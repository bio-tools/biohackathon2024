<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Artif Intell</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Artif Intell</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Artif. Intell.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Artificial Intelligence</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2624-8212</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9763446</article-id>
    <article-id pub-id-type="doi">10.3389/frai.2022.999289</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Artificial Intelligence</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepCausality: A general AI-powered causal inference framework for free text: A case study of LiverTox</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Xingqiao</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1296943/overview"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Xu</surname>
          <given-names>Xiaowei</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/989111/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tong</surname>
          <given-names>Weida</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/39650/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Qi</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Liu</surname>
          <given-names>Zhichao</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="c002" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/293118/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Department of Information Science, University of Arkansas at Little Rock</institution>, <addr-line>Little Rock, AR</addr-line>, <country>United States</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Division of Bioinformatics and Biostatistics, National Center for Toxicological Research, US Food and Drug Administration</institution>, <addr-line>Jefferson, AR</addr-line>, <country>United States</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Office of Clinical Pharmacology, Office of Translational Sciences, Center for Drug Evaluation and Research, US Food and Drug Administration</institution>, <addr-line>Silver Spring, MD</addr-line>, <country>United States</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Ramin Homayouni, Oakland University William Beaumont School of Medicine, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Yan Cui, University of Tennessee Health Science Center (UTHSC), United States; Jun Wu, East China Normal University, China; Zhining Wen, Sichuan University, China</p>
      </fn>
      <corresp id="c001">*Correspondence: Xiaowei Xu <email>xwxu@ualr.edu</email></corresp>
      <corresp id="c002">Zhichao Liu <email>Zhichao.Liu@fda.hhs.gov</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Medicine and Public Health, a section of the journal Frontiers in Artificial Intelligence</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>06</day>
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>5</volume>
    <elocation-id>999289</elocation-id>
    <history>
      <date date-type="received">
        <day>20</day>
        <month>7</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>16</day>
        <month>11</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2022 Wang, Xu, Tong, Liu and Liu.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Wang, Xu, Tong, Liu and Liu</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Causality plays an essential role in multiple scientific disciplines, including the social, behavioral, and biological sciences and portions of statistics and artificial intelligence. Manual-based causality assessment from a large number of free text-based documents is very time-consuming, labor-intensive, and sometimes even impractical. Herein, we proposed a general causal inference framework named DeepCausality to empirically estimate the causal factors for suspected endpoints embedded in the free text. The proposed DeepCausality seamlessly incorporates AI-powered language models, named entity recognition and Judea Pearl's Do-calculus, into a general framework for causal inference to fulfill different domain-specific applications. We exemplified the utility of the proposed DeepCausality framework by employing the LiverTox database to estimate idiosyncratic drug-induced liver injury (DILI)-related causal terms and generate a knowledge-based causal tree for idiosyncratic DILI patient stratification. Consequently, the DeepCausality yielded a prediction performance with an accuracy of 0.92 and an F-score of 0.84 for the DILI prediction. Notably, 90% of causal terms enriched by the DeepCausality were consistent with the clinical causal terms defined by the American College of Gastroenterology (ACG) clinical guideline for evaluating suspected idiosyncratic DILI (iDILI). Furthermore, we observed a high concordance of 0.91 between the iDILI severity scores generated by DeepCausality and domain experts. Altogether, the proposed DeepCausality framework could be a promising solution for causality assessment from free text and is publicly available through <ext-link xlink:href="https://github.com/XingqiaoWang/https-github.com-XingqiaoWang-DeepCausality-LiverTox" ext-link-type="uri">https://github.com/XingqiaoWang/https-github.com-XingqiaoWang-DeepCausality-LiverTox</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>AI</kwd>
      <kwd>causal inference analysis</kwd>
      <kwd>transformer</kwd>
      <kwd>NLP</kwd>
      <kwd>DILI</kwd>
    </kwd-group>
    <counts>
      <fig-count count="7"/>
      <table-count count="2"/>
      <equation-count count="5"/>
      <ref-count count="33"/>
      <page-count count="12"/>
      <word-count count="7180"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>Introduction</title>
    <p>Causality is the study of the relationship between causes and effects, which is the foundation of almost every scientific discipline to verify hypotheses and uncover underlying mechanisms (Pearl, <xref rid="B25" ref-type="bibr">2009</xref>). Notably, causal inference plays an essential role in medical practices to test scientific theories and decipher the etiology for advancing pharmacovigilance, optimize clinical trial designs, and establish real-world evidence (Naidu, <xref rid="B23" ref-type="bibr">2013</xref>; Mazhar et al., <xref rid="B22" ref-type="bibr">2020</xref>; Zheng et al., <xref rid="B33" ref-type="bibr">2020</xref>; Ho et al., <xref rid="B12" ref-type="bibr">2021</xref>). The conventional way to conduct causal inference relies on randomized controlled trials (RCTs) (Zheng et al., <xref rid="B33" ref-type="bibr">2020</xref>). In randomized clinical trials, the test subjects are randomly assigned to one of two groups: the treated group receiving the intervention (e.g., drug) tested and the control group receiving an alternative (e.g., placebo) treatment. Causality is established if the clinical outcome is statistically significant in the treated group over the control one. However, conducting a randomized clinical trial is time-consuming, labor-intensive, expensive, and sometimes even impractical.</p>
    <p>Consequently, there has been growing interest in alternative approaches, such as target trials based on observational data, to improve the causality assessment in real-world applications (Frieden, <xref rid="B7" ref-type="bibr">2017</xref>; Gajra et al., <xref rid="B8" ref-type="bibr">2020</xref>; Hernán, <xref rid="B11" ref-type="bibr">2021</xref>). For example, the U.S. Food and Drug Administration (FDA) released guidance on a real-world evidence (RWE) program to create a framework for evaluating the potential use of RWE to help support the approval of a new indication for a drug already approved under section 505(c) of the FD&amp;C Act or to help support or satisfy drug post-approval study requirements (<ext-link xlink:href="https://www.fda.gov/science-research/science-and-research-special-topics/real-world-evidence" ext-link-type="uri">https://www.fda.gov/science-research/science-and-research-special-topics/real-world-evidence</ext-link>). Under the 21<sup>st</sup> Century Cures Act, the FDA is mandated to evaluate the potential use of real-world data (RWD) and RWE to support the approval of a new indication for a drug. Draft guidance has been issued to address the generation of RWE, including the utilization of claims and electronic health records (EHRs), two major RWD sources, in support of regulatory decision-making. In addition, the FDA has prioritized the creation of an RWE Data Enterprise (the Sentinel System). An essential part of the initiative is incorporating EHR data from about 10 million individuals into the data infrastructure for FDA active drug safety surveillance (<ext-link xlink:href="https://www.fda.gov/news-events/fda-voices/fda-budget-matters-cross-cutting-data-enterprise-real-world-evidence" ext-link-type="uri">https://www.fda.gov/news-events/fda-voices/fda-budget-matters-cross-cutting-data-enterprise-real-world-evidence</ext-link>).</p>
    <p>In the past decade, the generation of EHRs has increased substantially in the U.S., partly due to the Health Information Technology for Economic and Clinical Health (HITECH) Act of 2009, which provided $30 billion in incentives for hospitals and physician practices to adopt EHR systems. Whereas administrative claims data are highly structured, much of the potentially useful information contained within EHRs is unstructured, in the form of laboratory data, visit notes (e.g., narrative descriptions of a patient's signs and symptoms, family history, social history), radiology reports or images, and discharge summaries. EHRs contain rich clinical information and complex relations in the data that may not be fully harnessed using more traditional approaches. The ability of EHRs to generate quality RWE depends on whether we can address the challenge in curating and analyzing unstructured data. In response, FDA seeks to incorporate emerging data science innovations, such as natural language processing (NLP) and machine learning, to establish the organizational framework for ensuring high-fidelity, fit-for-purpose EHR data. To inform the causal inference framework for EHR-based signal detection (hypothesis generating), we will evaluate the emerging approaches that have been proposed or tested.</p>
    <p>Accumulated observational data provide tremendous opportunities to promote target trials for causality establishment. Thus, there is an urgent need to develop novel statistical models to effectively estimate causal factors embedded in the extensive free text-based observational data. Artificial intelligence (AI) has made substantial progress in a variety of fields, such as computer vision (O'Mahony et al., <xref rid="B24" ref-type="bibr">2019</xref>), NLP (Liu et al., <xref rid="B21" ref-type="bibr">2021</xref>), speech recognition and generation (Hannun et al., <xref rid="B10" ref-type="bibr">2014</xref>), and decision-making (Shrestha et al., <xref rid="B29" ref-type="bibr">2019</xref>). Despite significant progress in AI, we still face a great challenge in understanding the mechanisms underlying intelligence, including reasoning, planning, and imagination (Schölkopf, <xref rid="B28" ref-type="bibr">2019</xref>). Recent hype of AI-powered language models (LMs) and advanced statistical measures seem to pave a promising way to enhance the ability of AI in reasoning, such as causal inference (Veitch et al., <xref rid="B31" ref-type="bibr">2020</xref>; Wang et al., <xref rid="B32" ref-type="bibr">2021</xref>). In our previous work, we proposed a transformer-based causal inference framework called InferBERT by integrating the A Lite Bidirectional Encoder Representations from Transformers (ALBERT) (Lan et al., <xref rid="B17" ref-type="bibr">2019</xref>) and Judea Pearl's Do-calculus (Wang et al., <xref rid="B32" ref-type="bibr">2021</xref>). The proposed InferBERT has been successfully applied for causality assessment in pharmacovigilance and exemplified estimation of the causal factors related to opioid-related acute liver failure and tramadol-related mortalities in the FDA Adverse Event Reporting System (FAERS) database. However, there is still much space for improvement for InferBERT to facilitate real-world applications. First, the proposed InferBERT has only been used for structure-based data sets (e.g., FAERS), limiting its application in the free text-based corpus. Although we proposed a synthetic approach to transforming the different clinical entities into a sentence-based representation, the performance of the proposed InferBERT in free text needs to be further investigated. Second, domain-specific knowledge was not considered for causal inference, resulting in false positives or introduction of Irrelevant causal factors.</p>
    <p>In this study, we proposed a general AI-powered framework called DeepCausality by fusing transformer, named entity recognition (NER), and Judea Pearl's Do-calculus for causal inference from free text-based documents. To demonstrate the validity of the proposed DeepCausality, we employed the LiverTox database (<ext-link xlink:href="https://www.ncbi.nlm.nih.gov/books/NBK547852/" ext-link-type="uri">https://www.ncbi.nlm.nih.gov/books/NBK547852/</ext-link>) to estimate the drug-induced liver injury (DILI)-related causal terms and further verified by using the American College of Gastroenterology (ACG) clinical guideline for idiosyncratic DILI (iDILI) (Chalasani et al., <xref rid="B3" ref-type="bibr">2021</xref>). Furthermore, we developed a causal tree based on verified causal DILI terms and utilized it for iDILI patient stratification based on DILI case reports.</p>
  </sec>
  <sec sec-type="materials and methods" id="s2">
    <title>Materials and methods</title>
    <sec>
      <title>DeepCausality overview</title>
      <p>The proposed DeepCausality is a general transformer-based causal inference framework for free text, consisting of data preprocessing, LM development, NER, and Do-calculus based causal inference (<xref rid="F1" ref-type="fig">Figure 1</xref>).</p>
      <fig position="float" id="F1">
        <label>Figure 1</label>
        <caption>
          <p>The workflow of the study: General framework of the DeepCausality, case study with LiverTox, and idiosyncratic DILI (iDILI) patient stratification.</p>
        </caption>
        <graphic xlink:href="frai-05-999289-g0001" position="float"/>
      </fig>
      <sec>
        <title>Data preprocessing</title>
        <p>First, the corpus of free text-based documents was split into sentences. Then, an endpoint was assigned to each sentence based on the investigational causal question. For example, suppose you investigate causal factors of lung cancer etiology. The sentences describing the patient with lung cancer and related symptoms and clinical outcomes were labeled as positives, and vice versa. Consequently, we used <italic><bold>D</bold></italic> to denote the preprocessed corpus of free text-based documents, where <italic>d</italic><sub><italic>i</italic></sub> = (<italic>x</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub>) ϵ <italic><bold>D</bold></italic> indicates the <italic>i-</italic>th instance in the dataset <italic><bold>D</bold></italic>, <italic>i</italic> = 1,2, …, <italic>N, N</italic> (total number of instances), with <italic>x</italic><sub><italic>i</italic></sub> (i.e., sentence) and <italic>y</italic><sub><italic>i</italic></sub> (i.e., endpoint) being the text sequence. We employed <italic>tf-idf</italic> [i.e., term frequency (tf)-inverse document frequency (idf)] values to investigate the distribution of terms in the corpus, which could be calculated based on the below formula,</p>
        <disp-formula id="E1">
          <label>(1)</label>
          <mml:math id="M1" overflow="scroll">
            <mml:mtable class="eqnarray" columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:mi>t</mml:mi>
                  <mml:mi>f</mml:mi>
                  <mml:mo>-</mml:mo>
                  <mml:mi>i</mml:mi>
                  <mml:mi>d</mml:mi>
                  <mml:mi>f</mml:mi>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:mi>d</mml:mi>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                  <mml:mo>=</mml:mo>
                  <mml:mi>t</mml:mi>
                  <mml:mi>f</mml:mi>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mrow>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mrow>
                          <mml:mi>t</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mi>d</mml:mi>
                        </mml:mrow>
                        <mml:mo stretchy="false">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>*</mml:mo>
                    </mml:mrow>
                  </mml:msup>
                  <mml:mi>i</mml:mi>
                  <mml:mi>d</mml:mi>
                  <mml:mi>f</mml:mi>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <disp-formula id="E2">
          <label>(2)</label>
          <mml:math id="M2" overflow="scroll">
            <mml:mtable class="eqnarray" columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:mi>t</mml:mi>
                  <mml:mi>f</mml:mi>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:mi>d</mml:mi>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                  <mml:mo>=</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mi>c</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>u</mml:mi>
                      <mml:mi>n</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mtext> </mml:mtext>
                      <mml:mi>o</mml:mi>
                      <mml:mi>f</mml:mi>
                      <mml:mtext> </mml:mtext>
                      <mml:mi>t</mml:mi>
                      <mml:mtext> </mml:mtext>
                      <mml:mi>i</mml:mi>
                      <mml:mi>n</mml:mi>
                      <mml:mtext> </mml:mtext>
                      <mml:mi>d</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>n</mml:mi>
                      <mml:mi>u</mml:mi>
                      <mml:mi>m</mml:mi>
                      <mml:mi>b</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mi>r</mml:mi>
                      <mml:mtext> </mml:mtext>
                      <mml:mi>o</mml:mi>
                      <mml:mi>f</mml:mi>
                      <mml:mtext> </mml:mtext>
                      <mml:mi>w</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>r</mml:mi>
                      <mml:mi>d</mml:mi>
                      <mml:mi>s</mml:mi>
                      <mml:mtext> </mml:mtext>
                      <mml:mi>i</mml:mi>
                      <mml:mi>n</mml:mi>
                      <mml:mtext> </mml:mtext>
                      <mml:mi>d</mml:mi>
                    </mml:mrow>
                  </mml:mfrac>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <disp-formula id="E3">
          <label>(3)</label>
          <mml:math id="M3" overflow="scroll">
            <mml:mtable class="eqnarray" columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:mi>i</mml:mi>
                  <mml:mi>d</mml:mi>
                  <mml:mi>f</mml:mi>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                  <mml:mo>=</mml:mo>
                  <mml:mi>l</mml:mi>
                  <mml:mi>o</mml:mi>
                  <mml:mi>g</mml:mi>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:mi>N</mml:mi>
                      <mml:mo>/</mml:mo>
                      <mml:mrow>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mrow>
                          <mml:mi>d</mml:mi>
                          <mml:mi>f</mml:mi>
                          <mml:mo>+</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mo stretchy="false">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <p>where <italic>t, d, N</italic> denote term, documents, and number of documents, respectively. The higher <italic>tf-idf</italic> value signified its importance in the document and corpus.</p>
      </sec>
      <sec>
        <title>Language model development</title>
        <p>Conditional probability distribution among words (i.e., tokens) in the text corpus is the basis for causal inference. LM uses various statistical and probabilistic techniques to determine joint probability among the words in the corpus. Specifically, a transformer-based LM could generate all joint probability among tokens as a gigantic probabilistic model using the Masked-Language Modeling (MLM) training strategy, allowing casual assessment among all the variables in the corpus. Two major types of transformer-based LM architectures, Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., <xref rid="B6" ref-type="bibr">2018</xref>) and its derives (Lan et al., <xref rid="B17" ref-type="bibr">2019</xref>; Liu et al., <xref rid="B20" ref-type="bibr">2019</xref>; Sanh et al., <xref rid="B27" ref-type="bibr">2019</xref>; Clark et al., <xref rid="B5" ref-type="bibr">2020</xref>), and Generative Pre-trained Transformer (GPT) models (Brown et al., <xref rid="B2" ref-type="bibr">2020</xref>), currently dominate the field. Furthermore, efforts have also been made to develop transformers based on the domain-specific corpus [e.g., BioBERT (Lee et al., <xref rid="B19" ref-type="bibr">2020</xref>), ClinicalBERT (Huang and Altosaar, <xref rid="B14" ref-type="bibr">2019</xref>), SciBERT (Beltagy and Lo, <xref rid="B1" ref-type="bibr">2019</xref>), LEGAL-BERT (Chalkidis et al., <xref rid="B4" ref-type="bibr">2020</xref>)] for performance enhancement in specialized domains. Some reports have demonstrated that domain-specific pre-training is a solid foundation for a wide range of downstream domain-specialized NLP tasks (Gu et al., <xref rid="B9" ref-type="bibr">2021</xref>).</p>
        <p>With the pre-trained LM, the conditional probability distribution given free text is estimated by the LM-based downstream task. The pre-trained LM computes the attention between tokens. Then, the classification ([CLS]) special token representing the semantic information of the whole sequence is fed into the input layer of the downstream classification model. The softMax layer is adopted as the output layer to access the conditional probability distribution. We use the following cross entropy loss function for the classification of input text sequences:</p>
        <disp-formula id="E5">
          <label>(4)</label>
          <mml:math id="M5" overflow="scroll">
            <mml:mtable columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:mi>L</mml:mi>
                  <mml:mi>O</mml:mi>
                  <mml:mi>S</mml:mi>
                  <mml:mi>S</mml:mi>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mi>D</mml:mi>
                  <mml:mo stretchy="false">)</mml:mo>
                  <mml:mo>=</mml:mo>
                  <mml:mo>−</mml:mo>
                  <mml:mstyle displaystyle="true">
                    <mml:msubsup>
                      <mml:mo>∑</mml:mo>
                      <mml:mn>1</mml:mn>
                      <mml:mi>N</mml:mi>
                    </mml:msubsup>
                    <mml:mo stretchy="false">(</mml:mo>
                  </mml:mstyle>
                  <mml:msub>
                    <mml:mi>y</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                  <mml:mo>∗</mml:mo>
                  <mml:mi>l</mml:mi>
                  <mml:mi>o</mml:mi>
                  <mml:mi>g</mml:mi>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mrow>
                      <mml:mi>p</mml:mi>
                      <mml:mrow>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mrow>
                          <mml:msub>
                            <mml:mi>x</mml:mi>
                            <mml:mi>i</mml:mi>
                          </mml:msub>
                        </mml:mrow>
                        <mml:mo stretchy="false">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                    <mml:mo>)</mml:mo>
                  </mml:mrow>
                </mml:mtd>
              </mml:mtr>
              <mml:mtr>
                <mml:mtd>
                  <mml:mtext>                       </mml:mtext>
                  <mml:mo>+</mml:mo>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mrow>
                      <mml:mn>1</mml:mn>
                      <mml:mo>−</mml:mo>
                      <mml:msub>
                        <mml:mi>y</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo>)</mml:mo>
                  </mml:mrow>
                  <mml:mo>∗</mml:mo>
                  <mml:mi>l</mml:mi>
                  <mml:mi>o</mml:mi>
                  <mml:mi>g</mml:mi>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mn>1</mml:mn>
                  <mml:mo>−</mml:mo>
                  <mml:mi>p</mml:mi>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                  <mml:mo stretchy="false">)</mml:mo>
                  <mml:mo stretchy="false">)</mml:mo>
                  <mml:mo stretchy="false">)</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mtext>  </mml:mtext>
                  <mml:mi>i</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mn>1</mml:mn>
                  <mml:mo>,</mml:mo>
                  <mml:mn>2</mml:mn>
                  <mml:mo>,</mml:mo>
                  <mml:mo>…</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mtext> </mml:mtext>
                  <mml:mi>N</mml:mi>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <p>where <italic>p</italic>(<italic>x</italic><sub><italic>i</italic></sub>) is the output of the classification model for text sequence <italic>x</italic><sub><italic>i</italic></sub>, which is a calculated probability of the predicted class of <italic>x</italic><sub><italic>i</italic></sub>. <italic>y</italic><sub><italic>i</italic></sub> is the ground truth label of <italic>x</italic><sub><italic>i</italic></sub>.</p>
        <p>By training the classifier with dataset <italic><bold>D</bold></italic>, we can estimate the conditional probability distribution <italic><bold>P</bold></italic>(<italic>endpoint</italic>|<italic><bold>X</bold></italic>), where training dataset <italic><bold>X</bold></italic> = {<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, …, <italic>x</italic><sub><italic>N</italic></sub>}. Then, we use the model to predict all the text sequences for each instance in the dataset <italic><bold>D</bold></italic>. We denote the output of the classifier as <italic>p</italic>(<italic>x</italic><sub><italic>i</italic></sub>), where <italic>p</italic>(<italic>x</italic><sub><italic>i</italic></sub>) is the probability of the endpoint presented for instance <italic>d</italic><sub><italic>i</italic></sub>.</p>
      </sec>
      <sec>
        <title>Name entity recognition</title>
        <p>According to the task field, our framework adopts a domain-specific NER method, a text mining technique, to extract the name entities in the free text. The NER method can predict the span and category of name entities in the text according to the task with a domain-specific NER method.</p>
        <p>For each instance, <italic>d</italic><sub><italic>i</italic></sub> in dataset <italic><bold>D</bold></italic>, the NER method recognizes all the name entities in the text sequence <italic>x</italic><sub><italic>i</italic></sub>. Then, we get the set of name entities <italic><bold>ner</bold></italic><sub><italic>i</italic></sub> corresponding to <italic>x</italic><sub><italic>i</italic></sub>, where <italic><bold>ner</bold></italic><sub><italic>i</italic></sub>= {<italic>ner</italic><sub><sub><italic>i</italic></sub>1</sub>, <italic>ner</italic><sub><italic>i</italic></sub>2, …, <italic>ner</italic><sub><italic>iM</italic></sub>}, with <italic>M</italic> being the total number of name entities in the text sequence <italic>x</italic><sub><italic>i</italic></sub>. Next, we combined and unified all the name entities in set <italic><bold>ner</bold></italic><sub><italic><bold>i</bold></italic></sub> corresponding to the text in the extracted dataset <italic><bold>D</bold></italic>. As a result, we obtained the unique name entity set <italic><bold>NER</bold></italic>, where <italic>NER</italic> = ⋃<italic>ner</italic><sub><bold>i</bold></sub>, <italic>i</italic> = 1, 2, …, <italic>N</italic>; It is the union of <italic><bold>ner</bold></italic><sub><italic><bold>i</bold></italic></sub>. Then, the recognized name entities were fed into the Do-calculus component of the framework as causal factor candidates.</p>
      </sec>
      <sec>
        <title>Do-calculus based causal inference</title>
        <p>In our previous work, we performed causal inference on structured data by using the Do-calculus mechanism to check whether each feature in the structured data was the cause of the endpoint. In this study, to perform causal inference on the free text, we first extracted name entities in free text and then considered these name entities as causality candidates to infer potential causal factors.</p>
        <p>In the proposed framework, the classifier model calculates the conditional probability distribution of the endpoint given the free text sequence. Then, the extracted name entities in each instance sequence act as the endpoint's candidate causal factors. To empirically estimate the candidate name entities in each instance causing the endpoint, we adopted Judea Pearl's Do-calculus framework (Tucci, <xref rid="B30" ref-type="bibr">2013</xref>; Pearl and Mackenzie, <xref rid="B26" ref-type="bibr">2018</xref>).</p>
        <p>Do-calculus aims to investigate the interventional conditional probability distribution of <italic><bold>P</bold></italic>[<italic>endpoint</italic> = true|<bold>DO</bold>(<italic>ner</italic>)] by counterfactually changing the appearance of the name entity <italic>ner</italic>. We use the conditional probability distribution expectation to represent the <bold>DO</bold>(<italic>ner</italic>) and <bold>NOT DO</bold>(<italic>ner</italic>). Suppose there exists a statistically significant difference when comparing the interventional conditional probability distributions of P[endpoint = true|<bold>DO</bold>(ner)] and P[endpoint = true|<bold>NOT DO</bold>(ner)]. In that case, the causality relationship will be established.</p>
        <p>Based on the Classification Prediction <italic><bold>p(<italic>x</italic><sub><italic>i</italic></sub>)</bold></italic> from the developed classifier, the Do-calculus procedure was performed to estimate the cause of the endpoint. The pseudo-code of the name entity-based Do-calculus procedure is shown below:</p>
        <table-wrap position="float" id="T3">
          <label>Algorithm 1</label>
          <caption>
            <p>Name entity-based Do-calculus algorithm.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <tbody>
              <tr>
                <td align="left" valign="top" rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="left" valign="top" rowspan="1" colspan="1">
                  <monospace><bold>Input:</bold>Classification Prediction result <italic>p</italic>(<italic>x</italic>), dataset <italic><bold>D</bold></italic>, <italic><bold>NER</bold></italic> results, statistic test threshold <italic>thr</italic>
</monospace>
                  <break/>
                  <monospace><bold>Output:</bold>Do-calculus results <italic><bold>C</bold></italic></monospace>
                  <break/>
                  <monospace>1. set <italic><bold>C</bold></italic> = {} // <italic><bold>C</bold></italic> is the set of established causes </monospace>
                  <break/>
                  <monospace>2. <bold>for</bold>
<italic>ner</italic> in <italic><bold>NER</bold></italic>
<bold>do</bold> // for each name entity </monospace>
                  <break/>
                  <monospace>3.     set <italic><bold>S1</bold></italic> = {} // <italic>S1</italic> contains all results of <bold>DO</bold> (<italic>ner</italic>) </monospace>
                  <break/>
                  <monospace>4.     set <italic><bold>S2</bold></italic> = {} // <italic>S2</italic> contains all results of <bold>NOT DO</bold> (<italic>ner</italic>) </monospace>
                  <break/>
                  <monospace>5.     <bold>for</bold>
<italic>d</italic><sub><italic>i</italic></sub> in <italic><bold>D</bold></italic>
<bold>do</bold> // for each instance in the dataset </monospace>
                  <break/>
                  <monospace>6.       <italic><bold>S1</bold></italic> ← <italic>p</italic>(<italic>endpoint</italic>|<bold>DO</bold> (<italic>ner</italic>) // probability of <bold>DO</bold> (<italic>ner</italic>). </monospace>
                  <break/>
                  <monospace>7.       <italic><bold>S2</bold></italic>← <italic>p</italic>(<italic>endpoint</italic>|<bold>NOT</bold>
<bold>DO</bold> (<italic>ner</italic>) // probability of <bold>NOT DO</bold> (<italic>ner</italic>). </monospace>
                  <break/>
                  <monospace>8.     z-score = z<sub>test</sub> (<italic><bold>S1</bold></italic>, <italic><bold>S2</bold></italic>) // perform z-test based on <italic><bold>S1</bold></italic> and <italic><bold>S2</bold></italic>
</monospace>
                  <break/>
                  <monospace>9.     <bold>if</bold> z-score &gt; <italic>thr</italic>
<bold>then</bold>
</monospace>
                  <break/>
                  <monospace>10.       <italic><bold>C</bold></italic>←<italic>ner</italic> // <italic><bold>C</bold></italic> consists of all established causes </monospace>
                  <break/>
                  <monospace>11.       <bold>return</bold>
<italic><bold>C</bold></italic>;</monospace>
                </td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>For all the extracted name entities, we applied the name entity-based Do-calculus algorithm to check whether it was the cause of the endpoint. For a name entity <italic>ner</italic>, if <italic>ner</italic> ϵ <italic>x</italic><sub><italic>i</italic></sub>, we say instance <italic>d</italic><sub><italic>i</italic></sub> meets the condition of <bold>DO</bold> (<italic>ner</italic>), while if <italic>ner</italic>
<inline-graphic xlink:href="frai-05-999289-i0001.jpg"/>
<italic>x</italic><sub><italic>i</italic></sub>, then it doesn't. For <italic>ner</italic>, we assigned the conditional probability <italic>p</italic>[<italic>endpoint</italic>|<bold>DO</bold> (<italic>ner</italic>)] or <italic>p</italic>[<italic>endpoint</italic>|<bold>NOT</bold>
<bold>DO</bold> (<italic>ner</italic>)] to sets <italic><bold>S1</bold></italic> and <italic><bold>S2</bold></italic> respectively. <italic><bold>S1</bold></italic> is the set of conditional probability of <bold>DO</bold> (<italic>ner</italic>), while <italic><bold>S2</bold></italic> consists of conditional probabilities of those instances NOT DO (<italic>ner</italic>). We used the one tail <italic>z</italic>-test to evaluate whether the probabilities in <italic><bold>S1</bold></italic> were significantly different to <italic><bold>S2</bold></italic>.</p>
        <p>We perform one tail <italic>z-</italic>test between S1 and S2. If the <italic>p</italic>-value is less than a threshold like 0.05, we view the <italic>ner</italic> as a cause of the endpoint. To establish all the causal terms of the endpoint, we evaluated every candidate name entity. The generated term set <italic><bold>C</bold></italic> is the set of all the name entities that satisfy the statistical significance test.</p>
      </sec>
    </sec>
    <sec>
      <title>Case study: Causal inference of idiosyncratic DILI based on LiverTox</title>
      <sec>
        <title>Clinical knowledge of idiosyncratic DILI</title>
        <p>iDILI is a rare adverse drug reaction, but common in gastroenterology and hepatology practices. The symptoms of iDILI have multiple presentations, characterized from asymptomatic elevations in liver biochemistries to hepatocellular or cholestatic jaundice, liver failure, or chronic hepatitis (Chalasani et al., <xref rid="B3" ref-type="bibr">2021</xref>). Causal factors associated with iDILI recommended by ACG Clinical Guideline could be divided into three types: host, environmental, and drug-related factors (Chalasani et al., <xref rid="B3" ref-type="bibr">2021</xref>). Specifically, host factors include age, gender, pregnancy, malnutrition, obesity, diabetes mellitus, co-morbidities (e.g., underlying liver disease), and indications for therapy. Environmental factors include smoking, alcohol consumption, infection, and inflammatory episodes. Drug-related factors consist of the daily dose, metabolic profiles, class effect and cross-sensitization, and drug interactions and polypharmacy. Furthermore, the ACG clinical guideline also suggested an algorithm to evaluate suspected iDILI by integrating DILI-related clinical measurements and iDILI-associated causal factors (Chalasani et al., <xref rid="B3" ref-type="bibr">2021</xref>).</p>
      </sec>
      <sec>
        <title>Data preprocessing of the LiverTox database</title>
        <p>LiverTox<sup>®</sup>, launched by the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK) and the National Library of Medicine (NLM), is a DILI atlas dedicated to providing up-to-date, easily accessed information and comprehensive clinical information on iDILI for both physicians and patients (Hoofnagle, <xref rid="B13" ref-type="bibr">2013</xref>). There are 1,095 drug records in the LiverTox database, which are available at <ext-link xlink:href="https://ftp.ncbi.nlm.nih.gov/pub/litarch/29/31/" ext-link-type="uri">https://ftp.ncbi.nlm.nih.gov/pub/litarch/29/31/</ext-link>. For each drug record, the information was organized based on different sections, including Introduction, Background, Hepatotoxicity, Mechanism of Liver Injury, Outcome and Management, Case reports, Chemical and Product Information, and References.</p>
        <p>To demonstrate the utility of the proposed DeepCausality framework, we employed drug records stored in the LiverTox<sup>®</sup> database. The purpose is to use our proposed DeepCausality to estimate the causal factors related to iDILI. For each drug record, we extracted the text from four sections, Introduction, Background, Hepatotoxicity, and Mechanism of Liver Injury, which are the major sections that describe the synthesized knowledge on hepatoxicity. The DILI Likelihood score is embedded in the hepatoxicity section. Each sentence except the one that included the DILI likelihood score in these four sections was considered as <italic>x</italic><sub><italic>i</italic></sub>, and all the extracted sentences were considered as <italic><bold>D</bold></italic>.</p>
        <p>Domain experts developed the DILI likelihood score to categorize drugs based on the likelihood of drugs associated with the known potential of DILI for causing liver injury. The DILI likelihood score is largely opinion-based and derived from published medical literature to categorize the possibility of the drug causing idiosyncratic liver injury, including <bold>Category A</bold> – well known, <bold>Category B</bold> – known or highly likely, <bold>Category C</bold> – probable, <bold>Category D</bold> – possible, <bold>Category E</bold> – not believed or unlikely, and <bold>Category X</bold> – unknown. We labeled each sentence <italic>x</italic><sub><italic>i</italic></sub> according to the DILI likelihood score. Specifically, if the sentence <italic>x</italic><sub><italic>i</italic></sub> from the drug with a DILI likelihood score was either Category A or Category B, we assigned the sentence a label <italic>y</italic><sub><italic>i</italic></sub> as iDILI positives. Otherwise, the sentence was labeled as iDILI negatives.</p>
      </sec>
      <sec>
        <title>Language model selection</title>
        <p>Considering the LiverTox database provided the summarized knowledge on DILI mainly based on medical literature, we selected BioBERT as the domain-specific language model to develop DeepCausality. BioBERT was developed on top of the pre-trained BERT model by further fine-tuning with biomedical-specific corpora, including PubMed abstracts (PubMed) and PubMed Central full-text articles (PMC) using MLM (Lee et al., <xref rid="B18" ref-type="bibr">2019</xref>). BioBERT has shown its superiority in various biomedical-related downstream tasks over the state-of-the-art NLP approaches. To make BioBERT more specific for the DILI application, we further fine-tuned the BioBERT model with the extracted sentences <italic><bold>D</bold></italic> from LiverTox. Consequently, the fine-tuned BioBERT could represent the joint conditional probability among words involved in the extracted sentence <italic><bold>D</bold></italic>.</p>
      </sec>
      <sec>
        <title>Biomedical entity recognition</title>
        <p>Given that many words in the corpus were not biomedical specific, there was the potential risk of bringing false positives during the causal inference process. Therefore, we employed biomedical entity recognition to extract different biomedical-related terms and limit the causal inference within these domain-relevant terms. In this study, we used biomedical entity recognition and a multi-type normalization tool (BERN) to extract biomedical-related terms, including gene/protein, disease, drug/chemical, species information, and genetic variants (Kim et al., <xref rid="B16" ref-type="bibr">2019</xref>). The BERN is a series of BioBERT-named entity recognition models with probability-based decision rules to recognize and discover different biomedical entities, accessible through <ext-link xlink:href="https://bern.korea.ac.kr" ext-link-type="uri">https://bern.korea.ac.kr</ext-link>. Here, we only considered extracted name entities with more than a frequency of 50 across the corpus as causal factor candidates for further analysis.</p>
      </sec>
      <sec>
        <title>NER-based Causal inference</title>
        <p>The named entity-based Do-calculus strategy was developed to carry out the causal inference within biomedical entities extracted using the BERN. The potential causal terms of iDILI were enriched if the adjusted <italic>p</italic> value was less than 0.05 based on the one-tail z-test calculation. Furthermore, other statistical measures were also provided, including z-score, average DO probability, and average not DO probability.</p>
        <p>We further developed a knowledge-based causal tree to organize the enriched causal factors by following the ACG clinical guideline for iDILI diagnosis (see <italic>Clinical knowledge</italic> section). Specifically, the enriched causal terms were classified into different causal factors of iDILI, including Concomitant diseases, History of other liver disorders, Physical findings, Laboratory results, Symptoms and Signs, Clinical outcome, Covering host, Environmental, and Drug-related. Furthermore, the liver enzymes test results were also incorporated into the proposed knowledge-based causal tree to facilitate the iDILI patient stratification.</p>
      </sec>
    </sec>
    <sec>
      <title>Real-world application: Idiosyncratic DILI (iDILI) patient stratification</title>
      <p>In the LiverTox database, some drug records contained one or more case reports related to DILI, which were curated from scientific literature or liver-specific clinical databases such as DILI Network (DILIN). The case report comprised the findings from a clinical laboratory, radiologic and histologic testing summarized in a formulaic table titled Key Points, and a short concluding discussion and comment on DILI severity. The key points included iDILI patterns and severity scores, which served as the ground truth for iDILI patient classification. The DILI patterns were divided into three categories (i.e., Hepatocellular - <italic>R</italic> &gt; 5, mixed - 2&lt;<italic>R</italic>&lt;5, and cholestatic - R &lt; 2) by the ratio between serum alanine transaminase (ALT) and aspartate transaminase (AST). The severity score was based on five levels: 1+, Mild; 2+, Moderate; 3+, Moderate to Severe; 4+, Severe; and 5+, Fatal.</p>
      <p>Because iDILI is a multifactorial endpoint caused by different underlying mechanisms, it was crucial to stratify iDILI patients into different DILI pattern subgroups to facilitate subsequent treatment regimen development. To demonstrate whether the developed knowledge-based causal tree could be utilized to categorize the iDILI patients, we extracted a total of 175 case reports from LiverTox for further analysis. First, we classified the patients by extracting the causal factors involved in the developed knowledge-based causal tree from each case report. Second, we verified the iDILI patient stratification results by comparing them to the ground truth classification results based on the DILI pattern and severity scores.</p>
    </sec>
    <sec>
      <title>Robustness evaluation</title>
      <p>The proposed DeepCausality framework employed transformer-based LMs to learn the joint probability among variables for causal inference. However, this process can be less robust due to different random seeds, even though the same hyper-parameters were chosen. Toward real-world applications, the robustness of the proposed framework was investigated based on the strategy developed in our previous study (Wang et al., <xref rid="B32" ref-type="bibr">2021</xref>). Specifically, we employed the proposed DeepCausality to run parallel experiments with the same hypermeters three times. Then, the enriched causal terms in the three repeated experiments were compared using a Venn diagram and the percentage of overlapped terms (POT) strategy (Wang et al., <xref rid="B32" ref-type="bibr">2021</xref>). The POT could be calculated based on two steps: (1) rank the enriched terms based on z scores from high to low in each run, and (2) calculate the POT using the number of the overlapping terms among three repeated runs divided by <italic>L</italic>. <italic>L</italic> denotes the number of enriched terms of each subset of the ranked enriched term list. In this study, <italic>L</italic> was set from 1 to 30 at one interval.</p>
    </sec>
    <sec>
      <title>Implementation of the DeepCausality</title>
      <p>To facilitate the application of our model, we developed a standalone package for the readers' convenience. The proposed DeepCausality framework was exemplified based on a BioBERT (BioBERT, <ext-link xlink:href="https://github.com/dmis-lab/biobert" ext-link-type="uri">https://github.com/dmis-lab/biobert</ext-link>) and BERN under Python 3.6 TensorFlow version 1.15. We evaluated our proposed DeepCausality model on one NVIDIA Tesla V100 GPU. For the LiverTox dataset, the average runtime was approximately 8 h. We incorporated the Do-calculus causal function into the BioBERT source code, which easily migrated into other transformers. All the source code and the processed data sets used in this study are publicly available through <ext-link xlink:href="https://github.com/XingqiaoWang/https-github.com-XingqiaoWang-DeepCausality-LiverTox" ext-link-type="uri">https://github.com/XingqiaoWang/https-github.com-XingqiaoWang-DeepCausality-LiverTox</ext-link>.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>Results</title>
    <sec>
      <title>Data preprocessing of the LiverTox dataset</title>
      <p><xref rid="F2" ref-type="fig">Figure 2</xref> illustrates the sequence length of the extracted 14,361 sentences from four sections (i.e., Introduction, Background, Hepatotoxicity, Mechanism of Liver Injury) of LiverTox. The average and standard deviation of the sequence length of the extracted 14,361 sentences is 26.84 ± 15.58. Furthermore, the extracted 14,361 sentences contain 15,804 unique words (<xref rid="SM1" ref-type="supplementary-material">Supplementary Table S1</xref>). We observed the top ten terms based on the term frequency-inverse document frequency (Tf-idf) values, including <italic>iu, hydroxycut, clobazam, dabrafenib, dapsone, germander, progesterone, asparaginase, barbiturate</italic>, and <italic>CDC</italic>. These top ten terms were not directly associated with any current knowledge of iDILI, indicating the causal factors could not be enriched by the simple frequency-based strategy.</p>
      <fig position="float" id="F2">
        <label>Figure 2</label>
        <caption>
          <p>The distribution of sequence length and the top 10 terms based on Tf-idf values.</p>
        </caption>
        <graphic xlink:href="frai-05-999289-g0002" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>Fine-tune BioBERT model with LiverTox data</title>
      <p>Considering that LiverTox is summarized from literature and clinical reports, we employed BioBERT to establish the joint probability between variables. For that, we divided the extracted 14,361 sentences into two sets with a ratio of 9:1 in a stratified manner, with the ratio between positives (i.e., iDILI positives) and negatives (i.e., iDILI negatives) kept constant for both sets. It resulted in 12,924 (14,361 × 90% = 12,924) and 1,437 (14,361 × 10% = 1,437) sentences in training and test sets, respectively (<xref rid="T1" ref-type="table">Table 1</xref>). Then, we employed BioBERT-Base v1.1 (+ PubMed 1M), consisting of 12 transformer layers, 128 embeddings, 768 hidden, and 12 heads with 11M parameters. We further fine-tuned the BioBERT<sub>base</sub> model with the 12,924 sentences in the training set. We determined the optimized models based on the text classification result in the test set for iDILI sentence prediction. Specifically, we set the maximum sequence length to 128 and the mini-batch size to 128. A total of 2,500 training steps were implemented with a 500-step warmup, and the checkpoint step was set to 200 for recording the prediction results.</p>
      <table-wrap position="float" id="T1">
        <label>Table 1</label>
        <caption>
          <p>Data information of preprocessed sentences in LiverTox.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Dataset</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>iDILI positive</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>iDILI negative</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Positive ratio</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Total</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Training set</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3,218</td>
              <td valign="top" align="center" rowspan="1" colspan="1">9,706</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.249</td>
              <td valign="top" align="center" rowspan="1" colspan="1">12,924</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Test set</td>
              <td valign="top" align="center" rowspan="1" colspan="1">360</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1,077</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.251</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1,437</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Total</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3,578</td>
              <td valign="top" align="center" rowspan="1" colspan="1">10,783</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.249</td>
              <td valign="top" align="center" rowspan="1" colspan="1">14,361</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p><xref rid="F3" ref-type="fig">Figure 3A</xref> depicted the trends of cross entropy loss and accuracy while increasing the number of training steps based on the text set. The cross-entropy loss decreased dramatically before 400 training steps and became stable between 400 and 800 training steps. Then, it increased after 1,000 steps, indicating the potential of overfitting phenomena. Meanwhile, the accuracies of the dataset tended to be stable after training step 400. Thus, we selected the optimized fine-tuned model based on the training step with the minimum loss (i.e., 800), where the accuracy value also showed no dramatic changes. The optimized fine-tuned model yielded a high accuracy of 0.92, an F1-score of 0.84, a precision of 0.86, and a recall of 0.82 in the test set, indicating the optimized fine-tuned model well captured the relationship between variables (<xref rid="F3" ref-type="fig">Figure 3B</xref>).</p>
      <fig position="float" id="F3">
        <label>Figure 3</label>
        <caption>
          <p><bold>(A)</bold> The trend of cross-entropy loss and accuracy across the different training steps in the fine-tuned BioBERT model; <bold>(B)</bold> Prediction performance metrics of the optimized fine-tuned BioBERT model on the test set.</p>
        </caption>
        <graphic xlink:href="frai-05-999289-g0003" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>Biomedical-based named entity recognition</title>
      <p>To carry out the causal inference within the biomedical-based NER terms, we employed the BERN to extract the biomedical-related terms from the preprocessed sentences. We obtained a total of 87 biomedical-related terms that were divided into three categories using BERN, including 16 drugs, 11 genes, and 60 diseases (see <xref rid="SM2" ref-type="supplementary-material">Supplementary Table S2</xref>). Through the biomedical-based NER, we narrowed down the total terms (unique words) in the preprocessed sentences from a total of 15,804 to 87, with a 99.4% compression rate.</p>
    </sec>
    <sec>
      <title>Causal inference using NER-based Do-calculus</title>
      <p>To further investigate whether the performance of the proposed DeepCausality could identify the causal terms of iDILI, we implemented NER-based Do-calculus to uncover the predictors from the fine-tuned BioBERT model (<xref rid="T2" ref-type="table">Table 2</xref>). Of 87 Biomedical-based name entities, 24 name entities were enriched with an adjusted <italic>p</italic> value &lt; 0.05 based on a one-tail z-test using the NER-based Do-calculus. We excluded 4 drug entities, including iron, isoniazid, rifampin, and acetaminophen, since our objective is to identify the causal factors related to iDILI. For example, acetaminophen is a protype drug for dose-dependent drug-induced liver injury (DILI), which is not idiosyncratic in nature (Jaeschke, <xref rid="B15" ref-type="bibr">2015</xref>). Furthermore, 18 of 20 enriched causal terms were highly consistent with current knowledge of iDILI, yielding an enrichment rate of 90% (Chalasani et al., <xref rid="B3" ref-type="bibr">2021</xref>). These name entities were distributed into different categories, including Liver Enzymes, Concomitant diseases, History of other liver disorders, Physical findings, Laboratory results, Symptoms and Signs, and Clinical outcomes based on the ACG clinical guideline for iDILI diagnosis.</p>
      <table-wrap position="float" id="T2">
        <label>Table 2</label>
        <caption>
          <p>Causal inference results for idiosyncratic DILI.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Elements</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Z score</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Probability of DO value</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Probability of not DO value</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Probability difference</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" colspan="5" rowspan="1">
                <bold>Liver Enzymes</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Alkaline phosphatase</td>
              <td valign="top" align="left" rowspan="1" colspan="1">3.772</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.398</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.244</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.154</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">ALT</td>
              <td valign="top" align="left" rowspan="1" colspan="1">2.561</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.307</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.244</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.064</td>
            </tr>
            <tr>
              <td valign="top" align="left" colspan="5" rowspan="1">
                <bold>Concomitant diseases</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Tuberculosis</td>
              <td valign="top" align="left" rowspan="1" colspan="1">2.470</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.382</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.244</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.138</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Rheumatoid arthritis</td>
              <td valign="top" align="left" rowspan="1" colspan="1">1.759</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.334</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.244</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.089</td>
            </tr>
            <tr>
              <td valign="top" align="left" colspan="5" rowspan="1">
                <bold>History of other liver disorder</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Cholestasis</td>
              <td valign="top" align="left" rowspan="1" colspan="1">4.827</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.547</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.244</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.303</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Cholestatic hepatitis</td>
              <td valign="top" align="left" rowspan="1" colspan="1">3.653</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.499</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.244</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.255</td>
            </tr>
            <tr>
              <td valign="top" align="left" colspan="5" rowspan="1">
                <bold>Physical findings</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Fever</td>
              <td valign="top" align="left" rowspan="1" colspan="1">6.508</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.383</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.241</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.141</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Pain</td>
              <td valign="top" align="left" rowspan="1" colspan="1">2.377</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.395</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.244</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.150</td>
            </tr>
            <tr>
              <td valign="top" align="left" colspan="5" rowspan="1">
                <bold>Laboratory results</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Lactic acidosis</td>
              <td valign="top" align="left" rowspan="1" colspan="1">3.181</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.460</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.244</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.216</td>
            </tr>
            <tr>
              <td valign="top" align="left" colspan="5" rowspan="1">
                <bold>Symptoms and signs</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Hypersensitivity</td>
              <td valign="top" align="left" rowspan="1" colspan="1">3.966</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.383</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.243</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.139</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Skin rash</td>
              <td valign="top" align="left" rowspan="1" colspan="1">2.066</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.333</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.244</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.088</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Jaundice</td>
              <td valign="top" align="left" rowspan="1" colspan="1">1.773</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.274</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.244</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.030</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Stevens Johnson syndrome</td>
              <td valign="top" align="left" rowspan="1" colspan="1">1.669</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.335</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.245</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.090</td>
            </tr>
            <tr>
              <td valign="top" align="left" colspan="5" rowspan="1">
                <bold>Clinical outcome</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Hepatic failure</td>
              <td valign="top" align="left" rowspan="1" colspan="1">4.119</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.437</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.244</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.193</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Cirrhosis</td>
              <td valign="top" align="left" rowspan="1" colspan="1">2.944</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.391</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.244</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.147</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Liver failure</td>
              <td valign="top" align="left" rowspan="1" colspan="1">2.905</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.366</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.244</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.122</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Sinusoidal obstruction syndrome</td>
              <td valign="top" align="left" rowspan="1" colspan="1">2.490</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.403</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.244</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.159</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Acute liver failure</td>
              <td valign="top" align="left" rowspan="1" colspan="1">1.669</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.326</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.244</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.082</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p><xref rid="T2" ref-type="table">Table 2</xref> lists enriched causal factors ranked based on the Z score. The causal factor (Z score) are as follows: for Liver Enzymes, alkaline phosphatase (3.772), ALT (2.561); for Concomitant diseases, tuberculosis (2.470), rheumatoid arthritis (1.759); for History of other liver disorder, cholestasis (4.827), cholestatic hepatitis (3.653); for Physical findings, fever (6.508), pain (2.377); for Laboratory results, lactic acidosis (3.181); for Symptoms and Signs, hypersensitivity (3.966), skin rash (2.066), jaundice (1.773), and Stevens-Johnson syndrome (1.669); for Clinical outcome, hepatic failure (4.119), cirrhosis (2.944), liver failure (2.905), sinusoidal obstruction syndrome (2.490), and acute liver failure (1.669).</p>
      <p><xref rid="F4" ref-type="fig">Figure 4</xref> illustrates the developed knowledge-based causal tree with enriched causal factors based on the ACG clinical guideline for iDILI diagnosis. The proposed knowledge-based prediction tree could be divided into two major components: liver enzyme test and clinical observations. The liver enzyme test, including ALT and AST, divides iDILI patients into different DILI patterns, including hepatocellular, mixed, and cholestatic. Clinical observations could further classify the iDILI patients based on their severity and clinical symptoms.</p>
      <fig position="float" id="F4">
        <label>Figure 4</label>
        <caption>
          <p>The proposed knowledge-based causal tree based on the ACG clinical guideline on iDILI patient diagnosis: ULN denotes upper limits of normal.</p>
        </caption>
        <graphic xlink:href="frai-05-999289-g0004" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>iDILI patient stratification</title>
      <p>To demonstrate the proposed knowledge-based causal tree could be utilized for iDILI patient stratification, we stratified 175 patients' case reports in the LiverTox dataset based on the developed causal tree and compared expert-based patient stratification results. There was a high correlation between the R (ALT/AST) values determined by DeepCausality and the experts, with a Pearson correlation coefficient of more than 0.9 (<xref rid="F5" ref-type="fig">Figure 5</xref>). Furthermore, we observed that the clinical observations in the developed causal tree could be used to classify the patients into different severity groups, distinguished by the R scores estimated by DeepCausality (<xref rid="F6" ref-type="fig">Figure 6</xref>).</p>
      <fig position="float" id="F5">
        <label>Figure 5</label>
        <caption>
          <p>The correction between the R scores (ALT/AST) calculated by DeepCausality and expert: ALT and AST stand for Alanine transaminase and aspartate transaminase, respectively.</p>
        </caption>
        <graphic xlink:href="frai-05-999289-g0005" position="float"/>
      </fig>
      <fig position="float" id="F6">
        <label>Figure 6</label>
        <caption>
          <p>The distribution of iDILI patients stratified by DeepCausality across the different severity levels defined by domain experts.</p>
        </caption>
        <graphic xlink:href="frai-05-999289-g0006" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>Robustness of DeepCausality</title>
      <p>To ensure the proposed DeepCausality could generate reproducible causal inference results, we investigated the robustness of causal inference results by running the DeepCausality three times (see <xref rid="SM3" ref-type="supplementary-material">Supplementary Table S3</xref>). <xref rid="F7" ref-type="fig">Figure 7</xref> depicted the POT enrichment after three different runs. We found highly reproducible results from three parallel runs of DeepCausality, with an average POT of 0.923. Furthermore, the Venn diagram indicates 87.5% commonality of enriched causal terms after three runs. Altogether, the proposed DeepCausality framework could generate highly repeatable results without interfering with factors such as initial seeds.</p>
      <fig position="float" id="F7">
        <label>Figure 7</label>
        <caption>
          <p>Robustness evaluation of the proposed DeepCausality: The Venn diagram illustrates the overlapping of the enriched causal terms by three parallel runs. The dotted-line curve illustrates the percentage of overlapping causal terms (POTs) among the three repeated runs across ranked order terms by z scores.</p>
        </caption>
        <graphic xlink:href="frai-05-999289-g0007" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>Discussion</title>
    <p>Causality is one of the most critical notions in every branch of science. Causal inference based on observational data has gained more and more momentum as an alternative to the conventional random controlled trial-based causality assessment. Notably, More and more advocates promote using RWD and RWE to monitor post-market safety and adverse events and make regulatory decisions in drug development. An essential resource of RWD, observational data such as EHRs, clinical reports, and patient narratives are typically free text-based, posing a significant challenge to uncovering hidden causal factors. AI-powered LMs such as transformers have shown great potential in various NLP tasks such as text classification, information retrieval, question &amp; answering, and sentimental analysis. However, leveraging these AI-powered LMs to conduct causal inference as a human does is still at the infant stage. To bridge this gap, we proposed DeepCausality, a general AI-powered causal inference framework for free text. We exemplified the utility of the proposed DeepCausality for iDILI-related causal factor identification based on LiverTox and applied it to iDILI patient stratification. Consequently, DeepCausality identified 20 causal factors for iDILI, and 18 (90%) were aligned with the current clinical knowledge of iDILI. Furthermore, the developed knowledge-based causal tree was used to classify iDILI patients, which was highly consistent with stratification results based on domain experts.</p>
    <p>AI-based language models such as transformers rely on a pre-trained model with a large corpus and then use the learned knowledge to solve the downstream tasks. In this study, without training on a large number of DILI-related literature and clinical reports, we hypothesized the accumulated knowledge from these large corpora of documents could be an alternative to accelerate the training process of transformer-based LMs. Furthermore, we introduced the domain-specific named entity recognition (NER) step into the general framework, aiming to eliminate the false positives and irrelevant enrichment in the causal inference process. If available, this step could also be substituted with domain-specific ontology and knowledge graphs.</p>
    <p>One of the initial attempts conveyed in this study was to use the developed knowledge-based causal tree for iDILI patient stratification. The high consistency of iDILI patient stratification results from DeepCausality with determination by experts is encouraging. However, it is worth pointing out the causal tree was developed based on prior knowledge of iDILI diagnosis, indicating that expert knowledge is still an indispensable component to facilitating AI-based approaches in real-world applications.</p>
    <p>It is also worth investigating a few aspects of the proposed DeepCausality for potential improvements. In this study, to showcase the proposed DeepCausality, we employed a biomedical-based free text in LiverTox. Additional validation of the utility in other domains is highly recommended. To facilitate the process, all developed codes, scripts, and processed datasets are open to the public through <ext-link xlink:href="https://github.com/XingqiaoWang/https-github.com-XingqiaoWang-DeepCausality-LiverTox" ext-link-type="uri">https://github.com/XingqiaoWang/https-github.com-XingqiaoWang-DeepCausality-LiverTox</ext-link>. Additionally, the BERT-based model was incorporated into the DeepCausality framework presented here. Some generative-based transformers, such as Generative Pre-trained Transformer 3 (GPT3), do not need intensive task-specific training (Brown et al., <xref rid="B2" ref-type="bibr">2020</xref>), which may be a more efficient way to conduct causal inference. Lastly, although DeepCausality could identify the causal factors, it could not classify the identified causal factors further into cofounders or colliders. It may be solved by developing directional DO-calculus statistics in the Bayesian networks derived from the transformers.</p>
    <p>In conclusion, DeepCausality provided an AI-powered solution for causal inference in free text by integrating transformers, NER, and Do-calculus into a unified framework. DeepCausality is proposed for real-world applications to promote RWE collection and utilization.</p>
  </sec>
  <sec sec-type="data-availability" id="s5">
    <title>Data availability statement</title>
    <p>The original contributions presented in the study are included in the article/<xref rid="s9" ref-type="sec">Supplementary material</xref>, further inquiries can be directed to the corresponding author/s.</p>
  </sec>
  <sec id="s6">
    <title>Author contributions</title>
    <p>XX devised the DeepCausality model applied in this study. ZL and WT conceived and designed the study and the utilization of the LiverTox database. XW coded the DeepCausality model. XW and ZL performed data analysis. ZL, XW, and XX wrote the manuscript. WT and QL revised the manuscript. All authors read and approved the final manuscript.</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s7">
    <title>Publisher's note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
  <sec id="s8">
    <title>Author disclaimer</title>
    <p>This manuscript reflects the views of the authors and does not necessarily reflect those of the Food and Drug Administration. Any mention of commercial products is for clarification only and is not intended as approval, endorsement, or recommendation.</p>
  </sec>
</body>
<back>
  <sec sec-type="supplementary-material" id="s9">
    <title>Supplementary material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link xlink:href="https://www.frontiersin.org/articles/10.3389/frai.2022.999289/full#supplementary-material" ext-link-type="uri">https://www.frontiersin.org/articles/10.3389/frai.2022.999289/full#supplementary-material</ext-link></p>
    <supplementary-material id="SM1" position="float" content-type="local-data">
      <media xlink:href="Table_1.XLSX">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="SM2" position="float" content-type="local-data">
      <media xlink:href="Table_2.XLSX">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="SM3" position="float" content-type="local-data">
      <media xlink:href="Table_3.XLSX">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beltagy</surname><given-names>I.</given-names></name><name><surname>Lo</surname><given-names>K.</given-names></name></person-group> (<year>2019</year>). <article-title>SciBERT: A pretrained language model for scientific text</article-title>. <source>arXiv [Preprint].</source> arXiv:1903.10676. <pub-id pub-id-type="doi">10.18653/v1/D19-1371</pub-id><?supplied-pmid 35062081?><pub-id pub-id-type="pmid">35062081</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>T.</given-names></name><name><surname>Mann</surname><given-names>B.</given-names></name><name><surname>Ryder</surname><given-names>N.</given-names></name><name><surname>Subbiah</surname><given-names>M.</given-names></name><name><surname>Kaplan</surname><given-names>J. D.</given-names></name><name><surname>Dhariwal</surname><given-names>P.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Language models are few-shot learners</article-title>. <source>Adv. Neural Inf. Process. Syst.</source><volume>33</volume>, <fpage>1877</fpage>–<lpage>1901</lpage>. <pub-id pub-id-type="doi">10.48550/arXiv.2005.14165</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chalasani</surname><given-names>N. P.</given-names></name><name><surname>Maddur</surname><given-names>H.</given-names></name><name><surname>Russo</surname><given-names>M. W.</given-names></name><name><surname>Wong</surname><given-names>R. J.</given-names></name></person-group> (<year>2021</year>). <article-title>ACG clinical guideline: diagnosis and management of idiosyncratic drug-induced liver injury</article-title>. <source>ACG</source>
<volume>116</volume>, <fpage>878</fpage>–<lpage>898</lpage>. <pub-id pub-id-type="doi">10.14309/ajg.0000000000001259</pub-id><?supplied-pmid 33929376?><pub-id pub-id-type="pmid">33929376</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chalkidis</surname><given-names>I.</given-names></name><name><surname>Fergadiotis</surname><given-names>M.</given-names></name><name><surname>Malakasiotis</surname><given-names>P.</given-names></name><name><surname>Aletras</surname><given-names>N.</given-names></name></person-group> (<year>2020</year>). <article-title>LEGAL-BERT: The muppets straight out of law school</article-title>. <source>arXiv [Preprint].</source> arXiv:2010.02559.</mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>K.</given-names></name><name><surname>Luong</surname><given-names>M. T.</given-names></name><name><surname>Le</surname><given-names>Q. V.</given-names></name></person-group> (<year>2020</year>). <article-title>Electra: Pre-training text encoders as discriminators rather than generators</article-title>. <source>arXiv [Preprint].</source> arXiv:2003.10555.<?supplied-pmid 34330259?><pub-id pub-id-type="pmid">34330259</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devlin</surname><given-names>J.</given-names></name><name><surname>Chang</surname><given-names>M. W.</given-names></name><name><surname>Lee</surname><given-names>K.</given-names></name></person-group> (<year>2018</year>). <article-title>Bert: Pretraining of deep bidirectional transformers for language understanding</article-title>. <source>arXiv [Preprint].</source> arXiv:1810.04805.<?supplied-pmid 32723719?><pub-id pub-id-type="pmid">32723719</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frieden</surname><given-names>T. R.</given-names></name></person-group> (<year>2017</year>). <article-title>Evidence for health decision making — beyond randomized, controlled trials</article-title>. <source>New Engl. J. Med.</source>
<volume>377</volume>, <fpage>465</fpage>–<lpage>475</lpage>. <pub-id pub-id-type="doi">10.1056/NEJMra1614394</pub-id><?supplied-pmid 28767357?><pub-id pub-id-type="pmid">28767357</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gajra</surname><given-names>A.</given-names></name><name><surname>Zettler</surname><given-names>M. E.</given-names></name><name><surname>Feinberg</surname><given-names>B. A.</given-names></name></person-group> (<year>2020</year>). <article-title>Randomization versus Real-World Evidence</article-title>. <source>New England J. Med.</source>
<volume>383</volume>, <fpage>e21</fpage>. <pub-id pub-id-type="doi">10.1056/NEJMc2020020</pub-id><?supplied-pmid 32706552?><pub-id pub-id-type="pmid">32706552</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Y.</given-names></name><name><surname>Tinn</surname><given-names>R.</given-names></name><name><surname>Cheng</surname><given-names>H.</given-names></name><name><surname>Lucas</surname><given-names>M.</given-names></name><name><surname>Usuyama</surname><given-names>N.</given-names></name><name><surname>Liu</surname><given-names>X.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>Domain-specific language model pretraining for biomedical natural language processing</article-title>. <source>ACM Trans. Comput. Healthcare (HEALTH)</source><volume>3</volume>, <fpage>1</fpage>–<lpage>23</lpage>. <pub-id pub-id-type="doi">10.1145/3458754</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hannun</surname><given-names>A.</given-names></name><name><surname>Case</surname><given-names>C.</given-names></name><name><surname>Casper</surname><given-names>J.</given-names></name><name><surname>Catanzaro</surname><given-names>B.</given-names></name><name><surname>Diamos</surname><given-names>G.</given-names></name><name><surname>Elsen</surname><given-names>E.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>Deep speech: Scaling up end-to-end speech recognition</article-title>. <source>arXiv [Preprint].</source> arXiv:1412.5567.</mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hernán</surname><given-names>M. A.</given-names></name></person-group> (<year>2021</year>). <article-title>Methods of public health research — strengthening causal inference from observational data</article-title>. <source>New Engl. J. Med.</source>
<volume>385</volume>, <fpage>1345</fpage>–<lpage>1348</lpage>. <pub-id pub-id-type="doi">10.1056/NEJMp2113319</pub-id><?supplied-pmid 34596980?><pub-id pub-id-type="pmid">34596980</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ho</surname><given-names>M.</given-names></name><name><surname>van der Laan</surname><given-names>M.</given-names></name><name><surname>Lee</surname><given-names>H.</given-names></name><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Lee</surname><given-names>K.</given-names></name><name><surname>Fang</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>The current landscape in biostatistics of real-world data and evidence: causal inference frameworks for study design and analysis</article-title>. <source>Stat. Biopharmaceut. Res.</source><volume>52</volume>, <fpage>511</fpage>–<lpage>525</lpage>. <pub-id pub-id-type="doi">10.1080/19466315.2021.1883475</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoofnagle</surname><given-names>J. H.</given-names></name></person-group> (<year>2013</year>). <article-title>LiverTox: a website on drug-induced liver injury,”</article-title> in <source>Drug-Induced Liver Disease</source> (Elsevier) <fpage>725</fpage>–<lpage>732</lpage>. <pub-id pub-id-type="doi">10.1016/B978-0-12-387817-5.00040-6</pub-id><?supplied-pmid 23456678?><pub-id pub-id-type="pmid">23456678</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>K.</given-names></name><name><surname>Altosaar</surname><given-names>J.</given-names></name></person-group> (<year>2019</year>). <article-title>Clinicalbert: Modeling clinical notes and predicting hospital readmission</article-title>. <source>arXiv [Preprint].</source> arXiv:1904.05342.</mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaeschke</surname><given-names>H.</given-names></name></person-group> (<year>2015</year>). <article-title>Acetaminophen: Dose-dependent drug hepatotoxicity and acute liver failure in patients</article-title>. <source>Dig. Dis.</source>
<volume>33</volume>, <fpage>464</fpage>–<lpage>471</lpage>. <pub-id pub-id-type="doi">10.1159/000374090</pub-id><?supplied-pmid 26159260?><pub-id pub-id-type="pmid">26159260</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>D.</given-names></name><name><surname>Lee</surname><given-names>J.</given-names></name><name><surname>So</surname><given-names>C. H.</given-names></name><name><surname>Jeon</surname><given-names>H.</given-names></name><name><surname>Jeong</surname><given-names>M.</given-names></name><name><surname>Choi</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>A neural named entity recognition and multi-type normalization tool for biomedical text mining</article-title>. <source>IEEE Access</source><volume>7</volume>, <fpage>73729</fpage>–<lpage>73740</lpage>. <pub-id pub-id-type="doi">10.1109/ACCESS.2019.2920708</pub-id><?supplied-pmid 30666476?><pub-id pub-id-type="pmid">30666476</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lan</surname><given-names>Z.</given-names></name><name><surname>Chen</surname><given-names>M.</given-names></name><name><surname>Goodman</surname><given-names>S.</given-names></name><name><surname>Gimpel</surname><given-names>K.</given-names></name><name><surname>Sharma</surname><given-names>P.</given-names></name><name><surname>Soricut</surname><given-names>R.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Albert: A lite bert for self-supervised learning of language representations</article-title>. <source>arXiv [Preprint].</source> arXiv:1909.11942.</mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>J.</given-names></name><name><surname>Yoon</surname><given-names>W.</given-names></name><name><surname>Kim</surname><given-names>S.</given-names></name><name><surname>Kim</surname><given-names>D.</given-names></name><name><surname>Kim</surname><given-names>S.</given-names></name><name><surname>So</surname><given-names>C. H.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>BioBERT: a pre-trained biomedical language representation model for biomedical text mining</article-title>. <source>Bioinformatics</source><volume>36</volume>, <fpage>1234</fpage>–<lpage>1240</lpage>.<?supplied-pmid 31501885?><pub-id pub-id-type="pmid">31501885</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>J.</given-names></name><name><surname>Yoon</surname><given-names>W.</given-names></name><name><surname>Kim</surname><given-names>S.</given-names></name><name><surname>Kim</surname><given-names>D.</given-names></name><name><surname>Kim</surname><given-names>S.</given-names></name><name><surname>So</surname><given-names>C. H.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>BioBERT: a pre-trained biomedical language representation model for biomedical text mining</article-title>. <source>Bioinformatics</source><volume>36</volume>, <fpage>1234</fpage>–<lpage>1240</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btz682</pub-id><?supplied-pmid 31501885?><pub-id pub-id-type="pmid">31501885</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Ott</surname><given-names>M.</given-names></name><name><surname>Goyal</surname><given-names>N.</given-names></name><name><surname>Du</surname><given-names>J.</given-names></name><name><surname>Joshi</surname><given-names>M.</given-names></name><name><surname>Chen</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Roberta: A robustly optimized bert pretraining approach</article-title>. <source>arXiv [Preprint].</source> arXiv:1907.11692.</mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Z.</given-names></name><name><surname>Roberts</surname><given-names>R. A.</given-names></name><name><surname>Lal-Nag</surname><given-names>M.</given-names></name><name><surname>Chen</surname><given-names>X.</given-names></name><name><surname>Huang</surname><given-names>R.</given-names></name><name><surname>Tong</surname><given-names>W.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>AI-based language models powering drug discovery and development</article-title>. <source>Drug Discov. Today</source><volume>26</volume>, <fpage>2593</fpage>–<lpage>2607</lpage>. <pub-id pub-id-type="doi">10.1016/j.drudis.2021.06.009</pub-id><?supplied-pmid 34216835?><pub-id pub-id-type="pmid">34216835</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazhar</surname><given-names>H.</given-names></name><name><surname>Foster</surname><given-names>B. C.</given-names></name><name><surname>Necyk</surname><given-names>C.</given-names></name><name><surname>Gardiner</surname><given-names>P. M.</given-names></name><name><surname>Harris</surname><given-names>C. S.</given-names></name><name><surname>Robaey</surname><given-names>P.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Natural health product-drug interaction causality assessment in pediatric adverse event reports associated with attention-deficit/hyperactivity disorder medication</article-title>. <source>J. Child Adolesc. Psychopharmacol.</source><volume>30</volume>, <fpage>38</fpage>–<lpage>47</lpage>. <pub-id pub-id-type="doi">10.1089/cap.2019.0102</pub-id><?supplied-pmid 31670573?><pub-id pub-id-type="pmid">31670573</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naidu</surname><given-names>R. P.</given-names></name></person-group> (<year>2013</year>). <article-title>Causality assessment: A brief insight into practices in pharmaceutical industry</article-title>. <source>Perspect. Clin. Res.</source>
<volume>4</volume>, <fpage>233</fpage>–<lpage>236</lpage>. <pub-id pub-id-type="doi">10.4103/2229-3485.120173</pub-id><?supplied-pmid 24312892?><pub-id pub-id-type="pmid">24312892</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>O'Mahony</surname><given-names>N.</given-names></name><name><surname>Campbell</surname><given-names>S.</given-names></name><name><surname>Carvalho</surname><given-names>A.</given-names></name><name><surname>Harapanahalli</surname><given-names>S.</given-names></name><name><surname>Hernandez</surname><given-names>G. V.</given-names></name><name><surname>Krpalkova</surname><given-names>L.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Deep learning vs. traditional computer vision,”</article-title> in <source>Science and Information Conference</source> (<publisher-loc>Springer</publisher-loc>) <fpage>128</fpage>–<lpage>144</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-030-17795-9_10</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pearl</surname><given-names>J.</given-names></name></person-group> (<year>2009</year>). <source>Causality: Models, Reasoning and Inference.</source>
<publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>. <pub-id pub-id-type="doi">10.1017/CBO9780511803161</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pearl</surname><given-names>J.</given-names></name><name><surname>Mackenzie</surname><given-names>D.</given-names></name></person-group> (<year>2018</year>). <source>The Book of Why: The New Science of Cause and Effect.</source>
<publisher-loc>Penguin</publisher-loc>: <publisher-name>Basic Books</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanh</surname><given-names>V.</given-names></name><name><surname>Debut</surname><given-names>L.</given-names></name><name><surname>Chaumond</surname><given-names>J.</given-names></name></person-group> (<year>2019</year>). <article-title>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</article-title>. <source>arXiv [Preprint].</source> arXiv:1910.01108.</mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schölkopf</surname><given-names>B.</given-names></name></person-group> (<year>2019</year>). <article-title>Causality for machine learning</article-title>. <source>arXiv [Preprint].</source> arXiv:1911.10500.</mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shrestha</surname><given-names>Y. R.</given-names></name><name><surname>Ben-Menahem</surname><given-names>S. M.</given-names></name><name><surname>Von Krogh</surname><given-names>G.</given-names></name></person-group> (<year>2019</year>). <article-title>Organizational decision-making structures in the age of artificial intelligence</article-title>. <source>California Manag. Rev.</source>
<volume>61</volume>, <fpage>66</fpage>–<lpage>83</lpage>. <pub-id pub-id-type="doi">10.1177/0008125619862257</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tucci</surname><given-names>R. R.</given-names></name></person-group> (<year>2013</year>). <article-title>Introduction to Judea Pearl's Do-Calculus</article-title>. <source>arXiv [Preprint].</source> arXiv:1305.5506.</mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veitch</surname><given-names>V.</given-names></name><name><surname>Sridhar</surname><given-names>D.</given-names></name><name><surname>Blei</surname><given-names>D.</given-names></name></person-group> (<year>2020</year>). <article-title>“Adapting text embeddings for causal inference,”</article-title> in <source>Conference on Uncertainty in Artificial Intelligence, PMLR</source>, <fpage>919</fpage>–<lpage>928</lpage>.</mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Xu</surname><given-names>X.</given-names></name><name><surname>Tong</surname><given-names>W.</given-names></name><name><surname>Roberts</surname><given-names>R.</given-names></name><name><surname>Liu</surname><given-names>Z.</given-names></name></person-group> (<year>2021</year>). <article-title>InferBERT: A transformer-based causal inference framework for enhancing pharmacovigilance</article-title>. <source>Front. Artific. Intell.</source>
<volume>4</volume>, <fpage>659622</fpage>. <pub-id pub-id-type="doi">10.3389/frai.2021.659622</pub-id><?supplied-pmid 34136800?><pub-id pub-id-type="pmid">34136800</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>C.</given-names></name><name><surname>Dai</surname><given-names>R.</given-names></name><name><surname>Gale</surname><given-names>R. P.</given-names></name><name><surname>Zhang</surname><given-names>M. J.</given-names></name></person-group> (<year>2020</year>). <article-title>Causal inference in randomized clinical trials</article-title>. <source>Bone Marrow Transpl.</source>
<volume>55</volume>, <fpage>4</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1038/s41409-018-0424-x</pub-id><?supplied-pmid 32643219?><pub-id pub-id-type="pmid">32643219</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
