<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8388038</article-id>
    <article-id pub-id-type="pmid">33244602</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btaa995</article-id>
    <article-id pub-id-type="publisher-id">btaa995</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Applications Notes</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Data and Text Mining</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>LexExp: a system for automatically expanding concept lexicons for noisy biomedical texts</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-7358-544X</contrib-id>
        <name>
          <surname>Sarker</surname>
          <given-names>Abeed</given-names>
        </name>
        <xref rid="btaa995-cor1" ref-type="corresp"/>
        <aff><institution>Department of Biomedical Informatics, School of Medicine, Emory University</institution>, Atlanta, GA 30322, <country country="US">USA</country></aff>
        <!--abeed@dbmi.emory.edu-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Jonathan</surname>
          <given-names>Wren</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btaa995-cor1">To whom correspondence should be addressed. <email>abeed@dbmi.emory.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>15</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2020-11-27">
      <day>27</day>
      <month>11</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>27</day>
      <month>11</month>
      <year>2020</year>
    </pub-date>
    <volume>37</volume>
    <issue>16</issue>
    <fpage>2499</fpage>
    <lpage>2501</lpage>
    <history>
      <date date-type="received">
        <day>31</day>
        <month>5</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>04</day>
        <month>10</month>
        <year>2020</year>
      </date>
      <date date-type="editorial-decision">
        <day>13</day>
        <month>11</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>17</day>
        <month>11</month>
        <year>2020</year>
      </date>
      <date date-type="corrected-typeset">
        <day>14</day>
        <month>2</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btaa995.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Summary</title>
        <p>LexExp is an open-source, data-centric lexicon expansion system that generates spelling variants of lexical expressions in a lexicon using a phrase embedding model, lexical similarity-based natural language processing methods and a set of tunable threshold decay functions. The system is customizable, can be optimized for recall or precision and can generate variants for multi-word expressions.</p>
      </sec>
      <sec id="s2">
        <title>Availability and implementation</title>
        <p>Code available at: <ext-link xlink:href="https://bitbucket.org/asarker/lexexp" ext-link-type="uri">https://bitbucket.org/asarker/lexexp</ext-link>; data and resources available at: <ext-link xlink:href="https://sarkerlab.org/lexexp" ext-link-type="uri">https://sarkerlab.org/lexexp</ext-link>.</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institute on Drug Abuse</institution>
            <institution-id institution-id-type="DOI">10.13039/100000026</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institutes of Health</institution>
            <institution-id institution-id-type="DOI">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>R01DA046619</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>NIH</institution>
            <institution-id institution-id-type="DOI">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="3"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Lexicon- or dictionary-based biomedical concept detection approaches require the manual curation of relevant lexical expressions, generally by domain experts (<xref rid="btaa995-B2" ref-type="bibr">Demner-Fushman and Elhadad, 2016</xref>; <xref rid="btaa995-B4" ref-type="bibr">Ghiassi and Lee, 2018</xref>; <xref rid="btaa995-B8" ref-type="bibr">Rebholz-Schuhmann <italic toggle="yes">et al.</italic>, 2013</xref>; <xref rid="btaa995-B15" ref-type="bibr">Shivade <italic toggle="yes">et al.</italic>, 2014</xref>). To aid the tedious process of lexicon creation, automated lexicon expansion methods have received considerable research attention, leading to resources such as the UMLS SPECIALIST system (<xref rid="btaa995-B5" ref-type="bibr">McCray <italic toggle="yes">et al.</italic>, 1993</xref>), which is utilized in MetaMap (<xref rid="btaa995-B1" ref-type="bibr">Aronson and Lang, 2010</xref>) and cTAKES (<xref rid="btaa995-B14" ref-type="bibr">Savova <italic toggle="yes">et al.</italic>, 2010</xref>). Such systems perform well for formal biomedical texts, but not noisy texts from sources such as social media and electronic health records. Due to the presence of non-standard expressions, misspellings and abbreviations, it is not possible to capture all possible concept variants in noisy texts using manual or traditional lexicon expansion approaches. The number of lexical variants of a given concept that may occur, although finite, cannot be predetermined. Biomedical concepts, such as symptoms and medications, are specifically likely to be misspelled compared to non-biomedical concepts (<xref rid="btaa995-B16" ref-type="bibr">Soualmia <italic toggle="yes">et al.</italic>, 2012</xref>; <xref rid="btaa995-B19" ref-type="bibr">Zhou <italic toggle="yes">et al.</italic>, 2015</xref>). Despite advances in machine learning based sequence labeling approaches, which typically outperform lexicon-based approaches and can detect inexact concept expressions, the latter are frequently used in biomedical research. This is non-exclusively because machine learning methods require manually annotated datasets, which may be time-consuming and expensive to create, and training and executing state-of-the-art machine learning approaches may require technical expertise and high-performance computers, which may not be available. In this article, we describe an unsupervised lexicon expansion system (<italic toggle="yes">LexExp</italic>), which automatically generates many lexical variants of expressions encoded in a lexicon.</p>
  </sec>
  <sec>
    <title>2 Materials and Methods</title>
    <p>LexExp builds on recent studies, including our own, which utilize the semantic similarities captured by word2vec-type dense-vector models (<xref rid="btaa995-B6" ref-type="bibr">Mikolov <italic toggle="yes">et al.</italic>, 2013</xref>) to automatically identify similar terms and variants (<xref rid="btaa995-B7" ref-type="bibr">Percha <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btaa995-B12" ref-type="bibr">Sarker and Gonzalez-Hernandez, 2018</xref>; <xref rid="btaa995-B17" ref-type="bibr">Viani <italic toggle="yes">et al.</italic>, 2019</xref>). LexExp employs customizable <italic toggle="yes">threshold decay</italic> functions (<italic toggle="yes">constant</italic>, <italic toggle="yes">linear</italic>, <italic toggle="yes">cosine</italic>, <italic toggle="yes">exponential</italic>), combined with dense-vector and lexical similarities, to generate many variants of lexicon entries. Similarity thresholding for determining lexical matches is popular (<xref rid="btaa995-B3" ref-type="bibr">Fischer, 1982</xref>); most approaches apply static thresholding while some recent studies have attempted to employ dynamic thresholding for misspelling correction or generation (<xref rid="btaa995-B12" ref-type="bibr">Sarker and Gonzalez-Hernandez, 2018</xref>; <xref rid="btaa995-B13" ref-type="bibr">Savary, 2002</xref>). However, there is no existing tool that enables the use of customizable thresholding options for these tasks. The objective of LexExp is to generate lexical variants of multi-word expressions using customized thresholding, not lexically dissimilar semantic variants (e.g. synonyms).</p>
    <p>Given a lexicon entry, LexExp first generates word <italic toggle="yes">n</italic>-grams (<italic toggle="yes">n</italic> = 1 and 2) from the entry (for one-word expressions, only unigrams are generated). For each <italic toggle="yes">n</italic>-gram within the entry, a dense embedding model is used to retrieve <italic toggle="yes">n</italic> most semantically similar words/phrases using cosine similarity, if the <italic toggle="yes">n</italic>-gram is present in the model. Next, all the words/phrases whose semantic similarities with the <italic toggle="yes">n</italic>-grams are higher than a threshold are included as candidate variants. For each candidate, its Levenshtein ratio is computed against the original <italic toggle="yes">n</italic>-gram and a separate threshold for lexical similarity (<italic toggle="yes">t</italic>) is applied. All candidates below the threshold are removed from the list of possible variants. The same process is applied recursively on each remaining candidate until no new variants with similarity above <italic toggle="yes">t</italic> are found. While we used an embedding model described in our past work (<xref rid="btaa995-B11" ref-type="bibr">Sarker and Gonzalez, 2017</xref>), any can be used for identifying semantically similar terms in LexExp.</p>
    <sec>
      <title>2.1 Lexical similarity thresholding functions</title>
      <p>LexExp provides the user with four functions that can be used to vary <italic toggle="yes">t</italic> based on the character lengths of the input <italic toggle="yes">n</italic>-grams. Typically, longer terms/phrases have <italic toggle="yes">true</italic> variants that are lexically more distant from the original entry. So, adjusting <italic toggle="yes">t</italic> based on the length of an expression may lead to better precision and/or recall. Note that recall and precision are both ill-defined in this context: recall—because there is no known bound for the total number of variants; precision—because the set of <italic toggle="yes">true variants</italic> depends on the research task. Given an expression, <italic toggle="yes">p</italic>, the four functions are:</p>
      <p><bold>Static</bold>: <italic toggle="yes">t</italic> = <italic toggle="yes">t<sub>i</sub></italic></p>
      <p><bold>Linear decay</bold>: 
<disp-formula id="E1"><mml:math id="M1" display="block" overflow="scroll"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">max</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mi>m</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="normal">len</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="normal"> </mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>100.0</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula></p>
      <p><bold>Cosine decay</bold>: 
<disp-formula id="E2"><mml:math id="M2" display="block" overflow="scroll"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">min</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="normal">max</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">max</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">len</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mn>2</mml:mn><mml:mo>π</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula></p>
      <p><bold>Exponential decay</bold>: 
<disp-formula id="E3"><mml:math id="M3" display="block" overflow="scroll"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">min</mml:mi><mml:mo>(</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">len</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="normal"> </mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>m</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mo>.</mml:mo><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">len</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula>where <italic toggle="yes">m</italic> and <italic toggle="yes">n</italic> are constants, <italic toggle="yes">t<sub>i</sub></italic> is the initial threshold and <italic toggle="yes">t<sub>l</sub></italic> is the lower bound for <italic toggle="yes">t</italic>. <xref rid="btaa995-F1" ref-type="fig">Figure 1</xref> illustrates how these thresholding methods vary for expressions of length 1–30 characters. These thresholding functions are carefully designed to provide the user with flexibility to vary them as per the needs of a task.</p>
      <fig position="float" id="btaa995-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>(<bold>top</bold>) Workflow of the LexExp system. Expressions from the lexicon are first tokenized into <italic toggle="yes">n</italic>-grams. The dense-vector model is then used to generate semantically similar expressions, which are filtered to keep only <italic toggle="yes">k</italic> most semantically similar expressions for each source token. These expressions are then passed to the lexical similarity filter, which employs one of the threshold decay functions described in the article to remove too lexically dissimilar expressions. Finally, the generated variants are combined to produce all combinations of multi-word expressions and repetitions are removed. Such repetitions occur often and typically when a variant of an <italic toggle="yes">n</italic>-gram token has a different <italic toggle="yes">n</italic> in the variant. For example, “panic attack” is often expressed as one word, “panicattack”, on social media (perhaps as a hashtag), which is generated as a semantic variant during the expansion process. The variant combination generator then combines the single-word expression with tokens in multi-word expressions leading to the generation of variants such as “panicattack attack”. The repetition filter then attempts to identify such repetitions and remove them. (<bold>bottom</bold>) The four threshold determination functions that can be used in LexExp. Static—no change in threshold; linear—threshold decreases in a linear fashion with length; cosine—gradient of threshold decay increases with length and exponential—gradient of threshold decay decreases with length. For all these curves, <italic toggle="yes">m</italic> = 2, <italic toggle="yes">n</italic> = 3, initial threshold (<italic toggle="yes">t<sub>i</sub></italic>) = 0.95 and threshold lower bound (<italic toggle="yes">t<sub>l</sub></italic>) = 0.30</p>
        </caption>
        <graphic xlink:href="btaa995f1" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.2 Multi-word variants</title>
      <p>A key functionality of LexExp is its ability to generate variants for multi-word expressions. Capturing variants of multi-word expressions comprehensively is particularly challenging via manual annotation since the number of possible word combinations can be very high. Also, phrase embedding models cannot capture the semantics of long multi-word expressions due to the sparsity of their occurrences.</p>
      <p>LexExp uses two functions for generating multi-word variants. The first is a unigram variant generation function that generates variants for each word based on a specific value of <italic toggle="yes">t</italic>, and then generates all combinations of the original expression based on the variants identified, keeping the ordering of the variants unchanged. Examples of variants generated by this function are shown below:</p>
      <p><italic toggle="yes">Original expression</italic>: eyes were excruciatingly sensitive and sore.</p>
      <p><italic toggle="yes">Sample variants</italic>:
</p>
      <list list-type="order">
        <list-item>
          <p>eyes were <bold>excrusiatingly sensistive</bold> and sore;</p>
        </list-item>
        <list-item>
          <p>eyes were excruciatingly <bold>sensitve</bold> and sore;</p>
        </list-item>
        <list-item>
          <p>eyes were <bold>excrusiatingly</bold> sensitive and sore;</p>
        </list-item>
        <list-item>
          <p>eyes were <bold>excrusiatingly sensetive</bold> and sore.</p>
        </list-item>
      </list>
      <p>The second is a bigram generation function, which first tokenizes the expressions into bigrams, then generates variants of the bigrams. These variants maybe uni- or multi-grams (e.g. stomach ache: stomachache, mild stomach ache). After the variants are generated, they are tokenized to unigrams and then all combinations of all unigrams are generated as described before. Recombining the bigrams following the generation of the variants can be complicated in some cases, as a term and its <italic toggle="yes">partial variant</italic> may both be present in a combination (see <xref rid="btaa995-F1" ref-type="fig">Figure 1</xref> caption). LexExp attempts to resolve these using a simple forward and backward pass through the list of words, removing all words identical to or substrings of the next/previous one.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Conclusion</title>
    <p>We ran LexExp on multiple lexicons, including lexicons for COVID-19 symptoms from Twitter (<xref rid="btaa995-B9" ref-type="bibr">Sarker <italic toggle="yes">et al.</italic>, 2020</xref>), adverse drug reactions (<xref rid="btaa995-B10" ref-type="bibr">Sarker and Gonzalez, 2015</xref>), a subset of the consumer health vocabulary (<xref rid="btaa995-B18" ref-type="bibr">Zeng and Tse, 2006</xref>) and psychosis symptoms from electronic health records (<xref rid="btaa995-B17" ref-type="bibr">Viani <italic toggle="yes">et al.</italic>, 2019</xref>). We also compared tweet retrieval numbers for COVID-19 symptom-mentioning tweets using the abovementioned lexicon with and without variants, and observed an increase of 16.6%. Further details about these experiments are provided in <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>. As a lexicon expansion system, the purpose of LexExp is not to obtain perfect accuracy—in fact, accuracy is not well-defined for this generation task. The objective, instead, is to automatically generate large sets of possible variants that can be readily used by human experts for information retrieval and extraction.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>Research reported in this publication was supported by the National Institute on Drug Abuse (NIDA) of the National Institutes of Health (NIH) under award No. R01DA046619. The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIH.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Conflict of Interest</title>
    <p>None declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btaa995_Supplementary_Data</label>
      <media xlink:href="btaa995_supplementary_data.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btaa995-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aronson</surname><given-names>A.R.</given-names></string-name>, <string-name><surname>Lang</surname><given-names>F.-M.</given-names></string-name></person-group> (<year>2010</year>) <article-title>An overview of MetaMap: historical perspective and recent advances</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>17</volume>, <fpage>229</fpage>–<lpage>236</lpage>.<pub-id pub-id-type="pmid">20442139</pub-id></mixed-citation>
    </ref>
    <ref id="btaa995-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Demner-Fushman</surname><given-names>D.</given-names></string-name>, <string-name><surname>Elhadad</surname><given-names>N.</given-names></string-name></person-group> (<year>2016</year>) <article-title>Aspiring to unintended consequences of natural language processing: a review of recent developments in clinical and consumer-generated text processing</article-title>. <source>IMIA Yearbook</source>, <volume>25</volume>, <fpage>224</fpage>–<lpage>233</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa995-B3">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Fischer</surname><given-names>R.-J.</given-names></string-name></person-group> (<year>1982</year>) <source>A Threshold Method of Approximate String Matching</source>. <publisher-name>Springer</publisher-name>, <publisher-loc>Berlin, Heidelberg</publisher-loc>, pp. <fpage>843</fpage>–<lpage>849</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa995-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ghiassi</surname><given-names>M.</given-names></string-name>, <string-name><surname>Lee</surname><given-names>S.</given-names></string-name></person-group> (<year>2018</year>) <article-title>A domain transferable lexicon set for Twitter sentiment analysis using a supervised machine learning approach</article-title>. <source>Expert Syst. Appl</source>., <volume>106</volume>, <fpage>197</fpage>–<lpage>216</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa995-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McCray</surname><given-names>A.T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>1993</year>) <article-title>UMLS<sup>®</sup> knowledge for biomedical language processing</article-title>. <source>Bull. Med. Libr. Assoc</source>., <volume>81</volume>, <fpage>184</fpage>–<lpage>194</lpage>.<pub-id pub-id-type="pmid">8472004</pub-id></mixed-citation>
    </ref>
    <ref id="btaa995-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mikolov</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2013</year>) <article-title>Distributed representations of words and phrases and their compositionality</article-title>. In: <italic toggle="yes">Proceedings of the 26th International Conference on Neural Information Processing Systems</italic> - Volume 2 (NIPS'13). Curran Associates Inc., Red Hook, NY, USA, pp. <fpage>3111</fpage>–<lpage>3119</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa995-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Percha</surname><given-names>B.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>Expanding a radiology lexicon using contextual patterns in radiology reports</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>25</volume>, <fpage>679</fpage>–<lpage>685</lpage>.<pub-id pub-id-type="pmid">29329435</pub-id></mixed-citation>
    </ref>
    <ref id="btaa995-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rebholz-Schuhmann</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2013</year>) <article-title>Evaluating gold standard corpora against gene/protein tagging solutions and lexical resources</article-title>. <source>J. Biomed. Sem</source>., <volume>4</volume>, <fpage>28</fpage>.</mixed-citation>
    </ref>
    <ref id="btaa995-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sarker</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Self-reported COVID-19 symptoms on Twitter: an analysis and a research resource</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>27</volume>, <fpage>1310</fpage>–<lpage>1315</lpage>.<pub-id pub-id-type="pmid">32620975</pub-id></mixed-citation>
    </ref>
    <ref id="btaa995-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sarker</surname><given-names>A.</given-names></string-name>, <string-name><surname>Gonzalez</surname><given-names>G.</given-names></string-name></person-group> (<year>2015</year>) <article-title>Portable automatic text classification for adverse drug reaction detection via multi-corpus training</article-title>. <source>J. Biomed. Inform</source>., <volume>53</volume>, <fpage>196</fpage>–<lpage>207</lpage>.<pub-id pub-id-type="pmid">25451103</pub-id></mixed-citation>
    </ref>
    <ref id="btaa995-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sarker</surname><given-names>A.</given-names></string-name>, <string-name><surname>Gonzalez</surname><given-names>G.</given-names></string-name></person-group> (<year>2017</year>) <article-title>A corpus for mining drug-related knowledge from Twitter chatter: language models and their utilities</article-title>. <source>Data Brief</source>., <volume>10</volume>, <fpage>122</fpage>–<lpage>131</lpage>.<pub-id pub-id-type="pmid">27981203</pub-id></mixed-citation>
    </ref>
    <ref id="btaa995-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sarker</surname><given-names>A.</given-names></string-name>, <string-name><surname>Gonzalez-Hernandez</surname><given-names>G.</given-names></string-name></person-group> (<year>2018</year>) <article-title>An unsupervised and customizable misspelling generator for mining noisy health-related text sources</article-title>. <source>J. Biomed. Inform</source>., <volume>88</volume>, <fpage>98</fpage>–<lpage>107</lpage>.<pub-id pub-id-type="pmid">30445220</pub-id></mixed-citation>
    </ref>
    <ref id="btaa995-B13">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Savary</surname><given-names>A.</given-names></string-name></person-group> (<year>2002</year>) <source>Typographical Nearest-Neighbor Search in a Finite-State Lexicon and its Application to Spelling Correction. Lecture Notes in Computer Science. Artificial Intelligence and Lecture Notes in Bioinformatics</source>. <publisher-name>Springer Verlag</publisher-name>, <publisher-loc>Berlin, Heidelberg</publisher-loc>, pp. <fpage>251</fpage>–<lpage>260</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa995-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Savova</surname><given-names>G.K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2010</year>) <article-title>Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>17</volume>, <fpage>507</fpage>–<lpage>513</lpage>.<pub-id pub-id-type="pmid">20819853</pub-id></mixed-citation>
    </ref>
    <ref id="btaa995-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shivade</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) <article-title>A review of approaches to identifying patient phenotype cohorts using electronic health records</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>21</volume>, <fpage>221</fpage>–<lpage>230</lpage>.<pub-id pub-id-type="pmid">24201027</pub-id></mixed-citation>
    </ref>
    <ref id="btaa995-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Soualmia</surname><given-names>L.F.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2012</year>) <article-title>Matching health information seekers’ queries to medical terms</article-title>. <source>BMC Bioinform</source>., <volume>13</volume>, <fpage>S11</fpage>.</mixed-citation>
    </ref>
    <ref id="btaa995-B17">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Viani</surname><given-names>N.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <source>Generating Positive Psychosis Symptom Keywords from Electronic Health Records. Lecture Notes in Computer Science. Artificial Intelligence and Lecture Notes in Bioinformatics</source>. <publisher-name>Springer Verlag</publisher-name>, <publisher-loc>Berlin, Heidelberg</publisher-loc>, pp. <fpage>298</fpage>–<lpage>303</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa995-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname><given-names>Q.T.</given-names></string-name>, <string-name><surname>Tse</surname><given-names>T.</given-names></string-name></person-group> (<year>2006</year>) <article-title>Exploring and developing consumer health vocabularies</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>13</volume>, <fpage>24</fpage>–<lpage>29</lpage>.<pub-id pub-id-type="pmid">16221948</pub-id></mixed-citation>
    </ref>
    <ref id="btaa995-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname><given-names>X.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) <article-title>Context-sensitive spelling correction of consumer-generated content on health care</article-title>. <source>JMIR Med. Inform</source>., <volume>3</volume>, <fpage>e27</fpage>.<pub-id pub-id-type="pmid">26232246</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
