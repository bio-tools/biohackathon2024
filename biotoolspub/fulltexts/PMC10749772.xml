<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10749772</article-id>
    <article-id pub-id-type="pmid">38109668</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btad752</article-id>
    <article-id pub-id-type="publisher-id">btad752</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Sequence Analysis</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>LncLocFormer: a Transformer-based deep learning model for multi-label lncRNA subcellular localization prediction by using localization-specific attention mechanism</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1726-0955</contrib-id>
        <name>
          <surname>Zeng</surname>
          <given-names>Min</given-names>
        </name>
        <aff><institution>School of Computer Science and Engineering, Central South University</institution>, Changsha, Hunan 410083, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wu</surname>
          <given-names>Yifan</given-names>
        </name>
        <aff><institution>School of Computer Science and Engineering, Central South University</institution>, Changsha, Hunan 410083, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Yiming</given-names>
        </name>
        <aff><institution>School of Computer Science and Engineering, Central South University</institution>, Changsha, Hunan 410083, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yin</surname>
          <given-names>Rui</given-names>
        </name>
        <aff><institution>Department of Health Outcomes and Biomedical Informatics, University of Florida</institution>, Gainesville, FL 32603, <country country="US">United States</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lu</surname>
          <given-names>Chengqian</given-names>
        </name>
        <aff><institution>School of Computer Science, Key Laboratory of Intelligent Computing and Information Processing, Xiangtan University</institution>, Xiangtan, Hunan 411105, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-4955-9270</contrib-id>
        <name>
          <surname>Duan</surname>
          <given-names>Junwen</given-names>
        </name>
        <aff><institution>School of Computer Science and Engineering, Central South University</institution>, Changsha, Hunan 410083, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0188-1394</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>Min</given-names>
        </name>
        <aff><institution>School of Computer Science and Engineering, Central South University</institution>, Changsha, Hunan 410083, <country country="CN">China</country></aff>
        <xref rid="btad752-cor1" ref-type="corresp"/>
        <!--limin@mail.csu.edu.cn-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Boeva</surname>
          <given-names>Valentina</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btad752-cor1">Corresponding author. School of Computer Science and Engineering, Central South University, Changsha, Hunan 410083, China. E-mail: <email>limin@mail.csu.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2023-12-18">
      <day>18</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>18</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <volume>39</volume>
    <issue>12</issue>
    <elocation-id>btad752</elocation-id>
    <history>
      <date date-type="received">
        <day>26</day>
        <month>7</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>13</day>
        <month>11</month>
        <year>2023</year>
      </date>
      <date date-type="editorial-decision">
        <day>07</day>
        <month>12</month>
        <year>2023</year>
      </date>
      <date date-type="corrected-typeset">
        <day>25</day>
        <month>12</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btad752.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>There is mounting evidence that the subcellular localization of lncRNAs can provide valuable insights into their biological functions. In the real world of transcriptomes, lncRNAs are usually localized in multiple subcellular localizations. Furthermore, lncRNAs have specific localization patterns for different subcellular localizations. Although several computational methods have been developed to predict the subcellular localization of lncRNAs, few of them are designed for lncRNAs that have multiple subcellular localizations, and none of them take motif specificity into consideration.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>In this study, we proposed a novel deep learning model, called LncLocFormer, which uses only lncRNA sequences to predict multi-label lncRNA subcellular localization. LncLocFormer utilizes eight Transformer blocks to model long-range dependencies within the lncRNA sequence and shares information across the lncRNA sequence. To exploit the relationship between different subcellular localizations and find distinct localization patterns for different subcellular localizations, LncLocFormer employs a localization-specific attention mechanism. The results demonstrate that LncLocFormer outperforms existing state-of-the-art predictors on the hold-out test set. Furthermore, we conducted a motif analysis and found LncLocFormer can capture known motifs. Ablation studies confirmed the contribution of the localization-specific attention mechanism in improving the prediction performance.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The LncLocFormer web server is available at <ext-link xlink:href="http://csuligroup.com:9000/LncLocFormer" ext-link-type="uri">http://csuligroup.com:9000/LncLocFormer</ext-link>. The source code can be obtained from <ext-link xlink:href="https://github.com/CSUBioGroup/LncLocFormer" ext-link-type="uri">https://github.com/CSUBioGroup/LncLocFormer</ext-link>.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Key Research and Development Program of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100012166</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>2022YFC3400300</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>62102457</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Hunan Provincial Natural Science Foundation of China</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2023JJ40763</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="11"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Long non-coding RNAs (lncRNAs) are a class of non-coding RNA molecules that comprised more than 200 nucleotides (<xref rid="btad752-B7" ref-type="bibr">Birney <italic toggle="yes">et al.</italic> 2007</xref>, <xref rid="btad752-B21" ref-type="bibr">Lu <italic toggle="yes">et al.</italic> 2018</xref>). They are involved in various important biological processes, including the regulation of gene expression, alternative splicing, nuclear organization, and genomic imprinting (<xref rid="btad752-B36" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2020</xref>). LncRNAs have the ability to interact with proteins, DNAs, and RNAs, and perform specific functions as a result of these interactions (<xref rid="btad752-B10" ref-type="bibr">Esteller 2011</xref>). For example, they can act as “miRNA sponge” to regulate miRNA levels and thereby influence the expression of miRNA targets (<xref rid="btad752-B9" ref-type="bibr">DiStefano 2018</xref>). Additionally, under particular stimulation, lncRNAs can influence transcriptional activity or pathways (<xref rid="btad752-B31" ref-type="bibr">Wang and Chang 2011</xref>). Because of the complexity of molecular functions and biological processes, lncRNA-related research has gained significant attention (<xref rid="btad752-B20" ref-type="bibr">Lu <italic toggle="yes">et al.</italic> 2020</xref>, <xref rid="btad752-B35" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2021</xref>).</p>
    <p>A growing amount of evidence reveals that lncRNA subcellular localizations can provide valuable insights into their biological functions (<xref rid="btad752-B28" ref-type="bibr">Savulescu <italic toggle="yes">et al.</italic> 2021</xref>). One wet-lab technique commonly used to study RNA subcellular localization is single-molecule fluorescent <italic toggle="yes">in situ</italic> hybridization (smFISH) technique (<xref rid="btad752-B24" ref-type="bibr">Moffitt and Zhuang 2016</xref>). Although the image data provided by the smFISH technique can accurately determine the subcellular localization of RNAs, the smFISH technique is expensive and time-consuming. Considering its limitations, it would be extremely beneficial for biologists to develop accurate computational methods to predict lncRNA subcellular localizations.</p>
    <p>Some computational methods have been proposed to predict lncRNA subcellular localization. To the best of our knowledge, LncLocator is the first predictor for lncRNA subcellular localization (<xref rid="btad752-B5" ref-type="bibr">Cao <italic toggle="yes">et al.</italic> 2018</xref>). It extracts 4-mer features and high-level features, and uses support vector machine (SVM) and random forest to make predictions. iLoc-lncRNA utilizes 8-mer features to encode lncRNA sequences, and applies SVM to perform the prediction task (<xref rid="btad752-B30" ref-type="bibr">Su <italic toggle="yes">et al.</italic> 2018</xref>). DeepLncRNA incorporates 2, 3, 4, and 5-mer features and uses a deep learning network to predict lncRNA subcellular localizations (<xref rid="btad752-B13" ref-type="bibr">Gudenas and Wang 2018</xref>). Locate-R incorporates the preselected <italic toggle="yes">k</italic>-mer features and applies SVM to construct a classifier (<xref rid="btad752-B1" ref-type="bibr">Ahmad <italic toggle="yes">et al.</italic> 2020</xref>). lncLocPred integrates multiple feature selection techniques to select optimal features, and adopts a logistic regression model to make predictions (<xref rid="btad752-B11" ref-type="bibr">Fan <italic toggle="yes">et al.</italic> 2020</xref>). LncLocation integrates the multi-source heterogeneous features, and uses SVM to construct a classifier (<xref rid="btad752-B12" ref-type="bibr">Feng <italic toggle="yes">et al.</italic> 2020</xref>). DeepLncLoc is a novel deep learning model, which uses subsequence embedding technique to encode lncRNA sequences, and uses a deep neural network to classify five localizations (<xref rid="btad752-B37" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2022</xref>). TACOS applies a tree-based stacking classifier to predict the subcellular localization of human lncRNA in 10 different cell types (<xref rid="btad752-B15" ref-type="bibr">Jeon <italic toggle="yes">et al.</italic> 2022</xref>). RNALight extracts <italic toggle="yes">k</italic>-mer features and uses LightGBM to predict the subcellular localizations of mRNAs and lncRNAs (<xref rid="btad752-B34" ref-type="bibr">Yuan <italic toggle="yes">et al.</italic> 2023</xref>). GraphLncLoc transforms lncRNA sequences into graphs, and utilizes graph convolutional networks to capture high-level features and make predictions (<xref rid="btad752-B17" ref-type="bibr">Li <italic toggle="yes">et al.</italic> 2023</xref>). Recently, LncLocator 2.0 (<xref rid="btad752-B19" ref-type="bibr">Lin <italic toggle="yes">et al.</italic> 2021</xref>) and iLoc-LncRNA(2.0) (<xref rid="btad752-B42" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> 2022</xref>) have been released, which provide more accurate prediction results than their previous versions.</p>
    <p>Although several computational models have been developed, few of these models are designed for lncRNAs that have multiple subcellular localizations. In reality, lncRNA subcellular localization is a dynamic process (<xref rid="btad752-B4" ref-type="bibr">Bridges <italic toggle="yes">et al.</italic> 2021</xref>). For example, lncRNA SNHG1 displays cytoplasmic distribution in human HCT116 colon cancer cells. However, upon DNA damage stress, it is retained in the nucleus compartment (<xref rid="btad752-B6" ref-type="bibr">Carlevaro-Fita and Johnson 2019</xref>). Another example is lncRNA Uchl1-AS1, which translocates from the nucleus to the cytoplasm under rapamycin treatment (<xref rid="btad752-B27" ref-type="bibr">Riva <italic toggle="yes">et al.</italic> 2016</xref>). However, the existing computational models usually only consider a single subcellular localization for each lncRNA.</p>
    <p>In addition, increasing evidence suggests that lncRNAs exhibit distinct localization patterns in different subcellular localizations. For example, Shukla <italic toggle="yes">et al.</italic> found that conserved long sequences (&gt;300 nt) with a common 15-nt C-rich pattern are responsible for nuclear localization (<xref rid="btad752-B29" ref-type="bibr">Shukla <italic toggle="yes">et al.</italic> 2018</xref>). Lubelsky <italic toggle="yes">et al.</italic> found a core 42-nt motif that drives nuclear RNA localization (<xref rid="btad752-B22" ref-type="bibr">Lubelsky and Ulitsky 2018</xref>). Despite these findings, existing computational methods do not take motif specificity in different subcellular localizations into account.</p>
    <p>To meet the need for lncRNA multiple subcellular localization predictions and to consider motif specificity for different subcellular localizations, we proposed LncLocFormer, which is a Transformer-based deep learning model using a localization-specific attention mechanism. Transformer is a class of powerful deep learning architecture that has achieved substantial breakthroughs in natural language processing (NLP), as it can capture both local and global features of sequences. Inspired by its success in NLP, we applied it to the prediction of lncRNA subcellular localization. By using the positional coding and multi-head attention mechanism in Transformer blocks, LncLocFormer can model long-range dependencies within the lncRNA sequence and share information across the lncRNA sequence. Different from previous computational methods, LncLocFormer can predict multiple subcellular localizations simultaneously for each lncRNA sequence. Furthermore, using the localization-specific attention mechanism, LncLocFormer learns different attention weights for different subcellular localizations, which can provide valuable information about the relationship between different labels.</p>
    <p>To evaluate the performance of LncLocFormer, we compared it with some deep learning baseline models and existing state-of-the-art predictors. The results of cross-validation (CV) and the hold-out test set demonstrate that LncLocFormer performs significantly better than other computational models. In addition, the results show that LncLocFormer is capable of capturing sequence motifs. To investigate which part of LncLocFormer is helpful in predicting lncRNA subcellular localizations, we conducted an ablation study by removing or replacing some components of LncLocFormer. The ablation study shows that the localization-specific attention mechanism is a crucial component in LncLocFormer. To facilitate the use of LncLocFormer, we developed a user-friendly web server.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Benchmark dataset</title>
      <p>The first important step in constructing a reliable predictor is to establish a reliable benchmark dataset. To achieve this, we retrieved known lncRNA subcellular localization information from the RNALocate v2.0 (<xref rid="btad752-B8" ref-type="bibr">Cui <italic toggle="yes">et al.</italic> 2022</xref>) database (<ext-link xlink:href="https://www.rna-society.org/rnalocate/" ext-link-type="uri">https://www.rna-society.org/rnalocate/</ext-link>), which collects more than 210 000 RNA-associated subcellular localization entries with experimental evidence, encompassing more than 110 000 RNAs with 171 subcellular localizations in 104 species. We generated a benchmark dataset to train and test our model by the following procedure:</p>
      <list list-type="order">
        <list-item>
          <p>We retrieved a total of 9128 Homo sapiens lncRNA-associated subcellular localization entries from the RNALocate v2.0 database. Since many lncRNAs have multiple entries, we merged the entries with the same gene symbol;</p>
        </list-item>
        <list-item>
          <p>We removed the lncRNAs that do not have sequence information in NCBI (<xref rid="btad752-B26" ref-type="bibr">Pruitt <italic toggle="yes">et al.</italic> 2007</xref>);</p>
        </list-item>
        <list-item>
          <p>To reduce data redundancy, we used the cd-hit-est tool (<xref rid="btad752-B14" ref-type="bibr">Huang <italic toggle="yes">et al.</italic> 2010</xref>) with a cutoff of 80%;</p>
        </list-item>
        <list-item>
          <p>Consider that some subcellular localizations have a small number lncRNA entries, we only selected the subcellular localizations with more than 40 lncRNA entries;</p>
        </list-item>
        <list-item>
          <p>In the RNALocate v2.0 database, a significant number of entries are localized in exosome. However, accumulating evidence suggests that lncRNAs are expressed in a cell-specific and/or tissue-specific manner, and most of them are located in the nucleus. Moreover, a lot of samples which belong to the exosome localization could hinder the prediction of other subcellular locations. Thus, we removed exosome-localized entries in our study.</p>
        </list-item>
      </list>
      <p>Finally, our benchmark dataset comprises 811 lncRNAs, covering four types of subcellular localizations: nucleus, cytoplasm, chromatin, and insoluble cytoplasm. <xref rid="btad752-F1" ref-type="fig">Figure 1</xref> shows the distribution of subcellular localizations in the constructed benchmark dataset.</p>
      <fig position="float" id="btad752-F1">
        <label>Figure 1.</label>
        <caption>
          <p>The distribution of subcellular localizations in the constructed benchmark dataset.</p>
        </caption>
        <graphic xlink:href="btad752f1" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.2 LncLocFormer architecture</title>
      <p><xref rid="btad752-F2" ref-type="fig">Figure 2</xref> illustrates the architecture of LncLocFormer, which comprises four main components: (i) the embedding part, (ii) eight Transformer blocks, (iii) a localization-specific attention mechanism, and (iv) a fully connected layer that performs the multi-label classification task.</p>
      <fig position="float" id="btad752-F2">
        <label>Figure 2.</label>
        <caption>
          <p>The architecture of LncLocFormer. LncLocFormer takes a lncRNA sequence as input, which is encoded using a subsequence embedding method. The embedding layer is immediately followed by eight Transformer blocks, which are used to model long-range dependencies within the lncRNA sequence and share information across the lncRNA sequence. These Transformer blocks have the same architecture: relative position encoding, multi-head attention mechanism, residual connection, position-wise feed-forward network, and “Add &amp; Norm” component. Following the Transformer blocks, the designed localization-specific attention layer is employed to learn distinct weights of nucleotide for each subcellular localization. Finally, a fully connected layer is used to perform the multi-label classification task.</p>
        </caption>
        <graphic xlink:href="btad752f2" position="float"/>
      </fig>
      <sec>
        <title>2.2.1 Sequence embedding</title>
        <p>Before feeding raw lncRNA sequences into a deep learning model, it is necessary to encode them as numeric vectors. The two most commonly used coding techniques are <italic toggle="yes">k</italic>-mer coding and one-hot coding. However, <italic toggle="yes">k</italic>-mer coding loses the sequence order information, while one-hot coding ignores the relationship between different nucleotides. In order to tackle these limitations, we employed an effective subsequence embedding method (<xref rid="btad752-B37" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2022</xref>), which can preserve the sequence order information of lncRNAs and reflect the relationship between different <italic toggle="yes">k</italic>-mers. The main idea of the subsequence embedding method is to split a lncRNA sequence into a number of consecutive, non-overlapping subsequences. Then, we extracted patterns from each subsequence and combined these patterns to obtain a complete representation of the lncRNA sequence.</p>
        <p>Specifically, the subsequence embedding method involves several steps. First, we split a lncRNA sequence into <italic toggle="yes">n</italic> consecutive, non-overlapping subsequences. Then, we used an embedding technique to encode each subsequence. Word2vec is a popular word embedding technique in NLP that has demonstrated potential in many bioinformatics tasks (<xref rid="btad752-B32" ref-type="bibr">Wu <italic toggle="yes">et al.</italic> 2022</xref>, <xref rid="btad752-B17" ref-type="bibr">Li <italic toggle="yes">et al.</italic> 2023</xref>). Thus, we pre-trained all lncRNA sequences in our dataset to obtain the distribution representation of <italic toggle="yes">k</italic>-mers by using the word2vec method, and then used the distribution representation of <italic toggle="yes">k</italic>-mer features to represent these subsequences. In the training process, the parameter <italic toggle="yes">k</italic> was chosen from {1, 2, 3, 4, 5, 6} to find the best value. The skip-gram model was applied to maximize the co-occurrence likelihood function of the central word and corresponding context words. The other settings of word2vec were kept default to train word vectors. Finally, in our study, we set <italic toggle="yes">k </italic>=<italic toggle="yes"> </italic>3, and the dimension of the word vector was 128. After pre-trained on the dataset, we obtained the word vectors and then combined these vectors to represent a lncRNA sequence. The whole subsequence embedding framework is shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S1</xref>. We refer to the original publication of the subsequence embedding method for more details (<xref rid="btad752-B37" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2022</xref>).</p>
      </sec>
      <sec>
        <title>2.2.2 Transformer blocks</title>
        <p>So far, we have obtained the representation of a lncRNA sequence. The next step is to extract high-level features from the lncRNA representation. Transformer is a class of deep learning models that has achieved substantial breakthroughs in NLP and has recently been applied to various bioinformatics tasks. In our task, we employed eight Transformer blocks to model long-range dependencies within the lncRNA sequence and share information across the lncRNA sequence. The Transformer blocks are inspired by RealFormer, which is a state-of-the-art variant version of Transformer. The detailed structure is shown in the right side of <xref rid="btad752-F2" ref-type="fig">Fig. 2</xref>. These Transformer blocks mainly consist of five components: relative positional encoding, multi-head attention, residual connection, “add &amp; norm” component, and position-wise feed-forward network.</p>
        <p>The first component is relative positional encoding. We know that Recurrent Neural Network (RNN) is a sequential structure that recurrently processes words one by one. Unlike RNN, the core part of Transformer is the attention mechanism. Using the attention mechanism to replace RNN loses the sequence order information, which causes that the model does not know the relative and absolute position information of each nucleotide in lncRNA sequences. Thus, it is necessary to add the sequence order information to assist the model in learning the position information. To use the sequence order information, we inject relative positional information by using relative positional encoding to the input representations. We borrowed the idea of traditional relative positional encoding and made some modifications. Specifically, the original formula of “traditional relative positional encoding” is as follows:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        <p>We made some modifications, resulting in the modified relative positional encoding formula to inject relative positional information:
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a learnable parameter, <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are learnable matrixes, and <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a relative position bias term. The main difference between the relative positional encoding and traditional relative positional encoding lies in the inclusion of the relative position scale term <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The advantage is, by adding this term, we can control the information passing for different relative locations more efficiently, making the model better able to capture the long-term dependencies within the lncRNA sequence.</p>
        <p>After relative positional coding, LncLocFormer applies self-attention to learn the attention weights for each nucleotide pair in the lncRNA sequence. The attention weights can reveal the importance of sequence regions for subcellular localization. Specifically, for each input lncRNA sequence:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mi mathvariant="normal">lncRNA</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where <italic toggle="yes">L</italic> denotes the length of the lncRNA, <italic toggle="yes">Nj</italic> is one of the four nucleotide bases (A, C, G, and U) at the <italic toggle="yes">j</italic> position of the lncRNA sequence. Self-attention learns an attention score for each pair of nucleotides <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic>. The attention score is computed by using a query <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mi>Q</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, a key <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mi>K</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, a value <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mi>V</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, a pre-softmax attention score Prev as follows:
<disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mi mathvariant="normal">Attention</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Softmax</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Prev</mml:mi></mml:mrow></mml:mfenced><mml:mi>V</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula>where Prev indicates the attention scores from the previous self-attention layer.</p>
        <p>Instead of using a single attention in the traditional Transformer architecture, computing attention scores using a set of queries, keys, and values enables the Transformer model to jointly attend to information at different positions, which is called the multi-head attention mechanism. In our study, the multi-head attention mechanism is applied to model long-range dependencies and share information across the lncRNA sequence. Each attention head<italic toggle="yes">i</italic> (<italic toggle="yes">i </italic>=<italic toggle="yes"> </italic>1, 2, …, <italic toggle="yes">H</italic>) is computed as:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5" display="block" overflow="scroll"><mml:mi mathvariant="normal">hea</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Attention</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>Q</mml:mi><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Pre</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, and <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are learnable parameter matrixes, Prev<italic toggle="yes">i</italic> is the slice of Prev corresponding to head<italic toggle="yes">i</italic>.</p>
        <p>Here, each attention head is independent. All head<italic toggle="yes">i</italic> are concatenated and transformed with another linear projection to obtain the final multi-head output values.
<disp-formula id="E6"><label>(6)</label><mml:math id="M6" display="block" overflow="scroll"><mml:mi mathvariant="normal">Multi</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal">head</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Concat</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi mathvariant="normal">hea</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal">hea</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="normal">hea</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfenced><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is a learnable parameter matrix and <italic toggle="yes">H</italic> is the number of heads. Based on the multi-head attention mechanism, each head may attend to different parts of the input lncRNA sequence.</p>
        <p>In addition to relative positional coding and multi-head attention, the Transformer block has a standard architecture, which includes residual connection, “add &amp; norm” component, and position-wise feed-forward network. Specifically, the queries, keys, and values are derived from the outputs of the previous Transformer block, a residual connection is employed to avoid gradient vanishing or gradient exploding problems. The “add &amp; norm” component has two operations: addition and layer normalization. This addition operation from the residual connection is immediately followed by layer normalization. The position-wise feed-forward network transforms the representation at all the sequence positions using a fully connected layer.</p>
      </sec>
      <sec>
        <title>2.2.3 Localization-specific attention</title>
        <p>The standard attention mechanisms in Transformer blocks only tell us which nucleotides are considered very important for the overall prediction. However, lncRNA subcellular localization is a dynamic process, which is treated as a multi-label classification problem in our study. Therefore, it would be more informative to analyze which nucleotides are considered important for each subcellular localization compartment. With this motivation, we designed a localization-specific attention mechanism after the Transformer blocks.</p>
        <p>In the study, we have four subcellular localization compartments (nucleus, cytoplasm, chromatin, and insoluble cytoplasm). We used the multi-head attention mechanism to obtain the weight (importance) of every nucleotide in one subcellular localization compartment. Then, we repeated the process four times (here, four represents the number of subcellular localization compartments). As a result, we obtained four kinds of attention scores for four subcellular localization compartments. Specifically, we trained four attention matrices <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>ϵ</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for each subcellular localization compartment.
<disp-formula id="E7"><label>(7)</label><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">softmax</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mn>3,4</mml:mn><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mi>V</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the representation of the lncRNA obtained by the Transformer blocks and <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>∈</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:math></inline-formula> are the learnable weight matrix and bias term. After that, we used <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to aggregate <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mi>V</mml:mi></mml:math></inline-formula> under label <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mi>j</mml:mi></mml:math></inline-formula> and used a dense layer with sigmoid activation function to obtain the localization probability:
<disp-formula id="E8"><label>(8)</label><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">sigmoid</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        <p>Finally, the <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mn>3,4</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> is used as the final prediction, and the cross-entropy loss is computed by <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> to perform gradient descent.</p>
        <p>Overall, the benefits of the localization-specific attention mechanism can be summarized as follows:</p>
        <list list-type="order">
          <list-item>
            <p>The localization-specific attention mechanism is a fine-grained interpretability technique that can provide support for the interpretability of each subcellular localization.</p>
          </list-item>
          <list-item>
            <p>The localization-specific attention mechanism tends to be less heavily biased toward the most frequent compartments, which alleviates the imbalance data distribution problem.</p>
          </list-item>
          <list-item>
            <p>The localization-specific attention learns multiple attention scores and uses them for prediction, resulting in more accurate and robust results.</p>
          </list-item>
        </list>
        <p>Finally, in the classification part, a fully connected layer is applied to perform the multi-label classification task.</p>
      </sec>
    </sec>
    <sec>
      <title>2.3 Deep learning baseline models and existing predictors</title>
      <p>In this study, we focus on constructing powerful deep learning models to predict lncRNA subcellular localizations. To demonstrate the effectiveness of LncLncFormer, we compared it with several deep learning baseline models.</p>
      <list list-type="order">
        <list-item>
          <p><italic toggle="yes">k</italic>-mer + MLP, this model extracts <italic toggle="yes">k</italic>-mer frequency features, which are fed into a MLP layer to output subcellular localizations.</p>
        </list-item>
        <list-item>
          <p>Word2vec + MLP, this model encodes lncRNA sequences by using the word2vec technique, followed by feeding the sequence representation to a MLP layer for subcellular localization prediction.</p>
        </list-item>
        <list-item>
          <p>Word2vec + CNN + MLP, this model converts lncRNA sequences to embedding vectors learned by the word2vec technique, followed by a CNN layer, then uses a MLP layer to predict the subcellular localizations.</p>
        </list-item>
        <list-item>
          <p>Word2vec + Bi-LSTM + MLP, this model converts lncRNA sequences to embedding vectors learned by the word2vec technique, followed by a Bi-LSTM layer, then uses a MLP layer to predict the subcellular localizations.</p>
        </list-item>
        <list-item>
          <p>Glove + MLP, this model encodes lncRNA sequences by using the Glove technique, followed by feeding the sequence representation to a MLP layer for subcellular localization prediction.</p>
        </list-item>
        <list-item>
          <p>Glove + CNN + MLP, this model converts lncRNA sequences to embedding vectors learned by the Glove technique, followed by a CNN layer, then uses a MLP layer to predict the subcellular localizations.</p>
        </list-item>
        <list-item>
          <p>Glove + Bi-LSTM + MLP, this model converts lncRNA sequences to embedding vectors learned by the Glove technique, followed by a Bi-LSTM layer, then uses a MLP layer to predict the subcellular localizations.</p>
        </list-item>
      </list>
      <p>In the study, we used grid search to find the optimal parameters for these deep learning baseline models.</p>
      <p>To further evaluate the performance of LncLocFormer in predicting lncRNA subcellular localizations, we compared LncLocFormer with several existing state-of-the-art predictors by using a hold-out test set. We selected lncLocator (<xref rid="btad752-B5" ref-type="bibr">Cao <italic toggle="yes">et al.</italic> 2018</xref>), iLoc-lncRNA (<xref rid="btad752-B30" ref-type="bibr">Su <italic toggle="yes">et al.</italic> 2018</xref>), Locate-R (<xref rid="btad752-B1" ref-type="bibr">Ahmad <italic toggle="yes">et al.</italic> 2020</xref>), DeepLncLoc (<xref rid="btad752-B37" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2022</xref>), iLoc-LncRNA(2.0) (<xref rid="btad752-B42" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> 2022</xref>), and GraphLncLoc (<xref rid="btad752-B17" ref-type="bibr">Li <italic toggle="yes">et al.</italic> 2023</xref>) as the compared predictors. LncLocator and DeepLncLoc can predict five types of subcellular localizations, including nucleus, cytoplasm, cytosol, ribosome, and exosome. iLoc-lncRNA, Locate-R, iLoc-LncRNA(2.0), and GraphLncLoc can predict four types of subcellular localizations, including nucleus, cytoplasm, ribosome, and exosome. We did not compare LncLocFormer with lncLocator 2.0 since lncLocator 2.0 only provides the predicted CNRCI values instead of probabilities.</p>
    </sec>
    <sec>
      <title>2.4 Evaluation metrics</title>
      <p>To evaluate the performance of LncLocFormer with deep learning baseline models, we selected some evaluation metrics which are widely used in multi-label classification problem (<xref rid="btad752-B16" ref-type="bibr">Li <italic toggle="yes">et al.</italic> 2019</xref>). These evaluation metrics include average <italic toggle="yes">F</italic>-measure (Ave-<italic toggle="yes">F</italic>1), micro precision (MiP), micro recall (MiR), micro <italic toggle="yes">F</italic>-measure (MiF), and each area under receiver operating characteristic curve (AUC) for each subcellular localization. For convenience, <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>∈</mml:mo><mml:mn>0,1</mml:mn></mml:math></inline-formula> are the ground truth and predicted value of lncRNA <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mi>i</mml:mi></mml:math></inline-formula> for subcellular localization <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mi>j</mml:mi></mml:math></inline-formula>, respectively, and <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> if <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>, otherwise <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>.</p>
      <p>Ave-<italic toggle="yes">F</italic>1 is the harmonic mean of average precision and average recall, which is used in the CAFA challenge (<xref rid="btad752-B39" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> 2019</xref>), a protein function prediction challenge. We compute Ave-<italic toggle="yes">F</italic>1 using the following formulas:
<disp-formula id="E9"><label>(9)</label><mml:math id="M9" display="block" overflow="scroll"><mml:mi mathvariant="normal">Ave</mml:mi><mml:mo>-</mml:mo><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">AvgPre</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>A</mml:mi><mml:mi mathvariant="normal">vgRec</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi mathvariant="normal">AvgPre</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi mathvariant="normal">AvgRec</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula><disp-formula id="E10"><label>(10)</label><mml:math id="M10" display="block" overflow="scroll"><mml:mi mathvariant="normal">AvgPre</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">pr</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula><disp-formula id="E11"><label>(11)</label><mml:math id="M11" display="block" overflow="scroll"><mml:mi mathvariant="normal">AvgRec</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">re</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where
<disp-formula id="E12"><mml:math id="M12" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">pre</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">rec</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p>
      <p>MiF is the harmonic mean of MiP and MiR, which is used in the BioASQ challenge (<xref rid="btad752-B33" ref-type="bibr">You <italic toggle="yes">et al.</italic> 2021</xref>), a challenge on large-scale biomedical semantic indexing and question answering. It is defined as follows:
<disp-formula id="E13"><label>(12)</label><mml:math id="M13" display="block" overflow="scroll"><mml:mi mathvariant="normal">MiF</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">MiP</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">MiR</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">MiP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">MiR</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where
<disp-formula id="E14"><mml:math id="M14" display="block" overflow="scroll"><mml:mi mathvariant="normal">MiP</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal">MiR</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p>
      <p>Considering that the existing predictors are designed as multi-class predictors rather than multi-label predictors, to evaluate the performance of LncLocFormer with existing predictors, we evaluated the performance from two perspectives: the multi-label and the multi-class perspectives.</p>
      <p>In the multi-label perspective, we used Precision@k (P@k, which represents the number of correct predictions over <italic toggle="yes">k</italic>) to evaluate the performance (Zhang <italic toggle="yes">et al.</italic>). It is defined as follows:
<disp-formula id="E15"><label>(13)</label><mml:math id="M15" display="block" overflow="scroll"><mml:mi>P</mml:mi><mml:mo>@</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mi mathvariant="normal">ran</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">rank</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mfenced></mml:math></inline-formula> returns the <italic toggle="yes">k</italic> largest indices of <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> ranked in descending order. In the study, we only focus on the subcellular localization with the highest probability, thus, <italic toggle="yes">k</italic> is set to 1.</p>
      <p>In the multi-class perspective, consistent with the current state-of-the-art lncRNA subcellular localization predictors, we used Accuracy (ACC), Macro F-measure (MaF), Macro Precision (MaP), Macro Recall (MaR), and AUC as evaluation metrics.
<disp-formula id="E16"><label>(14)</label><mml:math id="M16" display="block" overflow="scroll"><mml:mi mathvariant="normal">ACC</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula><disp-formula id="E17"><label>(15)</label><mml:math id="M17" display="block" overflow="scroll"><mml:mi mathvariant="normal">Ma</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="E18"><label>(16)</label><mml:math id="M18" display="block" overflow="scroll"><mml:mi mathvariant="normal">Ma</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="E19"><label>(17)</label><mml:math id="M19" display="block" overflow="scroll"><mml:mi mathvariant="normal">MaF</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">Ma</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:mi mathvariant="normal">Ma</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ma</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Ma</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
    </sec>
    <sec>
      <title>2.5 Implementation details</title>
      <p>LncLocFormer is implemented using PyTorch (Paszke <italic toggle="yes">et al.</italic>). A grid search strategy was employed to find the optimal parameters of LncLocFormer, and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref> provides a summary of the optimal hyper-parameters and the corresponding search space. The used loss function is the cross-entropy function. The skip-gram model (Mikolov <italic toggle="yes">et al.</italic>) is used to pre-train the <italic toggle="yes">k</italic>-mer embedding vectors. We used the Adam optimizer with a learning rate of 0.0003. The learning rate is warm-uped over the first four epochs and decayed linearly for the remaining training steps. The batch size is set to 64. In the Transformer blocks, we used eight heads and hidden size of 128. To prevent the low-rank bottleneck, we enhanced the size of query/key/value into 64 using a dense layer (Bhojanapalli <italic toggle="yes">et al.</italic>). We keep at most 8196 nt for each lncRNA and divide them into 512 subsequences by using the subsequence embedding method. The dropout rate is set to 0.2 for the embedding layer and 0.1 for other layers. The maximum relative distance in the position embedding is set to 25.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Comparison with deep learning baseline models</title>
      <p>In this section, we investigated the effectiveness of LncLocFormer (subsequence embedding + Transformer blocks + localization-specific attention + MLP). We conducted 5-fold CV to evaluate the performance of LncLocFormer with other deep learning baseline models. In particular, we split the benchmark dataset into a training set (90%) and a hold-out test set (10%). Next, we performed 5-fold CV by further splitting the training set into 80% training and 20% validation. The process was repeated five times, and the final prediction results were the average of five validation results. The performances of LncLocFormer and other deep learning baseline models using 5-fold CV are shown in <xref rid="btad752-T1" ref-type="table">Table 1</xref>. We can observe that LncLocFormer outperforms other deep learning baseline models, except for the MiP. Specifically, LncLocFormer obtains Ave-<italic toggle="yes">F</italic>1 of 0.719, MiR of 0.721, MiF of 0.701, and average AUC of 0.648, while GloVe + Bi-LSTM + MLP obtains the best MiP (0.712). These observations indicate the superiority of LncLocFormer network architecture.</p>
      <table-wrap position="float" id="btad752-T1">
        <label>Table 1.</label>
        <caption>
          <p>Performance of LncLocFormer and other deep learning baseline models using 5-fold CV.<xref rid="tblfn1" ref-type="table-fn"><sup>a</sup></xref></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="2" colspan="1">Model</th>
              <th rowspan="2" colspan="1">Ave-<italic toggle="yes">F</italic>1</th>
              <th rowspan="2" colspan="1">MiP</th>
              <th rowspan="2" colspan="1">MiR</th>
              <th rowspan="2" colspan="1">MiF</th>
              <th colspan="5" rowspan="1">AUC<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Nucleus</th>
              <th rowspan="1" colspan="1">Cytoplasm</th>
              <th rowspan="1" colspan="1">Chromatin</th>
              <th rowspan="1" colspan="1">Insoluble cytoplasm</th>
              <th rowspan="1" colspan="1">Average</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">K-mer + MLP</td>
              <td rowspan="1" colspan="1">0.670</td>
              <td rowspan="1" colspan="1">0.651</td>
              <td rowspan="1" colspan="1">0.645</td>
              <td rowspan="1" colspan="1">0.648</td>
              <td rowspan="1" colspan="1">0.680</td>
              <td rowspan="1" colspan="1">0.636</td>
              <td rowspan="1" colspan="1">0.616</td>
              <td rowspan="1" colspan="1">0.582</td>
              <td rowspan="1" colspan="1">0.629</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Word2vec + MLP</td>
              <td rowspan="1" colspan="1">0.690</td>
              <td rowspan="1" colspan="1">0.664</td>
              <td rowspan="1" colspan="1">0.660</td>
              <td rowspan="1" colspan="1">0.662</td>
              <td rowspan="1" colspan="1">0.660</td>
              <td rowspan="1" colspan="1">0.580</td>
              <td rowspan="1" colspan="1">0.561</td>
              <td rowspan="1" colspan="1">0.526</td>
              <td rowspan="1" colspan="1">0.582</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Word2vec + CNN + MLP</td>
              <td rowspan="1" colspan="1">0.697</td>
              <td rowspan="1" colspan="1">0.692</td>
              <td rowspan="1" colspan="1">0.652</td>
              <td rowspan="1" colspan="1">0.672</td>
              <td rowspan="1" colspan="1">0.679</td>
              <td rowspan="1" colspan="1">0.591</td>
              <td rowspan="1" colspan="1">0.610</td>
              <td rowspan="1" colspan="1">0.582</td>
              <td rowspan="1" colspan="1">0.616</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Word2vec + Bi-LSTM + MLP</td>
              <td rowspan="1" colspan="1">0.689</td>
              <td rowspan="1" colspan="1">0.670</td>
              <td rowspan="1" colspan="1">0.653</td>
              <td rowspan="1" colspan="1">0.661</td>
              <td rowspan="1" colspan="1">0.682</td>
              <td rowspan="1" colspan="1">0.595</td>
              <td rowspan="1" colspan="1">0.597</td>
              <td rowspan="1" colspan="1">0.622</td>
              <td rowspan="1" colspan="1">0.624</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GloVe + MLP</td>
              <td rowspan="1" colspan="1">0.697</td>
              <td rowspan="1" colspan="1">0.685</td>
              <td rowspan="1" colspan="1">0.656</td>
              <td rowspan="1" colspan="1">0.670</td>
              <td rowspan="1" colspan="1">0.662</td>
              <td rowspan="1" colspan="1">0.560</td>
              <td rowspan="1" colspan="1">0.588</td>
              <td rowspan="1" colspan="1">0.574</td>
              <td rowspan="1" colspan="1">0.596</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GloVe + CNN + MLP</td>
              <td rowspan="1" colspan="1">0.697</td>
              <td rowspan="1" colspan="1">0.674</td>
              <td rowspan="1" colspan="1">0.677</td>
              <td rowspan="1" colspan="1">0.675</td>
              <td rowspan="1" colspan="1">0.668</td>
              <td rowspan="1" colspan="1">0.636</td>
              <td rowspan="1" colspan="1">0.580</td>
              <td rowspan="1" colspan="1">0.541</td>
              <td rowspan="1" colspan="1">0.606</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GloVe + Bi-LSTM + MLP</td>
              <td rowspan="1" colspan="1">0.679</td>
              <td rowspan="1" colspan="1">
                <bold>0.712</bold>
              </td>
              <td rowspan="1" colspan="1">0.602</td>
              <td rowspan="1" colspan="1">0.652</td>
              <td rowspan="1" colspan="1">0.676</td>
              <td rowspan="1" colspan="1">0.573</td>
              <td rowspan="1" colspan="1">0.570</td>
              <td rowspan="1" colspan="1">0.510</td>
              <td rowspan="1" colspan="1">0.582</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LncLocFormer</td>
              <td rowspan="1" colspan="1">
                <bold>0.719</bold>
              </td>
              <td rowspan="1" colspan="1">0.683</td>
              <td rowspan="1" colspan="1">
                <bold>0.721</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.701</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.686</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.651</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.623</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.632</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.648</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <label>a</label>
            <p>The best performance values are highlighted in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.2 Comparison with existing predictors</title>
      <p>In the previous section, we performed 5-fold CV to obtain the best parameters and evaluated the performance of LncLocFormer with other deep learning baseline models. To further evaluate the performance of LncLocFormer in predicting lncRNA subcellular localizations, we compared LncLocFormer with several existing state-of-the-art predictors by using a hold-out test set. In particular, we selected the current predictors follow these criteria: (i) the availability of web server or stand-alone version; (ii) input that only needs lncRNA sequences; and (iii) outputs that include predictive probabilities for subcellular localization. Finally, we used the following web servers for comparison: lncLocator (<ext-link xlink:href="http://www.csbio.sjtu.edu.cn/bioinf/lncLocator/" ext-link-type="uri">http://www.csbio.sjtu.edu.cn/bioinf/lncLocator/</ext-link>), iLoc-lncRNA (<ext-link xlink:href="http://lin-group.cn/server/iLoc-LncRNA/" ext-link-type="uri">http://lin-group.cn/server/iLoc-LncRNA/</ext-link>), Locate-R (<ext-link xlink:href="http://locate-r.azurewebsites.net" ext-link-type="uri">http://locate-r.azurewebsites.net</ext-link>), DeepLncLoc (<ext-link xlink:href="http://bioinformatics.csu.edu.cn/DeepLncLoc/" ext-link-type="uri">http://bioinformatics.csu.edu.cn/DeepLncLoc/</ext-link>), iLoc-LncRNA(2.0) (<ext-link xlink:href="http://lin-group.cn/server/iLoc-LncRNA" ext-link-type="uri">http://lin-group.cn/server/iLoc-LncRNA</ext-link>(2.0)/), and GraphLncLoc (<ext-link xlink:href="http://csuligroup.com:8000/GraphLncLoc/" ext-link-type="uri">http://csuligroup.com:8000/GraphLncLoc/</ext-link>). The detailed prediction results on the hold-out test set are shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>. The P@1 of LncLocFormer and existing predictors on the hold-out test set is shown in <xref rid="btad752-T2" ref-type="table">Table 2</xref>.</p>
      <table-wrap position="float" id="btad752-T2">
        <label>Table 2.</label>
        <caption>
          <p>P@1 of LncLocFormer and existing predictors on the hold-out test set (RNALocate v2.0).<xref rid="tblfn2" ref-type="table-fn"><sup>a</sup></xref></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Predictor</th>
              <th rowspan="1" colspan="1">P@1</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">lncLocator</td>
              <td rowspan="1" colspan="1">0.232</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">iLoc-LncRNA</td>
              <td rowspan="1" colspan="1">0.348</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Locate-R</td>
              <td rowspan="1" colspan="1">0.275</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepLncLoc</td>
              <td rowspan="1" colspan="1">0.304</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">iLoc-LncRNA(2.0)</td>
              <td rowspan="1" colspan="1">0.333</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GraphLncLoc</td>
              <td rowspan="1" colspan="1">0.536</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LncLocFormer</td>
              <td rowspan="1" colspan="1">
                <bold>0.899</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <label>a</label>
            <p>The best performance values are highlighted in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>From <xref rid="btad752-T2" ref-type="table">Table 2</xref>, we can observe that LncLocFormer significantly outperforms existing predictors. Specifically, LncLocFormer obtains P@1 of 0.899, which is much higher than that of lncLocator (0.232), iLoc-LncRNA (0.348), Locate-R (0.275), DeepLncLoc (0.304), iLoc-LncRNA(2.0) (0.333), and GraphLncLoc (0.536). These results demonstrate that LncLocFormer has a powerful ability in predicting multi-label lncRNA subcellular localizations and achieves state-of-the-art performance on the hold-out test set. However, a natural question arises: why is there such a significant gap between LncLocFormer and the other predictors? We believe that it is impossible to achieve this by relying only on model architecture. One of the most possible explanations is that the used datasets are different. lncLocator, iLoc-LncRNA, Locate-R, DeepLncLoc, iLoc-LncRNA(2.0), and GraphLncLoc all used the RNALocate v1.0 database (<xref rid="btad752-B40" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> 2017</xref>) to train and test their models, while LncLocFormer is trained and tested by using the RNALocate v2.0 database (<xref rid="btad752-B8" ref-type="bibr">Cui <italic toggle="yes">et al.</italic> 2022</xref>). Therefore, we believe that the difference between the two datasets is the main reason for the large gap between LncLocFormer and other predictors.</p>
      <p>To investigate the difference between the two datasets, we plotted the distributions of the RNALocate v1.0 dataset used in the six predictors and the RNALocate v2.0 dataset used in LncLocFormer, as shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S2</xref>. By comparing <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S2a</xref> with b, we can observe that there are significant differences between the RNALocate v2.0 and RNALocate v1.0 datasets. For example, in the RNALocate v2.0 dataset, the number of lncRNAs located in the nucleus is much greater than the number of lncRNAs located in the cytoplasm. In contrast, in the RNALocate v1.0 dataset, the number of lncRNAs located in the cytoplasm is slightly larger than the number of lncRNAs located in the nucleus. These findings demonstrate that the data from the two datasets are not independently identical distributed (i.i.d.).</p>
      <p>To make a fairer comparison and to prove the superiority of LncLocFormer architecture, we used the RNALocate v1.0 database to retrain our model. Specifically, we employed the same training set and test set utilized in our previous predictor, DeepLncLoc, to retrain and test LncLocFormer. Since the dataset is generated for the multi-class prediction problem, we used a softmax activation function to replace the sigmoid activation function in the final fully connected layer to perform the multi-class prediction task. As with previous studies, we used ACC, MaF, MaP, and MaR as evaluation metrics to evaluate LncLocFormer and the existing predictors. The detailed prediction results on the RNALocate v1.0 test set are shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S3</xref>. The performance of LncLocFormer (using the RNALocate v1.0 dataset for training and test) and the existing predictors is shown in <xref rid="btad752-T3" ref-type="table">Table 3</xref>. In <xref rid="btad752-T3" ref-type="table">Table 3</xref>, the evaluation metrics we pay most attention to are MaF and ACC. From <xref rid="btad752-T3" ref-type="table">Table 3</xref>, we can observe that LncLocFormer still outperforms existing predictors in terms of MaF and ACC.</p>
      <table-wrap position="float" id="btad752-T3">
        <label>Table 3.</label>
        <caption>
          <p>Performance comparison of LncLocFormer (using the RNALocate v1.0 dataset for training and test) with the existing predictors.<xref rid="tblfn3" ref-type="table-fn"><sup>a</sup></xref></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Predictor</th>
              <th rowspan="1" colspan="1">MaP</th>
              <th rowspan="1" colspan="1">MaR</th>
              <th rowspan="1" colspan="1">MaF</th>
              <th rowspan="1" colspan="1">ACC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">lncLocator</td>
              <td rowspan="1" colspan="1">0.288</td>
              <td rowspan="1" colspan="1">0.292</td>
              <td rowspan="1" colspan="1">0.276</td>
              <td rowspan="1" colspan="1">0.433</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">iLoc-lncRNA</td>
              <td rowspan="1" colspan="1">0.488</td>
              <td rowspan="1" colspan="1">0.445</td>
              <td rowspan="1" colspan="1">0.458</td>
              <td rowspan="1" colspan="1">0.507</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Locate-R</td>
              <td rowspan="1" colspan="1">0.374</td>
              <td rowspan="1" colspan="1">0.317</td>
              <td rowspan="1" colspan="1">0.329</td>
              <td rowspan="1" colspan="1">0.403</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepLncLoc</td>
              <td rowspan="1" colspan="1">0.680</td>
              <td rowspan="1" colspan="1">0.543</td>
              <td rowspan="1" colspan="1">0.563</td>
              <td rowspan="1" colspan="1">0.537</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">iLoc-LncRNA(2.0)</td>
              <td rowspan="1" colspan="1">0.460</td>
              <td rowspan="1" colspan="1">0.384</td>
              <td rowspan="1" colspan="1">0.390</td>
              <td rowspan="1" colspan="1">0.433</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GraphLncLoc</td>
              <td rowspan="1" colspan="1">
                <bold>0.731</bold>
              </td>
              <td rowspan="1" colspan="1">0.549</td>
              <td rowspan="1" colspan="1">0.560</td>
              <td rowspan="1" colspan="1">0.522</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LncLocFormer</td>
              <td rowspan="1" colspan="1">0.696</td>
              <td rowspan="1" colspan="1">
                <bold>0.566</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.597</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.612</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn3">
            <label>a</label>
            <p>The best performance values are highlighted in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.3 Motif analysis</title>
      <p>In the study, we designed a localization-specific attention mechanism to obtain distinct attention weights for each subcellular localization and find the most likely motifs in lncRNA sequences. To investigate the performance of localization-specific attention mechanism in LncLocFormer, we conducted some motif analyses.</p>
      <p>First, we tested whether LncLocFormer could find the most frequently recurring motifs. In particular, we used the MEME suite (<xref rid="btad752-B2" ref-type="bibr">Bailey <italic toggle="yes">et al.</italic> 2009</xref>) to find the motifs in our dataset. The motifs are analyzed with the width of nine, and the <italic toggle="yes">E</italic>-value is set to 0.05. We used a threshold to determine the importance of attention weights. The threshold is set to the multiplicative inverse of the input sequence length. Because if the attention weights on the lncRNA sequence are randomly distributed, the mathematic expectation of all attention weights on the lncRNA sequence is the multiplicative inverse of the input sequence length. Only if the attention weight of a nucleotide is larger than the mathematic expectation, we believe that LncLocFormer pays attention to the nucleotide. If the attention weight of a nucleotide is smaller than the mathematic expectation, we believe that LncLocFormer does not focus on the nucleotide. <xref rid="btad752-F3" ref-type="fig">Figure 3</xref> displays the representative examples, with the left column depicting the motifs found by the MEME suite, the middle column showing the motifs discovered by LncLocFormer, and the right column displaying the <italic toggle="yes">E</italic>-values of the motifs found by the MEME suite. From <xref rid="btad752-F3" ref-type="fig">Fig. 3</xref>, we can observe that LncLocFormer can capture the motifs that are similar to those found by the MEME suite, which means LncLocFormer can capture the most frequently recurring motifs.</p>
      <fig position="float" id="btad752-F3">
        <label>Figure 3.</label>
        <caption>
          <p>Motifs discovered by MEME suite (left) and by LncLocFormer (middle). The right are the <italic toggle="yes">E</italic>-values of the motifs found by the MEME suite.</p>
        </caption>
        <graphic xlink:href="btad752f3" position="float"/>
      </fig>
      <p>Second, we investigated whether LncLocFormer could capture some known motifs. Specifically, we searched for some known motifs in recent literature that are related to subcellular localization. Lubelsky <italic toggle="yes">et al.</italic> (<xref rid="btad752-B22" ref-type="bibr">Lubelsky and Ulitsky 2018</xref>) found that the repeated motif RCCTCCC (where R denotes A/G) drives lncRNAs to be located in the nucleus. <xref rid="btad752-B38" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> (2014)</xref> identified the motif AGCCC act as a general nucleus localization signal. We used motifs RCCTCCC and AGCCC as examples to show the performance of LncLocFormer. The captured motifs by LncLocFormer for nucleus are shown in <xref rid="btad752-F4" ref-type="fig">Fig. 4</xref>. From <xref rid="btad752-F4" ref-type="fig">Fig. 4</xref>, we can observe that LncLocFormer can capture motifs that are similar to those that are already known.</p>
      <fig position="float" id="btad752-F4">
        <label>Figure 4.</label>
        <caption>
          <p>LncLocFormer captures two known motifs, which are related to nucleus localization.</p>
        </caption>
        <graphic xlink:href="btad752f4" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.4 Case study</title>
      <p>To better understand the role of the localization-specific attention, we visualized the attention matrices <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">and</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:math></inline-formula> which are computed by <xref rid="E7" ref-type="disp-formula">Equation (7)</xref> and gave a case study in <xref rid="btad752-F5" ref-type="fig">Fig. 5</xref> using lncRNA Cerox1 (cytoplasmic endogenous regulator of oxidative phosphorylation 1, NCBI ID: 115804232) as an example. The true label of lncRNA Cerox1 is nucleus. According to <xref rid="btad752-B38" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> (2014)</xref>, the motif AGCCC act as a general nucleus localization signal. We obtained four kinds of attention weights of different subcellular localizations, and highlighted the sequence using different degrees of red based on the values of attention weights. From <xref rid="btad752-F5" ref-type="fig">Fig. 5</xref>, we can observe that the attention matrix of nucleus captures an important region containing the core motif (AGCCC), while the attention matrices of other subcellular localizations fail to capture the motif AGCCC. Although LncLocFormer cannot find the exact known motifs, it can capture motifs that are very similar to the known motifs. The results suggest the potential of LncLocFormer in motif discovery.</p>
      <fig position="float" id="btad752-F5">
        <label>Figure 5.</label>
        <caption>
          <p>Attention weight visualization of lncRNA Cerox1.</p>
        </caption>
        <graphic xlink:href="btad752f5" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.5 Ablation study</title>
      <p>In order to discover the essential components of LncLocFormer, we conducted an ablation study by removing individual parts of LncLocFormer. In particular, we tested the model without localization-specific attention and the model without positional encoding. The former model lost multiple attention weights for different subcellular localizations, while the latter model lost the sequence order information. The results are shown in <xref rid="btad752-T4" ref-type="table">Table 4</xref>. In <xref rid="btad752-T4" ref-type="table">Table 4</xref>, the evaluation metrics we pay most attention to are Ave-<italic toggle="yes">F</italic>1, MiF, and average AUC. From <xref rid="btad752-T4" ref-type="table">Table 4</xref>, we can observe that localization-specific attention is the most important part of LncLocFormer. Without localization-specific attention, Ave-<italic toggle="yes">F</italic>1, MiF, and average AUC decrease from 0.719, 0.701, and 0.648 to 0.627, 0.591, and 0.614, respectively. Additionally, positional encoding is also useful in LncLocFormer. Without positional encoding, Ave-<italic toggle="yes">F</italic>1, MiF, and average AUC decrease from 0.719, 0.701, and 0.648 to 0.710, 0.691, and 0.643, respectively. The results confirm the effectiveness of localization-specific attention and positional encoding in LncLocFormer.</p>
      <table-wrap position="float" id="btad752-T4">
        <label>Table 4.</label>
        <caption>
          <p>The performances of various models in the ablation study.<xref rid="tblfn4" ref-type="table-fn"><sup>a</sup></xref></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="2" colspan="1">Model</th>
              <th rowspan="2" colspan="1">Ave-<italic toggle="yes">F</italic>1</th>
              <th rowspan="2" colspan="1">MiP</th>
              <th rowspan="2" colspan="1">MiR</th>
              <th rowspan="2" colspan="1">MiF</th>
              <th colspan="5" rowspan="1">AUC<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Nucleus</th>
              <th rowspan="1" colspan="1">Cytoplasm</th>
              <th rowspan="1" colspan="1">Chromatin</th>
              <th rowspan="1" colspan="1">Insoluble cytoplasm</th>
              <th rowspan="1" colspan="1">Average</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Without localization-specific attention</td>
              <td rowspan="1" colspan="1">0.627</td>
              <td rowspan="1" colspan="1">0.550</td>
              <td rowspan="1" colspan="1">0.640</td>
              <td rowspan="1" colspan="1">0.591</td>
              <td rowspan="1" colspan="1">
                <bold>0.688</bold>
              </td>
              <td rowspan="1" colspan="1">0.638</td>
              <td rowspan="1" colspan="1">0.599</td>
              <td rowspan="1" colspan="1">0.532</td>
              <td rowspan="1" colspan="1">0.614</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Without positional encoding</td>
              <td rowspan="1" colspan="1">0.710</td>
              <td rowspan="1" colspan="1">0.653</td>
              <td rowspan="1" colspan="1">
                <bold>0.734</bold>
              </td>
              <td rowspan="1" colspan="1">0.691</td>
              <td rowspan="1" colspan="1">0.672</td>
              <td rowspan="1" colspan="1">
                <bold>0.660</bold>
              </td>
              <td rowspan="1" colspan="1">0.611</td>
              <td rowspan="1" colspan="1">0.628</td>
              <td rowspan="1" colspan="1">0.643</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LncLocFormer</td>
              <td rowspan="1" colspan="1">
                <bold>0.719</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.683</bold>
              </td>
              <td rowspan="1" colspan="1">0.721</td>
              <td rowspan="1" colspan="1">
                <bold>0.701</bold>
              </td>
              <td rowspan="1" colspan="1">0.686</td>
              <td rowspan="1" colspan="1">0.651</td>
              <td rowspan="1" colspan="1">
                <bold>0.623</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.632</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.648</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn4">
            <label>a</label>
            <p>The best performance values are highlighted in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>In addition, we observed an interesting phenomenon, i.e. models without localization-specific attention can produce better results for the nucleus localization, while models without positional encoding can produce better results for the cytoplasm localization. Regarding the better results for nucleus localization with models lacking localization-specific attention, one possible explanation is that the benchmark dataset is imbalanced. The nucleus localization has a larger number of samples compared to the other three classes. When localization-specific attention is removed, the model may exhibit a bias toward the classes with more samples because this can lead to higher overall accuracy. This bias could result in improved predictions for the nucleus localization while leading to poorer predictions for the other three subcellular localizations. As for the better results for cytoplasm localization with models lacking positional encoding, the possible reason is that the lncRNA sequences belonging to the cytoplasm localization in the benchmark dataset are often quite long. In very long sequences, the relative position encoding may not effectively capture the relative distance relationship between nucleotides and may forget what has been learned in the sequence. Instead, the addition of relative position encoding may introduce noise or unnecessary information, resulting in a decline in prediction performance for the cytoplasm localization.</p>
    </sec>
    <sec>
      <title>3.6 Web server</title>
      <p>To facilitate the use of LncLocFormer, we developed a user-friendly web server, <ext-link xlink:href="http://csuligroup.com:9000/LncLocFormer" ext-link-type="uri">http://csuligroup.com:9000/LncLocFormer</ext-link>. LncLocFormer requires lncRNA sequences with more than 200 and &lt;10 000 nucleotides as input. Users can paste the lncRNA sequence into the input box and click on the submit button to see the predicted results. For each lncRNA sequence, the predicted probabilities and attention weights for each subcellular localization are displayed on the screen. In general, LncLocFormer takes &lt;10 s to predict the subcellular localization of a given lncRNA sequence. We believe that LncLocFormer is a convenient and efficient tool in the field of lncRNA subcellular localization prediction.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p>In the study, we proposed LncLocFormer, a multi-label lncRNA subcellular localization predictor that utilizes Transformer and localization-specific attention mechanism. Unlike many previous computational methods that only consider a single subcellular localization for a lncRNA sequence, LncLocFormer can predict multiple subcellular localizations simultaneously for each lncRNA sequence. Due to the uncertainty of the number of labels for each lncRNA sequence and the implicit relationship between the labels, the multi-label classification problem is more complicated than conventional multi-class classification tasks. By using Transformer blocks and localization-specific attention mechanism, LncLocFormer can predict lncRNA multiple subcellular localizations accurately, learn different motifs for each subcellular localization, and capture some motifs that are very similar to known motifs. Our extensive experimental results demonstrate that LncLocFormer outperforms existing state-of-the-art predictors. We believe that LncLocFormer can serve as a useful tool for predicting lncRNA multiple subcellular localizations.</p>
    <p>Although LncLocFormer shows promising results, there are some limitations that may influence the performance of LncLocFormer. The performance of LncLocFormer is limited by the number of samples in the RNALocate dataset. In the study, we only have 811 samples for the multi-label classification task. With lncRNA subcellular localization becoming a more important research topic, we could obtain more reliable data that can be used for training and test. Alternatively, we could consider transferring some data from other domains to aid in the research topic.</p>
    <p>Furthermore, LncLocFormer utilizes eight Transformer blocks and localization-specific attention, resulting in a lot of parameters that need to be tuned. Consequently, the training time of LncLocFormer is very long. With the development of deep learning techniques, more and more advanced knowledge distillation and network pruning techniques will be proposed. As a result, using a lightweight network architecture to predict lncRNA subcellular localization is a promising future direction.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btad752_Supplementary_Data</label>
      <media xlink:href="btad752_supplementary_data.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <sec>
    <title>Supplementary data</title>
    <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Conflict of interest</title>
    <p>None declared.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the National Key Research and Development Program of China [No. 2022YFC3400300]; the National Natural Science Foundation of China [No. 62102457]; Hunan Provincial Natural Science Foundation of China [No. 2023JJ40763]; Hunan Provincial Science and Technology Program [No. 2021RC4008]; and the Fundamental Research Funds for the Central Universities of Central South University [No. 2023ZZTS0627]. This work was carried out in part using computing resources at the High Performance Computing Center of Central South University.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btad752-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ahmad</surname><given-names>A</given-names></string-name>, <string-name><surname>Lin</surname><given-names>H</given-names></string-name>, <string-name><surname>Shatabda</surname><given-names>S.</given-names></string-name></person-group><article-title>Locate-R: subcellular localization of long non-coding RNAs using nucleotide compositions</article-title>. <source>Genomics</source><year>2020</year>;<volume>112</volume>:<fpage>2583</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">32068122</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bailey</surname><given-names>TL</given-names></string-name>, <string-name><surname>Boden</surname><given-names>M</given-names></string-name>, <string-name><surname>Buske</surname><given-names>FA</given-names></string-name></person-group><etal>et al</etal><article-title>MEME SUITE: tools for motif discovery and searching</article-title>. <source>Nucleic Acids Res</source><year>2009</year>;<volume>37</volume>:<fpage>W202</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">19458158</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B3">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bhojanapalli</surname><given-names>S</given-names></string-name>, <string-name><surname>Yun</surname><given-names>C</given-names></string-name>, <string-name><surname>Rawat</surname><given-names>AS</given-names></string-name></person-group><etal>et al</etal> Low-rank bottleneck in multi-head attention models. In: <italic toggle="yes">International conference on machine learning, online event, </italic>Vol. 119<italic toggle="yes">.</italic> PMLR. <fpage>864</fpage>–<lpage>73</lpage>.</mixed-citation>
    </ref>
    <ref id="btad752-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Birney</surname><given-names>E</given-names></string-name>, <string-name><surname>Stamatoyannopoulos</surname><given-names>JA</given-names></string-name>, <string-name><surname>Dutta</surname><given-names>A</given-names></string-name></person-group><etal>et al</etal>; <collab>Children's Hospital Oakland Research Institute</collab>. <article-title>Identification and analysis of functional elements in 1% of the human genome by the ENCODE pilot project</article-title>. <source>Nature</source><year>2007</year>;<volume>447</volume>:<fpage>799</fpage>–<lpage>816</lpage>.<pub-id pub-id-type="pmid">17571346</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bridges</surname><given-names>MC</given-names></string-name>, <string-name><surname>Daulagala</surname><given-names>AC</given-names></string-name>, <string-name><surname>Kourtidis</surname><given-names>A.</given-names></string-name></person-group><article-title>LNCcation: lncRNA localization and function</article-title>. <source>J Cell Biol</source><year>2021</year>;<volume>220</volume>:<fpage>e202009045</fpage>.<pub-id pub-id-type="pmid">33464299</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cao</surname><given-names>Z</given-names></string-name>, <string-name><surname>Pan</surname><given-names>X</given-names></string-name>, <string-name><surname>Yang</surname><given-names>Y</given-names></string-name></person-group><etal>et al</etal><article-title>The lncLocator: a subcellular localization predictor for long non-coding RNAs based on a stacked ensemble classifier</article-title>. <source>Bioinformatics</source><year>2018</year>;<volume>34</volume>:<fpage>2185</fpage>–<lpage>94</lpage>.<pub-id pub-id-type="pmid">29462250</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carlevaro-Fita</surname><given-names>J</given-names></string-name>, <string-name><surname>Johnson</surname><given-names>R.</given-names></string-name></person-group><article-title>Global positioning system: understanding long noncoding RNAs through subcellular localization</article-title>. <source>Mol Cell</source><year>2019</year>;<volume>73</volume>:<fpage>869</fpage>–<lpage>83</lpage>.<pub-id pub-id-type="pmid">30849394</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cui</surname><given-names>T</given-names></string-name>, <string-name><surname>Dou</surname><given-names>Y</given-names></string-name>, <string-name><surname>Tan</surname><given-names>P</given-names></string-name></person-group><etal>et al</etal><article-title>RNALocate v2.0: an updated resource for RNA subcellular localization with increased coverage and annotation</article-title>. <source>Nucleic Acids Res</source><year>2022</year>;<volume>50</volume>:<fpage>D333</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">34551440</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>DiStefano</surname><given-names>JK.</given-names></string-name></person-group><article-title>The emerging role of long noncoding RNAs in human disease</article-title>. <source>Methods Mol Biol</source><year>2018</year>;<volume>1706</volume>:<fpage>91</fpage>–<lpage>110</lpage>.<pub-id pub-id-type="pmid">29423795</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Esteller</surname><given-names>M.</given-names></string-name></person-group><article-title>Non-coding RNAs in human disease</article-title>. <source>Nat Rev Genet</source><year>2011</year>;<volume>12</volume>:<fpage>861</fpage>–<lpage>74</lpage>.<pub-id pub-id-type="pmid">22094949</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fan</surname><given-names>YX</given-names></string-name>, <string-name><surname>Chen</surname><given-names>MJ</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>QQ.</given-names></string-name></person-group><article-title>lncLocPred: predicting LncRNA subcellular localization using multiple sequence feature information</article-title>. <source>IEEE Access</source><year>2020</year>;<volume>8</volume>:<fpage>124702</fpage>–<lpage>11</lpage>.</mixed-citation>
    </ref>
    <ref id="btad752-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feng</surname><given-names>S</given-names></string-name>, <string-name><surname>Liang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Du</surname><given-names>W</given-names></string-name></person-group><etal>et al</etal><article-title>LncLocation: efficient subcellular location prediction of long non-coding RNA-based multi-source heterogeneous feature fusion</article-title>. <source>Int J Mol Sci</source><year>2020</year>;<volume>21</volume>:<fpage>7221</fpage>.<pub-id pub-id-type="pmid">33007849</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gudenas</surname><given-names>BL</given-names></string-name>, <string-name><surname>Wang</surname><given-names>L.</given-names></string-name></person-group><article-title>Prediction of LncRNA subcellular localization with deep learning from sequence features</article-title>. <source>Sci Rep</source><year>2018</year>;<volume>8</volume>:<fpage>16385</fpage>.<pub-id pub-id-type="pmid">30401954</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Niu</surname><given-names>B</given-names></string-name>, <string-name><surname>Gao</surname><given-names>Y</given-names></string-name></person-group><etal>et al</etal><article-title>CD-HIT suite: a web server for clustering and comparing biological sequences</article-title>. <source>Bioinformatics</source><year>2010</year>;<volume>26</volume>:<fpage>680</fpage>–<lpage>2</lpage>.<pub-id pub-id-type="pmid">20053844</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jeon</surname><given-names>YJ</given-names></string-name>, <string-name><surname>Hasan</surname><given-names>MM</given-names></string-name>, <string-name><surname>Park</surname><given-names>HW</given-names></string-name></person-group><etal>et al</etal><article-title>TACOS: a novel approach for accurate prediction of cell-specific long noncoding RNAs subcellular localization</article-title>. <source>Brief Bioinform</source><year>2022</year>;<volume>23</volume>:<fpage>bbac243</fpage>.<pub-id pub-id-type="pmid">35753698</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>M</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>B</given-names></string-name>, <string-name><surname>Yin</surname><given-names>R</given-names></string-name></person-group><etal>et al</etal><article-title>GraphLncLoc: long non-coding RNA subcellular localization prediction using graph convolutional networks based on sequence to graph transformation</article-title>. <source>Brief Bioinform</source><year>2023</year>;<volume>24</volume>:<fpage>bbac565</fpage>.<pub-id pub-id-type="pmid">36545797</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>M</given-names></string-name>, <string-name><surname>Fei</surname><given-names>Z</given-names></string-name>, <string-name><surname>Zeng</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Automated ICD-9 coding via a deep learning approach</article-title>. <source>IEEE/ACM Trans Comput Biol Bioinform</source><year>2019</year>;<volume>16</volume>:<fpage>1193</fpage>–<lpage>202</lpage>.<pub-id pub-id-type="pmid">29994157</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>Y</given-names></string-name>, <string-name><surname>Zeng</surname><given-names>M</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>DeepCellEss: cell line-specific essential protein prediction with attention-based interpretable deep learning</article-title>. <source>Bioinformatics</source><year>2023</year>;<volume>39</volume>:<fpage>btac779</fpage>.<pub-id pub-id-type="pmid">36458923</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>Y</given-names></string-name>, <string-name><surname>Pan</surname><given-names>X</given-names></string-name>, <string-name><surname>Shen</surname><given-names>HB.</given-names></string-name></person-group><article-title>lncLocator 2.0: a cell-line-specific subcellular localization predictor for long non-coding RNAs with interpretable deep learning</article-title>. <source>Bioinformatics</source><year>2021</year>;<volume>37</volume>:<fpage>2308</fpage>–<lpage>16</lpage>.<pub-id pub-id-type="pmid">33630066</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lu</surname><given-names>C</given-names></string-name>, <string-name><surname>Yang</surname><given-names>M</given-names></string-name>, <string-name><surname>Li</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Predicting human lncRNA-disease associations based on geometric matrix completion</article-title>. <source>IEEE J Biomed Health Inform</source><year>2020</year>;<volume>24</volume>:<fpage>2420</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">31825885</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lu</surname><given-names>C</given-names></string-name>, <string-name><surname>Yang</surname><given-names>M</given-names></string-name>, <string-name><surname>Luo</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>Prediction of lncRNA-disease associations based on inductive matrix completion</article-title>. <source>Bioinformatics</source><year>2018</year>;<volume>34</volume>:<fpage>3357</fpage>–<lpage>64</lpage>.<pub-id pub-id-type="pmid">29718113</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lubelsky</surname><given-names>Y</given-names></string-name>, <string-name><surname>Ulitsky</surname><given-names>I.</given-names></string-name></person-group><article-title>Sequences enriched in Alu repeats drive nuclear localization of long RNAs in human cells</article-title>. <source>Nature</source><year>2018</year>;<volume>555</volume>:<fpage>107</fpage>–<lpage>11</lpage>.<pub-id pub-id-type="pmid">29466324</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B23">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Mikolov</surname><given-names>T</given-names></string-name>, <string-name><surname>Chen</surname><given-names>K</given-names></string-name>, <string-name><surname>Corrado</surname><given-names>G</given-names></string-name></person-group><etal>et al</etal> Efficient estimation of word representations in vector space. arXiv, arXiv:1301.3781. <year>2013</year>.</mixed-citation>
    </ref>
    <ref id="btad752-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moffitt</surname><given-names>JR</given-names></string-name>, <string-name><surname>Zhuang</surname><given-names>X.</given-names></string-name></person-group><article-title>RNA imaging with multiplexed Error-Robust fluorescence in situ hybridization (MERFISH)</article-title>. <source>Methods Enzymol</source><year>2016</year>;<volume>572</volume>:<fpage>1</fpage>–<lpage>49</lpage>.<pub-id pub-id-type="pmid">27241748</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Paszke</surname><given-names>A</given-names></string-name>, <string-name><surname>Gross</surname><given-names>S</given-names></string-name>, <string-name><surname>Massa</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>PyTorch: an imperative style, high-performance deep learning library</article-title>. <source>Adv Neural Inf Process Syst</source> 2019;<volume>32</volume>.</mixed-citation>
    </ref>
    <ref id="btad752-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pruitt</surname><given-names>KD</given-names></string-name>, <string-name><surname>Tatusova</surname><given-names>T</given-names></string-name>, <string-name><surname>Maglott</surname><given-names>DR.</given-names></string-name></person-group><article-title>NCBI reference sequences (RefSeq): a curated non-redundant sequence database of genomes, transcripts and proteins</article-title>. <source>Nucleic Acids Res</source><year>2007</year>;<volume>35</volume>:<fpage>D61</fpage>–<lpage>5</lpage>.<pub-id pub-id-type="pmid">17130148</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Riva</surname><given-names>P</given-names></string-name>, <string-name><surname>Ratti</surname><given-names>A</given-names></string-name>, <string-name><surname>Venturin</surname><given-names>M.</given-names></string-name></person-group><article-title>The long non-coding RNAs in neurodegenerative diseases: novel mechanisms of pathogenesis</article-title>. <source>CAR</source><year>2016</year>;<volume>13</volume>:<fpage>1219</fpage>–<lpage>31</lpage>.</mixed-citation>
    </ref>
    <ref id="btad752-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Savulescu</surname><given-names>AF</given-names></string-name>, <string-name><surname>Bouilhol</surname><given-names>E</given-names></string-name>, <string-name><surname>Beaume</surname><given-names>N</given-names></string-name></person-group><etal>et al</etal><article-title>Prediction of RNA subcellular localization: learning from heterogeneous data sources</article-title>. <source>iScience</source><year>2021</year>;<volume>24</volume>:<fpage>103298</fpage>.<pub-id pub-id-type="pmid">34765919</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shukla</surname><given-names>CJ</given-names></string-name>, <string-name><surname>McCorkindale</surname><given-names>AL</given-names></string-name>, <string-name><surname>Gerhardinger</surname><given-names>C</given-names></string-name></person-group><etal>et al</etal><article-title>High-throughput identification of RNA nuclear enrichment sequences</article-title>. <source>EMBO J</source><year>2018</year>;<volume>37</volume>:<fpage>e98452</fpage>.<pub-id pub-id-type="pmid">29335281</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Su</surname><given-names>Z-D</given-names></string-name>, <string-name><surname>Huang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Z-Y</given-names></string-name></person-group><etal>et al</etal><article-title>iLoc-lncRNA: predict the subcellular location of lncRNAs by incorporating octamer composition into general PseKNC</article-title>. <source>Bioinformatics</source><year>2018</year>;<volume>34</volume>:<fpage>4196</fpage>–<lpage>204</lpage>.<pub-id pub-id-type="pmid">29931187</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>KC</given-names></string-name>, <string-name><surname>Chang</surname><given-names>HY.</given-names></string-name></person-group><article-title>Molecular mechanisms of long noncoding RNAs</article-title>. <source>Mol Cell</source><year>2011</year>;<volume>43</volume>:<fpage>904</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">21925379</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Gao</surname><given-names>M</given-names></string-name>, <string-name><surname>Zeng</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>BridgeDPI: a novel graph neural network for predicting drug-protein interactions</article-title>. <source>Bioinformatics</source><year>2022</year>;<volume>38</volume>:<fpage>2571</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">35274672</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>You</surname><given-names>R</given-names></string-name>, <string-name><surname>Liu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Mamitsuka</surname><given-names>H</given-names></string-name></person-group><etal>et al</etal><article-title>BERTMeSH: deep contextual representation learning for large-scale high-performance MeSH indexing with full text</article-title>. <source>Bioinformatics</source><year>2021</year>;<volume>37</volume>:<fpage>684</fpage>–<lpage>92</lpage>.<pub-id pub-id-type="pmid">32976559</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yuan</surname><given-names>GH</given-names></string-name>, <string-name><surname>Wang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname><given-names>GZ</given-names></string-name></person-group><etal>et al</etal><article-title>RNAlight: a machine learning model to identify nucleotide features determining RNA subcellular localization</article-title>. <source>Brief Bioinform</source><year>2023</year>;<volume>24</volume>:<fpage>bbac509</fpage>.<pub-id pub-id-type="pmid">36464487</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname><given-names>M</given-names></string-name>, <string-name><surname>Wu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Lu</surname><given-names>C</given-names></string-name></person-group><etal>et al</etal><article-title>DeepLncLoc: a deep learning framework for long non-coding RNA subcellular localization prediction based on subsequence embedding</article-title>. <source>Brief Bioinform</source><year>2022</year>;<volume>23</volume>:<fpage>bbab360</fpage>.<pub-id pub-id-type="pmid">34498677</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname><given-names>M</given-names></string-name>, <string-name><surname>Lu</surname><given-names>C</given-names></string-name>, <string-name><surname>Fei</surname><given-names>Z</given-names></string-name></person-group><etal>et al</etal><article-title>DMFLDA: a deep learning framework for predicting lncRNA-disease associations</article-title>. <source>IEEE/ACM Trans Comput Biol Bioinform</source><year>2021</year>;<volume>18</volume>:<fpage>2353</fpage>–<lpage>63</lpage>.<pub-id pub-id-type="pmid">32248123</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname><given-names>M</given-names></string-name>, <string-name><surname>Lu</surname><given-names>C</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>SDLDA: lncRNA-disease association prediction based on singular value decomposition and deep learning</article-title>. <source>Methods</source><year>2020</year>;<volume>179</volume>:<fpage>73</fpage>–<lpage>80</lpage>.<pub-id pub-id-type="pmid">32387314</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>B</given-names></string-name>, <string-name><surname>Gunawardane</surname><given-names>L</given-names></string-name>, <string-name><surname>Niazi</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>A novel RNA motif mediates the strict nuclear localization of a long noncoding RNA</article-title>. <source>Mol Cell Biol</source><year>2014</year>;<volume>34</volume>:<fpage>2318</fpage>–<lpage>29</lpage>.<pub-id pub-id-type="pmid">24732794</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>F</given-names></string-name>, <string-name><surname>Song</surname><given-names>H</given-names></string-name>, <string-name><surname>Zeng</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>DeepFunc: a deep learning framework for accurate prediction of protein functions from protein sequences and interactions</article-title>. <source>Proteomics</source><year>2019</year>;<volume>19</volume>:<fpage>e1900019</fpage>.<pub-id pub-id-type="pmid">30941889</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>T</given-names></string-name>, <string-name><surname>Tan</surname><given-names>P</given-names></string-name>, <string-name><surname>Wang</surname><given-names>L</given-names></string-name></person-group><etal>et al</etal><article-title>RNALocate: a resource for RNA subcellular localizations</article-title>. <source>Nucleic Acids Res</source><year>2017</year>;<volume>45</volume>:<fpage>D135</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">27543076</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B41">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>W</given-names></string-name>, <string-name><surname>Yan</surname><given-names>J</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X</given-names></string-name></person-group><etal>et al</etal> Deep extreme multi-label learning. New York: Association for Computing Machinery 2018. <fpage>100</fpage>–<lpage>7</lpage>.</mixed-citation>
    </ref>
    <ref id="btad752-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>ZY</given-names></string-name>, <string-name><surname>Sun</surname><given-names>ZJ</given-names></string-name>, <string-name><surname>Yang</surname><given-names>YH</given-names></string-name></person-group><etal>et al</etal><article-title>Towards a better prediction of subcellular location of long non-coding RNA</article-title>. <source>Front Comput Sci</source><year>2022</year>;<volume>16</volume>:<fpage>165903</fpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10749772</article-id>
    <article-id pub-id-type="pmid">38109668</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btad752</article-id>
    <article-id pub-id-type="publisher-id">btad752</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Sequence Analysis</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>LncLocFormer: a Transformer-based deep learning model for multi-label lncRNA subcellular localization prediction by using localization-specific attention mechanism</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1726-0955</contrib-id>
        <name>
          <surname>Zeng</surname>
          <given-names>Min</given-names>
        </name>
        <aff><institution>School of Computer Science and Engineering, Central South University</institution>, Changsha, Hunan 410083, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wu</surname>
          <given-names>Yifan</given-names>
        </name>
        <aff><institution>School of Computer Science and Engineering, Central South University</institution>, Changsha, Hunan 410083, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Yiming</given-names>
        </name>
        <aff><institution>School of Computer Science and Engineering, Central South University</institution>, Changsha, Hunan 410083, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yin</surname>
          <given-names>Rui</given-names>
        </name>
        <aff><institution>Department of Health Outcomes and Biomedical Informatics, University of Florida</institution>, Gainesville, FL 32603, <country country="US">United States</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lu</surname>
          <given-names>Chengqian</given-names>
        </name>
        <aff><institution>School of Computer Science, Key Laboratory of Intelligent Computing and Information Processing, Xiangtan University</institution>, Xiangtan, Hunan 411105, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-4955-9270</contrib-id>
        <name>
          <surname>Duan</surname>
          <given-names>Junwen</given-names>
        </name>
        <aff><institution>School of Computer Science and Engineering, Central South University</institution>, Changsha, Hunan 410083, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0188-1394</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>Min</given-names>
        </name>
        <aff><institution>School of Computer Science and Engineering, Central South University</institution>, Changsha, Hunan 410083, <country country="CN">China</country></aff>
        <xref rid="btad752-cor1" ref-type="corresp"/>
        <!--limin@mail.csu.edu.cn-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Boeva</surname>
          <given-names>Valentina</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btad752-cor1">Corresponding author. School of Computer Science and Engineering, Central South University, Changsha, Hunan 410083, China. E-mail: <email>limin@mail.csu.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2023-12-18">
      <day>18</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>18</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <volume>39</volume>
    <issue>12</issue>
    <elocation-id>btad752</elocation-id>
    <history>
      <date date-type="received">
        <day>26</day>
        <month>7</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>13</day>
        <month>11</month>
        <year>2023</year>
      </date>
      <date date-type="editorial-decision">
        <day>07</day>
        <month>12</month>
        <year>2023</year>
      </date>
      <date date-type="corrected-typeset">
        <day>25</day>
        <month>12</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btad752.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>There is mounting evidence that the subcellular localization of lncRNAs can provide valuable insights into their biological functions. In the real world of transcriptomes, lncRNAs are usually localized in multiple subcellular localizations. Furthermore, lncRNAs have specific localization patterns for different subcellular localizations. Although several computational methods have been developed to predict the subcellular localization of lncRNAs, few of them are designed for lncRNAs that have multiple subcellular localizations, and none of them take motif specificity into consideration.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>In this study, we proposed a novel deep learning model, called LncLocFormer, which uses only lncRNA sequences to predict multi-label lncRNA subcellular localization. LncLocFormer utilizes eight Transformer blocks to model long-range dependencies within the lncRNA sequence and shares information across the lncRNA sequence. To exploit the relationship between different subcellular localizations and find distinct localization patterns for different subcellular localizations, LncLocFormer employs a localization-specific attention mechanism. The results demonstrate that LncLocFormer outperforms existing state-of-the-art predictors on the hold-out test set. Furthermore, we conducted a motif analysis and found LncLocFormer can capture known motifs. Ablation studies confirmed the contribution of the localization-specific attention mechanism in improving the prediction performance.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The LncLocFormer web server is available at <ext-link xlink:href="http://csuligroup.com:9000/LncLocFormer" ext-link-type="uri">http://csuligroup.com:9000/LncLocFormer</ext-link>. The source code can be obtained from <ext-link xlink:href="https://github.com/CSUBioGroup/LncLocFormer" ext-link-type="uri">https://github.com/CSUBioGroup/LncLocFormer</ext-link>.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Key Research and Development Program of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100012166</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>2022YFC3400300</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>62102457</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Hunan Provincial Natural Science Foundation of China</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2023JJ40763</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="11"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Long non-coding RNAs (lncRNAs) are a class of non-coding RNA molecules that comprised more than 200 nucleotides (<xref rid="btad752-B7" ref-type="bibr">Birney <italic toggle="yes">et al.</italic> 2007</xref>, <xref rid="btad752-B21" ref-type="bibr">Lu <italic toggle="yes">et al.</italic> 2018</xref>). They are involved in various important biological processes, including the regulation of gene expression, alternative splicing, nuclear organization, and genomic imprinting (<xref rid="btad752-B36" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2020</xref>). LncRNAs have the ability to interact with proteins, DNAs, and RNAs, and perform specific functions as a result of these interactions (<xref rid="btad752-B10" ref-type="bibr">Esteller 2011</xref>). For example, they can act as “miRNA sponge” to regulate miRNA levels and thereby influence the expression of miRNA targets (<xref rid="btad752-B9" ref-type="bibr">DiStefano 2018</xref>). Additionally, under particular stimulation, lncRNAs can influence transcriptional activity or pathways (<xref rid="btad752-B31" ref-type="bibr">Wang and Chang 2011</xref>). Because of the complexity of molecular functions and biological processes, lncRNA-related research has gained significant attention (<xref rid="btad752-B20" ref-type="bibr">Lu <italic toggle="yes">et al.</italic> 2020</xref>, <xref rid="btad752-B35" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2021</xref>).</p>
    <p>A growing amount of evidence reveals that lncRNA subcellular localizations can provide valuable insights into their biological functions (<xref rid="btad752-B28" ref-type="bibr">Savulescu <italic toggle="yes">et al.</italic> 2021</xref>). One wet-lab technique commonly used to study RNA subcellular localization is single-molecule fluorescent <italic toggle="yes">in situ</italic> hybridization (smFISH) technique (<xref rid="btad752-B24" ref-type="bibr">Moffitt and Zhuang 2016</xref>). Although the image data provided by the smFISH technique can accurately determine the subcellular localization of RNAs, the smFISH technique is expensive and time-consuming. Considering its limitations, it would be extremely beneficial for biologists to develop accurate computational methods to predict lncRNA subcellular localizations.</p>
    <p>Some computational methods have been proposed to predict lncRNA subcellular localization. To the best of our knowledge, LncLocator is the first predictor for lncRNA subcellular localization (<xref rid="btad752-B5" ref-type="bibr">Cao <italic toggle="yes">et al.</italic> 2018</xref>). It extracts 4-mer features and high-level features, and uses support vector machine (SVM) and random forest to make predictions. iLoc-lncRNA utilizes 8-mer features to encode lncRNA sequences, and applies SVM to perform the prediction task (<xref rid="btad752-B30" ref-type="bibr">Su <italic toggle="yes">et al.</italic> 2018</xref>). DeepLncRNA incorporates 2, 3, 4, and 5-mer features and uses a deep learning network to predict lncRNA subcellular localizations (<xref rid="btad752-B13" ref-type="bibr">Gudenas and Wang 2018</xref>). Locate-R incorporates the preselected <italic toggle="yes">k</italic>-mer features and applies SVM to construct a classifier (<xref rid="btad752-B1" ref-type="bibr">Ahmad <italic toggle="yes">et al.</italic> 2020</xref>). lncLocPred integrates multiple feature selection techniques to select optimal features, and adopts a logistic regression model to make predictions (<xref rid="btad752-B11" ref-type="bibr">Fan <italic toggle="yes">et al.</italic> 2020</xref>). LncLocation integrates the multi-source heterogeneous features, and uses SVM to construct a classifier (<xref rid="btad752-B12" ref-type="bibr">Feng <italic toggle="yes">et al.</italic> 2020</xref>). DeepLncLoc is a novel deep learning model, which uses subsequence embedding technique to encode lncRNA sequences, and uses a deep neural network to classify five localizations (<xref rid="btad752-B37" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2022</xref>). TACOS applies a tree-based stacking classifier to predict the subcellular localization of human lncRNA in 10 different cell types (<xref rid="btad752-B15" ref-type="bibr">Jeon <italic toggle="yes">et al.</italic> 2022</xref>). RNALight extracts <italic toggle="yes">k</italic>-mer features and uses LightGBM to predict the subcellular localizations of mRNAs and lncRNAs (<xref rid="btad752-B34" ref-type="bibr">Yuan <italic toggle="yes">et al.</italic> 2023</xref>). GraphLncLoc transforms lncRNA sequences into graphs, and utilizes graph convolutional networks to capture high-level features and make predictions (<xref rid="btad752-B17" ref-type="bibr">Li <italic toggle="yes">et al.</italic> 2023</xref>). Recently, LncLocator 2.0 (<xref rid="btad752-B19" ref-type="bibr">Lin <italic toggle="yes">et al.</italic> 2021</xref>) and iLoc-LncRNA(2.0) (<xref rid="btad752-B42" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> 2022</xref>) have been released, which provide more accurate prediction results than their previous versions.</p>
    <p>Although several computational models have been developed, few of these models are designed for lncRNAs that have multiple subcellular localizations. In reality, lncRNA subcellular localization is a dynamic process (<xref rid="btad752-B4" ref-type="bibr">Bridges <italic toggle="yes">et al.</italic> 2021</xref>). For example, lncRNA SNHG1 displays cytoplasmic distribution in human HCT116 colon cancer cells. However, upon DNA damage stress, it is retained in the nucleus compartment (<xref rid="btad752-B6" ref-type="bibr">Carlevaro-Fita and Johnson 2019</xref>). Another example is lncRNA Uchl1-AS1, which translocates from the nucleus to the cytoplasm under rapamycin treatment (<xref rid="btad752-B27" ref-type="bibr">Riva <italic toggle="yes">et al.</italic> 2016</xref>). However, the existing computational models usually only consider a single subcellular localization for each lncRNA.</p>
    <p>In addition, increasing evidence suggests that lncRNAs exhibit distinct localization patterns in different subcellular localizations. For example, Shukla <italic toggle="yes">et al.</italic> found that conserved long sequences (&gt;300 nt) with a common 15-nt C-rich pattern are responsible for nuclear localization (<xref rid="btad752-B29" ref-type="bibr">Shukla <italic toggle="yes">et al.</italic> 2018</xref>). Lubelsky <italic toggle="yes">et al.</italic> found a core 42-nt motif that drives nuclear RNA localization (<xref rid="btad752-B22" ref-type="bibr">Lubelsky and Ulitsky 2018</xref>). Despite these findings, existing computational methods do not take motif specificity in different subcellular localizations into account.</p>
    <p>To meet the need for lncRNA multiple subcellular localization predictions and to consider motif specificity for different subcellular localizations, we proposed LncLocFormer, which is a Transformer-based deep learning model using a localization-specific attention mechanism. Transformer is a class of powerful deep learning architecture that has achieved substantial breakthroughs in natural language processing (NLP), as it can capture both local and global features of sequences. Inspired by its success in NLP, we applied it to the prediction of lncRNA subcellular localization. By using the positional coding and multi-head attention mechanism in Transformer blocks, LncLocFormer can model long-range dependencies within the lncRNA sequence and share information across the lncRNA sequence. Different from previous computational methods, LncLocFormer can predict multiple subcellular localizations simultaneously for each lncRNA sequence. Furthermore, using the localization-specific attention mechanism, LncLocFormer learns different attention weights for different subcellular localizations, which can provide valuable information about the relationship between different labels.</p>
    <p>To evaluate the performance of LncLocFormer, we compared it with some deep learning baseline models and existing state-of-the-art predictors. The results of cross-validation (CV) and the hold-out test set demonstrate that LncLocFormer performs significantly better than other computational models. In addition, the results show that LncLocFormer is capable of capturing sequence motifs. To investigate which part of LncLocFormer is helpful in predicting lncRNA subcellular localizations, we conducted an ablation study by removing or replacing some components of LncLocFormer. The ablation study shows that the localization-specific attention mechanism is a crucial component in LncLocFormer. To facilitate the use of LncLocFormer, we developed a user-friendly web server.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Benchmark dataset</title>
      <p>The first important step in constructing a reliable predictor is to establish a reliable benchmark dataset. To achieve this, we retrieved known lncRNA subcellular localization information from the RNALocate v2.0 (<xref rid="btad752-B8" ref-type="bibr">Cui <italic toggle="yes">et al.</italic> 2022</xref>) database (<ext-link xlink:href="https://www.rna-society.org/rnalocate/" ext-link-type="uri">https://www.rna-society.org/rnalocate/</ext-link>), which collects more than 210 000 RNA-associated subcellular localization entries with experimental evidence, encompassing more than 110 000 RNAs with 171 subcellular localizations in 104 species. We generated a benchmark dataset to train and test our model by the following procedure:</p>
      <list list-type="order">
        <list-item>
          <p>We retrieved a total of 9128 Homo sapiens lncRNA-associated subcellular localization entries from the RNALocate v2.0 database. Since many lncRNAs have multiple entries, we merged the entries with the same gene symbol;</p>
        </list-item>
        <list-item>
          <p>We removed the lncRNAs that do not have sequence information in NCBI (<xref rid="btad752-B26" ref-type="bibr">Pruitt <italic toggle="yes">et al.</italic> 2007</xref>);</p>
        </list-item>
        <list-item>
          <p>To reduce data redundancy, we used the cd-hit-est tool (<xref rid="btad752-B14" ref-type="bibr">Huang <italic toggle="yes">et al.</italic> 2010</xref>) with a cutoff of 80%;</p>
        </list-item>
        <list-item>
          <p>Consider that some subcellular localizations have a small number lncRNA entries, we only selected the subcellular localizations with more than 40 lncRNA entries;</p>
        </list-item>
        <list-item>
          <p>In the RNALocate v2.0 database, a significant number of entries are localized in exosome. However, accumulating evidence suggests that lncRNAs are expressed in a cell-specific and/or tissue-specific manner, and most of them are located in the nucleus. Moreover, a lot of samples which belong to the exosome localization could hinder the prediction of other subcellular locations. Thus, we removed exosome-localized entries in our study.</p>
        </list-item>
      </list>
      <p>Finally, our benchmark dataset comprises 811 lncRNAs, covering four types of subcellular localizations: nucleus, cytoplasm, chromatin, and insoluble cytoplasm. <xref rid="btad752-F1" ref-type="fig">Figure 1</xref> shows the distribution of subcellular localizations in the constructed benchmark dataset.</p>
      <fig position="float" id="btad752-F1">
        <label>Figure 1.</label>
        <caption>
          <p>The distribution of subcellular localizations in the constructed benchmark dataset.</p>
        </caption>
        <graphic xlink:href="btad752f1" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.2 LncLocFormer architecture</title>
      <p><xref rid="btad752-F2" ref-type="fig">Figure 2</xref> illustrates the architecture of LncLocFormer, which comprises four main components: (i) the embedding part, (ii) eight Transformer blocks, (iii) a localization-specific attention mechanism, and (iv) a fully connected layer that performs the multi-label classification task.</p>
      <fig position="float" id="btad752-F2">
        <label>Figure 2.</label>
        <caption>
          <p>The architecture of LncLocFormer. LncLocFormer takes a lncRNA sequence as input, which is encoded using a subsequence embedding method. The embedding layer is immediately followed by eight Transformer blocks, which are used to model long-range dependencies within the lncRNA sequence and share information across the lncRNA sequence. These Transformer blocks have the same architecture: relative position encoding, multi-head attention mechanism, residual connection, position-wise feed-forward network, and “Add &amp; Norm” component. Following the Transformer blocks, the designed localization-specific attention layer is employed to learn distinct weights of nucleotide for each subcellular localization. Finally, a fully connected layer is used to perform the multi-label classification task.</p>
        </caption>
        <graphic xlink:href="btad752f2" position="float"/>
      </fig>
      <sec>
        <title>2.2.1 Sequence embedding</title>
        <p>Before feeding raw lncRNA sequences into a deep learning model, it is necessary to encode them as numeric vectors. The two most commonly used coding techniques are <italic toggle="yes">k</italic>-mer coding and one-hot coding. However, <italic toggle="yes">k</italic>-mer coding loses the sequence order information, while one-hot coding ignores the relationship between different nucleotides. In order to tackle these limitations, we employed an effective subsequence embedding method (<xref rid="btad752-B37" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2022</xref>), which can preserve the sequence order information of lncRNAs and reflect the relationship between different <italic toggle="yes">k</italic>-mers. The main idea of the subsequence embedding method is to split a lncRNA sequence into a number of consecutive, non-overlapping subsequences. Then, we extracted patterns from each subsequence and combined these patterns to obtain a complete representation of the lncRNA sequence.</p>
        <p>Specifically, the subsequence embedding method involves several steps. First, we split a lncRNA sequence into <italic toggle="yes">n</italic> consecutive, non-overlapping subsequences. Then, we used an embedding technique to encode each subsequence. Word2vec is a popular word embedding technique in NLP that has demonstrated potential in many bioinformatics tasks (<xref rid="btad752-B32" ref-type="bibr">Wu <italic toggle="yes">et al.</italic> 2022</xref>, <xref rid="btad752-B17" ref-type="bibr">Li <italic toggle="yes">et al.</italic> 2023</xref>). Thus, we pre-trained all lncRNA sequences in our dataset to obtain the distribution representation of <italic toggle="yes">k</italic>-mers by using the word2vec method, and then used the distribution representation of <italic toggle="yes">k</italic>-mer features to represent these subsequences. In the training process, the parameter <italic toggle="yes">k</italic> was chosen from {1, 2, 3, 4, 5, 6} to find the best value. The skip-gram model was applied to maximize the co-occurrence likelihood function of the central word and corresponding context words. The other settings of word2vec were kept default to train word vectors. Finally, in our study, we set <italic toggle="yes">k </italic>=<italic toggle="yes"> </italic>3, and the dimension of the word vector was 128. After pre-trained on the dataset, we obtained the word vectors and then combined these vectors to represent a lncRNA sequence. The whole subsequence embedding framework is shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S1</xref>. We refer to the original publication of the subsequence embedding method for more details (<xref rid="btad752-B37" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2022</xref>).</p>
      </sec>
      <sec>
        <title>2.2.2 Transformer blocks</title>
        <p>So far, we have obtained the representation of a lncRNA sequence. The next step is to extract high-level features from the lncRNA representation. Transformer is a class of deep learning models that has achieved substantial breakthroughs in NLP and has recently been applied to various bioinformatics tasks. In our task, we employed eight Transformer blocks to model long-range dependencies within the lncRNA sequence and share information across the lncRNA sequence. The Transformer blocks are inspired by RealFormer, which is a state-of-the-art variant version of Transformer. The detailed structure is shown in the right side of <xref rid="btad752-F2" ref-type="fig">Fig. 2</xref>. These Transformer blocks mainly consist of five components: relative positional encoding, multi-head attention, residual connection, “add &amp; norm” component, and position-wise feed-forward network.</p>
        <p>The first component is relative positional encoding. We know that Recurrent Neural Network (RNN) is a sequential structure that recurrently processes words one by one. Unlike RNN, the core part of Transformer is the attention mechanism. Using the attention mechanism to replace RNN loses the sequence order information, which causes that the model does not know the relative and absolute position information of each nucleotide in lncRNA sequences. Thus, it is necessary to add the sequence order information to assist the model in learning the position information. To use the sequence order information, we inject relative positional information by using relative positional encoding to the input representations. We borrowed the idea of traditional relative positional encoding and made some modifications. Specifically, the original formula of “traditional relative positional encoding” is as follows:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        <p>We made some modifications, resulting in the modified relative positional encoding formula to inject relative positional information:
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a learnable parameter, <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are learnable matrixes, and <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a relative position bias term. The main difference between the relative positional encoding and traditional relative positional encoding lies in the inclusion of the relative position scale term <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The advantage is, by adding this term, we can control the information passing for different relative locations more efficiently, making the model better able to capture the long-term dependencies within the lncRNA sequence.</p>
        <p>After relative positional coding, LncLocFormer applies self-attention to learn the attention weights for each nucleotide pair in the lncRNA sequence. The attention weights can reveal the importance of sequence regions for subcellular localization. Specifically, for each input lncRNA sequence:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mi mathvariant="normal">lncRNA</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where <italic toggle="yes">L</italic> denotes the length of the lncRNA, <italic toggle="yes">Nj</italic> is one of the four nucleotide bases (A, C, G, and U) at the <italic toggle="yes">j</italic> position of the lncRNA sequence. Self-attention learns an attention score for each pair of nucleotides <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic>. The attention score is computed by using a query <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mi>Q</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, a key <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mi>K</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, a value <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mi>V</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, a pre-softmax attention score Prev as follows:
<disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mi mathvariant="normal">Attention</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Softmax</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Prev</mml:mi></mml:mrow></mml:mfenced><mml:mi>V</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula>where Prev indicates the attention scores from the previous self-attention layer.</p>
        <p>Instead of using a single attention in the traditional Transformer architecture, computing attention scores using a set of queries, keys, and values enables the Transformer model to jointly attend to information at different positions, which is called the multi-head attention mechanism. In our study, the multi-head attention mechanism is applied to model long-range dependencies and share information across the lncRNA sequence. Each attention head<italic toggle="yes">i</italic> (<italic toggle="yes">i </italic>=<italic toggle="yes"> </italic>1, 2, …, <italic toggle="yes">H</italic>) is computed as:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5" display="block" overflow="scroll"><mml:mi mathvariant="normal">hea</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Attention</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>Q</mml:mi><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Pre</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, and <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are learnable parameter matrixes, Prev<italic toggle="yes">i</italic> is the slice of Prev corresponding to head<italic toggle="yes">i</italic>.</p>
        <p>Here, each attention head is independent. All head<italic toggle="yes">i</italic> are concatenated and transformed with another linear projection to obtain the final multi-head output values.
<disp-formula id="E6"><label>(6)</label><mml:math id="M6" display="block" overflow="scroll"><mml:mi mathvariant="normal">Multi</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal">head</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Concat</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi mathvariant="normal">hea</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal">hea</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="normal">hea</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfenced><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is a learnable parameter matrix and <italic toggle="yes">H</italic> is the number of heads. Based on the multi-head attention mechanism, each head may attend to different parts of the input lncRNA sequence.</p>
        <p>In addition to relative positional coding and multi-head attention, the Transformer block has a standard architecture, which includes residual connection, “add &amp; norm” component, and position-wise feed-forward network. Specifically, the queries, keys, and values are derived from the outputs of the previous Transformer block, a residual connection is employed to avoid gradient vanishing or gradient exploding problems. The “add &amp; norm” component has two operations: addition and layer normalization. This addition operation from the residual connection is immediately followed by layer normalization. The position-wise feed-forward network transforms the representation at all the sequence positions using a fully connected layer.</p>
      </sec>
      <sec>
        <title>2.2.3 Localization-specific attention</title>
        <p>The standard attention mechanisms in Transformer blocks only tell us which nucleotides are considered very important for the overall prediction. However, lncRNA subcellular localization is a dynamic process, which is treated as a multi-label classification problem in our study. Therefore, it would be more informative to analyze which nucleotides are considered important for each subcellular localization compartment. With this motivation, we designed a localization-specific attention mechanism after the Transformer blocks.</p>
        <p>In the study, we have four subcellular localization compartments (nucleus, cytoplasm, chromatin, and insoluble cytoplasm). We used the multi-head attention mechanism to obtain the weight (importance) of every nucleotide in one subcellular localization compartment. Then, we repeated the process four times (here, four represents the number of subcellular localization compartments). As a result, we obtained four kinds of attention scores for four subcellular localization compartments. Specifically, we trained four attention matrices <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>ϵ</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for each subcellular localization compartment.
<disp-formula id="E7"><label>(7)</label><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">softmax</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mn>3,4</mml:mn><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mi>V</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the representation of the lncRNA obtained by the Transformer blocks and <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>∈</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:math></inline-formula> are the learnable weight matrix and bias term. After that, we used <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to aggregate <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mi>V</mml:mi></mml:math></inline-formula> under label <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mi>j</mml:mi></mml:math></inline-formula> and used a dense layer with sigmoid activation function to obtain the localization probability:
<disp-formula id="E8"><label>(8)</label><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">sigmoid</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        <p>Finally, the <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mn>3,4</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> is used as the final prediction, and the cross-entropy loss is computed by <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> to perform gradient descent.</p>
        <p>Overall, the benefits of the localization-specific attention mechanism can be summarized as follows:</p>
        <list list-type="order">
          <list-item>
            <p>The localization-specific attention mechanism is a fine-grained interpretability technique that can provide support for the interpretability of each subcellular localization.</p>
          </list-item>
          <list-item>
            <p>The localization-specific attention mechanism tends to be less heavily biased toward the most frequent compartments, which alleviates the imbalance data distribution problem.</p>
          </list-item>
          <list-item>
            <p>The localization-specific attention learns multiple attention scores and uses them for prediction, resulting in more accurate and robust results.</p>
          </list-item>
        </list>
        <p>Finally, in the classification part, a fully connected layer is applied to perform the multi-label classification task.</p>
      </sec>
    </sec>
    <sec>
      <title>2.3 Deep learning baseline models and existing predictors</title>
      <p>In this study, we focus on constructing powerful deep learning models to predict lncRNA subcellular localizations. To demonstrate the effectiveness of LncLncFormer, we compared it with several deep learning baseline models.</p>
      <list list-type="order">
        <list-item>
          <p><italic toggle="yes">k</italic>-mer + MLP, this model extracts <italic toggle="yes">k</italic>-mer frequency features, which are fed into a MLP layer to output subcellular localizations.</p>
        </list-item>
        <list-item>
          <p>Word2vec + MLP, this model encodes lncRNA sequences by using the word2vec technique, followed by feeding the sequence representation to a MLP layer for subcellular localization prediction.</p>
        </list-item>
        <list-item>
          <p>Word2vec + CNN + MLP, this model converts lncRNA sequences to embedding vectors learned by the word2vec technique, followed by a CNN layer, then uses a MLP layer to predict the subcellular localizations.</p>
        </list-item>
        <list-item>
          <p>Word2vec + Bi-LSTM + MLP, this model converts lncRNA sequences to embedding vectors learned by the word2vec technique, followed by a Bi-LSTM layer, then uses a MLP layer to predict the subcellular localizations.</p>
        </list-item>
        <list-item>
          <p>Glove + MLP, this model encodes lncRNA sequences by using the Glove technique, followed by feeding the sequence representation to a MLP layer for subcellular localization prediction.</p>
        </list-item>
        <list-item>
          <p>Glove + CNN + MLP, this model converts lncRNA sequences to embedding vectors learned by the Glove technique, followed by a CNN layer, then uses a MLP layer to predict the subcellular localizations.</p>
        </list-item>
        <list-item>
          <p>Glove + Bi-LSTM + MLP, this model converts lncRNA sequences to embedding vectors learned by the Glove technique, followed by a Bi-LSTM layer, then uses a MLP layer to predict the subcellular localizations.</p>
        </list-item>
      </list>
      <p>In the study, we used grid search to find the optimal parameters for these deep learning baseline models.</p>
      <p>To further evaluate the performance of LncLocFormer in predicting lncRNA subcellular localizations, we compared LncLocFormer with several existing state-of-the-art predictors by using a hold-out test set. We selected lncLocator (<xref rid="btad752-B5" ref-type="bibr">Cao <italic toggle="yes">et al.</italic> 2018</xref>), iLoc-lncRNA (<xref rid="btad752-B30" ref-type="bibr">Su <italic toggle="yes">et al.</italic> 2018</xref>), Locate-R (<xref rid="btad752-B1" ref-type="bibr">Ahmad <italic toggle="yes">et al.</italic> 2020</xref>), DeepLncLoc (<xref rid="btad752-B37" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2022</xref>), iLoc-LncRNA(2.0) (<xref rid="btad752-B42" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> 2022</xref>), and GraphLncLoc (<xref rid="btad752-B17" ref-type="bibr">Li <italic toggle="yes">et al.</italic> 2023</xref>) as the compared predictors. LncLocator and DeepLncLoc can predict five types of subcellular localizations, including nucleus, cytoplasm, cytosol, ribosome, and exosome. iLoc-lncRNA, Locate-R, iLoc-LncRNA(2.0), and GraphLncLoc can predict four types of subcellular localizations, including nucleus, cytoplasm, ribosome, and exosome. We did not compare LncLocFormer with lncLocator 2.0 since lncLocator 2.0 only provides the predicted CNRCI values instead of probabilities.</p>
    </sec>
    <sec>
      <title>2.4 Evaluation metrics</title>
      <p>To evaluate the performance of LncLocFormer with deep learning baseline models, we selected some evaluation metrics which are widely used in multi-label classification problem (<xref rid="btad752-B16" ref-type="bibr">Li <italic toggle="yes">et al.</italic> 2019</xref>). These evaluation metrics include average <italic toggle="yes">F</italic>-measure (Ave-<italic toggle="yes">F</italic>1), micro precision (MiP), micro recall (MiR), micro <italic toggle="yes">F</italic>-measure (MiF), and each area under receiver operating characteristic curve (AUC) for each subcellular localization. For convenience, <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>∈</mml:mo><mml:mn>0,1</mml:mn></mml:math></inline-formula> are the ground truth and predicted value of lncRNA <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mi>i</mml:mi></mml:math></inline-formula> for subcellular localization <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mi>j</mml:mi></mml:math></inline-formula>, respectively, and <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> if <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>, otherwise <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>.</p>
      <p>Ave-<italic toggle="yes">F</italic>1 is the harmonic mean of average precision and average recall, which is used in the CAFA challenge (<xref rid="btad752-B39" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> 2019</xref>), a protein function prediction challenge. We compute Ave-<italic toggle="yes">F</italic>1 using the following formulas:
<disp-formula id="E9"><label>(9)</label><mml:math id="M9" display="block" overflow="scroll"><mml:mi mathvariant="normal">Ave</mml:mi><mml:mo>-</mml:mo><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">AvgPre</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>A</mml:mi><mml:mi mathvariant="normal">vgRec</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi mathvariant="normal">AvgPre</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi mathvariant="normal">AvgRec</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula><disp-formula id="E10"><label>(10)</label><mml:math id="M10" display="block" overflow="scroll"><mml:mi mathvariant="normal">AvgPre</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">pr</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula><disp-formula id="E11"><label>(11)</label><mml:math id="M11" display="block" overflow="scroll"><mml:mi mathvariant="normal">AvgRec</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">re</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where
<disp-formula id="E12"><mml:math id="M12" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">pre</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">rec</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p>
      <p>MiF is the harmonic mean of MiP and MiR, which is used in the BioASQ challenge (<xref rid="btad752-B33" ref-type="bibr">You <italic toggle="yes">et al.</italic> 2021</xref>), a challenge on large-scale biomedical semantic indexing and question answering. It is defined as follows:
<disp-formula id="E13"><label>(12)</label><mml:math id="M13" display="block" overflow="scroll"><mml:mi mathvariant="normal">MiF</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">MiP</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">MiR</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">MiP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">MiR</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where
<disp-formula id="E14"><mml:math id="M14" display="block" overflow="scroll"><mml:mi mathvariant="normal">MiP</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal">MiR</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p>
      <p>Considering that the existing predictors are designed as multi-class predictors rather than multi-label predictors, to evaluate the performance of LncLocFormer with existing predictors, we evaluated the performance from two perspectives: the multi-label and the multi-class perspectives.</p>
      <p>In the multi-label perspective, we used Precision@k (P@k, which represents the number of correct predictions over <italic toggle="yes">k</italic>) to evaluate the performance (Zhang <italic toggle="yes">et al.</italic>). It is defined as follows:
<disp-formula id="E15"><label>(13)</label><mml:math id="M15" display="block" overflow="scroll"><mml:mi>P</mml:mi><mml:mo>@</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mi mathvariant="normal">ran</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">rank</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mfenced></mml:math></inline-formula> returns the <italic toggle="yes">k</italic> largest indices of <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> ranked in descending order. In the study, we only focus on the subcellular localization with the highest probability, thus, <italic toggle="yes">k</italic> is set to 1.</p>
      <p>In the multi-class perspective, consistent with the current state-of-the-art lncRNA subcellular localization predictors, we used Accuracy (ACC), Macro F-measure (MaF), Macro Precision (MaP), Macro Recall (MaR), and AUC as evaluation metrics.
<disp-formula id="E16"><label>(14)</label><mml:math id="M16" display="block" overflow="scroll"><mml:mi mathvariant="normal">ACC</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula><disp-formula id="E17"><label>(15)</label><mml:math id="M17" display="block" overflow="scroll"><mml:mi mathvariant="normal">Ma</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="E18"><label>(16)</label><mml:math id="M18" display="block" overflow="scroll"><mml:mi mathvariant="normal">Ma</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="E19"><label>(17)</label><mml:math id="M19" display="block" overflow="scroll"><mml:mi mathvariant="normal">MaF</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">Ma</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:mi mathvariant="normal">Ma</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ma</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Ma</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
    </sec>
    <sec>
      <title>2.5 Implementation details</title>
      <p>LncLocFormer is implemented using PyTorch (Paszke <italic toggle="yes">et al.</italic>). A grid search strategy was employed to find the optimal parameters of LncLocFormer, and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref> provides a summary of the optimal hyper-parameters and the corresponding search space. The used loss function is the cross-entropy function. The skip-gram model (Mikolov <italic toggle="yes">et al.</italic>) is used to pre-train the <italic toggle="yes">k</italic>-mer embedding vectors. We used the Adam optimizer with a learning rate of 0.0003. The learning rate is warm-uped over the first four epochs and decayed linearly for the remaining training steps. The batch size is set to 64. In the Transformer blocks, we used eight heads and hidden size of 128. To prevent the low-rank bottleneck, we enhanced the size of query/key/value into 64 using a dense layer (Bhojanapalli <italic toggle="yes">et al.</italic>). We keep at most 8196 nt for each lncRNA and divide them into 512 subsequences by using the subsequence embedding method. The dropout rate is set to 0.2 for the embedding layer and 0.1 for other layers. The maximum relative distance in the position embedding is set to 25.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Comparison with deep learning baseline models</title>
      <p>In this section, we investigated the effectiveness of LncLocFormer (subsequence embedding + Transformer blocks + localization-specific attention + MLP). We conducted 5-fold CV to evaluate the performance of LncLocFormer with other deep learning baseline models. In particular, we split the benchmark dataset into a training set (90%) and a hold-out test set (10%). Next, we performed 5-fold CV by further splitting the training set into 80% training and 20% validation. The process was repeated five times, and the final prediction results were the average of five validation results. The performances of LncLocFormer and other deep learning baseline models using 5-fold CV are shown in <xref rid="btad752-T1" ref-type="table">Table 1</xref>. We can observe that LncLocFormer outperforms other deep learning baseline models, except for the MiP. Specifically, LncLocFormer obtains Ave-<italic toggle="yes">F</italic>1 of 0.719, MiR of 0.721, MiF of 0.701, and average AUC of 0.648, while GloVe + Bi-LSTM + MLP obtains the best MiP (0.712). These observations indicate the superiority of LncLocFormer network architecture.</p>
      <table-wrap position="float" id="btad752-T1">
        <label>Table 1.</label>
        <caption>
          <p>Performance of LncLocFormer and other deep learning baseline models using 5-fold CV.<xref rid="tblfn1" ref-type="table-fn"><sup>a</sup></xref></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="2" colspan="1">Model</th>
              <th rowspan="2" colspan="1">Ave-<italic toggle="yes">F</italic>1</th>
              <th rowspan="2" colspan="1">MiP</th>
              <th rowspan="2" colspan="1">MiR</th>
              <th rowspan="2" colspan="1">MiF</th>
              <th colspan="5" rowspan="1">AUC<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Nucleus</th>
              <th rowspan="1" colspan="1">Cytoplasm</th>
              <th rowspan="1" colspan="1">Chromatin</th>
              <th rowspan="1" colspan="1">Insoluble cytoplasm</th>
              <th rowspan="1" colspan="1">Average</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">K-mer + MLP</td>
              <td rowspan="1" colspan="1">0.670</td>
              <td rowspan="1" colspan="1">0.651</td>
              <td rowspan="1" colspan="1">0.645</td>
              <td rowspan="1" colspan="1">0.648</td>
              <td rowspan="1" colspan="1">0.680</td>
              <td rowspan="1" colspan="1">0.636</td>
              <td rowspan="1" colspan="1">0.616</td>
              <td rowspan="1" colspan="1">0.582</td>
              <td rowspan="1" colspan="1">0.629</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Word2vec + MLP</td>
              <td rowspan="1" colspan="1">0.690</td>
              <td rowspan="1" colspan="1">0.664</td>
              <td rowspan="1" colspan="1">0.660</td>
              <td rowspan="1" colspan="1">0.662</td>
              <td rowspan="1" colspan="1">0.660</td>
              <td rowspan="1" colspan="1">0.580</td>
              <td rowspan="1" colspan="1">0.561</td>
              <td rowspan="1" colspan="1">0.526</td>
              <td rowspan="1" colspan="1">0.582</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Word2vec + CNN + MLP</td>
              <td rowspan="1" colspan="1">0.697</td>
              <td rowspan="1" colspan="1">0.692</td>
              <td rowspan="1" colspan="1">0.652</td>
              <td rowspan="1" colspan="1">0.672</td>
              <td rowspan="1" colspan="1">0.679</td>
              <td rowspan="1" colspan="1">0.591</td>
              <td rowspan="1" colspan="1">0.610</td>
              <td rowspan="1" colspan="1">0.582</td>
              <td rowspan="1" colspan="1">0.616</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Word2vec + Bi-LSTM + MLP</td>
              <td rowspan="1" colspan="1">0.689</td>
              <td rowspan="1" colspan="1">0.670</td>
              <td rowspan="1" colspan="1">0.653</td>
              <td rowspan="1" colspan="1">0.661</td>
              <td rowspan="1" colspan="1">0.682</td>
              <td rowspan="1" colspan="1">0.595</td>
              <td rowspan="1" colspan="1">0.597</td>
              <td rowspan="1" colspan="1">0.622</td>
              <td rowspan="1" colspan="1">0.624</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GloVe + MLP</td>
              <td rowspan="1" colspan="1">0.697</td>
              <td rowspan="1" colspan="1">0.685</td>
              <td rowspan="1" colspan="1">0.656</td>
              <td rowspan="1" colspan="1">0.670</td>
              <td rowspan="1" colspan="1">0.662</td>
              <td rowspan="1" colspan="1">0.560</td>
              <td rowspan="1" colspan="1">0.588</td>
              <td rowspan="1" colspan="1">0.574</td>
              <td rowspan="1" colspan="1">0.596</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GloVe + CNN + MLP</td>
              <td rowspan="1" colspan="1">0.697</td>
              <td rowspan="1" colspan="1">0.674</td>
              <td rowspan="1" colspan="1">0.677</td>
              <td rowspan="1" colspan="1">0.675</td>
              <td rowspan="1" colspan="1">0.668</td>
              <td rowspan="1" colspan="1">0.636</td>
              <td rowspan="1" colspan="1">0.580</td>
              <td rowspan="1" colspan="1">0.541</td>
              <td rowspan="1" colspan="1">0.606</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GloVe + Bi-LSTM + MLP</td>
              <td rowspan="1" colspan="1">0.679</td>
              <td rowspan="1" colspan="1">
                <bold>0.712</bold>
              </td>
              <td rowspan="1" colspan="1">0.602</td>
              <td rowspan="1" colspan="1">0.652</td>
              <td rowspan="1" colspan="1">0.676</td>
              <td rowspan="1" colspan="1">0.573</td>
              <td rowspan="1" colspan="1">0.570</td>
              <td rowspan="1" colspan="1">0.510</td>
              <td rowspan="1" colspan="1">0.582</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LncLocFormer</td>
              <td rowspan="1" colspan="1">
                <bold>0.719</bold>
              </td>
              <td rowspan="1" colspan="1">0.683</td>
              <td rowspan="1" colspan="1">
                <bold>0.721</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.701</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.686</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.651</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.623</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.632</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.648</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <label>a</label>
            <p>The best performance values are highlighted in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.2 Comparison with existing predictors</title>
      <p>In the previous section, we performed 5-fold CV to obtain the best parameters and evaluated the performance of LncLocFormer with other deep learning baseline models. To further evaluate the performance of LncLocFormer in predicting lncRNA subcellular localizations, we compared LncLocFormer with several existing state-of-the-art predictors by using a hold-out test set. In particular, we selected the current predictors follow these criteria: (i) the availability of web server or stand-alone version; (ii) input that only needs lncRNA sequences; and (iii) outputs that include predictive probabilities for subcellular localization. Finally, we used the following web servers for comparison: lncLocator (<ext-link xlink:href="http://www.csbio.sjtu.edu.cn/bioinf/lncLocator/" ext-link-type="uri">http://www.csbio.sjtu.edu.cn/bioinf/lncLocator/</ext-link>), iLoc-lncRNA (<ext-link xlink:href="http://lin-group.cn/server/iLoc-LncRNA/" ext-link-type="uri">http://lin-group.cn/server/iLoc-LncRNA/</ext-link>), Locate-R (<ext-link xlink:href="http://locate-r.azurewebsites.net" ext-link-type="uri">http://locate-r.azurewebsites.net</ext-link>), DeepLncLoc (<ext-link xlink:href="http://bioinformatics.csu.edu.cn/DeepLncLoc/" ext-link-type="uri">http://bioinformatics.csu.edu.cn/DeepLncLoc/</ext-link>), iLoc-LncRNA(2.0) (<ext-link xlink:href="http://lin-group.cn/server/iLoc-LncRNA" ext-link-type="uri">http://lin-group.cn/server/iLoc-LncRNA</ext-link>(2.0)/), and GraphLncLoc (<ext-link xlink:href="http://csuligroup.com:8000/GraphLncLoc/" ext-link-type="uri">http://csuligroup.com:8000/GraphLncLoc/</ext-link>). The detailed prediction results on the hold-out test set are shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>. The P@1 of LncLocFormer and existing predictors on the hold-out test set is shown in <xref rid="btad752-T2" ref-type="table">Table 2</xref>.</p>
      <table-wrap position="float" id="btad752-T2">
        <label>Table 2.</label>
        <caption>
          <p>P@1 of LncLocFormer and existing predictors on the hold-out test set (RNALocate v2.0).<xref rid="tblfn2" ref-type="table-fn"><sup>a</sup></xref></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Predictor</th>
              <th rowspan="1" colspan="1">P@1</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">lncLocator</td>
              <td rowspan="1" colspan="1">0.232</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">iLoc-LncRNA</td>
              <td rowspan="1" colspan="1">0.348</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Locate-R</td>
              <td rowspan="1" colspan="1">0.275</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepLncLoc</td>
              <td rowspan="1" colspan="1">0.304</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">iLoc-LncRNA(2.0)</td>
              <td rowspan="1" colspan="1">0.333</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GraphLncLoc</td>
              <td rowspan="1" colspan="1">0.536</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LncLocFormer</td>
              <td rowspan="1" colspan="1">
                <bold>0.899</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <label>a</label>
            <p>The best performance values are highlighted in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>From <xref rid="btad752-T2" ref-type="table">Table 2</xref>, we can observe that LncLocFormer significantly outperforms existing predictors. Specifically, LncLocFormer obtains P@1 of 0.899, which is much higher than that of lncLocator (0.232), iLoc-LncRNA (0.348), Locate-R (0.275), DeepLncLoc (0.304), iLoc-LncRNA(2.0) (0.333), and GraphLncLoc (0.536). These results demonstrate that LncLocFormer has a powerful ability in predicting multi-label lncRNA subcellular localizations and achieves state-of-the-art performance on the hold-out test set. However, a natural question arises: why is there such a significant gap between LncLocFormer and the other predictors? We believe that it is impossible to achieve this by relying only on model architecture. One of the most possible explanations is that the used datasets are different. lncLocator, iLoc-LncRNA, Locate-R, DeepLncLoc, iLoc-LncRNA(2.0), and GraphLncLoc all used the RNALocate v1.0 database (<xref rid="btad752-B40" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> 2017</xref>) to train and test their models, while LncLocFormer is trained and tested by using the RNALocate v2.0 database (<xref rid="btad752-B8" ref-type="bibr">Cui <italic toggle="yes">et al.</italic> 2022</xref>). Therefore, we believe that the difference between the two datasets is the main reason for the large gap between LncLocFormer and other predictors.</p>
      <p>To investigate the difference between the two datasets, we plotted the distributions of the RNALocate v1.0 dataset used in the six predictors and the RNALocate v2.0 dataset used in LncLocFormer, as shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S2</xref>. By comparing <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S2a</xref> with b, we can observe that there are significant differences between the RNALocate v2.0 and RNALocate v1.0 datasets. For example, in the RNALocate v2.0 dataset, the number of lncRNAs located in the nucleus is much greater than the number of lncRNAs located in the cytoplasm. In contrast, in the RNALocate v1.0 dataset, the number of lncRNAs located in the cytoplasm is slightly larger than the number of lncRNAs located in the nucleus. These findings demonstrate that the data from the two datasets are not independently identical distributed (i.i.d.).</p>
      <p>To make a fairer comparison and to prove the superiority of LncLocFormer architecture, we used the RNALocate v1.0 database to retrain our model. Specifically, we employed the same training set and test set utilized in our previous predictor, DeepLncLoc, to retrain and test LncLocFormer. Since the dataset is generated for the multi-class prediction problem, we used a softmax activation function to replace the sigmoid activation function in the final fully connected layer to perform the multi-class prediction task. As with previous studies, we used ACC, MaF, MaP, and MaR as evaluation metrics to evaluate LncLocFormer and the existing predictors. The detailed prediction results on the RNALocate v1.0 test set are shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S3</xref>. The performance of LncLocFormer (using the RNALocate v1.0 dataset for training and test) and the existing predictors is shown in <xref rid="btad752-T3" ref-type="table">Table 3</xref>. In <xref rid="btad752-T3" ref-type="table">Table 3</xref>, the evaluation metrics we pay most attention to are MaF and ACC. From <xref rid="btad752-T3" ref-type="table">Table 3</xref>, we can observe that LncLocFormer still outperforms existing predictors in terms of MaF and ACC.</p>
      <table-wrap position="float" id="btad752-T3">
        <label>Table 3.</label>
        <caption>
          <p>Performance comparison of LncLocFormer (using the RNALocate v1.0 dataset for training and test) with the existing predictors.<xref rid="tblfn3" ref-type="table-fn"><sup>a</sup></xref></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Predictor</th>
              <th rowspan="1" colspan="1">MaP</th>
              <th rowspan="1" colspan="1">MaR</th>
              <th rowspan="1" colspan="1">MaF</th>
              <th rowspan="1" colspan="1">ACC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">lncLocator</td>
              <td rowspan="1" colspan="1">0.288</td>
              <td rowspan="1" colspan="1">0.292</td>
              <td rowspan="1" colspan="1">0.276</td>
              <td rowspan="1" colspan="1">0.433</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">iLoc-lncRNA</td>
              <td rowspan="1" colspan="1">0.488</td>
              <td rowspan="1" colspan="1">0.445</td>
              <td rowspan="1" colspan="1">0.458</td>
              <td rowspan="1" colspan="1">0.507</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Locate-R</td>
              <td rowspan="1" colspan="1">0.374</td>
              <td rowspan="1" colspan="1">0.317</td>
              <td rowspan="1" colspan="1">0.329</td>
              <td rowspan="1" colspan="1">0.403</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepLncLoc</td>
              <td rowspan="1" colspan="1">0.680</td>
              <td rowspan="1" colspan="1">0.543</td>
              <td rowspan="1" colspan="1">0.563</td>
              <td rowspan="1" colspan="1">0.537</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">iLoc-LncRNA(2.0)</td>
              <td rowspan="1" colspan="1">0.460</td>
              <td rowspan="1" colspan="1">0.384</td>
              <td rowspan="1" colspan="1">0.390</td>
              <td rowspan="1" colspan="1">0.433</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GraphLncLoc</td>
              <td rowspan="1" colspan="1">
                <bold>0.731</bold>
              </td>
              <td rowspan="1" colspan="1">0.549</td>
              <td rowspan="1" colspan="1">0.560</td>
              <td rowspan="1" colspan="1">0.522</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LncLocFormer</td>
              <td rowspan="1" colspan="1">0.696</td>
              <td rowspan="1" colspan="1">
                <bold>0.566</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.597</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.612</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn3">
            <label>a</label>
            <p>The best performance values are highlighted in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.3 Motif analysis</title>
      <p>In the study, we designed a localization-specific attention mechanism to obtain distinct attention weights for each subcellular localization and find the most likely motifs in lncRNA sequences. To investigate the performance of localization-specific attention mechanism in LncLocFormer, we conducted some motif analyses.</p>
      <p>First, we tested whether LncLocFormer could find the most frequently recurring motifs. In particular, we used the MEME suite (<xref rid="btad752-B2" ref-type="bibr">Bailey <italic toggle="yes">et al.</italic> 2009</xref>) to find the motifs in our dataset. The motifs are analyzed with the width of nine, and the <italic toggle="yes">E</italic>-value is set to 0.05. We used a threshold to determine the importance of attention weights. The threshold is set to the multiplicative inverse of the input sequence length. Because if the attention weights on the lncRNA sequence are randomly distributed, the mathematic expectation of all attention weights on the lncRNA sequence is the multiplicative inverse of the input sequence length. Only if the attention weight of a nucleotide is larger than the mathematic expectation, we believe that LncLocFormer pays attention to the nucleotide. If the attention weight of a nucleotide is smaller than the mathematic expectation, we believe that LncLocFormer does not focus on the nucleotide. <xref rid="btad752-F3" ref-type="fig">Figure 3</xref> displays the representative examples, with the left column depicting the motifs found by the MEME suite, the middle column showing the motifs discovered by LncLocFormer, and the right column displaying the <italic toggle="yes">E</italic>-values of the motifs found by the MEME suite. From <xref rid="btad752-F3" ref-type="fig">Fig. 3</xref>, we can observe that LncLocFormer can capture the motifs that are similar to those found by the MEME suite, which means LncLocFormer can capture the most frequently recurring motifs.</p>
      <fig position="float" id="btad752-F3">
        <label>Figure 3.</label>
        <caption>
          <p>Motifs discovered by MEME suite (left) and by LncLocFormer (middle). The right are the <italic toggle="yes">E</italic>-values of the motifs found by the MEME suite.</p>
        </caption>
        <graphic xlink:href="btad752f3" position="float"/>
      </fig>
      <p>Second, we investigated whether LncLocFormer could capture some known motifs. Specifically, we searched for some known motifs in recent literature that are related to subcellular localization. Lubelsky <italic toggle="yes">et al.</italic> (<xref rid="btad752-B22" ref-type="bibr">Lubelsky and Ulitsky 2018</xref>) found that the repeated motif RCCTCCC (where R denotes A/G) drives lncRNAs to be located in the nucleus. <xref rid="btad752-B38" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> (2014)</xref> identified the motif AGCCC act as a general nucleus localization signal. We used motifs RCCTCCC and AGCCC as examples to show the performance of LncLocFormer. The captured motifs by LncLocFormer for nucleus are shown in <xref rid="btad752-F4" ref-type="fig">Fig. 4</xref>. From <xref rid="btad752-F4" ref-type="fig">Fig. 4</xref>, we can observe that LncLocFormer can capture motifs that are similar to those that are already known.</p>
      <fig position="float" id="btad752-F4">
        <label>Figure 4.</label>
        <caption>
          <p>LncLocFormer captures two known motifs, which are related to nucleus localization.</p>
        </caption>
        <graphic xlink:href="btad752f4" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.4 Case study</title>
      <p>To better understand the role of the localization-specific attention, we visualized the attention matrices <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">and</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:math></inline-formula> which are computed by <xref rid="E7" ref-type="disp-formula">Equation (7)</xref> and gave a case study in <xref rid="btad752-F5" ref-type="fig">Fig. 5</xref> using lncRNA Cerox1 (cytoplasmic endogenous regulator of oxidative phosphorylation 1, NCBI ID: 115804232) as an example. The true label of lncRNA Cerox1 is nucleus. According to <xref rid="btad752-B38" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> (2014)</xref>, the motif AGCCC act as a general nucleus localization signal. We obtained four kinds of attention weights of different subcellular localizations, and highlighted the sequence using different degrees of red based on the values of attention weights. From <xref rid="btad752-F5" ref-type="fig">Fig. 5</xref>, we can observe that the attention matrix of nucleus captures an important region containing the core motif (AGCCC), while the attention matrices of other subcellular localizations fail to capture the motif AGCCC. Although LncLocFormer cannot find the exact known motifs, it can capture motifs that are very similar to the known motifs. The results suggest the potential of LncLocFormer in motif discovery.</p>
      <fig position="float" id="btad752-F5">
        <label>Figure 5.</label>
        <caption>
          <p>Attention weight visualization of lncRNA Cerox1.</p>
        </caption>
        <graphic xlink:href="btad752f5" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.5 Ablation study</title>
      <p>In order to discover the essential components of LncLocFormer, we conducted an ablation study by removing individual parts of LncLocFormer. In particular, we tested the model without localization-specific attention and the model without positional encoding. The former model lost multiple attention weights for different subcellular localizations, while the latter model lost the sequence order information. The results are shown in <xref rid="btad752-T4" ref-type="table">Table 4</xref>. In <xref rid="btad752-T4" ref-type="table">Table 4</xref>, the evaluation metrics we pay most attention to are Ave-<italic toggle="yes">F</italic>1, MiF, and average AUC. From <xref rid="btad752-T4" ref-type="table">Table 4</xref>, we can observe that localization-specific attention is the most important part of LncLocFormer. Without localization-specific attention, Ave-<italic toggle="yes">F</italic>1, MiF, and average AUC decrease from 0.719, 0.701, and 0.648 to 0.627, 0.591, and 0.614, respectively. Additionally, positional encoding is also useful in LncLocFormer. Without positional encoding, Ave-<italic toggle="yes">F</italic>1, MiF, and average AUC decrease from 0.719, 0.701, and 0.648 to 0.710, 0.691, and 0.643, respectively. The results confirm the effectiveness of localization-specific attention and positional encoding in LncLocFormer.</p>
      <table-wrap position="float" id="btad752-T4">
        <label>Table 4.</label>
        <caption>
          <p>The performances of various models in the ablation study.<xref rid="tblfn4" ref-type="table-fn"><sup>a</sup></xref></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="2" colspan="1">Model</th>
              <th rowspan="2" colspan="1">Ave-<italic toggle="yes">F</italic>1</th>
              <th rowspan="2" colspan="1">MiP</th>
              <th rowspan="2" colspan="1">MiR</th>
              <th rowspan="2" colspan="1">MiF</th>
              <th colspan="5" rowspan="1">AUC<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Nucleus</th>
              <th rowspan="1" colspan="1">Cytoplasm</th>
              <th rowspan="1" colspan="1">Chromatin</th>
              <th rowspan="1" colspan="1">Insoluble cytoplasm</th>
              <th rowspan="1" colspan="1">Average</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Without localization-specific attention</td>
              <td rowspan="1" colspan="1">0.627</td>
              <td rowspan="1" colspan="1">0.550</td>
              <td rowspan="1" colspan="1">0.640</td>
              <td rowspan="1" colspan="1">0.591</td>
              <td rowspan="1" colspan="1">
                <bold>0.688</bold>
              </td>
              <td rowspan="1" colspan="1">0.638</td>
              <td rowspan="1" colspan="1">0.599</td>
              <td rowspan="1" colspan="1">0.532</td>
              <td rowspan="1" colspan="1">0.614</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Without positional encoding</td>
              <td rowspan="1" colspan="1">0.710</td>
              <td rowspan="1" colspan="1">0.653</td>
              <td rowspan="1" colspan="1">
                <bold>0.734</bold>
              </td>
              <td rowspan="1" colspan="1">0.691</td>
              <td rowspan="1" colspan="1">0.672</td>
              <td rowspan="1" colspan="1">
                <bold>0.660</bold>
              </td>
              <td rowspan="1" colspan="1">0.611</td>
              <td rowspan="1" colspan="1">0.628</td>
              <td rowspan="1" colspan="1">0.643</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LncLocFormer</td>
              <td rowspan="1" colspan="1">
                <bold>0.719</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.683</bold>
              </td>
              <td rowspan="1" colspan="1">0.721</td>
              <td rowspan="1" colspan="1">
                <bold>0.701</bold>
              </td>
              <td rowspan="1" colspan="1">0.686</td>
              <td rowspan="1" colspan="1">0.651</td>
              <td rowspan="1" colspan="1">
                <bold>0.623</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.632</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.648</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn4">
            <label>a</label>
            <p>The best performance values are highlighted in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>In addition, we observed an interesting phenomenon, i.e. models without localization-specific attention can produce better results for the nucleus localization, while models without positional encoding can produce better results for the cytoplasm localization. Regarding the better results for nucleus localization with models lacking localization-specific attention, one possible explanation is that the benchmark dataset is imbalanced. The nucleus localization has a larger number of samples compared to the other three classes. When localization-specific attention is removed, the model may exhibit a bias toward the classes with more samples because this can lead to higher overall accuracy. This bias could result in improved predictions for the nucleus localization while leading to poorer predictions for the other three subcellular localizations. As for the better results for cytoplasm localization with models lacking positional encoding, the possible reason is that the lncRNA sequences belonging to the cytoplasm localization in the benchmark dataset are often quite long. In very long sequences, the relative position encoding may not effectively capture the relative distance relationship between nucleotides and may forget what has been learned in the sequence. Instead, the addition of relative position encoding may introduce noise or unnecessary information, resulting in a decline in prediction performance for the cytoplasm localization.</p>
    </sec>
    <sec>
      <title>3.6 Web server</title>
      <p>To facilitate the use of LncLocFormer, we developed a user-friendly web server, <ext-link xlink:href="http://csuligroup.com:9000/LncLocFormer" ext-link-type="uri">http://csuligroup.com:9000/LncLocFormer</ext-link>. LncLocFormer requires lncRNA sequences with more than 200 and &lt;10 000 nucleotides as input. Users can paste the lncRNA sequence into the input box and click on the submit button to see the predicted results. For each lncRNA sequence, the predicted probabilities and attention weights for each subcellular localization are displayed on the screen. In general, LncLocFormer takes &lt;10 s to predict the subcellular localization of a given lncRNA sequence. We believe that LncLocFormer is a convenient and efficient tool in the field of lncRNA subcellular localization prediction.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p>In the study, we proposed LncLocFormer, a multi-label lncRNA subcellular localization predictor that utilizes Transformer and localization-specific attention mechanism. Unlike many previous computational methods that only consider a single subcellular localization for a lncRNA sequence, LncLocFormer can predict multiple subcellular localizations simultaneously for each lncRNA sequence. Due to the uncertainty of the number of labels for each lncRNA sequence and the implicit relationship between the labels, the multi-label classification problem is more complicated than conventional multi-class classification tasks. By using Transformer blocks and localization-specific attention mechanism, LncLocFormer can predict lncRNA multiple subcellular localizations accurately, learn different motifs for each subcellular localization, and capture some motifs that are very similar to known motifs. Our extensive experimental results demonstrate that LncLocFormer outperforms existing state-of-the-art predictors. We believe that LncLocFormer can serve as a useful tool for predicting lncRNA multiple subcellular localizations.</p>
    <p>Although LncLocFormer shows promising results, there are some limitations that may influence the performance of LncLocFormer. The performance of LncLocFormer is limited by the number of samples in the RNALocate dataset. In the study, we only have 811 samples for the multi-label classification task. With lncRNA subcellular localization becoming a more important research topic, we could obtain more reliable data that can be used for training and test. Alternatively, we could consider transferring some data from other domains to aid in the research topic.</p>
    <p>Furthermore, LncLocFormer utilizes eight Transformer blocks and localization-specific attention, resulting in a lot of parameters that need to be tuned. Consequently, the training time of LncLocFormer is very long. With the development of deep learning techniques, more and more advanced knowledge distillation and network pruning techniques will be proposed. As a result, using a lightweight network architecture to predict lncRNA subcellular localization is a promising future direction.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btad752_Supplementary_Data</label>
      <media xlink:href="btad752_supplementary_data.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <sec>
    <title>Supplementary data</title>
    <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Conflict of interest</title>
    <p>None declared.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the National Key Research and Development Program of China [No. 2022YFC3400300]; the National Natural Science Foundation of China [No. 62102457]; Hunan Provincial Natural Science Foundation of China [No. 2023JJ40763]; Hunan Provincial Science and Technology Program [No. 2021RC4008]; and the Fundamental Research Funds for the Central Universities of Central South University [No. 2023ZZTS0627]. This work was carried out in part using computing resources at the High Performance Computing Center of Central South University.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btad752-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ahmad</surname><given-names>A</given-names></string-name>, <string-name><surname>Lin</surname><given-names>H</given-names></string-name>, <string-name><surname>Shatabda</surname><given-names>S.</given-names></string-name></person-group><article-title>Locate-R: subcellular localization of long non-coding RNAs using nucleotide compositions</article-title>. <source>Genomics</source><year>2020</year>;<volume>112</volume>:<fpage>2583</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">32068122</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bailey</surname><given-names>TL</given-names></string-name>, <string-name><surname>Boden</surname><given-names>M</given-names></string-name>, <string-name><surname>Buske</surname><given-names>FA</given-names></string-name></person-group><etal>et al</etal><article-title>MEME SUITE: tools for motif discovery and searching</article-title>. <source>Nucleic Acids Res</source><year>2009</year>;<volume>37</volume>:<fpage>W202</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">19458158</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B3">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bhojanapalli</surname><given-names>S</given-names></string-name>, <string-name><surname>Yun</surname><given-names>C</given-names></string-name>, <string-name><surname>Rawat</surname><given-names>AS</given-names></string-name></person-group><etal>et al</etal> Low-rank bottleneck in multi-head attention models. In: <italic toggle="yes">International conference on machine learning, online event, </italic>Vol. 119<italic toggle="yes">.</italic> PMLR. <fpage>864</fpage>–<lpage>73</lpage>.</mixed-citation>
    </ref>
    <ref id="btad752-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Birney</surname><given-names>E</given-names></string-name>, <string-name><surname>Stamatoyannopoulos</surname><given-names>JA</given-names></string-name>, <string-name><surname>Dutta</surname><given-names>A</given-names></string-name></person-group><etal>et al</etal>; <collab>Children's Hospital Oakland Research Institute</collab>. <article-title>Identification and analysis of functional elements in 1% of the human genome by the ENCODE pilot project</article-title>. <source>Nature</source><year>2007</year>;<volume>447</volume>:<fpage>799</fpage>–<lpage>816</lpage>.<pub-id pub-id-type="pmid">17571346</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bridges</surname><given-names>MC</given-names></string-name>, <string-name><surname>Daulagala</surname><given-names>AC</given-names></string-name>, <string-name><surname>Kourtidis</surname><given-names>A.</given-names></string-name></person-group><article-title>LNCcation: lncRNA localization and function</article-title>. <source>J Cell Biol</source><year>2021</year>;<volume>220</volume>:<fpage>e202009045</fpage>.<pub-id pub-id-type="pmid">33464299</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cao</surname><given-names>Z</given-names></string-name>, <string-name><surname>Pan</surname><given-names>X</given-names></string-name>, <string-name><surname>Yang</surname><given-names>Y</given-names></string-name></person-group><etal>et al</etal><article-title>The lncLocator: a subcellular localization predictor for long non-coding RNAs based on a stacked ensemble classifier</article-title>. <source>Bioinformatics</source><year>2018</year>;<volume>34</volume>:<fpage>2185</fpage>–<lpage>94</lpage>.<pub-id pub-id-type="pmid">29462250</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carlevaro-Fita</surname><given-names>J</given-names></string-name>, <string-name><surname>Johnson</surname><given-names>R.</given-names></string-name></person-group><article-title>Global positioning system: understanding long noncoding RNAs through subcellular localization</article-title>. <source>Mol Cell</source><year>2019</year>;<volume>73</volume>:<fpage>869</fpage>–<lpage>83</lpage>.<pub-id pub-id-type="pmid">30849394</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cui</surname><given-names>T</given-names></string-name>, <string-name><surname>Dou</surname><given-names>Y</given-names></string-name>, <string-name><surname>Tan</surname><given-names>P</given-names></string-name></person-group><etal>et al</etal><article-title>RNALocate v2.0: an updated resource for RNA subcellular localization with increased coverage and annotation</article-title>. <source>Nucleic Acids Res</source><year>2022</year>;<volume>50</volume>:<fpage>D333</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">34551440</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>DiStefano</surname><given-names>JK.</given-names></string-name></person-group><article-title>The emerging role of long noncoding RNAs in human disease</article-title>. <source>Methods Mol Biol</source><year>2018</year>;<volume>1706</volume>:<fpage>91</fpage>–<lpage>110</lpage>.<pub-id pub-id-type="pmid">29423795</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Esteller</surname><given-names>M.</given-names></string-name></person-group><article-title>Non-coding RNAs in human disease</article-title>. <source>Nat Rev Genet</source><year>2011</year>;<volume>12</volume>:<fpage>861</fpage>–<lpage>74</lpage>.<pub-id pub-id-type="pmid">22094949</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fan</surname><given-names>YX</given-names></string-name>, <string-name><surname>Chen</surname><given-names>MJ</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>QQ.</given-names></string-name></person-group><article-title>lncLocPred: predicting LncRNA subcellular localization using multiple sequence feature information</article-title>. <source>IEEE Access</source><year>2020</year>;<volume>8</volume>:<fpage>124702</fpage>–<lpage>11</lpage>.</mixed-citation>
    </ref>
    <ref id="btad752-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feng</surname><given-names>S</given-names></string-name>, <string-name><surname>Liang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Du</surname><given-names>W</given-names></string-name></person-group><etal>et al</etal><article-title>LncLocation: efficient subcellular location prediction of long non-coding RNA-based multi-source heterogeneous feature fusion</article-title>. <source>Int J Mol Sci</source><year>2020</year>;<volume>21</volume>:<fpage>7221</fpage>.<pub-id pub-id-type="pmid">33007849</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gudenas</surname><given-names>BL</given-names></string-name>, <string-name><surname>Wang</surname><given-names>L.</given-names></string-name></person-group><article-title>Prediction of LncRNA subcellular localization with deep learning from sequence features</article-title>. <source>Sci Rep</source><year>2018</year>;<volume>8</volume>:<fpage>16385</fpage>.<pub-id pub-id-type="pmid">30401954</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Niu</surname><given-names>B</given-names></string-name>, <string-name><surname>Gao</surname><given-names>Y</given-names></string-name></person-group><etal>et al</etal><article-title>CD-HIT suite: a web server for clustering and comparing biological sequences</article-title>. <source>Bioinformatics</source><year>2010</year>;<volume>26</volume>:<fpage>680</fpage>–<lpage>2</lpage>.<pub-id pub-id-type="pmid">20053844</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jeon</surname><given-names>YJ</given-names></string-name>, <string-name><surname>Hasan</surname><given-names>MM</given-names></string-name>, <string-name><surname>Park</surname><given-names>HW</given-names></string-name></person-group><etal>et al</etal><article-title>TACOS: a novel approach for accurate prediction of cell-specific long noncoding RNAs subcellular localization</article-title>. <source>Brief Bioinform</source><year>2022</year>;<volume>23</volume>:<fpage>bbac243</fpage>.<pub-id pub-id-type="pmid">35753698</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>M</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>B</given-names></string-name>, <string-name><surname>Yin</surname><given-names>R</given-names></string-name></person-group><etal>et al</etal><article-title>GraphLncLoc: long non-coding RNA subcellular localization prediction using graph convolutional networks based on sequence to graph transformation</article-title>. <source>Brief Bioinform</source><year>2023</year>;<volume>24</volume>:<fpage>bbac565</fpage>.<pub-id pub-id-type="pmid">36545797</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>M</given-names></string-name>, <string-name><surname>Fei</surname><given-names>Z</given-names></string-name>, <string-name><surname>Zeng</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Automated ICD-9 coding via a deep learning approach</article-title>. <source>IEEE/ACM Trans Comput Biol Bioinform</source><year>2019</year>;<volume>16</volume>:<fpage>1193</fpage>–<lpage>202</lpage>.<pub-id pub-id-type="pmid">29994157</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>Y</given-names></string-name>, <string-name><surname>Zeng</surname><given-names>M</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>DeepCellEss: cell line-specific essential protein prediction with attention-based interpretable deep learning</article-title>. <source>Bioinformatics</source><year>2023</year>;<volume>39</volume>:<fpage>btac779</fpage>.<pub-id pub-id-type="pmid">36458923</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>Y</given-names></string-name>, <string-name><surname>Pan</surname><given-names>X</given-names></string-name>, <string-name><surname>Shen</surname><given-names>HB.</given-names></string-name></person-group><article-title>lncLocator 2.0: a cell-line-specific subcellular localization predictor for long non-coding RNAs with interpretable deep learning</article-title>. <source>Bioinformatics</source><year>2021</year>;<volume>37</volume>:<fpage>2308</fpage>–<lpage>16</lpage>.<pub-id pub-id-type="pmid">33630066</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lu</surname><given-names>C</given-names></string-name>, <string-name><surname>Yang</surname><given-names>M</given-names></string-name>, <string-name><surname>Li</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Predicting human lncRNA-disease associations based on geometric matrix completion</article-title>. <source>IEEE J Biomed Health Inform</source><year>2020</year>;<volume>24</volume>:<fpage>2420</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">31825885</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lu</surname><given-names>C</given-names></string-name>, <string-name><surname>Yang</surname><given-names>M</given-names></string-name>, <string-name><surname>Luo</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>Prediction of lncRNA-disease associations based on inductive matrix completion</article-title>. <source>Bioinformatics</source><year>2018</year>;<volume>34</volume>:<fpage>3357</fpage>–<lpage>64</lpage>.<pub-id pub-id-type="pmid">29718113</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lubelsky</surname><given-names>Y</given-names></string-name>, <string-name><surname>Ulitsky</surname><given-names>I.</given-names></string-name></person-group><article-title>Sequences enriched in Alu repeats drive nuclear localization of long RNAs in human cells</article-title>. <source>Nature</source><year>2018</year>;<volume>555</volume>:<fpage>107</fpage>–<lpage>11</lpage>.<pub-id pub-id-type="pmid">29466324</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B23">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Mikolov</surname><given-names>T</given-names></string-name>, <string-name><surname>Chen</surname><given-names>K</given-names></string-name>, <string-name><surname>Corrado</surname><given-names>G</given-names></string-name></person-group><etal>et al</etal> Efficient estimation of word representations in vector space. arXiv, arXiv:1301.3781. <year>2013</year>.</mixed-citation>
    </ref>
    <ref id="btad752-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moffitt</surname><given-names>JR</given-names></string-name>, <string-name><surname>Zhuang</surname><given-names>X.</given-names></string-name></person-group><article-title>RNA imaging with multiplexed Error-Robust fluorescence in situ hybridization (MERFISH)</article-title>. <source>Methods Enzymol</source><year>2016</year>;<volume>572</volume>:<fpage>1</fpage>–<lpage>49</lpage>.<pub-id pub-id-type="pmid">27241748</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Paszke</surname><given-names>A</given-names></string-name>, <string-name><surname>Gross</surname><given-names>S</given-names></string-name>, <string-name><surname>Massa</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>PyTorch: an imperative style, high-performance deep learning library</article-title>. <source>Adv Neural Inf Process Syst</source> 2019;<volume>32</volume>.</mixed-citation>
    </ref>
    <ref id="btad752-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pruitt</surname><given-names>KD</given-names></string-name>, <string-name><surname>Tatusova</surname><given-names>T</given-names></string-name>, <string-name><surname>Maglott</surname><given-names>DR.</given-names></string-name></person-group><article-title>NCBI reference sequences (RefSeq): a curated non-redundant sequence database of genomes, transcripts and proteins</article-title>. <source>Nucleic Acids Res</source><year>2007</year>;<volume>35</volume>:<fpage>D61</fpage>–<lpage>5</lpage>.<pub-id pub-id-type="pmid">17130148</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Riva</surname><given-names>P</given-names></string-name>, <string-name><surname>Ratti</surname><given-names>A</given-names></string-name>, <string-name><surname>Venturin</surname><given-names>M.</given-names></string-name></person-group><article-title>The long non-coding RNAs in neurodegenerative diseases: novel mechanisms of pathogenesis</article-title>. <source>CAR</source><year>2016</year>;<volume>13</volume>:<fpage>1219</fpage>–<lpage>31</lpage>.</mixed-citation>
    </ref>
    <ref id="btad752-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Savulescu</surname><given-names>AF</given-names></string-name>, <string-name><surname>Bouilhol</surname><given-names>E</given-names></string-name>, <string-name><surname>Beaume</surname><given-names>N</given-names></string-name></person-group><etal>et al</etal><article-title>Prediction of RNA subcellular localization: learning from heterogeneous data sources</article-title>. <source>iScience</source><year>2021</year>;<volume>24</volume>:<fpage>103298</fpage>.<pub-id pub-id-type="pmid">34765919</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shukla</surname><given-names>CJ</given-names></string-name>, <string-name><surname>McCorkindale</surname><given-names>AL</given-names></string-name>, <string-name><surname>Gerhardinger</surname><given-names>C</given-names></string-name></person-group><etal>et al</etal><article-title>High-throughput identification of RNA nuclear enrichment sequences</article-title>. <source>EMBO J</source><year>2018</year>;<volume>37</volume>:<fpage>e98452</fpage>.<pub-id pub-id-type="pmid">29335281</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Su</surname><given-names>Z-D</given-names></string-name>, <string-name><surname>Huang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Z-Y</given-names></string-name></person-group><etal>et al</etal><article-title>iLoc-lncRNA: predict the subcellular location of lncRNAs by incorporating octamer composition into general PseKNC</article-title>. <source>Bioinformatics</source><year>2018</year>;<volume>34</volume>:<fpage>4196</fpage>–<lpage>204</lpage>.<pub-id pub-id-type="pmid">29931187</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>KC</given-names></string-name>, <string-name><surname>Chang</surname><given-names>HY.</given-names></string-name></person-group><article-title>Molecular mechanisms of long noncoding RNAs</article-title>. <source>Mol Cell</source><year>2011</year>;<volume>43</volume>:<fpage>904</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">21925379</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Gao</surname><given-names>M</given-names></string-name>, <string-name><surname>Zeng</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>BridgeDPI: a novel graph neural network for predicting drug-protein interactions</article-title>. <source>Bioinformatics</source><year>2022</year>;<volume>38</volume>:<fpage>2571</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">35274672</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>You</surname><given-names>R</given-names></string-name>, <string-name><surname>Liu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Mamitsuka</surname><given-names>H</given-names></string-name></person-group><etal>et al</etal><article-title>BERTMeSH: deep contextual representation learning for large-scale high-performance MeSH indexing with full text</article-title>. <source>Bioinformatics</source><year>2021</year>;<volume>37</volume>:<fpage>684</fpage>–<lpage>92</lpage>.<pub-id pub-id-type="pmid">32976559</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yuan</surname><given-names>GH</given-names></string-name>, <string-name><surname>Wang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname><given-names>GZ</given-names></string-name></person-group><etal>et al</etal><article-title>RNAlight: a machine learning model to identify nucleotide features determining RNA subcellular localization</article-title>. <source>Brief Bioinform</source><year>2023</year>;<volume>24</volume>:<fpage>bbac509</fpage>.<pub-id pub-id-type="pmid">36464487</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname><given-names>M</given-names></string-name>, <string-name><surname>Wu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Lu</surname><given-names>C</given-names></string-name></person-group><etal>et al</etal><article-title>DeepLncLoc: a deep learning framework for long non-coding RNA subcellular localization prediction based on subsequence embedding</article-title>. <source>Brief Bioinform</source><year>2022</year>;<volume>23</volume>:<fpage>bbab360</fpage>.<pub-id pub-id-type="pmid">34498677</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname><given-names>M</given-names></string-name>, <string-name><surname>Lu</surname><given-names>C</given-names></string-name>, <string-name><surname>Fei</surname><given-names>Z</given-names></string-name></person-group><etal>et al</etal><article-title>DMFLDA: a deep learning framework for predicting lncRNA-disease associations</article-title>. <source>IEEE/ACM Trans Comput Biol Bioinform</source><year>2021</year>;<volume>18</volume>:<fpage>2353</fpage>–<lpage>63</lpage>.<pub-id pub-id-type="pmid">32248123</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname><given-names>M</given-names></string-name>, <string-name><surname>Lu</surname><given-names>C</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>SDLDA: lncRNA-disease association prediction based on singular value decomposition and deep learning</article-title>. <source>Methods</source><year>2020</year>;<volume>179</volume>:<fpage>73</fpage>–<lpage>80</lpage>.<pub-id pub-id-type="pmid">32387314</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>B</given-names></string-name>, <string-name><surname>Gunawardane</surname><given-names>L</given-names></string-name>, <string-name><surname>Niazi</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>A novel RNA motif mediates the strict nuclear localization of a long noncoding RNA</article-title>. <source>Mol Cell Biol</source><year>2014</year>;<volume>34</volume>:<fpage>2318</fpage>–<lpage>29</lpage>.<pub-id pub-id-type="pmid">24732794</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>F</given-names></string-name>, <string-name><surname>Song</surname><given-names>H</given-names></string-name>, <string-name><surname>Zeng</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>DeepFunc: a deep learning framework for accurate prediction of protein functions from protein sequences and interactions</article-title>. <source>Proteomics</source><year>2019</year>;<volume>19</volume>:<fpage>e1900019</fpage>.<pub-id pub-id-type="pmid">30941889</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>T</given-names></string-name>, <string-name><surname>Tan</surname><given-names>P</given-names></string-name>, <string-name><surname>Wang</surname><given-names>L</given-names></string-name></person-group><etal>et al</etal><article-title>RNALocate: a resource for RNA subcellular localizations</article-title>. <source>Nucleic Acids Res</source><year>2017</year>;<volume>45</volume>:<fpage>D135</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">27543076</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B41">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>W</given-names></string-name>, <string-name><surname>Yan</surname><given-names>J</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X</given-names></string-name></person-group><etal>et al</etal> Deep extreme multi-label learning. New York: Association for Computing Machinery 2018. <fpage>100</fpage>–<lpage>7</lpage>.</mixed-citation>
    </ref>
    <ref id="btad752-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>ZY</given-names></string-name>, <string-name><surname>Sun</surname><given-names>ZJ</given-names></string-name>, <string-name><surname>Yang</surname><given-names>YH</given-names></string-name></person-group><etal>et al</etal><article-title>Towards a better prediction of subcellular location of long non-coding RNA</article-title>. <source>Front Comput Sci</source><year>2022</year>;<volume>16</volume>:<fpage>165903</fpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10749772</article-id>
    <article-id pub-id-type="pmid">38109668</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btad752</article-id>
    <article-id pub-id-type="publisher-id">btad752</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Sequence Analysis</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>LncLocFormer: a Transformer-based deep learning model for multi-label lncRNA subcellular localization prediction by using localization-specific attention mechanism</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1726-0955</contrib-id>
        <name>
          <surname>Zeng</surname>
          <given-names>Min</given-names>
        </name>
        <aff><institution>School of Computer Science and Engineering, Central South University</institution>, Changsha, Hunan 410083, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wu</surname>
          <given-names>Yifan</given-names>
        </name>
        <aff><institution>School of Computer Science and Engineering, Central South University</institution>, Changsha, Hunan 410083, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Yiming</given-names>
        </name>
        <aff><institution>School of Computer Science and Engineering, Central South University</institution>, Changsha, Hunan 410083, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yin</surname>
          <given-names>Rui</given-names>
        </name>
        <aff><institution>Department of Health Outcomes and Biomedical Informatics, University of Florida</institution>, Gainesville, FL 32603, <country country="US">United States</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lu</surname>
          <given-names>Chengqian</given-names>
        </name>
        <aff><institution>School of Computer Science, Key Laboratory of Intelligent Computing and Information Processing, Xiangtan University</institution>, Xiangtan, Hunan 411105, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-4955-9270</contrib-id>
        <name>
          <surname>Duan</surname>
          <given-names>Junwen</given-names>
        </name>
        <aff><institution>School of Computer Science and Engineering, Central South University</institution>, Changsha, Hunan 410083, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0188-1394</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>Min</given-names>
        </name>
        <aff><institution>School of Computer Science and Engineering, Central South University</institution>, Changsha, Hunan 410083, <country country="CN">China</country></aff>
        <xref rid="btad752-cor1" ref-type="corresp"/>
        <!--limin@mail.csu.edu.cn-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Boeva</surname>
          <given-names>Valentina</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btad752-cor1">Corresponding author. School of Computer Science and Engineering, Central South University, Changsha, Hunan 410083, China. E-mail: <email>limin@mail.csu.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2023-12-18">
      <day>18</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>18</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <volume>39</volume>
    <issue>12</issue>
    <elocation-id>btad752</elocation-id>
    <history>
      <date date-type="received">
        <day>26</day>
        <month>7</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>13</day>
        <month>11</month>
        <year>2023</year>
      </date>
      <date date-type="editorial-decision">
        <day>07</day>
        <month>12</month>
        <year>2023</year>
      </date>
      <date date-type="corrected-typeset">
        <day>25</day>
        <month>12</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btad752.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>There is mounting evidence that the subcellular localization of lncRNAs can provide valuable insights into their biological functions. In the real world of transcriptomes, lncRNAs are usually localized in multiple subcellular localizations. Furthermore, lncRNAs have specific localization patterns for different subcellular localizations. Although several computational methods have been developed to predict the subcellular localization of lncRNAs, few of them are designed for lncRNAs that have multiple subcellular localizations, and none of them take motif specificity into consideration.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>In this study, we proposed a novel deep learning model, called LncLocFormer, which uses only lncRNA sequences to predict multi-label lncRNA subcellular localization. LncLocFormer utilizes eight Transformer blocks to model long-range dependencies within the lncRNA sequence and shares information across the lncRNA sequence. To exploit the relationship between different subcellular localizations and find distinct localization patterns for different subcellular localizations, LncLocFormer employs a localization-specific attention mechanism. The results demonstrate that LncLocFormer outperforms existing state-of-the-art predictors on the hold-out test set. Furthermore, we conducted a motif analysis and found LncLocFormer can capture known motifs. Ablation studies confirmed the contribution of the localization-specific attention mechanism in improving the prediction performance.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The LncLocFormer web server is available at <ext-link xlink:href="http://csuligroup.com:9000/LncLocFormer" ext-link-type="uri">http://csuligroup.com:9000/LncLocFormer</ext-link>. The source code can be obtained from <ext-link xlink:href="https://github.com/CSUBioGroup/LncLocFormer" ext-link-type="uri">https://github.com/CSUBioGroup/LncLocFormer</ext-link>.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Key Research and Development Program of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100012166</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>2022YFC3400300</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>62102457</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Hunan Provincial Natural Science Foundation of China</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2023JJ40763</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="11"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Long non-coding RNAs (lncRNAs) are a class of non-coding RNA molecules that comprised more than 200 nucleotides (<xref rid="btad752-B7" ref-type="bibr">Birney <italic toggle="yes">et al.</italic> 2007</xref>, <xref rid="btad752-B21" ref-type="bibr">Lu <italic toggle="yes">et al.</italic> 2018</xref>). They are involved in various important biological processes, including the regulation of gene expression, alternative splicing, nuclear organization, and genomic imprinting (<xref rid="btad752-B36" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2020</xref>). LncRNAs have the ability to interact with proteins, DNAs, and RNAs, and perform specific functions as a result of these interactions (<xref rid="btad752-B10" ref-type="bibr">Esteller 2011</xref>). For example, they can act as “miRNA sponge” to regulate miRNA levels and thereby influence the expression of miRNA targets (<xref rid="btad752-B9" ref-type="bibr">DiStefano 2018</xref>). Additionally, under particular stimulation, lncRNAs can influence transcriptional activity or pathways (<xref rid="btad752-B31" ref-type="bibr">Wang and Chang 2011</xref>). Because of the complexity of molecular functions and biological processes, lncRNA-related research has gained significant attention (<xref rid="btad752-B20" ref-type="bibr">Lu <italic toggle="yes">et al.</italic> 2020</xref>, <xref rid="btad752-B35" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2021</xref>).</p>
    <p>A growing amount of evidence reveals that lncRNA subcellular localizations can provide valuable insights into their biological functions (<xref rid="btad752-B28" ref-type="bibr">Savulescu <italic toggle="yes">et al.</italic> 2021</xref>). One wet-lab technique commonly used to study RNA subcellular localization is single-molecule fluorescent <italic toggle="yes">in situ</italic> hybridization (smFISH) technique (<xref rid="btad752-B24" ref-type="bibr">Moffitt and Zhuang 2016</xref>). Although the image data provided by the smFISH technique can accurately determine the subcellular localization of RNAs, the smFISH technique is expensive and time-consuming. Considering its limitations, it would be extremely beneficial for biologists to develop accurate computational methods to predict lncRNA subcellular localizations.</p>
    <p>Some computational methods have been proposed to predict lncRNA subcellular localization. To the best of our knowledge, LncLocator is the first predictor for lncRNA subcellular localization (<xref rid="btad752-B5" ref-type="bibr">Cao <italic toggle="yes">et al.</italic> 2018</xref>). It extracts 4-mer features and high-level features, and uses support vector machine (SVM) and random forest to make predictions. iLoc-lncRNA utilizes 8-mer features to encode lncRNA sequences, and applies SVM to perform the prediction task (<xref rid="btad752-B30" ref-type="bibr">Su <italic toggle="yes">et al.</italic> 2018</xref>). DeepLncRNA incorporates 2, 3, 4, and 5-mer features and uses a deep learning network to predict lncRNA subcellular localizations (<xref rid="btad752-B13" ref-type="bibr">Gudenas and Wang 2018</xref>). Locate-R incorporates the preselected <italic toggle="yes">k</italic>-mer features and applies SVM to construct a classifier (<xref rid="btad752-B1" ref-type="bibr">Ahmad <italic toggle="yes">et al.</italic> 2020</xref>). lncLocPred integrates multiple feature selection techniques to select optimal features, and adopts a logistic regression model to make predictions (<xref rid="btad752-B11" ref-type="bibr">Fan <italic toggle="yes">et al.</italic> 2020</xref>). LncLocation integrates the multi-source heterogeneous features, and uses SVM to construct a classifier (<xref rid="btad752-B12" ref-type="bibr">Feng <italic toggle="yes">et al.</italic> 2020</xref>). DeepLncLoc is a novel deep learning model, which uses subsequence embedding technique to encode lncRNA sequences, and uses a deep neural network to classify five localizations (<xref rid="btad752-B37" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2022</xref>). TACOS applies a tree-based stacking classifier to predict the subcellular localization of human lncRNA in 10 different cell types (<xref rid="btad752-B15" ref-type="bibr">Jeon <italic toggle="yes">et al.</italic> 2022</xref>). RNALight extracts <italic toggle="yes">k</italic>-mer features and uses LightGBM to predict the subcellular localizations of mRNAs and lncRNAs (<xref rid="btad752-B34" ref-type="bibr">Yuan <italic toggle="yes">et al.</italic> 2023</xref>). GraphLncLoc transforms lncRNA sequences into graphs, and utilizes graph convolutional networks to capture high-level features and make predictions (<xref rid="btad752-B17" ref-type="bibr">Li <italic toggle="yes">et al.</italic> 2023</xref>). Recently, LncLocator 2.0 (<xref rid="btad752-B19" ref-type="bibr">Lin <italic toggle="yes">et al.</italic> 2021</xref>) and iLoc-LncRNA(2.0) (<xref rid="btad752-B42" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> 2022</xref>) have been released, which provide more accurate prediction results than their previous versions.</p>
    <p>Although several computational models have been developed, few of these models are designed for lncRNAs that have multiple subcellular localizations. In reality, lncRNA subcellular localization is a dynamic process (<xref rid="btad752-B4" ref-type="bibr">Bridges <italic toggle="yes">et al.</italic> 2021</xref>). For example, lncRNA SNHG1 displays cytoplasmic distribution in human HCT116 colon cancer cells. However, upon DNA damage stress, it is retained in the nucleus compartment (<xref rid="btad752-B6" ref-type="bibr">Carlevaro-Fita and Johnson 2019</xref>). Another example is lncRNA Uchl1-AS1, which translocates from the nucleus to the cytoplasm under rapamycin treatment (<xref rid="btad752-B27" ref-type="bibr">Riva <italic toggle="yes">et al.</italic> 2016</xref>). However, the existing computational models usually only consider a single subcellular localization for each lncRNA.</p>
    <p>In addition, increasing evidence suggests that lncRNAs exhibit distinct localization patterns in different subcellular localizations. For example, Shukla <italic toggle="yes">et al.</italic> found that conserved long sequences (&gt;300 nt) with a common 15-nt C-rich pattern are responsible for nuclear localization (<xref rid="btad752-B29" ref-type="bibr">Shukla <italic toggle="yes">et al.</italic> 2018</xref>). Lubelsky <italic toggle="yes">et al.</italic> found a core 42-nt motif that drives nuclear RNA localization (<xref rid="btad752-B22" ref-type="bibr">Lubelsky and Ulitsky 2018</xref>). Despite these findings, existing computational methods do not take motif specificity in different subcellular localizations into account.</p>
    <p>To meet the need for lncRNA multiple subcellular localization predictions and to consider motif specificity for different subcellular localizations, we proposed LncLocFormer, which is a Transformer-based deep learning model using a localization-specific attention mechanism. Transformer is a class of powerful deep learning architecture that has achieved substantial breakthroughs in natural language processing (NLP), as it can capture both local and global features of sequences. Inspired by its success in NLP, we applied it to the prediction of lncRNA subcellular localization. By using the positional coding and multi-head attention mechanism in Transformer blocks, LncLocFormer can model long-range dependencies within the lncRNA sequence and share information across the lncRNA sequence. Different from previous computational methods, LncLocFormer can predict multiple subcellular localizations simultaneously for each lncRNA sequence. Furthermore, using the localization-specific attention mechanism, LncLocFormer learns different attention weights for different subcellular localizations, which can provide valuable information about the relationship between different labels.</p>
    <p>To evaluate the performance of LncLocFormer, we compared it with some deep learning baseline models and existing state-of-the-art predictors. The results of cross-validation (CV) and the hold-out test set demonstrate that LncLocFormer performs significantly better than other computational models. In addition, the results show that LncLocFormer is capable of capturing sequence motifs. To investigate which part of LncLocFormer is helpful in predicting lncRNA subcellular localizations, we conducted an ablation study by removing or replacing some components of LncLocFormer. The ablation study shows that the localization-specific attention mechanism is a crucial component in LncLocFormer. To facilitate the use of LncLocFormer, we developed a user-friendly web server.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Benchmark dataset</title>
      <p>The first important step in constructing a reliable predictor is to establish a reliable benchmark dataset. To achieve this, we retrieved known lncRNA subcellular localization information from the RNALocate v2.0 (<xref rid="btad752-B8" ref-type="bibr">Cui <italic toggle="yes">et al.</italic> 2022</xref>) database (<ext-link xlink:href="https://www.rna-society.org/rnalocate/" ext-link-type="uri">https://www.rna-society.org/rnalocate/</ext-link>), which collects more than 210 000 RNA-associated subcellular localization entries with experimental evidence, encompassing more than 110 000 RNAs with 171 subcellular localizations in 104 species. We generated a benchmark dataset to train and test our model by the following procedure:</p>
      <list list-type="order">
        <list-item>
          <p>We retrieved a total of 9128 Homo sapiens lncRNA-associated subcellular localization entries from the RNALocate v2.0 database. Since many lncRNAs have multiple entries, we merged the entries with the same gene symbol;</p>
        </list-item>
        <list-item>
          <p>We removed the lncRNAs that do not have sequence information in NCBI (<xref rid="btad752-B26" ref-type="bibr">Pruitt <italic toggle="yes">et al.</italic> 2007</xref>);</p>
        </list-item>
        <list-item>
          <p>To reduce data redundancy, we used the cd-hit-est tool (<xref rid="btad752-B14" ref-type="bibr">Huang <italic toggle="yes">et al.</italic> 2010</xref>) with a cutoff of 80%;</p>
        </list-item>
        <list-item>
          <p>Consider that some subcellular localizations have a small number lncRNA entries, we only selected the subcellular localizations with more than 40 lncRNA entries;</p>
        </list-item>
        <list-item>
          <p>In the RNALocate v2.0 database, a significant number of entries are localized in exosome. However, accumulating evidence suggests that lncRNAs are expressed in a cell-specific and/or tissue-specific manner, and most of them are located in the nucleus. Moreover, a lot of samples which belong to the exosome localization could hinder the prediction of other subcellular locations. Thus, we removed exosome-localized entries in our study.</p>
        </list-item>
      </list>
      <p>Finally, our benchmark dataset comprises 811 lncRNAs, covering four types of subcellular localizations: nucleus, cytoplasm, chromatin, and insoluble cytoplasm. <xref rid="btad752-F1" ref-type="fig">Figure 1</xref> shows the distribution of subcellular localizations in the constructed benchmark dataset.</p>
      <fig position="float" id="btad752-F1">
        <label>Figure 1.</label>
        <caption>
          <p>The distribution of subcellular localizations in the constructed benchmark dataset.</p>
        </caption>
        <graphic xlink:href="btad752f1" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.2 LncLocFormer architecture</title>
      <p><xref rid="btad752-F2" ref-type="fig">Figure 2</xref> illustrates the architecture of LncLocFormer, which comprises four main components: (i) the embedding part, (ii) eight Transformer blocks, (iii) a localization-specific attention mechanism, and (iv) a fully connected layer that performs the multi-label classification task.</p>
      <fig position="float" id="btad752-F2">
        <label>Figure 2.</label>
        <caption>
          <p>The architecture of LncLocFormer. LncLocFormer takes a lncRNA sequence as input, which is encoded using a subsequence embedding method. The embedding layer is immediately followed by eight Transformer blocks, which are used to model long-range dependencies within the lncRNA sequence and share information across the lncRNA sequence. These Transformer blocks have the same architecture: relative position encoding, multi-head attention mechanism, residual connection, position-wise feed-forward network, and “Add &amp; Norm” component. Following the Transformer blocks, the designed localization-specific attention layer is employed to learn distinct weights of nucleotide for each subcellular localization. Finally, a fully connected layer is used to perform the multi-label classification task.</p>
        </caption>
        <graphic xlink:href="btad752f2" position="float"/>
      </fig>
      <sec>
        <title>2.2.1 Sequence embedding</title>
        <p>Before feeding raw lncRNA sequences into a deep learning model, it is necessary to encode them as numeric vectors. The two most commonly used coding techniques are <italic toggle="yes">k</italic>-mer coding and one-hot coding. However, <italic toggle="yes">k</italic>-mer coding loses the sequence order information, while one-hot coding ignores the relationship between different nucleotides. In order to tackle these limitations, we employed an effective subsequence embedding method (<xref rid="btad752-B37" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2022</xref>), which can preserve the sequence order information of lncRNAs and reflect the relationship between different <italic toggle="yes">k</italic>-mers. The main idea of the subsequence embedding method is to split a lncRNA sequence into a number of consecutive, non-overlapping subsequences. Then, we extracted patterns from each subsequence and combined these patterns to obtain a complete representation of the lncRNA sequence.</p>
        <p>Specifically, the subsequence embedding method involves several steps. First, we split a lncRNA sequence into <italic toggle="yes">n</italic> consecutive, non-overlapping subsequences. Then, we used an embedding technique to encode each subsequence. Word2vec is a popular word embedding technique in NLP that has demonstrated potential in many bioinformatics tasks (<xref rid="btad752-B32" ref-type="bibr">Wu <italic toggle="yes">et al.</italic> 2022</xref>, <xref rid="btad752-B17" ref-type="bibr">Li <italic toggle="yes">et al.</italic> 2023</xref>). Thus, we pre-trained all lncRNA sequences in our dataset to obtain the distribution representation of <italic toggle="yes">k</italic>-mers by using the word2vec method, and then used the distribution representation of <italic toggle="yes">k</italic>-mer features to represent these subsequences. In the training process, the parameter <italic toggle="yes">k</italic> was chosen from {1, 2, 3, 4, 5, 6} to find the best value. The skip-gram model was applied to maximize the co-occurrence likelihood function of the central word and corresponding context words. The other settings of word2vec were kept default to train word vectors. Finally, in our study, we set <italic toggle="yes">k </italic>=<italic toggle="yes"> </italic>3, and the dimension of the word vector was 128. After pre-trained on the dataset, we obtained the word vectors and then combined these vectors to represent a lncRNA sequence. The whole subsequence embedding framework is shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S1</xref>. We refer to the original publication of the subsequence embedding method for more details (<xref rid="btad752-B37" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2022</xref>).</p>
      </sec>
      <sec>
        <title>2.2.2 Transformer blocks</title>
        <p>So far, we have obtained the representation of a lncRNA sequence. The next step is to extract high-level features from the lncRNA representation. Transformer is a class of deep learning models that has achieved substantial breakthroughs in NLP and has recently been applied to various bioinformatics tasks. In our task, we employed eight Transformer blocks to model long-range dependencies within the lncRNA sequence and share information across the lncRNA sequence. The Transformer blocks are inspired by RealFormer, which is a state-of-the-art variant version of Transformer. The detailed structure is shown in the right side of <xref rid="btad752-F2" ref-type="fig">Fig. 2</xref>. These Transformer blocks mainly consist of five components: relative positional encoding, multi-head attention, residual connection, “add &amp; norm” component, and position-wise feed-forward network.</p>
        <p>The first component is relative positional encoding. We know that Recurrent Neural Network (RNN) is a sequential structure that recurrently processes words one by one. Unlike RNN, the core part of Transformer is the attention mechanism. Using the attention mechanism to replace RNN loses the sequence order information, which causes that the model does not know the relative and absolute position information of each nucleotide in lncRNA sequences. Thus, it is necessary to add the sequence order information to assist the model in learning the position information. To use the sequence order information, we inject relative positional information by using relative positional encoding to the input representations. We borrowed the idea of traditional relative positional encoding and made some modifications. Specifically, the original formula of “traditional relative positional encoding” is as follows:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        <p>We made some modifications, resulting in the modified relative positional encoding formula to inject relative positional information:
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a learnable parameter, <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are learnable matrixes, and <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a relative position bias term. The main difference between the relative positional encoding and traditional relative positional encoding lies in the inclusion of the relative position scale term <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The advantage is, by adding this term, we can control the information passing for different relative locations more efficiently, making the model better able to capture the long-term dependencies within the lncRNA sequence.</p>
        <p>After relative positional coding, LncLocFormer applies self-attention to learn the attention weights for each nucleotide pair in the lncRNA sequence. The attention weights can reveal the importance of sequence regions for subcellular localization. Specifically, for each input lncRNA sequence:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mi mathvariant="normal">lncRNA</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where <italic toggle="yes">L</italic> denotes the length of the lncRNA, <italic toggle="yes">Nj</italic> is one of the four nucleotide bases (A, C, G, and U) at the <italic toggle="yes">j</italic> position of the lncRNA sequence. Self-attention learns an attention score for each pair of nucleotides <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic>. The attention score is computed by using a query <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mi>Q</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, a key <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mi>K</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, a value <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mi>V</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, a pre-softmax attention score Prev as follows:
<disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mi mathvariant="normal">Attention</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Softmax</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Prev</mml:mi></mml:mrow></mml:mfenced><mml:mi>V</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula>where Prev indicates the attention scores from the previous self-attention layer.</p>
        <p>Instead of using a single attention in the traditional Transformer architecture, computing attention scores using a set of queries, keys, and values enables the Transformer model to jointly attend to information at different positions, which is called the multi-head attention mechanism. In our study, the multi-head attention mechanism is applied to model long-range dependencies and share information across the lncRNA sequence. Each attention head<italic toggle="yes">i</italic> (<italic toggle="yes">i </italic>=<italic toggle="yes"> </italic>1, 2, …, <italic toggle="yes">H</italic>) is computed as:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5" display="block" overflow="scroll"><mml:mi mathvariant="normal">hea</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Attention</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>Q</mml:mi><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Pre</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, and <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are learnable parameter matrixes, Prev<italic toggle="yes">i</italic> is the slice of Prev corresponding to head<italic toggle="yes">i</italic>.</p>
        <p>Here, each attention head is independent. All head<italic toggle="yes">i</italic> are concatenated and transformed with another linear projection to obtain the final multi-head output values.
<disp-formula id="E6"><label>(6)</label><mml:math id="M6" display="block" overflow="scroll"><mml:mi mathvariant="normal">Multi</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal">head</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Concat</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi mathvariant="normal">hea</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal">hea</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="normal">hea</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfenced><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is a learnable parameter matrix and <italic toggle="yes">H</italic> is the number of heads. Based on the multi-head attention mechanism, each head may attend to different parts of the input lncRNA sequence.</p>
        <p>In addition to relative positional coding and multi-head attention, the Transformer block has a standard architecture, which includes residual connection, “add &amp; norm” component, and position-wise feed-forward network. Specifically, the queries, keys, and values are derived from the outputs of the previous Transformer block, a residual connection is employed to avoid gradient vanishing or gradient exploding problems. The “add &amp; norm” component has two operations: addition and layer normalization. This addition operation from the residual connection is immediately followed by layer normalization. The position-wise feed-forward network transforms the representation at all the sequence positions using a fully connected layer.</p>
      </sec>
      <sec>
        <title>2.2.3 Localization-specific attention</title>
        <p>The standard attention mechanisms in Transformer blocks only tell us which nucleotides are considered very important for the overall prediction. However, lncRNA subcellular localization is a dynamic process, which is treated as a multi-label classification problem in our study. Therefore, it would be more informative to analyze which nucleotides are considered important for each subcellular localization compartment. With this motivation, we designed a localization-specific attention mechanism after the Transformer blocks.</p>
        <p>In the study, we have four subcellular localization compartments (nucleus, cytoplasm, chromatin, and insoluble cytoplasm). We used the multi-head attention mechanism to obtain the weight (importance) of every nucleotide in one subcellular localization compartment. Then, we repeated the process four times (here, four represents the number of subcellular localization compartments). As a result, we obtained four kinds of attention scores for four subcellular localization compartments. Specifically, we trained four attention matrices <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>ϵ</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for each subcellular localization compartment.
<disp-formula id="E7"><label>(7)</label><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">softmax</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mn>3,4</mml:mn><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mi>V</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the representation of the lncRNA obtained by the Transformer blocks and <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>∈</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:math></inline-formula> are the learnable weight matrix and bias term. After that, we used <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to aggregate <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mi>V</mml:mi></mml:math></inline-formula> under label <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mi>j</mml:mi></mml:math></inline-formula> and used a dense layer with sigmoid activation function to obtain the localization probability:
<disp-formula id="E8"><label>(8)</label><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">sigmoid</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        <p>Finally, the <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mn>3,4</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> is used as the final prediction, and the cross-entropy loss is computed by <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> to perform gradient descent.</p>
        <p>Overall, the benefits of the localization-specific attention mechanism can be summarized as follows:</p>
        <list list-type="order">
          <list-item>
            <p>The localization-specific attention mechanism is a fine-grained interpretability technique that can provide support for the interpretability of each subcellular localization.</p>
          </list-item>
          <list-item>
            <p>The localization-specific attention mechanism tends to be less heavily biased toward the most frequent compartments, which alleviates the imbalance data distribution problem.</p>
          </list-item>
          <list-item>
            <p>The localization-specific attention learns multiple attention scores and uses them for prediction, resulting in more accurate and robust results.</p>
          </list-item>
        </list>
        <p>Finally, in the classification part, a fully connected layer is applied to perform the multi-label classification task.</p>
      </sec>
    </sec>
    <sec>
      <title>2.3 Deep learning baseline models and existing predictors</title>
      <p>In this study, we focus on constructing powerful deep learning models to predict lncRNA subcellular localizations. To demonstrate the effectiveness of LncLncFormer, we compared it with several deep learning baseline models.</p>
      <list list-type="order">
        <list-item>
          <p><italic toggle="yes">k</italic>-mer + MLP, this model extracts <italic toggle="yes">k</italic>-mer frequency features, which are fed into a MLP layer to output subcellular localizations.</p>
        </list-item>
        <list-item>
          <p>Word2vec + MLP, this model encodes lncRNA sequences by using the word2vec technique, followed by feeding the sequence representation to a MLP layer for subcellular localization prediction.</p>
        </list-item>
        <list-item>
          <p>Word2vec + CNN + MLP, this model converts lncRNA sequences to embedding vectors learned by the word2vec technique, followed by a CNN layer, then uses a MLP layer to predict the subcellular localizations.</p>
        </list-item>
        <list-item>
          <p>Word2vec + Bi-LSTM + MLP, this model converts lncRNA sequences to embedding vectors learned by the word2vec technique, followed by a Bi-LSTM layer, then uses a MLP layer to predict the subcellular localizations.</p>
        </list-item>
        <list-item>
          <p>Glove + MLP, this model encodes lncRNA sequences by using the Glove technique, followed by feeding the sequence representation to a MLP layer for subcellular localization prediction.</p>
        </list-item>
        <list-item>
          <p>Glove + CNN + MLP, this model converts lncRNA sequences to embedding vectors learned by the Glove technique, followed by a CNN layer, then uses a MLP layer to predict the subcellular localizations.</p>
        </list-item>
        <list-item>
          <p>Glove + Bi-LSTM + MLP, this model converts lncRNA sequences to embedding vectors learned by the Glove technique, followed by a Bi-LSTM layer, then uses a MLP layer to predict the subcellular localizations.</p>
        </list-item>
      </list>
      <p>In the study, we used grid search to find the optimal parameters for these deep learning baseline models.</p>
      <p>To further evaluate the performance of LncLocFormer in predicting lncRNA subcellular localizations, we compared LncLocFormer with several existing state-of-the-art predictors by using a hold-out test set. We selected lncLocator (<xref rid="btad752-B5" ref-type="bibr">Cao <italic toggle="yes">et al.</italic> 2018</xref>), iLoc-lncRNA (<xref rid="btad752-B30" ref-type="bibr">Su <italic toggle="yes">et al.</italic> 2018</xref>), Locate-R (<xref rid="btad752-B1" ref-type="bibr">Ahmad <italic toggle="yes">et al.</italic> 2020</xref>), DeepLncLoc (<xref rid="btad752-B37" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2022</xref>), iLoc-LncRNA(2.0) (<xref rid="btad752-B42" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> 2022</xref>), and GraphLncLoc (<xref rid="btad752-B17" ref-type="bibr">Li <italic toggle="yes">et al.</italic> 2023</xref>) as the compared predictors. LncLocator and DeepLncLoc can predict five types of subcellular localizations, including nucleus, cytoplasm, cytosol, ribosome, and exosome. iLoc-lncRNA, Locate-R, iLoc-LncRNA(2.0), and GraphLncLoc can predict four types of subcellular localizations, including nucleus, cytoplasm, ribosome, and exosome. We did not compare LncLocFormer with lncLocator 2.0 since lncLocator 2.0 only provides the predicted CNRCI values instead of probabilities.</p>
    </sec>
    <sec>
      <title>2.4 Evaluation metrics</title>
      <p>To evaluate the performance of LncLocFormer with deep learning baseline models, we selected some evaluation metrics which are widely used in multi-label classification problem (<xref rid="btad752-B16" ref-type="bibr">Li <italic toggle="yes">et al.</italic> 2019</xref>). These evaluation metrics include average <italic toggle="yes">F</italic>-measure (Ave-<italic toggle="yes">F</italic>1), micro precision (MiP), micro recall (MiR), micro <italic toggle="yes">F</italic>-measure (MiF), and each area under receiver operating characteristic curve (AUC) for each subcellular localization. For convenience, <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>∈</mml:mo><mml:mn>0,1</mml:mn></mml:math></inline-formula> are the ground truth and predicted value of lncRNA <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mi>i</mml:mi></mml:math></inline-formula> for subcellular localization <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mi>j</mml:mi></mml:math></inline-formula>, respectively, and <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> if <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>, otherwise <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>.</p>
      <p>Ave-<italic toggle="yes">F</italic>1 is the harmonic mean of average precision and average recall, which is used in the CAFA challenge (<xref rid="btad752-B39" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> 2019</xref>), a protein function prediction challenge. We compute Ave-<italic toggle="yes">F</italic>1 using the following formulas:
<disp-formula id="E9"><label>(9)</label><mml:math id="M9" display="block" overflow="scroll"><mml:mi mathvariant="normal">Ave</mml:mi><mml:mo>-</mml:mo><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">AvgPre</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>A</mml:mi><mml:mi mathvariant="normal">vgRec</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi mathvariant="normal">AvgPre</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi mathvariant="normal">AvgRec</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula><disp-formula id="E10"><label>(10)</label><mml:math id="M10" display="block" overflow="scroll"><mml:mi mathvariant="normal">AvgPre</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">pr</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula><disp-formula id="E11"><label>(11)</label><mml:math id="M11" display="block" overflow="scroll"><mml:mi mathvariant="normal">AvgRec</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">re</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where
<disp-formula id="E12"><mml:math id="M12" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">pre</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">rec</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p>
      <p>MiF is the harmonic mean of MiP and MiR, which is used in the BioASQ challenge (<xref rid="btad752-B33" ref-type="bibr">You <italic toggle="yes">et al.</italic> 2021</xref>), a challenge on large-scale biomedical semantic indexing and question answering. It is defined as follows:
<disp-formula id="E13"><label>(12)</label><mml:math id="M13" display="block" overflow="scroll"><mml:mi mathvariant="normal">MiF</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">MiP</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">MiR</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">MiP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">MiR</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where
<disp-formula id="E14"><mml:math id="M14" display="block" overflow="scroll"><mml:mi mathvariant="normal">MiP</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal">MiR</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p>
      <p>Considering that the existing predictors are designed as multi-class predictors rather than multi-label predictors, to evaluate the performance of LncLocFormer with existing predictors, we evaluated the performance from two perspectives: the multi-label and the multi-class perspectives.</p>
      <p>In the multi-label perspective, we used Precision@k (P@k, which represents the number of correct predictions over <italic toggle="yes">k</italic>) to evaluate the performance (Zhang <italic toggle="yes">et al.</italic>). It is defined as follows:
<disp-formula id="E15"><label>(13)</label><mml:math id="M15" display="block" overflow="scroll"><mml:mi>P</mml:mi><mml:mo>@</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mi mathvariant="normal">ran</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">rank</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mfenced></mml:math></inline-formula> returns the <italic toggle="yes">k</italic> largest indices of <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> ranked in descending order. In the study, we only focus on the subcellular localization with the highest probability, thus, <italic toggle="yes">k</italic> is set to 1.</p>
      <p>In the multi-class perspective, consistent with the current state-of-the-art lncRNA subcellular localization predictors, we used Accuracy (ACC), Macro F-measure (MaF), Macro Precision (MaP), Macro Recall (MaR), and AUC as evaluation metrics.
<disp-formula id="E16"><label>(14)</label><mml:math id="M16" display="block" overflow="scroll"><mml:mi mathvariant="normal">ACC</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula><disp-formula id="E17"><label>(15)</label><mml:math id="M17" display="block" overflow="scroll"><mml:mi mathvariant="normal">Ma</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="E18"><label>(16)</label><mml:math id="M18" display="block" overflow="scroll"><mml:mi mathvariant="normal">Ma</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="E19"><label>(17)</label><mml:math id="M19" display="block" overflow="scroll"><mml:mi mathvariant="normal">MaF</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">Ma</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:mi mathvariant="normal">Ma</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ma</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Ma</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
    </sec>
    <sec>
      <title>2.5 Implementation details</title>
      <p>LncLocFormer is implemented using PyTorch (Paszke <italic toggle="yes">et al.</italic>). A grid search strategy was employed to find the optimal parameters of LncLocFormer, and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref> provides a summary of the optimal hyper-parameters and the corresponding search space. The used loss function is the cross-entropy function. The skip-gram model (Mikolov <italic toggle="yes">et al.</italic>) is used to pre-train the <italic toggle="yes">k</italic>-mer embedding vectors. We used the Adam optimizer with a learning rate of 0.0003. The learning rate is warm-uped over the first four epochs and decayed linearly for the remaining training steps. The batch size is set to 64. In the Transformer blocks, we used eight heads and hidden size of 128. To prevent the low-rank bottleneck, we enhanced the size of query/key/value into 64 using a dense layer (Bhojanapalli <italic toggle="yes">et al.</italic>). We keep at most 8196 nt for each lncRNA and divide them into 512 subsequences by using the subsequence embedding method. The dropout rate is set to 0.2 for the embedding layer and 0.1 for other layers. The maximum relative distance in the position embedding is set to 25.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Comparison with deep learning baseline models</title>
      <p>In this section, we investigated the effectiveness of LncLocFormer (subsequence embedding + Transformer blocks + localization-specific attention + MLP). We conducted 5-fold CV to evaluate the performance of LncLocFormer with other deep learning baseline models. In particular, we split the benchmark dataset into a training set (90%) and a hold-out test set (10%). Next, we performed 5-fold CV by further splitting the training set into 80% training and 20% validation. The process was repeated five times, and the final prediction results were the average of five validation results. The performances of LncLocFormer and other deep learning baseline models using 5-fold CV are shown in <xref rid="btad752-T1" ref-type="table">Table 1</xref>. We can observe that LncLocFormer outperforms other deep learning baseline models, except for the MiP. Specifically, LncLocFormer obtains Ave-<italic toggle="yes">F</italic>1 of 0.719, MiR of 0.721, MiF of 0.701, and average AUC of 0.648, while GloVe + Bi-LSTM + MLP obtains the best MiP (0.712). These observations indicate the superiority of LncLocFormer network architecture.</p>
      <table-wrap position="float" id="btad752-T1">
        <label>Table 1.</label>
        <caption>
          <p>Performance of LncLocFormer and other deep learning baseline models using 5-fold CV.<xref rid="tblfn1" ref-type="table-fn"><sup>a</sup></xref></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="2" colspan="1">Model</th>
              <th rowspan="2" colspan="1">Ave-<italic toggle="yes">F</italic>1</th>
              <th rowspan="2" colspan="1">MiP</th>
              <th rowspan="2" colspan="1">MiR</th>
              <th rowspan="2" colspan="1">MiF</th>
              <th colspan="5" rowspan="1">AUC<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Nucleus</th>
              <th rowspan="1" colspan="1">Cytoplasm</th>
              <th rowspan="1" colspan="1">Chromatin</th>
              <th rowspan="1" colspan="1">Insoluble cytoplasm</th>
              <th rowspan="1" colspan="1">Average</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">K-mer + MLP</td>
              <td rowspan="1" colspan="1">0.670</td>
              <td rowspan="1" colspan="1">0.651</td>
              <td rowspan="1" colspan="1">0.645</td>
              <td rowspan="1" colspan="1">0.648</td>
              <td rowspan="1" colspan="1">0.680</td>
              <td rowspan="1" colspan="1">0.636</td>
              <td rowspan="1" colspan="1">0.616</td>
              <td rowspan="1" colspan="1">0.582</td>
              <td rowspan="1" colspan="1">0.629</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Word2vec + MLP</td>
              <td rowspan="1" colspan="1">0.690</td>
              <td rowspan="1" colspan="1">0.664</td>
              <td rowspan="1" colspan="1">0.660</td>
              <td rowspan="1" colspan="1">0.662</td>
              <td rowspan="1" colspan="1">0.660</td>
              <td rowspan="1" colspan="1">0.580</td>
              <td rowspan="1" colspan="1">0.561</td>
              <td rowspan="1" colspan="1">0.526</td>
              <td rowspan="1" colspan="1">0.582</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Word2vec + CNN + MLP</td>
              <td rowspan="1" colspan="1">0.697</td>
              <td rowspan="1" colspan="1">0.692</td>
              <td rowspan="1" colspan="1">0.652</td>
              <td rowspan="1" colspan="1">0.672</td>
              <td rowspan="1" colspan="1">0.679</td>
              <td rowspan="1" colspan="1">0.591</td>
              <td rowspan="1" colspan="1">0.610</td>
              <td rowspan="1" colspan="1">0.582</td>
              <td rowspan="1" colspan="1">0.616</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Word2vec + Bi-LSTM + MLP</td>
              <td rowspan="1" colspan="1">0.689</td>
              <td rowspan="1" colspan="1">0.670</td>
              <td rowspan="1" colspan="1">0.653</td>
              <td rowspan="1" colspan="1">0.661</td>
              <td rowspan="1" colspan="1">0.682</td>
              <td rowspan="1" colspan="1">0.595</td>
              <td rowspan="1" colspan="1">0.597</td>
              <td rowspan="1" colspan="1">0.622</td>
              <td rowspan="1" colspan="1">0.624</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GloVe + MLP</td>
              <td rowspan="1" colspan="1">0.697</td>
              <td rowspan="1" colspan="1">0.685</td>
              <td rowspan="1" colspan="1">0.656</td>
              <td rowspan="1" colspan="1">0.670</td>
              <td rowspan="1" colspan="1">0.662</td>
              <td rowspan="1" colspan="1">0.560</td>
              <td rowspan="1" colspan="1">0.588</td>
              <td rowspan="1" colspan="1">0.574</td>
              <td rowspan="1" colspan="1">0.596</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GloVe + CNN + MLP</td>
              <td rowspan="1" colspan="1">0.697</td>
              <td rowspan="1" colspan="1">0.674</td>
              <td rowspan="1" colspan="1">0.677</td>
              <td rowspan="1" colspan="1">0.675</td>
              <td rowspan="1" colspan="1">0.668</td>
              <td rowspan="1" colspan="1">0.636</td>
              <td rowspan="1" colspan="1">0.580</td>
              <td rowspan="1" colspan="1">0.541</td>
              <td rowspan="1" colspan="1">0.606</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GloVe + Bi-LSTM + MLP</td>
              <td rowspan="1" colspan="1">0.679</td>
              <td rowspan="1" colspan="1">
                <bold>0.712</bold>
              </td>
              <td rowspan="1" colspan="1">0.602</td>
              <td rowspan="1" colspan="1">0.652</td>
              <td rowspan="1" colspan="1">0.676</td>
              <td rowspan="1" colspan="1">0.573</td>
              <td rowspan="1" colspan="1">0.570</td>
              <td rowspan="1" colspan="1">0.510</td>
              <td rowspan="1" colspan="1">0.582</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LncLocFormer</td>
              <td rowspan="1" colspan="1">
                <bold>0.719</bold>
              </td>
              <td rowspan="1" colspan="1">0.683</td>
              <td rowspan="1" colspan="1">
                <bold>0.721</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.701</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.686</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.651</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.623</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.632</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.648</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <label>a</label>
            <p>The best performance values are highlighted in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.2 Comparison with existing predictors</title>
      <p>In the previous section, we performed 5-fold CV to obtain the best parameters and evaluated the performance of LncLocFormer with other deep learning baseline models. To further evaluate the performance of LncLocFormer in predicting lncRNA subcellular localizations, we compared LncLocFormer with several existing state-of-the-art predictors by using a hold-out test set. In particular, we selected the current predictors follow these criteria: (i) the availability of web server or stand-alone version; (ii) input that only needs lncRNA sequences; and (iii) outputs that include predictive probabilities for subcellular localization. Finally, we used the following web servers for comparison: lncLocator (<ext-link xlink:href="http://www.csbio.sjtu.edu.cn/bioinf/lncLocator/" ext-link-type="uri">http://www.csbio.sjtu.edu.cn/bioinf/lncLocator/</ext-link>), iLoc-lncRNA (<ext-link xlink:href="http://lin-group.cn/server/iLoc-LncRNA/" ext-link-type="uri">http://lin-group.cn/server/iLoc-LncRNA/</ext-link>), Locate-R (<ext-link xlink:href="http://locate-r.azurewebsites.net" ext-link-type="uri">http://locate-r.azurewebsites.net</ext-link>), DeepLncLoc (<ext-link xlink:href="http://bioinformatics.csu.edu.cn/DeepLncLoc/" ext-link-type="uri">http://bioinformatics.csu.edu.cn/DeepLncLoc/</ext-link>), iLoc-LncRNA(2.0) (<ext-link xlink:href="http://lin-group.cn/server/iLoc-LncRNA" ext-link-type="uri">http://lin-group.cn/server/iLoc-LncRNA</ext-link>(2.0)/), and GraphLncLoc (<ext-link xlink:href="http://csuligroup.com:8000/GraphLncLoc/" ext-link-type="uri">http://csuligroup.com:8000/GraphLncLoc/</ext-link>). The detailed prediction results on the hold-out test set are shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>. The P@1 of LncLocFormer and existing predictors on the hold-out test set is shown in <xref rid="btad752-T2" ref-type="table">Table 2</xref>.</p>
      <table-wrap position="float" id="btad752-T2">
        <label>Table 2.</label>
        <caption>
          <p>P@1 of LncLocFormer and existing predictors on the hold-out test set (RNALocate v2.0).<xref rid="tblfn2" ref-type="table-fn"><sup>a</sup></xref></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Predictor</th>
              <th rowspan="1" colspan="1">P@1</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">lncLocator</td>
              <td rowspan="1" colspan="1">0.232</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">iLoc-LncRNA</td>
              <td rowspan="1" colspan="1">0.348</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Locate-R</td>
              <td rowspan="1" colspan="1">0.275</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepLncLoc</td>
              <td rowspan="1" colspan="1">0.304</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">iLoc-LncRNA(2.0)</td>
              <td rowspan="1" colspan="1">0.333</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GraphLncLoc</td>
              <td rowspan="1" colspan="1">0.536</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LncLocFormer</td>
              <td rowspan="1" colspan="1">
                <bold>0.899</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <label>a</label>
            <p>The best performance values are highlighted in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>From <xref rid="btad752-T2" ref-type="table">Table 2</xref>, we can observe that LncLocFormer significantly outperforms existing predictors. Specifically, LncLocFormer obtains P@1 of 0.899, which is much higher than that of lncLocator (0.232), iLoc-LncRNA (0.348), Locate-R (0.275), DeepLncLoc (0.304), iLoc-LncRNA(2.0) (0.333), and GraphLncLoc (0.536). These results demonstrate that LncLocFormer has a powerful ability in predicting multi-label lncRNA subcellular localizations and achieves state-of-the-art performance on the hold-out test set. However, a natural question arises: why is there such a significant gap between LncLocFormer and the other predictors? We believe that it is impossible to achieve this by relying only on model architecture. One of the most possible explanations is that the used datasets are different. lncLocator, iLoc-LncRNA, Locate-R, DeepLncLoc, iLoc-LncRNA(2.0), and GraphLncLoc all used the RNALocate v1.0 database (<xref rid="btad752-B40" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> 2017</xref>) to train and test their models, while LncLocFormer is trained and tested by using the RNALocate v2.0 database (<xref rid="btad752-B8" ref-type="bibr">Cui <italic toggle="yes">et al.</italic> 2022</xref>). Therefore, we believe that the difference between the two datasets is the main reason for the large gap between LncLocFormer and other predictors.</p>
      <p>To investigate the difference between the two datasets, we plotted the distributions of the RNALocate v1.0 dataset used in the six predictors and the RNALocate v2.0 dataset used in LncLocFormer, as shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S2</xref>. By comparing <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S2a</xref> with b, we can observe that there are significant differences between the RNALocate v2.0 and RNALocate v1.0 datasets. For example, in the RNALocate v2.0 dataset, the number of lncRNAs located in the nucleus is much greater than the number of lncRNAs located in the cytoplasm. In contrast, in the RNALocate v1.0 dataset, the number of lncRNAs located in the cytoplasm is slightly larger than the number of lncRNAs located in the nucleus. These findings demonstrate that the data from the two datasets are not independently identical distributed (i.i.d.).</p>
      <p>To make a fairer comparison and to prove the superiority of LncLocFormer architecture, we used the RNALocate v1.0 database to retrain our model. Specifically, we employed the same training set and test set utilized in our previous predictor, DeepLncLoc, to retrain and test LncLocFormer. Since the dataset is generated for the multi-class prediction problem, we used a softmax activation function to replace the sigmoid activation function in the final fully connected layer to perform the multi-class prediction task. As with previous studies, we used ACC, MaF, MaP, and MaR as evaluation metrics to evaluate LncLocFormer and the existing predictors. The detailed prediction results on the RNALocate v1.0 test set are shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S3</xref>. The performance of LncLocFormer (using the RNALocate v1.0 dataset for training and test) and the existing predictors is shown in <xref rid="btad752-T3" ref-type="table">Table 3</xref>. In <xref rid="btad752-T3" ref-type="table">Table 3</xref>, the evaluation metrics we pay most attention to are MaF and ACC. From <xref rid="btad752-T3" ref-type="table">Table 3</xref>, we can observe that LncLocFormer still outperforms existing predictors in terms of MaF and ACC.</p>
      <table-wrap position="float" id="btad752-T3">
        <label>Table 3.</label>
        <caption>
          <p>Performance comparison of LncLocFormer (using the RNALocate v1.0 dataset for training and test) with the existing predictors.<xref rid="tblfn3" ref-type="table-fn"><sup>a</sup></xref></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Predictor</th>
              <th rowspan="1" colspan="1">MaP</th>
              <th rowspan="1" colspan="1">MaR</th>
              <th rowspan="1" colspan="1">MaF</th>
              <th rowspan="1" colspan="1">ACC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">lncLocator</td>
              <td rowspan="1" colspan="1">0.288</td>
              <td rowspan="1" colspan="1">0.292</td>
              <td rowspan="1" colspan="1">0.276</td>
              <td rowspan="1" colspan="1">0.433</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">iLoc-lncRNA</td>
              <td rowspan="1" colspan="1">0.488</td>
              <td rowspan="1" colspan="1">0.445</td>
              <td rowspan="1" colspan="1">0.458</td>
              <td rowspan="1" colspan="1">0.507</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Locate-R</td>
              <td rowspan="1" colspan="1">0.374</td>
              <td rowspan="1" colspan="1">0.317</td>
              <td rowspan="1" colspan="1">0.329</td>
              <td rowspan="1" colspan="1">0.403</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepLncLoc</td>
              <td rowspan="1" colspan="1">0.680</td>
              <td rowspan="1" colspan="1">0.543</td>
              <td rowspan="1" colspan="1">0.563</td>
              <td rowspan="1" colspan="1">0.537</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">iLoc-LncRNA(2.0)</td>
              <td rowspan="1" colspan="1">0.460</td>
              <td rowspan="1" colspan="1">0.384</td>
              <td rowspan="1" colspan="1">0.390</td>
              <td rowspan="1" colspan="1">0.433</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GraphLncLoc</td>
              <td rowspan="1" colspan="1">
                <bold>0.731</bold>
              </td>
              <td rowspan="1" colspan="1">0.549</td>
              <td rowspan="1" colspan="1">0.560</td>
              <td rowspan="1" colspan="1">0.522</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LncLocFormer</td>
              <td rowspan="1" colspan="1">0.696</td>
              <td rowspan="1" colspan="1">
                <bold>0.566</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.597</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.612</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn3">
            <label>a</label>
            <p>The best performance values are highlighted in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.3 Motif analysis</title>
      <p>In the study, we designed a localization-specific attention mechanism to obtain distinct attention weights for each subcellular localization and find the most likely motifs in lncRNA sequences. To investigate the performance of localization-specific attention mechanism in LncLocFormer, we conducted some motif analyses.</p>
      <p>First, we tested whether LncLocFormer could find the most frequently recurring motifs. In particular, we used the MEME suite (<xref rid="btad752-B2" ref-type="bibr">Bailey <italic toggle="yes">et al.</italic> 2009</xref>) to find the motifs in our dataset. The motifs are analyzed with the width of nine, and the <italic toggle="yes">E</italic>-value is set to 0.05. We used a threshold to determine the importance of attention weights. The threshold is set to the multiplicative inverse of the input sequence length. Because if the attention weights on the lncRNA sequence are randomly distributed, the mathematic expectation of all attention weights on the lncRNA sequence is the multiplicative inverse of the input sequence length. Only if the attention weight of a nucleotide is larger than the mathematic expectation, we believe that LncLocFormer pays attention to the nucleotide. If the attention weight of a nucleotide is smaller than the mathematic expectation, we believe that LncLocFormer does not focus on the nucleotide. <xref rid="btad752-F3" ref-type="fig">Figure 3</xref> displays the representative examples, with the left column depicting the motifs found by the MEME suite, the middle column showing the motifs discovered by LncLocFormer, and the right column displaying the <italic toggle="yes">E</italic>-values of the motifs found by the MEME suite. From <xref rid="btad752-F3" ref-type="fig">Fig. 3</xref>, we can observe that LncLocFormer can capture the motifs that are similar to those found by the MEME suite, which means LncLocFormer can capture the most frequently recurring motifs.</p>
      <fig position="float" id="btad752-F3">
        <label>Figure 3.</label>
        <caption>
          <p>Motifs discovered by MEME suite (left) and by LncLocFormer (middle). The right are the <italic toggle="yes">E</italic>-values of the motifs found by the MEME suite.</p>
        </caption>
        <graphic xlink:href="btad752f3" position="float"/>
      </fig>
      <p>Second, we investigated whether LncLocFormer could capture some known motifs. Specifically, we searched for some known motifs in recent literature that are related to subcellular localization. Lubelsky <italic toggle="yes">et al.</italic> (<xref rid="btad752-B22" ref-type="bibr">Lubelsky and Ulitsky 2018</xref>) found that the repeated motif RCCTCCC (where R denotes A/G) drives lncRNAs to be located in the nucleus. <xref rid="btad752-B38" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> (2014)</xref> identified the motif AGCCC act as a general nucleus localization signal. We used motifs RCCTCCC and AGCCC as examples to show the performance of LncLocFormer. The captured motifs by LncLocFormer for nucleus are shown in <xref rid="btad752-F4" ref-type="fig">Fig. 4</xref>. From <xref rid="btad752-F4" ref-type="fig">Fig. 4</xref>, we can observe that LncLocFormer can capture motifs that are similar to those that are already known.</p>
      <fig position="float" id="btad752-F4">
        <label>Figure 4.</label>
        <caption>
          <p>LncLocFormer captures two known motifs, which are related to nucleus localization.</p>
        </caption>
        <graphic xlink:href="btad752f4" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.4 Case study</title>
      <p>To better understand the role of the localization-specific attention, we visualized the attention matrices <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">and</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:math></inline-formula> which are computed by <xref rid="E7" ref-type="disp-formula">Equation (7)</xref> and gave a case study in <xref rid="btad752-F5" ref-type="fig">Fig. 5</xref> using lncRNA Cerox1 (cytoplasmic endogenous regulator of oxidative phosphorylation 1, NCBI ID: 115804232) as an example. The true label of lncRNA Cerox1 is nucleus. According to <xref rid="btad752-B38" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> (2014)</xref>, the motif AGCCC act as a general nucleus localization signal. We obtained four kinds of attention weights of different subcellular localizations, and highlighted the sequence using different degrees of red based on the values of attention weights. From <xref rid="btad752-F5" ref-type="fig">Fig. 5</xref>, we can observe that the attention matrix of nucleus captures an important region containing the core motif (AGCCC), while the attention matrices of other subcellular localizations fail to capture the motif AGCCC. Although LncLocFormer cannot find the exact known motifs, it can capture motifs that are very similar to the known motifs. The results suggest the potential of LncLocFormer in motif discovery.</p>
      <fig position="float" id="btad752-F5">
        <label>Figure 5.</label>
        <caption>
          <p>Attention weight visualization of lncRNA Cerox1.</p>
        </caption>
        <graphic xlink:href="btad752f5" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.5 Ablation study</title>
      <p>In order to discover the essential components of LncLocFormer, we conducted an ablation study by removing individual parts of LncLocFormer. In particular, we tested the model without localization-specific attention and the model without positional encoding. The former model lost multiple attention weights for different subcellular localizations, while the latter model lost the sequence order information. The results are shown in <xref rid="btad752-T4" ref-type="table">Table 4</xref>. In <xref rid="btad752-T4" ref-type="table">Table 4</xref>, the evaluation metrics we pay most attention to are Ave-<italic toggle="yes">F</italic>1, MiF, and average AUC. From <xref rid="btad752-T4" ref-type="table">Table 4</xref>, we can observe that localization-specific attention is the most important part of LncLocFormer. Without localization-specific attention, Ave-<italic toggle="yes">F</italic>1, MiF, and average AUC decrease from 0.719, 0.701, and 0.648 to 0.627, 0.591, and 0.614, respectively. Additionally, positional encoding is also useful in LncLocFormer. Without positional encoding, Ave-<italic toggle="yes">F</italic>1, MiF, and average AUC decrease from 0.719, 0.701, and 0.648 to 0.710, 0.691, and 0.643, respectively. The results confirm the effectiveness of localization-specific attention and positional encoding in LncLocFormer.</p>
      <table-wrap position="float" id="btad752-T4">
        <label>Table 4.</label>
        <caption>
          <p>The performances of various models in the ablation study.<xref rid="tblfn4" ref-type="table-fn"><sup>a</sup></xref></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="2" colspan="1">Model</th>
              <th rowspan="2" colspan="1">Ave-<italic toggle="yes">F</italic>1</th>
              <th rowspan="2" colspan="1">MiP</th>
              <th rowspan="2" colspan="1">MiR</th>
              <th rowspan="2" colspan="1">MiF</th>
              <th colspan="5" rowspan="1">AUC<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Nucleus</th>
              <th rowspan="1" colspan="1">Cytoplasm</th>
              <th rowspan="1" colspan="1">Chromatin</th>
              <th rowspan="1" colspan="1">Insoluble cytoplasm</th>
              <th rowspan="1" colspan="1">Average</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Without localization-specific attention</td>
              <td rowspan="1" colspan="1">0.627</td>
              <td rowspan="1" colspan="1">0.550</td>
              <td rowspan="1" colspan="1">0.640</td>
              <td rowspan="1" colspan="1">0.591</td>
              <td rowspan="1" colspan="1">
                <bold>0.688</bold>
              </td>
              <td rowspan="1" colspan="1">0.638</td>
              <td rowspan="1" colspan="1">0.599</td>
              <td rowspan="1" colspan="1">0.532</td>
              <td rowspan="1" colspan="1">0.614</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Without positional encoding</td>
              <td rowspan="1" colspan="1">0.710</td>
              <td rowspan="1" colspan="1">0.653</td>
              <td rowspan="1" colspan="1">
                <bold>0.734</bold>
              </td>
              <td rowspan="1" colspan="1">0.691</td>
              <td rowspan="1" colspan="1">0.672</td>
              <td rowspan="1" colspan="1">
                <bold>0.660</bold>
              </td>
              <td rowspan="1" colspan="1">0.611</td>
              <td rowspan="1" colspan="1">0.628</td>
              <td rowspan="1" colspan="1">0.643</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LncLocFormer</td>
              <td rowspan="1" colspan="1">
                <bold>0.719</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.683</bold>
              </td>
              <td rowspan="1" colspan="1">0.721</td>
              <td rowspan="1" colspan="1">
                <bold>0.701</bold>
              </td>
              <td rowspan="1" colspan="1">0.686</td>
              <td rowspan="1" colspan="1">0.651</td>
              <td rowspan="1" colspan="1">
                <bold>0.623</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.632</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.648</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn4">
            <label>a</label>
            <p>The best performance values are highlighted in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>In addition, we observed an interesting phenomenon, i.e. models without localization-specific attention can produce better results for the nucleus localization, while models without positional encoding can produce better results for the cytoplasm localization. Regarding the better results for nucleus localization with models lacking localization-specific attention, one possible explanation is that the benchmark dataset is imbalanced. The nucleus localization has a larger number of samples compared to the other three classes. When localization-specific attention is removed, the model may exhibit a bias toward the classes with more samples because this can lead to higher overall accuracy. This bias could result in improved predictions for the nucleus localization while leading to poorer predictions for the other three subcellular localizations. As for the better results for cytoplasm localization with models lacking positional encoding, the possible reason is that the lncRNA sequences belonging to the cytoplasm localization in the benchmark dataset are often quite long. In very long sequences, the relative position encoding may not effectively capture the relative distance relationship between nucleotides and may forget what has been learned in the sequence. Instead, the addition of relative position encoding may introduce noise or unnecessary information, resulting in a decline in prediction performance for the cytoplasm localization.</p>
    </sec>
    <sec>
      <title>3.6 Web server</title>
      <p>To facilitate the use of LncLocFormer, we developed a user-friendly web server, <ext-link xlink:href="http://csuligroup.com:9000/LncLocFormer" ext-link-type="uri">http://csuligroup.com:9000/LncLocFormer</ext-link>. LncLocFormer requires lncRNA sequences with more than 200 and &lt;10 000 nucleotides as input. Users can paste the lncRNA sequence into the input box and click on the submit button to see the predicted results. For each lncRNA sequence, the predicted probabilities and attention weights for each subcellular localization are displayed on the screen. In general, LncLocFormer takes &lt;10 s to predict the subcellular localization of a given lncRNA sequence. We believe that LncLocFormer is a convenient and efficient tool in the field of lncRNA subcellular localization prediction.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p>In the study, we proposed LncLocFormer, a multi-label lncRNA subcellular localization predictor that utilizes Transformer and localization-specific attention mechanism. Unlike many previous computational methods that only consider a single subcellular localization for a lncRNA sequence, LncLocFormer can predict multiple subcellular localizations simultaneously for each lncRNA sequence. Due to the uncertainty of the number of labels for each lncRNA sequence and the implicit relationship between the labels, the multi-label classification problem is more complicated than conventional multi-class classification tasks. By using Transformer blocks and localization-specific attention mechanism, LncLocFormer can predict lncRNA multiple subcellular localizations accurately, learn different motifs for each subcellular localization, and capture some motifs that are very similar to known motifs. Our extensive experimental results demonstrate that LncLocFormer outperforms existing state-of-the-art predictors. We believe that LncLocFormer can serve as a useful tool for predicting lncRNA multiple subcellular localizations.</p>
    <p>Although LncLocFormer shows promising results, there are some limitations that may influence the performance of LncLocFormer. The performance of LncLocFormer is limited by the number of samples in the RNALocate dataset. In the study, we only have 811 samples for the multi-label classification task. With lncRNA subcellular localization becoming a more important research topic, we could obtain more reliable data that can be used for training and test. Alternatively, we could consider transferring some data from other domains to aid in the research topic.</p>
    <p>Furthermore, LncLocFormer utilizes eight Transformer blocks and localization-specific attention, resulting in a lot of parameters that need to be tuned. Consequently, the training time of LncLocFormer is very long. With the development of deep learning techniques, more and more advanced knowledge distillation and network pruning techniques will be proposed. As a result, using a lightweight network architecture to predict lncRNA subcellular localization is a promising future direction.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btad752_Supplementary_Data</label>
      <media xlink:href="btad752_supplementary_data.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <sec>
    <title>Supplementary data</title>
    <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Conflict of interest</title>
    <p>None declared.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the National Key Research and Development Program of China [No. 2022YFC3400300]; the National Natural Science Foundation of China [No. 62102457]; Hunan Provincial Natural Science Foundation of China [No. 2023JJ40763]; Hunan Provincial Science and Technology Program [No. 2021RC4008]; and the Fundamental Research Funds for the Central Universities of Central South University [No. 2023ZZTS0627]. This work was carried out in part using computing resources at the High Performance Computing Center of Central South University.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btad752-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ahmad</surname><given-names>A</given-names></string-name>, <string-name><surname>Lin</surname><given-names>H</given-names></string-name>, <string-name><surname>Shatabda</surname><given-names>S.</given-names></string-name></person-group><article-title>Locate-R: subcellular localization of long non-coding RNAs using nucleotide compositions</article-title>. <source>Genomics</source><year>2020</year>;<volume>112</volume>:<fpage>2583</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">32068122</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bailey</surname><given-names>TL</given-names></string-name>, <string-name><surname>Boden</surname><given-names>M</given-names></string-name>, <string-name><surname>Buske</surname><given-names>FA</given-names></string-name></person-group><etal>et al</etal><article-title>MEME SUITE: tools for motif discovery and searching</article-title>. <source>Nucleic Acids Res</source><year>2009</year>;<volume>37</volume>:<fpage>W202</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">19458158</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B3">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bhojanapalli</surname><given-names>S</given-names></string-name>, <string-name><surname>Yun</surname><given-names>C</given-names></string-name>, <string-name><surname>Rawat</surname><given-names>AS</given-names></string-name></person-group><etal>et al</etal> Low-rank bottleneck in multi-head attention models. In: <italic toggle="yes">International conference on machine learning, online event, </italic>Vol. 119<italic toggle="yes">.</italic> PMLR. <fpage>864</fpage>–<lpage>73</lpage>.</mixed-citation>
    </ref>
    <ref id="btad752-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Birney</surname><given-names>E</given-names></string-name>, <string-name><surname>Stamatoyannopoulos</surname><given-names>JA</given-names></string-name>, <string-name><surname>Dutta</surname><given-names>A</given-names></string-name></person-group><etal>et al</etal>; <collab>Children's Hospital Oakland Research Institute</collab>. <article-title>Identification and analysis of functional elements in 1% of the human genome by the ENCODE pilot project</article-title>. <source>Nature</source><year>2007</year>;<volume>447</volume>:<fpage>799</fpage>–<lpage>816</lpage>.<pub-id pub-id-type="pmid">17571346</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bridges</surname><given-names>MC</given-names></string-name>, <string-name><surname>Daulagala</surname><given-names>AC</given-names></string-name>, <string-name><surname>Kourtidis</surname><given-names>A.</given-names></string-name></person-group><article-title>LNCcation: lncRNA localization and function</article-title>. <source>J Cell Biol</source><year>2021</year>;<volume>220</volume>:<fpage>e202009045</fpage>.<pub-id pub-id-type="pmid">33464299</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cao</surname><given-names>Z</given-names></string-name>, <string-name><surname>Pan</surname><given-names>X</given-names></string-name>, <string-name><surname>Yang</surname><given-names>Y</given-names></string-name></person-group><etal>et al</etal><article-title>The lncLocator: a subcellular localization predictor for long non-coding RNAs based on a stacked ensemble classifier</article-title>. <source>Bioinformatics</source><year>2018</year>;<volume>34</volume>:<fpage>2185</fpage>–<lpage>94</lpage>.<pub-id pub-id-type="pmid">29462250</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carlevaro-Fita</surname><given-names>J</given-names></string-name>, <string-name><surname>Johnson</surname><given-names>R.</given-names></string-name></person-group><article-title>Global positioning system: understanding long noncoding RNAs through subcellular localization</article-title>. <source>Mol Cell</source><year>2019</year>;<volume>73</volume>:<fpage>869</fpage>–<lpage>83</lpage>.<pub-id pub-id-type="pmid">30849394</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cui</surname><given-names>T</given-names></string-name>, <string-name><surname>Dou</surname><given-names>Y</given-names></string-name>, <string-name><surname>Tan</surname><given-names>P</given-names></string-name></person-group><etal>et al</etal><article-title>RNALocate v2.0: an updated resource for RNA subcellular localization with increased coverage and annotation</article-title>. <source>Nucleic Acids Res</source><year>2022</year>;<volume>50</volume>:<fpage>D333</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">34551440</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>DiStefano</surname><given-names>JK.</given-names></string-name></person-group><article-title>The emerging role of long noncoding RNAs in human disease</article-title>. <source>Methods Mol Biol</source><year>2018</year>;<volume>1706</volume>:<fpage>91</fpage>–<lpage>110</lpage>.<pub-id pub-id-type="pmid">29423795</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Esteller</surname><given-names>M.</given-names></string-name></person-group><article-title>Non-coding RNAs in human disease</article-title>. <source>Nat Rev Genet</source><year>2011</year>;<volume>12</volume>:<fpage>861</fpage>–<lpage>74</lpage>.<pub-id pub-id-type="pmid">22094949</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fan</surname><given-names>YX</given-names></string-name>, <string-name><surname>Chen</surname><given-names>MJ</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>QQ.</given-names></string-name></person-group><article-title>lncLocPred: predicting LncRNA subcellular localization using multiple sequence feature information</article-title>. <source>IEEE Access</source><year>2020</year>;<volume>8</volume>:<fpage>124702</fpage>–<lpage>11</lpage>.</mixed-citation>
    </ref>
    <ref id="btad752-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feng</surname><given-names>S</given-names></string-name>, <string-name><surname>Liang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Du</surname><given-names>W</given-names></string-name></person-group><etal>et al</etal><article-title>LncLocation: efficient subcellular location prediction of long non-coding RNA-based multi-source heterogeneous feature fusion</article-title>. <source>Int J Mol Sci</source><year>2020</year>;<volume>21</volume>:<fpage>7221</fpage>.<pub-id pub-id-type="pmid">33007849</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gudenas</surname><given-names>BL</given-names></string-name>, <string-name><surname>Wang</surname><given-names>L.</given-names></string-name></person-group><article-title>Prediction of LncRNA subcellular localization with deep learning from sequence features</article-title>. <source>Sci Rep</source><year>2018</year>;<volume>8</volume>:<fpage>16385</fpage>.<pub-id pub-id-type="pmid">30401954</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Niu</surname><given-names>B</given-names></string-name>, <string-name><surname>Gao</surname><given-names>Y</given-names></string-name></person-group><etal>et al</etal><article-title>CD-HIT suite: a web server for clustering and comparing biological sequences</article-title>. <source>Bioinformatics</source><year>2010</year>;<volume>26</volume>:<fpage>680</fpage>–<lpage>2</lpage>.<pub-id pub-id-type="pmid">20053844</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jeon</surname><given-names>YJ</given-names></string-name>, <string-name><surname>Hasan</surname><given-names>MM</given-names></string-name>, <string-name><surname>Park</surname><given-names>HW</given-names></string-name></person-group><etal>et al</etal><article-title>TACOS: a novel approach for accurate prediction of cell-specific long noncoding RNAs subcellular localization</article-title>. <source>Brief Bioinform</source><year>2022</year>;<volume>23</volume>:<fpage>bbac243</fpage>.<pub-id pub-id-type="pmid">35753698</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>M</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>B</given-names></string-name>, <string-name><surname>Yin</surname><given-names>R</given-names></string-name></person-group><etal>et al</etal><article-title>GraphLncLoc: long non-coding RNA subcellular localization prediction using graph convolutional networks based on sequence to graph transformation</article-title>. <source>Brief Bioinform</source><year>2023</year>;<volume>24</volume>:<fpage>bbac565</fpage>.<pub-id pub-id-type="pmid">36545797</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>M</given-names></string-name>, <string-name><surname>Fei</surname><given-names>Z</given-names></string-name>, <string-name><surname>Zeng</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Automated ICD-9 coding via a deep learning approach</article-title>. <source>IEEE/ACM Trans Comput Biol Bioinform</source><year>2019</year>;<volume>16</volume>:<fpage>1193</fpage>–<lpage>202</lpage>.<pub-id pub-id-type="pmid">29994157</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>Y</given-names></string-name>, <string-name><surname>Zeng</surname><given-names>M</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>DeepCellEss: cell line-specific essential protein prediction with attention-based interpretable deep learning</article-title>. <source>Bioinformatics</source><year>2023</year>;<volume>39</volume>:<fpage>btac779</fpage>.<pub-id pub-id-type="pmid">36458923</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>Y</given-names></string-name>, <string-name><surname>Pan</surname><given-names>X</given-names></string-name>, <string-name><surname>Shen</surname><given-names>HB.</given-names></string-name></person-group><article-title>lncLocator 2.0: a cell-line-specific subcellular localization predictor for long non-coding RNAs with interpretable deep learning</article-title>. <source>Bioinformatics</source><year>2021</year>;<volume>37</volume>:<fpage>2308</fpage>–<lpage>16</lpage>.<pub-id pub-id-type="pmid">33630066</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lu</surname><given-names>C</given-names></string-name>, <string-name><surname>Yang</surname><given-names>M</given-names></string-name>, <string-name><surname>Li</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Predicting human lncRNA-disease associations based on geometric matrix completion</article-title>. <source>IEEE J Biomed Health Inform</source><year>2020</year>;<volume>24</volume>:<fpage>2420</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">31825885</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lu</surname><given-names>C</given-names></string-name>, <string-name><surname>Yang</surname><given-names>M</given-names></string-name>, <string-name><surname>Luo</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>Prediction of lncRNA-disease associations based on inductive matrix completion</article-title>. <source>Bioinformatics</source><year>2018</year>;<volume>34</volume>:<fpage>3357</fpage>–<lpage>64</lpage>.<pub-id pub-id-type="pmid">29718113</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lubelsky</surname><given-names>Y</given-names></string-name>, <string-name><surname>Ulitsky</surname><given-names>I.</given-names></string-name></person-group><article-title>Sequences enriched in Alu repeats drive nuclear localization of long RNAs in human cells</article-title>. <source>Nature</source><year>2018</year>;<volume>555</volume>:<fpage>107</fpage>–<lpage>11</lpage>.<pub-id pub-id-type="pmid">29466324</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B23">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Mikolov</surname><given-names>T</given-names></string-name>, <string-name><surname>Chen</surname><given-names>K</given-names></string-name>, <string-name><surname>Corrado</surname><given-names>G</given-names></string-name></person-group><etal>et al</etal> Efficient estimation of word representations in vector space. arXiv, arXiv:1301.3781. <year>2013</year>.</mixed-citation>
    </ref>
    <ref id="btad752-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moffitt</surname><given-names>JR</given-names></string-name>, <string-name><surname>Zhuang</surname><given-names>X.</given-names></string-name></person-group><article-title>RNA imaging with multiplexed Error-Robust fluorescence in situ hybridization (MERFISH)</article-title>. <source>Methods Enzymol</source><year>2016</year>;<volume>572</volume>:<fpage>1</fpage>–<lpage>49</lpage>.<pub-id pub-id-type="pmid">27241748</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Paszke</surname><given-names>A</given-names></string-name>, <string-name><surname>Gross</surname><given-names>S</given-names></string-name>, <string-name><surname>Massa</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>PyTorch: an imperative style, high-performance deep learning library</article-title>. <source>Adv Neural Inf Process Syst</source> 2019;<volume>32</volume>.</mixed-citation>
    </ref>
    <ref id="btad752-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pruitt</surname><given-names>KD</given-names></string-name>, <string-name><surname>Tatusova</surname><given-names>T</given-names></string-name>, <string-name><surname>Maglott</surname><given-names>DR.</given-names></string-name></person-group><article-title>NCBI reference sequences (RefSeq): a curated non-redundant sequence database of genomes, transcripts and proteins</article-title>. <source>Nucleic Acids Res</source><year>2007</year>;<volume>35</volume>:<fpage>D61</fpage>–<lpage>5</lpage>.<pub-id pub-id-type="pmid">17130148</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Riva</surname><given-names>P</given-names></string-name>, <string-name><surname>Ratti</surname><given-names>A</given-names></string-name>, <string-name><surname>Venturin</surname><given-names>M.</given-names></string-name></person-group><article-title>The long non-coding RNAs in neurodegenerative diseases: novel mechanisms of pathogenesis</article-title>. <source>CAR</source><year>2016</year>;<volume>13</volume>:<fpage>1219</fpage>–<lpage>31</lpage>.</mixed-citation>
    </ref>
    <ref id="btad752-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Savulescu</surname><given-names>AF</given-names></string-name>, <string-name><surname>Bouilhol</surname><given-names>E</given-names></string-name>, <string-name><surname>Beaume</surname><given-names>N</given-names></string-name></person-group><etal>et al</etal><article-title>Prediction of RNA subcellular localization: learning from heterogeneous data sources</article-title>. <source>iScience</source><year>2021</year>;<volume>24</volume>:<fpage>103298</fpage>.<pub-id pub-id-type="pmid">34765919</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shukla</surname><given-names>CJ</given-names></string-name>, <string-name><surname>McCorkindale</surname><given-names>AL</given-names></string-name>, <string-name><surname>Gerhardinger</surname><given-names>C</given-names></string-name></person-group><etal>et al</etal><article-title>High-throughput identification of RNA nuclear enrichment sequences</article-title>. <source>EMBO J</source><year>2018</year>;<volume>37</volume>:<fpage>e98452</fpage>.<pub-id pub-id-type="pmid">29335281</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Su</surname><given-names>Z-D</given-names></string-name>, <string-name><surname>Huang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Z-Y</given-names></string-name></person-group><etal>et al</etal><article-title>iLoc-lncRNA: predict the subcellular location of lncRNAs by incorporating octamer composition into general PseKNC</article-title>. <source>Bioinformatics</source><year>2018</year>;<volume>34</volume>:<fpage>4196</fpage>–<lpage>204</lpage>.<pub-id pub-id-type="pmid">29931187</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>KC</given-names></string-name>, <string-name><surname>Chang</surname><given-names>HY.</given-names></string-name></person-group><article-title>Molecular mechanisms of long noncoding RNAs</article-title>. <source>Mol Cell</source><year>2011</year>;<volume>43</volume>:<fpage>904</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">21925379</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Gao</surname><given-names>M</given-names></string-name>, <string-name><surname>Zeng</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>BridgeDPI: a novel graph neural network for predicting drug-protein interactions</article-title>. <source>Bioinformatics</source><year>2022</year>;<volume>38</volume>:<fpage>2571</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">35274672</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>You</surname><given-names>R</given-names></string-name>, <string-name><surname>Liu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Mamitsuka</surname><given-names>H</given-names></string-name></person-group><etal>et al</etal><article-title>BERTMeSH: deep contextual representation learning for large-scale high-performance MeSH indexing with full text</article-title>. <source>Bioinformatics</source><year>2021</year>;<volume>37</volume>:<fpage>684</fpage>–<lpage>92</lpage>.<pub-id pub-id-type="pmid">32976559</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yuan</surname><given-names>GH</given-names></string-name>, <string-name><surname>Wang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname><given-names>GZ</given-names></string-name></person-group><etal>et al</etal><article-title>RNAlight: a machine learning model to identify nucleotide features determining RNA subcellular localization</article-title>. <source>Brief Bioinform</source><year>2023</year>;<volume>24</volume>:<fpage>bbac509</fpage>.<pub-id pub-id-type="pmid">36464487</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname><given-names>M</given-names></string-name>, <string-name><surname>Wu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Lu</surname><given-names>C</given-names></string-name></person-group><etal>et al</etal><article-title>DeepLncLoc: a deep learning framework for long non-coding RNA subcellular localization prediction based on subsequence embedding</article-title>. <source>Brief Bioinform</source><year>2022</year>;<volume>23</volume>:<fpage>bbab360</fpage>.<pub-id pub-id-type="pmid">34498677</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname><given-names>M</given-names></string-name>, <string-name><surname>Lu</surname><given-names>C</given-names></string-name>, <string-name><surname>Fei</surname><given-names>Z</given-names></string-name></person-group><etal>et al</etal><article-title>DMFLDA: a deep learning framework for predicting lncRNA-disease associations</article-title>. <source>IEEE/ACM Trans Comput Biol Bioinform</source><year>2021</year>;<volume>18</volume>:<fpage>2353</fpage>–<lpage>63</lpage>.<pub-id pub-id-type="pmid">32248123</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname><given-names>M</given-names></string-name>, <string-name><surname>Lu</surname><given-names>C</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>SDLDA: lncRNA-disease association prediction based on singular value decomposition and deep learning</article-title>. <source>Methods</source><year>2020</year>;<volume>179</volume>:<fpage>73</fpage>–<lpage>80</lpage>.<pub-id pub-id-type="pmid">32387314</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>B</given-names></string-name>, <string-name><surname>Gunawardane</surname><given-names>L</given-names></string-name>, <string-name><surname>Niazi</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>A novel RNA motif mediates the strict nuclear localization of a long noncoding RNA</article-title>. <source>Mol Cell Biol</source><year>2014</year>;<volume>34</volume>:<fpage>2318</fpage>–<lpage>29</lpage>.<pub-id pub-id-type="pmid">24732794</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>F</given-names></string-name>, <string-name><surname>Song</surname><given-names>H</given-names></string-name>, <string-name><surname>Zeng</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>DeepFunc: a deep learning framework for accurate prediction of protein functions from protein sequences and interactions</article-title>. <source>Proteomics</source><year>2019</year>;<volume>19</volume>:<fpage>e1900019</fpage>.<pub-id pub-id-type="pmid">30941889</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>T</given-names></string-name>, <string-name><surname>Tan</surname><given-names>P</given-names></string-name>, <string-name><surname>Wang</surname><given-names>L</given-names></string-name></person-group><etal>et al</etal><article-title>RNALocate: a resource for RNA subcellular localizations</article-title>. <source>Nucleic Acids Res</source><year>2017</year>;<volume>45</volume>:<fpage>D135</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">27543076</pub-id></mixed-citation>
    </ref>
    <ref id="btad752-B41">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>W</given-names></string-name>, <string-name><surname>Yan</surname><given-names>J</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X</given-names></string-name></person-group><etal>et al</etal> Deep extreme multi-label learning. New York: Association for Computing Machinery 2018. <fpage>100</fpage>–<lpage>7</lpage>.</mixed-citation>
    </ref>
    <ref id="btad752-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>ZY</given-names></string-name>, <string-name><surname>Sun</surname><given-names>ZJ</given-names></string-name>, <string-name><surname>Yang</surname><given-names>YH</given-names></string-name></person-group><etal>et al</etal><article-title>Towards a better prediction of subcellular location of long non-coding RNA</article-title>. <source>Front Comput Sci</source><year>2022</year>;<volume>16</volume>:<fpage>165903</fpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
