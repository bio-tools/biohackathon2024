<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with OASIS Tables with MathML3 v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-archive-oasis-article1-mathml3.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jpoasis-nisons2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Hum Brain Mapp</journal-id>
    <journal-id journal-id-type="iso-abbrev">Hum Brain Mapp</journal-id>
    <journal-id journal-id-type="doi">10.1002/(ISSN)1097-0193</journal-id>
    <journal-id journal-id-type="publisher-id">HBM</journal-id>
    <journal-title-group>
      <journal-title>Human Brain Mapping</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1065-9471</issn>
    <issn pub-type="epub">1097-0193</issn>
    <publisher>
      <publisher-name>John Wiley &amp; Sons, Inc.</publisher-name>
      <publisher-loc>Hoboken, USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7267905</article-id>
    <article-id pub-id-type="pmid">31609046</article-id>
    <article-id pub-id-type="doi">10.1002/hbm.24811</article-id>
    <article-id pub-id-type="publisher-id">HBM24811</article-id>
    <article-categories>
      <subj-group subj-group-type="overline">
        <subject>Technical Report</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Technical Report</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Hippocampal segmentation for brains with extensive atrophy using three‐dimensional convolutional neural networks</article-title>
      <alt-title alt-title-type="left-running-head">Goubran et al.</alt-title>
    </title-group>
    <contrib-group>
      <contrib id="hbm24811-cr-0001" contrib-type="author" corresp="yes">
        <name>
          <surname>Goubran</surname>
          <given-names>Maged</given-names>
        </name>
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-5880-0818</contrib-id>
        <xref ref-type="aff" rid="hbm24811-aff-0001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="hbm24811-aff-0002">
          <sup>2</sup>
        </xref>
        <address>
          <email>maged.goubran@sri.utoronto.ca</email>
        </address>
      </contrib>
      <contrib id="hbm24811-cr-0002" contrib-type="author">
        <name>
          <surname>Ntiri</surname>
          <given-names>Emmanuel Edward</given-names>
        </name>
        <xref ref-type="aff" rid="hbm24811-aff-0001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="hbm24811-aff-0002">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib id="hbm24811-cr-0003" contrib-type="author">
        <name>
          <surname>Akhavein</surname>
          <given-names>Hassan</given-names>
        </name>
        <xref ref-type="aff" rid="hbm24811-aff-0001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="hbm24811-aff-0002">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib id="hbm24811-cr-0004" contrib-type="author">
        <name>
          <surname>Holmes</surname>
          <given-names>Melissa</given-names>
        </name>
        <xref ref-type="aff" rid="hbm24811-aff-0001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="hbm24811-aff-0002">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib id="hbm24811-cr-0005" contrib-type="author">
        <name>
          <surname>Nestor</surname>
          <given-names>Sean</given-names>
        </name>
        <xref ref-type="aff" rid="hbm24811-aff-0001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="hbm24811-aff-0003">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib id="hbm24811-cr-0006" contrib-type="author">
        <name>
          <surname>Ramirez</surname>
          <given-names>Joel</given-names>
        </name>
        <xref ref-type="aff" rid="hbm24811-aff-0001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="hbm24811-aff-0002">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib id="hbm24811-cr-0007" contrib-type="author">
        <name>
          <surname>Adamo</surname>
          <given-names>Sabrina</given-names>
        </name>
        <xref ref-type="aff" rid="hbm24811-aff-0001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="hbm24811-aff-0002">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib id="hbm24811-cr-0008" contrib-type="author">
        <name>
          <surname>Ozzoude</surname>
          <given-names>Miracle</given-names>
        </name>
        <xref ref-type="aff" rid="hbm24811-aff-0001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="hbm24811-aff-0002">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib id="hbm24811-cr-0009" contrib-type="author">
        <name>
          <surname>Scott</surname>
          <given-names>Christopher</given-names>
        </name>
        <xref ref-type="aff" rid="hbm24811-aff-0001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="hbm24811-aff-0002">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib id="hbm24811-cr-0010" contrib-type="author">
        <name>
          <surname>Gao</surname>
          <given-names>Fuqiang</given-names>
        </name>
        <xref ref-type="aff" rid="hbm24811-aff-0001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="hbm24811-aff-0002">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib id="hbm24811-cr-0011" contrib-type="author">
        <name>
          <surname>Martel</surname>
          <given-names>Anne</given-names>
        </name>
        <xref ref-type="aff" rid="hbm24811-aff-0004">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib id="hbm24811-cr-0012" contrib-type="author">
        <name>
          <surname>Swardfager</surname>
          <given-names>Walter</given-names>
        </name>
        <xref ref-type="aff" rid="hbm24811-aff-0002">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="hbm24811-aff-0005">
          <sup>5</sup>
        </xref>
      </contrib>
      <contrib id="hbm24811-cr-0013" contrib-type="author">
        <name>
          <surname>Masellis</surname>
          <given-names>Mario</given-names>
        </name>
        <xref ref-type="aff" rid="hbm24811-aff-0002">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="hbm24811-aff-0006">
          <sup>6</sup>
        </xref>
      </contrib>
      <contrib id="hbm24811-cr-0014" contrib-type="author">
        <name>
          <surname>Swartz</surname>
          <given-names>Richard</given-names>
        </name>
        <xref ref-type="aff" rid="hbm24811-aff-0001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="hbm24811-aff-0002">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="hbm24811-aff-0006">
          <sup>6</sup>
        </xref>
      </contrib>
      <contrib id="hbm24811-cr-0015" contrib-type="author">
        <name>
          <surname>MacIntosh</surname>
          <given-names>Bradley</given-names>
        </name>
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-7300-2355</contrib-id>
        <xref ref-type="aff" rid="hbm24811-aff-0002">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="hbm24811-aff-0004">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib id="hbm24811-cr-0016" contrib-type="author">
        <name>
          <surname>Black</surname>
          <given-names>Sandra E.</given-names>
        </name>
        <xref ref-type="aff" rid="hbm24811-aff-0001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="hbm24811-aff-0002">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="hbm24811-aff-0007">
          <sup>7</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="hbm24811-aff-0001">
      <label>
        <sup>1</sup>
      </label>
      <named-content content-type="organisation-division">LC Campbell Cognitive Neurology Unit</named-content>
      <institution>Hurvitz Brain Sciences Research Program, Sunnybrook Research Institute, University of Toronto</institution>
      <city>Toronto</city>
      <named-content content-type="country-part">Ontario</named-content>
      <country country="CA">Canada</country>
    </aff>
    <aff id="hbm24811-aff-0002">
      <label>
        <sup>2</sup>
      </label>
      <named-content content-type="organisation-division">Canadian Partnership for Stroke Recovery</named-content>
      <institution>Heart and Stroke Foundation</institution>
      <city>Toronto</city>
      <named-content content-type="country-part">Ontario</named-content>
      <country country="CA">Canada</country>
    </aff>
    <aff id="hbm24811-aff-0003">
      <label>
        <sup>3</sup>
      </label>
      <named-content content-type="organisation-division">Department of Psychiatry</named-content>
      <institution>University of Toronto</institution>
      <city>Toronto</city>
      <named-content content-type="country-part">Ontario</named-content>
      <country country="CA">Canada</country>
    </aff>
    <aff id="hbm24811-aff-0004">
      <label>
        <sup>4</sup>
      </label>
      <named-content content-type="organisation-division">Department of Medical Biophysics</named-content>
      <institution>University of Toronto</institution>
      <city>Toronto</city>
      <named-content content-type="country-part">Ontario</named-content>
      <country country="CA">Canada</country>
    </aff>
    <aff id="hbm24811-aff-0005">
      <label>
        <sup>5</sup>
      </label>
      <named-content content-type="organisation-division">Department of Pharmacology and Toxicology</named-content>
      <institution>University of Toronto</institution>
      <city>Toronto</city>
      <named-content content-type="country-part">Ontario</named-content>
      <country country="CA">Canada</country>
    </aff>
    <aff id="hbm24811-aff-0006">
      <label>
        <sup>6</sup>
      </label>
      <named-content content-type="organisation-division">Department of Medicine (Neurology division)</named-content>
      <institution>University of Toronto</institution>
      <city>Toronto</city>
      <named-content content-type="country-part">Ontario</named-content>
      <country country="CA">Canada</country>
    </aff>
    <aff id="hbm24811-aff-0007">
      <label>
        <sup>7</sup>
      </label>
      <named-content content-type="organisation-division">Department of Medical Imaging</named-content>
      <institution>University of Toronto</institution>
      <city>Toronto</city>
      <named-content content-type="country-part">Ontario</named-content>
      <country country="CA">Canada</country>
    </aff>
    <author-notes>
      <corresp id="correspondenceTo"><label>*</label><bold>Correspondence</bold><break/>
Maged Goubran, 2075 Bayview Avenue, M6 West RM 176, Toronto, ON M4N 3M5, Canada.<break/>
Email: <email>maged.goubran@sri.utoronto.ca</email><break/></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>14</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <day>1</day>
      <month>2</month>
      <year>2020</year>
    </pub-date>
    <volume>41</volume>
    <issue>2</issue>
    <issue-id pub-id-type="doi">10.1002/hbm.v41.2</issue-id>
    <fpage>291</fpage>
    <lpage>308</lpage>
    <history>
      <date date-type="received">
        <day>06</day>
        <month>5</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>09</day>
        <month>9</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>19</day>
        <month>9</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <!--<copyright-statement content-type="issue-copyright"> &#x000a9; 2020 Wiley Periodicals, Inc. <copyright-statement>-->
      <copyright-statement content-type="article-copyright">© 2019 The Authors. <italic>Human Brain Mapping</italic> published by Wiley Periodicals, Inc.</copyright-statement>
      <license license-type="creativeCommonsBy-nc">
        <license-p>This is an open access article under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link> License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited and is not used for commercial purposes.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="file:HBM-41-291.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>Hippocampal volumetry is a critical biomarker of aging and dementia, and it is widely used as a predictor of cognitive performance; however, automated hippocampal segmentation methods are limited because the algorithms are (a) not publicly available, (b) subject to error with significant brain atrophy, cerebrovascular disease and lesions, and/or (c) computationally expensive or require parameter tuning. In this study, we trained a 3D convolutional neural network using 259 bilateral manually delineated segmentations collected from three studies, acquired at multiple sites on different scanners with variable protocols. Our training dataset consisted of elderly cases difficult to segment due to extensive atrophy, vascular disease, and lesions. Our algorithm, (HippMapp3r), was validated against four other publicly available state‐of‐the‐art techniques (HippoDeep, FreeSurfer, SBHV, volBrain, and FIRST). HippMapp3r outperformed the other techniques on all three metrics, generating an average Dice of 0.89 and a correlation coefficient of 0.95. It was two orders of magnitude faster than some of the tested techniques. Further validation was performed on 200 subjects from two other disease populations (frontotemporal dementia and vascular cognitive impairment), highlighting our method's low outlier rate. We finally tested the methods on real and simulated “clinical adversarial” cases to study their robustness to corrupt, low‐quality scans. The pipeline and models are available at: <ext-link ext-link-type="uri" xlink:href="https://hippmapp3r.readthedocs.io">https://hippmapp3r.readthedocs.io</ext-link>to facilitate the study of the hippocampus in large multisite studies.</p>
    </abstract>
    <kwd-group kwd-group-type="author-generated">
      <kwd id="hbm24811-kwd-0001">brain atrophy</kwd>
      <kwd id="hbm24811-kwd-0002">convolutional neural networks</kwd>
      <kwd id="hbm24811-kwd-0003">deep learning</kwd>
      <kwd id="hbm24811-kwd-0004">dementia</kwd>
      <kwd id="hbm24811-kwd-0005">hippocampus</kwd>
      <kwd id="hbm24811-kwd-0006">image segmentation</kwd>
    </kwd-group>
    <funding-group>
      <award-group id="funding-0001">
        <funding-source>
          <institution-wrap>
            <institution>Canadian Institute for Health Research (CIHR) MOP Grant </institution>
            <institution-id institution-id-type="open-funder-registry">10.13039/501100000024</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>13129</award-id>
      </award-group>
      <award-group id="funding-0002">
        <funding-source>CIHR Foundation</funding-source>
        <award-id>159910</award-id>
      </award-group>
      <award-group id="funding-0003">
        <funding-source>L.C. Campbell Foundation</funding-source>
      </award-group>
      <award-group id="funding-0004">
        <funding-source>Alzheimer's Disease Neuroimaging Initiative (ADNI)</funding-source>
        <award-id>U01 AG024904</award-id>
      </award-group>
      <award-group id="funding-0005">
        <funding-source>Department of Defense ADNI</funding-source>
        <award-id>W81XWH‐12‐2‐0012</award-id>
      </award-group>
      <award-group id="funding-0006">
        <funding-source>
          <institution-wrap>
            <institution>National Institute on Aging </institution>
            <institution-id institution-id-type="open-funder-registry">10.13039/100000049</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group id="funding-0007">
        <funding-source>
          <institution-wrap>
            <institution>National Institute of Biomedical Imaging and Bioengineering </institution>
            <institution-id institution-id-type="open-funder-registry">10.13039/100000070</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group id="funding-0008">
        <funding-source>
          <institution-wrap>
            <institution>AbbVie, Alzheimer's Association </institution>
            <institution-id institution-id-type="open-funder-registry">10.13039/100006483</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group id="funding-0009">
        <funding-source>
          <institution-wrap>
            <institution>Alzheimer's Drug Discovery Foundation </institution>
            <institution-id institution-id-type="open-funder-registry">10.13039/100002565</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group id="funding-0010">
        <funding-source>Araclon Biotech</funding-source>
      </award-group>
      <award-group id="funding-0011">
        <funding-source>
          <institution-wrap>
            <institution>BioClinica, Inc. </institution>
            <institution-id institution-id-type="open-funder-registry">10.13039/100007742</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group id="funding-0012">
        <funding-source>
          <institution-wrap>
            <institution>Biogen </institution>
            <institution-id institution-id-type="open-funder-registry">10.13039/100005614</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group id="funding-0013">
        <funding-source>
          <institution-wrap>
            <institution>Bristol‐Myers Squibb Company </institution>
            <institution-id institution-id-type="open-funder-registry">10.13039/100002491</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group id="funding-0014">
        <funding-source>CereSpir, Inc.</funding-source>
      </award-group>
      <award-group id="funding-0015">
        <funding-source>Cogstate</funding-source>
      </award-group>
      <award-group id="funding-0016">
        <funding-source>Eisai Inc.</funding-source>
      </award-group>
      <award-group id="funding-0017">
        <funding-source>Elan Pharmaceuticals, Inc.</funding-source>
      </award-group>
      <award-group id="funding-0018">
        <funding-source>
          <institution-wrap>
            <institution>Eli Lilly and Company </institution>
            <institution-id institution-id-type="open-funder-registry">10.13039/100004312</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group id="funding-0019">
        <funding-source>
          <institution-wrap>
            <institution>EuroImmun </institution>
            <institution-id institution-id-type="open-funder-registry">10.13039/100008337</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group id="funding-0020">
        <funding-source>F. Hoffmann‐La Roche Ltd and its affiliated company Genentech, Inc.</funding-source>
      </award-group>
      <award-group id="funding-0021">
        <funding-source>
          <institution-wrap>
            <institution>Fujirebio </institution>
            <institution-id institution-id-type="open-funder-registry">10.13039/501100005062</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group id="funding-0022">
        <funding-source>
          <institution-wrap>
            <institution>GE Healthcare </institution>
            <institution-id institution-id-type="open-funder-registry">10.13039/100006775</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group id="funding-0023">
        <funding-source>IXICO Ltd.</funding-source>
      </award-group>
      <award-group id="funding-0024">
        <funding-source>Janssen Alzheimer Immunotherapy Research and Development LLC.</funding-source>
      </award-group>
      <award-group id="funding-0025">
        <funding-source>Johnson and Johnson Pharmaceutical Research and Development LLC.</funding-source>
      </award-group>
      <award-group id="funding-0026">
        <funding-source>Lumosity</funding-source>
      </award-group>
      <award-group id="funding-0027">
        <funding-source>
          <institution-wrap>
            <institution>Lundbeck </institution>
            <institution-id institution-id-type="open-funder-registry">10.13039/501100013327</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group id="funding-0028">
        <funding-source>Merck and Co., Inc.</funding-source>
      </award-group>
      <award-group id="funding-0029">
        <funding-source>Meso Scale Diagnostics, LLC.</funding-source>
      </award-group>
      <award-group id="funding-0030">
        <funding-source>NeuroRx Research</funding-source>
      </award-group>
      <award-group id="funding-0031">
        <funding-source>Neurotrack Technologies</funding-source>
      </award-group>
      <award-group id="funding-0032">
        <funding-source>
          <institution-wrap>
            <institution>Novartis Pharmaceuticals Corporation </institution>
            <institution-id institution-id-type="open-funder-registry">10.13039/100008272</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group id="funding-0033">
        <funding-source>
          <institution-wrap>
            <institution>Pfizer Inc. </institution>
            <institution-id institution-id-type="open-funder-registry">10.13039/100004319</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group id="funding-0034">
        <funding-source>Piramal Imaging</funding-source>
      </award-group>
      <award-group id="funding-0035">
        <funding-source>
          <institution-wrap>
            <institution>Servier </institution>
            <institution-id institution-id-type="open-funder-registry">10.13039/501100011725</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group id="funding-0036">
        <funding-source>
          <institution-wrap>
            <institution>Takeda Pharmaceutical Company </institution>
            <institution-id institution-id-type="open-funder-registry">10.13039/100008373</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group id="funding-0037">
        <funding-source>Transition Therapeutics</funding-source>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="9"/>
      <table-count count="3"/>
      <page-count count="18"/>
      <word-count count="11068"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>source-schema-version-number</meta-name>
        <meta-value>2.0</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>cover-date</meta-name>
        <meta-value>February 1, 2020</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>details-of-publishers-convertor</meta-name>
        <meta-value>Converter:WILEY_ML3GV2_TO_JATSPMC version:5.8.3 mode:remove_FC converted:03.06.2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <p content-type="self-citation">
      <mixed-citation publication-type="journal" id="hbm24811-cit-9001"><string-name><surname>Goubran</surname><given-names>M</given-names></string-name>, <string-name><surname>Ntiri</surname><given-names>EE</given-names></string-name>, <string-name><surname>Akhavein</surname><given-names>H</given-names></string-name>, et al. <article-title>Hippocampal segmentation for brains with extensive atrophy using three‐dimensional convolutional neural networks</article-title>. <source xml:lang="en">Hum Brain Mapp</source>. <year>2020</year>;<volume>41</volume>:<fpage>291</fpage>–<lpage>308</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.24811</pub-id><pub-id pub-id-type="pmid">31609046</pub-id></mixed-citation>
    </p>
    <fn-group>
      <fn id="hbm24811-note-1002">
        <p><bold>Funding information</bold> Canadian Institute for Health Research (CIHR) MOP Grant, Grant/Award Number: 13129; CIHR Foundation, Grant/Award Number: 159910; L.C. Campbell Foundation; Alzheimer's Disease Neuroimaging Initiative (ADNI), Grant/Award Number: U01 AG024904; Department of Defense ADNI, Grant/Award Number: W81XWH‐12‐2‐0012; National Institute on Aging; National Institute of Biomedical Imaging and Bioengineering; AbbVie, Alzheimer's Association; Alzheimer's Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol‐Myers Squibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoffmann‐La Roche Ltd and its affiliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research and Development LLC.; Johnson and Johnson Pharmaceutical Research and Development LLC.; Lumosity; Lundbeck; Merck and Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda Pharmaceutical Company; Transition Therapeutics</p>
      </fn>
    </fn-group>
  </notes>
</front>
<body id="hbm24811-body-0001">
  <sec id="hbm24811-sec-0001">
    <label>1</label>
    <title>INTRODUCTION</title>
    <p>The hippocampus is implicated in many neurological diseases including Alzheimer's disease (AD), epilepsy, and schizophrenia, among others (Barnes et al., <xref rid="hbm24811-bib-0002" ref-type="ref">2009</xref>; Bernasconi, Natsume, &amp; Bernasconi, <xref rid="hbm24811-bib-0003" ref-type="ref">2005</xref>; Goubran et al., <xref rid="hbm24811-bib-0011" ref-type="ref">2016</xref>; Santyr et al., <xref rid="hbm24811-bib-0035" ref-type="ref">2017</xref>; Steen, Mull, McClure, Hamer, &amp; Lieberman, <xref rid="hbm24811-bib-0038" ref-type="ref">2006</xref>). Hippocampal volumetry has been found to be a critical biomarker of aging and dementia (Courchesne et al., <xref rid="hbm24811-bib-0006" ref-type="ref">2000</xref>; Jack Jr et al., <xref rid="hbm24811-bib-0019" ref-type="ref">1998</xref>). It represents a central correlate of memory function and is widely used as a predictor of cognitive decline, both in research and clinical settings (Jack et al., <xref rid="hbm24811-bib-0020" ref-type="ref">2000</xref>; Rusinek et al., <xref rid="hbm24811-bib-0034" ref-type="ref">2003</xref>; Scahill et al., <xref rid="hbm24811-bib-0036" ref-type="ref">2003</xref>; Sullivan, <xref rid="hbm24811-bib-0039" ref-type="ref">2002</xref>). Hippocampal delineation is also employed for investigation of the hippocampal–neocortical connectivity and studying diffusion magnetic resonance imaging (MRI) changes in the medial temporal lobe. The hippocampal anatomy is variable, and its complex structure is selectively affected in different disorders (Goubran et al., <xref rid="hbm24811-bib-0012" ref-type="ref">2014</xref>). Manual segmentation of the hippocampus is very time consuming and may suffer in reproducibility across different raters. While numerous algorithms have been developed for automated segmentation of the whole hippocampus (Fischl et al., <xref rid="hbm24811-bib-0010" ref-type="ref">2002</xref>; Iglesias et al., <xref rid="hbm24811-bib-0016" ref-type="ref">2015</xref>; Nestor et al., <xref rid="hbm24811-bib-0029" ref-type="ref">2013</xref>; Thyreau, Sato, Fukuda, &amp; Taki, <xref rid="hbm24811-bib-0040" ref-type="ref">2018</xref>), the overwhelming majority suffer from at least one the following issues: (a) the algorithms are not made publicly available or the trained models are not released, (b) they have been trained on young adult brain images and are unable to accurately deal with brain atrophy or lesions associated with aging and neurodegeneration, or (c) they require parameter tuning, large computational time or advanced programming knowledge to execute them. With the increasing amounts of data and large multicenter studies, there is a great need for efficient, easy‐to‐use software that performs accurate quantification of structural biomarkers in elderly subjects while also enabling personalized assessments.</p>
    <p>Recently, deep neural networks, and particularly convolutional neural networks (CNNs), have shown superior performance to other machine learning techniques on computer vision tasks such as image classification (Krizhevsky, Sutskever, &amp; Hinton, <xref rid="hbm24811-bib-0024" ref-type="ref">2012</xref>) and semantic segmentation (Long, Shelhamer, &amp; Darrell, <xref rid="hbm24811-bib-0026" ref-type="ref">2015</xref>). These deep networks have been more recently applied in medical imaging (Çiçek, Abdulkadir, Lienkamp, Brox, &amp; Ronneberger, <xref rid="hbm24811-bib-0005" ref-type="ref">2016</xref>; Kamnitsas et al., <xref rid="hbm24811-bib-0021" ref-type="ref">2017</xref>; Kayalibay, Jensen, &amp; van der Smagt, <xref rid="hbm24811-bib-0022" ref-type="ref">2017</xref>; Milletari, Navab, &amp; Ahmadi, <xref rid="hbm24811-bib-0028" ref-type="ref">2016</xref>; Ronneberger, Fischer, &amp; Brox, <xref rid="hbm24811-bib-0033" ref-type="ref">2015</xref>), among other domains. However, there are several challenges to applying these networks to biomedical data. These supervised machine learning techniques, specifically deep networks, require very large amounts of labeled (ground truth) data, typically in the thousands or millions in the computer vision field, in order to train and optimize millions of weights. Creating databases of manually delineated ground truth labels (in 3D) for medical images requires a large amount of time and training, and hence these databases are scarce or commonly consist of smaller cohorts, typically in the hundreds or even less. Most of the networks developed for computer vision applications rely on a 2D architecture which is suitable for stacks of 2D images or 3D images with small depth. Most whole brain T1‐weighted structural brain scans have close to isotropic resolutions, that is, similar sizes in each dimension, making slice‐by‐slice application of 2D architectures inefficient (Kayalibay et al., <xref rid="hbm24811-bib-0022" ref-type="ref">2017</xref>). Novel network architectures tend to train on larger sections or entire images as opposed to small patches. This training approach creates a class imbalance as it is bound to the original distribution of classes in the dataset, which in medical images is dominated by background (negative) voxels (Kamnitsas et al., <xref rid="hbm24811-bib-0021" ref-type="ref">2017</xref>).</p>
    <p>In this paper, we present HippMapp3r, an open‐source, efficient whole hippocampal segmentation algorithm based on 3D CNNs that is robust to brain atrophy due to neurodegenerative changes. Our deep learning‐based segmentation model was trained on a large database consisting of 209 meticulously hand‐drawn segmentations of elderly subjects with brain atrophy and lesions from multiple studies. Individuals in the current cohort spanned a range of cognitive neurology presentations: cognitively unimpaired controls, patients with mild cognitive impairment (MCI), AD, or temporal lobe epilepsy (TLE). Consequently, numerous types of brain injury were included in the datasets: gray matter atrophy, ventricular enlargement, white matter hyperintensities (WMH) and perivascular spaces. These scans were part of multisite studies using different scanners, field strengths, and scanning protocols. We built 3D CNNs with a U‐net architecture, residual units, and a weighted dice coefficient loss function to deal with class imbalance. The developed model was validated against state‐of‐the‐art techniques to highlight its accuracy and efficiency. We also investigated outliers and failure rates in all tested methods using two additional patient populations, frontotemporal dementia (FTD) and vascular cognitive impairment (VCI). We further tested our model on corrupt data that did not pass quality control and on simulated realistic (clinical) adversarial attacks through sharp decreases in resolution, signal‐to‐noise ratios (SNR), and cropping of field of view (FOV). We are making our tools and models available to the research community and developed an easy‐to‐use pipeline with a graphical user interface (GUI) and thorough documentation to make it accessible to users without extensive programming knowledge.</p>
  </sec>
  <sec id="hbm24811-sec-0002">
    <label>2</label>
    <title>METHODS</title>
    <sec id="hbm24811-sec-0003">
      <label>2.1</label>
      <title>Participants and image acquisition</title>
      <p>A total of 259 participants were used for the hippocampal segmentation model, combined from three separate studies: 100 were recruited from the Sunnybrook Dementia Study (SDS) (Deshpande et al., <xref rid="hbm24811-bib-0008" ref-type="ref">2004</xref>), 135 from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database and the harmonization study (Boccardi et al., <xref rid="hbm24811-bib-0004" ref-type="ref">2015</xref>) and 24 from the University of Pennsylvania (UPenn) TLE atlas (Das et al., <xref rid="hbm24811-bib-0007" ref-type="ref">2009</xref>). Table <xref rid="hbm24811-tbl-0001" ref-type="table">1</xref> presents left and right hippocampal volumes, as well as WMH and stroke volumes to describes the ranges of hippocampal atrophy and lesions in these cohorts.</p>
      <table-wrap id="hbm24811-tbl-0001" xml:lang="en" orientation="portrait" position="float">
        <label>Table 1</label>
        <caption>
          <p>Participants study demographics, clinical diagnosis, MMSE scores, WMH and stroke volumes, and MRI field strength in the train and test datasets</p>
        </caption>
        <table frame="hsides" rules="groups">
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="char" char="." span="1"/>
          <col align="char" char="±" span="1"/>
          <col align="char" char="/" span="1"/>
          <col align="char" char="," span="1"/>
          <col align="char" char="±" span="1"/>
          <col align="char" char="±" span="1"/>
          <col align="center" span="1"/>
          <col align="center" span="1"/>
          <col align="center" span="1"/>
          <col align="center" span="1"/>
          <col align="left" span="1"/>
          <thead valign="bottom">
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" valign="bottom" rowspan="1" colspan="1"/>
              <th align="left" valign="bottom" rowspan="1" colspan="1">Population</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">N</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">Age</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">Sex</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">Dx</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">R Hp volume (mm<sup>3</sup>)</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">L Hp volume (mm<sup>3</sup>)</th>
              <th align="center" valign="bottom" rowspan="1" colspan="1">WMH volume (cc)</th>
              <th align="center" valign="bottom" rowspan="1" colspan="1">Stroke volume (cc)</th>
              <th align="center" valign="bottom" rowspan="1" colspan="1">MMSE</th>
              <th align="center" valign="bottom" rowspan="1" colspan="1">Field strength</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">GT</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td rowspan="3" align="left" valign="top" colspan="1">Train (<italic>N</italic> = 209)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">SDS</td>
              <td align="char" valign="top" rowspan="1" colspan="1">80</td>
              <td align="char" valign="top" rowspan="1" colspan="1">85.3 ± 11.2</td>
              <td align="char" valign="top" rowspan="1" colspan="1">56% M/44% F</td>
              <td align="char" valign="top" rowspan="1" colspan="1">58% AD, 26% NC, 16% VCI</td>
              <td align="char" valign="top" rowspan="1" colspan="1">2,700.8 ± 537.5 (1,610.8, 3,815.6)</td>
              <td align="char" valign="top" rowspan="1" colspan="1">2,633.57 ± 553.82 (1,417.2, 3,854.9)</td>
              <td align="center" valign="top" rowspan="1" colspan="1">11.37 ± 16.47</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0</td>
              <td align="center" valign="top" rowspan="1" colspan="1">21.41 ± 15.12</td>
              <td align="center" valign="top" rowspan="1" colspan="1">1.5 T</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Y</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">ADNI</td>
              <td align="char" valign="top" rowspan="1" colspan="1">109</td>
              <td align="char" valign="top" rowspan="1" colspan="1">74.1 ± 7.8</td>
              <td align="char" valign="top" rowspan="1" colspan="1">51% M/49% F</td>
              <td align="char" valign="top" rowspan="1" colspan="1">43% AD, 16% NC, 10% MCI</td>
              <td align="char" valign="top" rowspan="1" colspan="1">2,797.4 ± 582.32 (1,054.0, 5,029.0)</td>
              <td align="char" valign="top" rowspan="1" colspan="1">2,693.57 ± 586.28 (1,428.1, 5,140.0)</td>
              <td align="center" valign="top" rowspan="1" colspan="1">1.71 ± 3.95</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0</td>
              <td align="center" valign="top" rowspan="1" colspan="1">22.02 ± 6.99</td>
              <td align="center" valign="top" rowspan="1" colspan="1">1.5 T/3 T (50/50%)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Y</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Upenn</td>
              <td align="char" valign="top" rowspan="1" colspan="1">20</td>
              <td align="char" valign="top" rowspan="1" colspan="1"/>
              <td align="char" valign="top" rowspan="1" colspan="1"/>
              <td align="char" valign="top" rowspan="1" colspan="1">100% TLE</td>
              <td align="char" valign="top" rowspan="1" colspan="1">3,224.4 ± 482.9 (2096.6, 4,948.6)</td>
              <td align="char" valign="top" rowspan="1" colspan="1">3,124.6 ± 482.5 (2059.4, 4,948.6)</td>
              <td align="center" valign="top" rowspan="1" colspan="1">—</td>
              <td align="center" valign="top" rowspan="1" colspan="1">—</td>
              <td align="center" valign="top" rowspan="1" colspan="1">—</td>
              <td align="center" valign="top" rowspan="1" colspan="1">3 T</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Y</td>
            </tr>
            <tr>
              <td rowspan="4" align="left" valign="top" colspan="1">Test (<italic>N</italic> = 300)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Mixture from train set</td>
              <td align="char" valign="top" rowspan="1" colspan="1">50</td>
              <td align="char" valign="top" rowspan="1" colspan="1"/>
              <td align="char" valign="top" rowspan="1" colspan="1"/>
              <td align="char" valign="top" rowspan="1" colspan="1"/>
              <td align="char" valign="top" rowspan="1" colspan="1"/>
              <td align="char" valign="top" rowspan="1" colspan="1"/>
              <td align="center" valign="top" rowspan="1" colspan="1"/>
              <td align="center" valign="top" rowspan="1" colspan="1"/>
              <td align="center" valign="top" rowspan="1" colspan="1"/>
              <td align="center" valign="top" rowspan="1" colspan="1">1.5 T/3 T</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Y</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">FTLD</td>
              <td align="char" valign="top" rowspan="1" colspan="1">95</td>
              <td align="char" valign="top" rowspan="1" colspan="1">78.0 ± 11.4</td>
              <td align="char" valign="top" rowspan="1" colspan="1">52% M/48% F</td>
              <td align="char" valign="top" rowspan="1" colspan="1">100% FTLD</td>
              <td align="char" valign="top" rowspan="1" colspan="1">3,039.6 ± 591.3 (1,470.3, 4,061.6)</td>
              <td align="char" valign="top" rowspan="1" colspan="1">2,813.6 ± 563.9 (1,321.1, 3,938.7)</td>
              <td align="center" valign="top" rowspan="1" colspan="1">5.49 ± 9.36</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0</td>
              <td align="center" valign="top" rowspan="1" colspan="1">22.27 ± 7.11</td>
              <td align="center" valign="top" rowspan="1" colspan="1">1.5 T</td>
              <td align="left" valign="top" rowspan="1" colspan="1">N</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">VCI</td>
              <td align="char" valign="top" rowspan="1" colspan="1">105</td>
              <td align="char" valign="top" rowspan="1" colspan="1">90.6 ± 9.5</td>
              <td align="char" valign="top" rowspan="1" colspan="1">54% M/46% F</td>
              <td align="char" valign="top" rowspan="1" colspan="1">100% VCI</td>
              <td align="char" valign="top" rowspan="1" colspan="1">3,043.6 ± 618.2 (494.5, 4,385)</td>
              <td align="char" valign="top" rowspan="1" colspan="1">2,898.8 ± 634.7 (558.8, 4,515)</td>
              <td align="center" valign="top" rowspan="1" colspan="1">18.86 ± 19.93</td>
              <td align="center" valign="top" rowspan="1" colspan="1">17.14 ± 34.15</td>
              <td align="center" valign="top" rowspan="1" colspan="1">23.95 ± 4.58</td>
              <td align="center" valign="top" rowspan="1" colspan="1">1.5 T</td>
              <td align="left" valign="top" rowspan="1" colspan="1">N</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Adversarial cases</td>
              <td align="char" valign="top" rowspan="1" colspan="1">50</td>
              <td align="char" valign="top" rowspan="1" colspan="1">90.5 ± 5.6</td>
              <td align="char" valign="top" rowspan="1" colspan="1">64% M/36% F</td>
              <td align="char" valign="top" rowspan="1" colspan="1">44% AD, 24% NC, 18% VCI, 14% FTD</td>
              <td align="char" valign="top" rowspan="1" colspan="1">3,173.6 ± 620.1 (1,772.9, 4,457.0)</td>
              <td align="char" valign="top" rowspan="1" colspan="1">3,050.5 ± 639.0 (1,282.0, 4,040.0)</td>
              <td align="center" valign="top" rowspan="1" colspan="1">—</td>
              <td align="center" valign="top" rowspan="1" colspan="1">—</td>
              <td align="center" valign="top" rowspan="1" colspan="1">21.38 ± 6.34</td>
              <td align="center" valign="top" rowspan="1" colspan="1">1.5 T</td>
              <td align="left" valign="top" rowspan="1" colspan="1">N</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot id="hbm24811-ntgp-0001">
          <fn id="hbm24811-note-0001">
            <p>Abbreviations: AD, Alzheimer's disease; ADNI, Alzheimer's disease neuroimaging initiative; Dx, diagnosis; FTLD, fronto‐temporal lobar dementia; GT, ground truth; Hp, hippocampus; MMSE, mini‐mental state examination; MRI, magnetic resonance imaging; NC, normal controls; SDS, Sunnybrook Dementia Study; Upenn, University of Pennsylvania; VCI, vascular cognitive impairment; WMH, white matter hyperintensity.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>All SDS patients were recruited from the LC Campbell Cognitive Neurology Research Unit, Sunnybrook Health Sciences Centre at the University of Toronto (age: 71 ± 10, 28 Males). The T1‐weighted three‐dimensional volumetric scan was acquired using a 1.5 Tesla Signa system (GE Healthcare, Chicago, Illinois). The acquisition parameters for the T1 spoiled gradient echo sequence were: 124 slices; matrix, 256 × 192; 22 × 16.5 cm FOV; number of excitations, 1; echo time/repetition time, 35 ms/5 ms; flip angle, 35°, and in‐plane resolution of 0.859 × 0.859 mm with slice thickness between 1.2 and 1.4 mm depending on head size. ADNI subjects were scanned at multiple sites using a mixture of 1.5 T (<italic>N</italic> = 68) and 3 T (<italic>N</italic> = 67) scanners (age: 75 ± 8, 70 males). Participants were nearly equal in cohort size for the three diagnostic groups (normal, MCI and AD), and three scanner major manufacturers (GE, Siemens, and Philips). The ADNI‐GO/2 MRI protocol has been optimized to provide comparable images from different 3 T platforms from the three manufacturers. The T1 weighted magnetization prepared rapid gradient echo (MPRAGE) had the following parameters: TR/TE/TI = 2300/2.95/900 ms, sagittal, 1.1 × 1.1 × 1.2 mm spatial resolution. UPenn subjects were scanned at a 3 T Siemens Trio scanner using an eight‐channel head coil and body coil transmitter. The T1‐weighted structural MRI scan used the MPRAGE sequence with the following parameters: TR = 1,620 ms, TE = 3.87 ms, TI = 950 ms, flip angle = 15°, and voxel size 0.9375 × 0.9375 × 1 mm.</p>
    </sec>
    <sec id="hbm24811-sec-0004">
      <label>2.2</label>
      <title>Model architecture and contributions</title>
      <p>Our algorithm consists of a serial “ensemble” of two networks, an initial network trained on the whole brain and a second network with the same architecture trained on the first network's output (operating on a reduced FOV centered around the initial segmentation). Defining the architecture of deep CNN networks and loss functions are important factors in the construction of a deep model and are often guided by the specific application the network is set to achieve. Our CNN networks are based on a convolutional autoencoder‐like (U‐net) architecture (Çiçek et al., <xref rid="hbm24811-bib-0005" ref-type="ref">2016</xref>; Milletari et al., <xref rid="hbm24811-bib-0028" ref-type="ref">2016</xref>; Ronneberger et al., <xref rid="hbm24811-bib-0033" ref-type="ref">2015</xref>), which consists of contracting and expanding pathways (stages) and is trained on the entire image rather than patches. The contracting pathway encodes context (representations of the input) and the expanding pathway recombines encoded representations with shallower features to enable precise localization of the voxels of interest (i.e., the hippocampus). The overall architecture of the proposed 3D network (Figure <xref rid="hbm24811-fig-0001" ref-type="fig">1</xref>) is inspired by the original 2D U‐net (Ronneberger et al., <xref rid="hbm24811-bib-0033" ref-type="ref">2015</xref>) with a few modifications: (a) In this work we updated the original design with residual blocks (He, Zhang, Ren, &amp; Sun, <xref rid="hbm24811-bib-0015" ref-type="ref">2016</xref>; Kayalibay et al., <xref rid="hbm24811-bib-0022" ref-type="ref">2017</xref>; Milletari et al., <xref rid="hbm24811-bib-0028" ref-type="ref">2016</xref>) that ease optimization convergence by improving gradient flow and enable higher accuracy through a deeper network. (b) Our custom residual blocks consisted of two convolution blocks (convolution layer with normalization and nonlinearities) separated by a dropout layer to avoid overfitting. (c) We employed deep supervision (Kayalibay et al., <xref rid="hbm24811-bib-0022" ref-type="ref">2017</xref>; Lee, Xie, Gallagher, Zhang, &amp; Tu, <xref rid="hbm24811-bib-0025" ref-type="ref">2014</xref>) at the expanding pathway by adding earlier feature maps at different levels of the network and combining them via element‐wise summation to form the final network output. (d) Since the 3D network is memory expensive, we opted to use instance normalization (Isensee, Kickingereder, Wick, Bendszus, &amp; Maier‐Hein, <xref rid="hbm24811-bib-0017" ref-type="ref">2018</xref>; Ulyanov, Vedaldi, &amp; Lempitsky, <xref rid="hbm24811-bib-0042" ref-type="ref">2016</xref>) instead of the commonly used batch normalization as the stochasticity generated by a small batch size may destabilize batch normalization. (e) To generate segmentation maps from the entire input image, we relied on trainable deconvolution kernels as the upsampling operations. (f) Finally, we chose a loss function based on the Dice similarity coefficient (Dice, <xref rid="hbm24811-bib-0009" ref-type="ref">1945</xref>) (see <xref rid="hbm24811-sec-0005" ref-type="sec">Section 2.3</xref>).</p>
      <fig fig-type="Figure" xml:lang="en" id="hbm24811-fig-0001" orientation="portrait" position="float">
        <label>Figure 1</label>
        <caption>
          <p>Proposed base network architecture with insets for the convolution, residual, feature and upsample blocks. (a) The 3D layers are color‐coded to distinguish their different functionality and presented as 2D representations for simplification. For each layer, the number of features is shown at the bottom. “<italic>Conv</italic>” represents a convolution, “<italic>k</italic>” represents the kernel, “<italic>s</italic>” denotes the number of strides. (b) Overall scheme demonstrating the use of an ensemble of two consecutive networks to produce the final segmentation [Color figure can be viewed at <ext-link ext-link-type="uri" xlink:href="http://wileyonlinelibrary.com">http://wileyonlinelibrary.com</ext-link>]</p>
        </caption>
        <graphic id="nlm-graphic-1" xlink:href="HBM-41-291-g001"/>
      </fig>
      <p>The network is fully convolutional, that is no fully connected layers are added, and hence can predict a variable number of voxels in a forward pass without the need for architectural changes (Long et al., <xref rid="hbm24811-bib-0026" ref-type="ref">2015</xref>). It employs skip connections to combine feature maps across stages through concatenation. The network has a depth of five and 16 initial filters, with the number of filters doubling every contraction step. The building blocks of the networks are convolution blocks, consisting of a convolution layer followed by a normalization layer and a nonlinearity. We chose the leaky ReLU as the activation function with a negative slope of 10<sup>−2</sup> for all feature map convolutions.</p>
      <p>The residual blocks separate the input data into two paths, the first applies weights and nonlinearities, and the second applies an identity mapping (the input data is unchanged). The two paths are finally merged with the element‐wise sum, resulting in the following formulation:<disp-formula id="hbm24811-disp-0001"><label>(1)</label><mml:math id="nlm-math-1"><mml:mi mathvariant="normal">y</mml:mi><mml:mfenced open="(" close=")"><mml:mi mathvariant="normal">x</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="normal">σ</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi mathvariant="normal">W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi mathvariant="normal">W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mfenced></mml:math></disp-formula>where <italic>W</italic>
<sub><italic>1</italic></sub> and <italic>W</italic>
<sub><italic>2</italic></sub> are the weights of the convolutional layers and <italic>σ</italic> is the activation function. The residual block consisted of two convolution blocks, separated by a dropout layer. We chose a dropout rate of 0.3 and a small convolution kernel of 3 × 3 × 3, which enables a higher nonlinearity capacity for the same receptive field with a lower number of parameters needed. The residual blocks are preceded by a strided convolution layer with a kernel of 3 × 3 × 3 and a stride of two, effectively reducing the number of feature dimensions by a factor of two while adding more features as the network depth increases.</p>
      <p>In the expanding pathway, we relied on upsampling blocks that repeat the feature voxels twice in each spatial dimension, followed by concatenation to combine the upsampled features with those from the corresponding contracting pathway. After concatenation, a feature block recombines these features together and halves the number of feature maps at each step to reduce memory consumption. Upsampling blocks consisted of deconvolution layers, followed by a convolution block with kernel 3 × 3 × 3. As for the feature blocks, they consisted of two consecutive convolution blocks with the first having a convolution layer with a kernel size of 3 × 3 × 3 and the second having a kernel of 1 × 1 × 1. As discussed before, we added earlier feature maps at different levels (<italic>n</italic> = 3) of the network and combined them via element‐wise summation to form the final output. Feature maps from the last layer were passed to a softmax function that generates pseudo‐class probability maps as:<disp-formula id="hbm24811-disp-0002"><label>(2)</label><mml:math id="nlm-math-2"><mml:msub><mml:mi>ρ</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:msub><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mfenced open="(" close=")"><mml:msub><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:msubsup><mml:mi>exp</mml:mi><mml:mfenced open="(" close=")"><mml:msub><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>∀</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula>where c denotes class and C the total number of classes, that is, either a hippocampus voxel or not, in‐turn producing a class‐likelihood probability for each voxel in the image.</p>
    </sec>
    <sec id="hbm24811-sec-0005">
      <label>2.3</label>
      <title>Loss function</title>
      <p>To mitigate the class imbalance issue (the majority of image voxels do not represent the structure of interest), previous work (Çiçek et al., <xref rid="hbm24811-bib-0005" ref-type="ref">2016</xref>; Ronneberger et al., <xref rid="hbm24811-bib-0033" ref-type="ref">2015</xref>) applied a weight map to the categorical cross‐entropy loss (objective) function. We sought to maximize the Dice similarity coefficient (Dice, <xref rid="hbm24811-bib-0009" ref-type="ref">1945</xref>) as suggested by (Milletari et al., <xref rid="hbm24811-bib-0028" ref-type="ref">2016</xref>). We employed a formulation of Dice similarity as an equally weighted dice loss function. An advantage of this approach is that it does not rely on hyperparameters. The Dice similarity (Dice, <xref rid="hbm24811-bib-0009" ref-type="ref">1945</xref>) is an overlap metric commonly used to quantify segmentation accuracy. It is defined as follows between two binary volumes:<disp-formula id="hbm24811-disp-0003"><label>(3)</label><mml:math id="nlm-math-3"><mml:mi mathvariant="normal">D</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mo>∑</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:msubsup><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:msubsup><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:msubsup><mml:msubsup><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:math></disp-formula> where <italic>p</italic>
<sub><italic>i</italic></sub> are voxels of the predicted binary segmentation volume <italic>p</italic>
<sub><italic>i</italic></sub> ? <italic>P</italic> and <italic>g</italic>
<sub><italic>i</italic></sub> are voxels of the ground truth volume <italic>g</italic>
<sub><italic>i</italic></sub> ? <italic>G</italic>. This formulation is differentiable and can be incorporated in the network, yielding the following gradient:<disp-formula id="hbm24811-disp-0004"><label>(4)</label><mml:math id="nlm-math-4"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi mathvariant="normal">j</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mfenced open="[" close="]"><mml:mfrac><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi mathvariant="normal">j</mml:mi></mml:msub><mml:mfenced open="[" close="]"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:msubsup><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:msubsup><mml:msubsup><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>p</mml:mi><mml:mi mathvariant="normal">j</mml:mi></mml:msub><mml:mfenced open="[" close="]"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:msubsup><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:msup><mml:mfenced open="[" close="]"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:msubsup><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:msubsup><mml:msubsup><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mfenced></mml:math></disp-formula>
</p>
    </sec>
    <sec id="hbm24811-sec-0006">
      <label>2.4</label>
      <title>Data preprocessing, augmentation and model training</title>
      <p>Prior to training, all images were bias field corrected for B1 inhomogeneities using <italic>N4</italic> (Tustison et al., <xref rid="hbm24811-bib-0041" ref-type="ref">2010</xref>). They were then standardized to have a zero mean and unit variance within a local neighborhood of 50 voxels using <italic>c3d</italic> (Yushkevich et al., <xref rid="hbm24811-bib-0043" ref-type="ref">2006</xref>). We opted for neighborhood normalization instead of global image normalization to better preserve local features. We performed a total of three augmentations per scan including flipping images along Left–Right and ± 15° rotations in the L–R axis. SDS datasets were not L–R flipped as the data comes from 50 unique subjects segmented twice each (after L–R flipping), generating 100 ground truth segmentations. No deformable augmentation was employed, relatively preserving the anatomy of input images. Models were trained for 300 epochs and early stopping was set to 50 epochs where validation loss did not improve. We used the Adam optimizer (Kingma &amp; Ba, <xref rid="hbm24811-bib-0023" ref-type="ref">2014</xref>) with an initial learning rate of 5 × 10<sup>−3</sup>, a patience of 10 epochs for the validation loss and a learning rate drop (decay factor) of 0.5.</p>
      <p>Both networks trained on single contrast T1‐weighted image inputs. The initial 3D U‐net trained on downsampled T1 images (to a size of 128 × 128 × 128), and the second network trained on a limited FOV of 112 × 112 × 64 voxels centered around the initial segmentation. The initial network was trained on a mixture of skull‐stripped and intact‐skull images. Each network was trained with five‐fold cross validation. Out of the 259 datasets with manual hippocampal segmentations used in this study, 184 (~70%) were used for training, 50 (~20%) for testing and 25 (~10%) for validation during training. With data augmentation, this split generated a total of 502 training samples. The networks were implemented using Keras (using Tensorflow backend) and trained on a GeForce GTX1080 Ti graphics card with 11Gb of memory and a Pascal architecture (NVIDIA, Santa Clara, CA). The algorithm and trained model are available at: <ext-link ext-link-type="uri" xlink:href="https://hippmapp3r.readthedocs.io">https://hippmapp3r.readthedocs.io</ext-link>.</p>
    </sec>
    <sec id="hbm24811-sec-0007">
      <label>2.5</label>
      <title>Evaluation of clinical datasets</title>
      <p>The model was tested on two datasets (Table <xref rid="hbm24811-tbl-0001" ref-type="table">1</xref>). Fifty subjects with manually traced ground truth from the aforementioned studies were used for the first dataset. Images from 200 additional subjects participating in the SDS were used in the second dataset from two separate disease cohorts: FTD who have severe atrophy particularly in the temporal regions, and VCI who typically have strokes and severe WMH burden. The disease cohorts in this test set are characterized by atrophy, vascular lesions and features not present in our training set. The second set was employed to investigate outliers and failure rates. Manual ground truth segmentation was not available for this set.</p>
      <p>The model was compared against established state‐of‐the‐art techniques for the two test sets: FreeSurfer's whole hippocampus segmentation (version 6.0) (Fischl et al., <xref rid="hbm24811-bib-0010" ref-type="ref">2002</xref>), FSL's subcortical segmentation (FIRST) (v. 5.0.10) (Patenaude, Smith, Kennedy, &amp; Jenkinson, <xref rid="hbm24811-bib-0030" ref-type="ref">2011</xref>), an in‐house developed segmentation tool (SBHV) (v. 1.0) (Nestor et al., <xref rid="hbm24811-bib-0029" ref-type="ref">2013</xref>), a multi‐atlas patch‐based model (volBrain) (v. 1.0) (Manjón &amp; Coupé, <xref rid="hbm24811-bib-0027" ref-type="ref">2016</xref>), and a newly developed CNN‐based model (Hippodeep) (v. 0.1) (Thyreau et al., <xref rid="hbm24811-bib-0040" ref-type="ref">2018</xref>). FreeSurfer's hippocampus algorithm combines in‐vivo and ex‐vivo tracings into a computational atlas that employs Bayesian inference to segment the hippocampal subfields. FIRST is a Bayesian model‐based tool with deformable surfaces that rely on shape and appearance for segmentation. SBHV employs a multi‐atlas‐based segmentation approach. Hippodeep uses a relatively shallow CNN trained on both FreeSurfer's hippocampal segmentation from multiple large cohorts and augmented iterations of a much smaller manually ground‐truth dataset. volBrain is an open‐source, automatic online tool that provides a series of image segmentation tasks. It employs a modified edition of nonlocal label fusion for subcortical structure segmentation.</p>
      <p>While there were slight differences in protocols between the tested segmentation methods and the ground truth used for our models, all protocols used very similar border definitions for the hippocampus. ADNI, volBrain, and SBHV used the EADC‐ADNI Harmonized Protocol (HarP) (Boccardi et al., <xref rid="hbm24811-bib-0004" ref-type="ref">2015</xref>), consisting of the hippocampal head, body, the alveus/fimbria up to the separation from the fornix, the medial border of the hippocampal body and subiculum, and the whole hippocampal tail. The UPenn atlas used data that was segmented using a semiautomated pipeline (Pluta et al., <xref rid="hbm24811-bib-0032" ref-type="ref">2009</xref>) and included the hippocampus proper, the dentate gyrus, the alveus, the fimbria, and the subiculum (Anon, <xref rid="hbm24811-bib-0001" ref-type="ref">2005</xref>; Hasboun et al., <xref rid="hbm24811-bib-0013" ref-type="ref">1996</xref>). Like the previous methods, the tracings included all rostrocaudal parts of the hippocampus. FIRST, Hippodeep, and Freesurfer share the same protocol for segmenting the hippocampus, which included the dentate gyrus, the hippocampus proper, the prosubiculum, and the subiculum (<ext-link ext-link-type="uri" xlink:href="http://freesurfer.net/fswiki/CMA">http://freesurfer.net/fswiki/CMA</ext-link>). Careful distinction is paid between the hippocampus, the amygdala, the thalamus and the temporal horn of the lateral ventricle (Schoemaker et al., <xref rid="hbm24811-bib-0037" ref-type="ref">2016</xref>).</p>
    </sec>
    <sec id="hbm24811-sec-0008">
      <label>2.6</label>
      <title>Clinical adversarial attacks</title>
      <p>To further validate our model, we tested it on challenging images (<italic>n</italic> = 50) that did not pass our quality control (QC) protocol and were deemed corrupt due to motion, low SNR and ringing artifacts; herein referred to as “adversarial attacks,” a commonly used term in the deep learning field referring to engineered inputs with perturbations presented to neural networks in order to drive them to produce errors and study their robustness toward different inputs.</p>
      <p>Furthermore, we performed additional validation experiments whereby we simulated low‐quality clinical grade or challenging scans acquired with different acquisitions and scanners. These simulated adversarial attacks included: (a) decreases in resolution, (b) addition of noise, and (c) cropping the FOV. Input images were downsampled by a factor of two in all dimensions, and 2× in‐plane with 4× out‐of‐plane (in the <italic>z</italic>‐dimension), to simulate clinical imaging protocols which tend to acquire images with lower resolutions and thicker slices. Speckle noise with a <italic>σ</italic> = {0.1, 0.3} was applied to input images to simulate scans with lower SNR and/or low‐field strength acquisitions. Salt and pepper noise with a <italic>σ</italic> = 0.1 were also applied to simulate signal dropout in random voxels. Finally, we also cropped the input sequences in the Superior–Inferior plane 15% from each side to simulate scans with a limited FOV.</p>
    </sec>
    <sec id="hbm24811-sec-0009">
      <label>2.7</label>
      <title>Validation metrics</title>
      <p>Four metrics were used to evaluate model performance against manual segmentation: the Pearson R correlation coefficient of the volumes, the Dice similarity coefficient, the Jaccard coefficient, and the Hausdorff distance. We used the Pearson R correlation coefficient (Pearson, <xref rid="hbm24811-bib-0031" ref-type="ref">1895</xref>) between manually segmented volumes and volumes generated through model predictions to assess the clinical utility of the predictions. The Dice coefficient has been described in <xref rid="hbm24811-sec-0005" ref-type="sec">Section 2.3</xref> and the Jaccard coefficient (Jaccard, <xref rid="hbm24811-bib-0018" ref-type="ref">1912</xref>) is another metric to assess the degree of similarity between two sets (sometimes referred to as the intersection over the union or IoU) as defined by:<disp-formula id="hbm24811-disp-0005"><label>(5)</label><mml:math id="nlm-math-5"><mml:mtext>Jacc</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mfenced open="|" close="|"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>∩</mml:mo><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:mfenced><mml:mfenced open="|" close="|"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>∪</mml:mo><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:mfenced></mml:mfrac></mml:math></disp-formula>where <italic>P</italic> and <italic>G</italic> are the predicted and ground truth masks. The Hausdorff distance was used to evaluate the similarity in shape between the ground truth and each segmentation method. The Hausdorff distance of two objects in the same space is defined as the largest distance in a set of all closest distances between both sets of points:<disp-formula id="hbm24811-disp-0006"><mml:math id="nlm-math-6"><mml:mi>H</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:mfenced open="{" close="}"><mml:mrow><mml:msub><mml:mi>max</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mfenced open="{" close="}"><mml:mrow><mml:msub><mml:mi>min</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mfenced open="{" close="}"><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mi>max</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mfenced open="{" close="}"><mml:mrow><mml:msub><mml:mi>min</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mfenced open="{" close="}"><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></disp-formula>
</p>
      <p>Volumes and segmentations for both hemispheres were used for the quantitative analyses across all models. In the second test set, since no manual ground truth labels were available, we relied on computing Z‐scores of the hippocampal surface areas of the subjects to highlight outliers and failed segmentations. For every subject, a mean surface area was computed from the different tested methods while excluding any values lower than the lower quartile (25th percentile) or greater than the upper quartile (75th percentile) by more than 1.5× the interquartile range (iqr = upper quartile − lower quartile). The results were visually checked to further exclude any segmentations that did not pass quality control for quantification of subject means. Surface area <italic>z</italic>‐scores were then computed for every method based on the filtered subject mean per participant, for each hemisphere separately. Very high or low <italic>z</italic>‐score values then highlight potential outliers or failure cases (substantial deviation from the filtered mean computed across all methods).</p>
    </sec>
  </sec>
  <sec id="hbm24811-sec-0010">
    <label>3</label>
    <title>RESULTS</title>
    <sec id="hbm24811-sec-0011">
      <label>3.1</label>
      <title>Evaluation of clinical datasets</title>
      <p>While all tested techniques provided significant correlations between predicted and manual volumes, our model generated the highest agreement with ground truth labels (<italic>r</italic> = 0.95) and the lowest number of outliers (Figure <xref rid="hbm24811-fig-0002" ref-type="fig">2</xref>a). The distributions of Dice and Jaccard coefficients between ground truth manual labels of the hippocampus and predicted segmentations are presented in Figure <xref rid="hbm24811-fig-0002" ref-type="fig">2</xref>b. The shape of the distributions was computed using kernel density estimation with Gaussian (normal) kernels. Our network had a tight Dice coefficient distribution (similarly with Jaccard) centered around 0.869 ± 0.033 (0.870 ± 0.030 left, 0.868 ± 0.036 right) followed by SBHV, volBrain, FIRST, Hippodeep, and FreeSurfer. The SBHV pipeline had a better dice and standard deviation than volBrain, FIRST, Hippodeep, and FreeSurfer but with a trade‐off of the longest computational time, taking at least 8 hrs per subject. While volBrain had a better Dice, Jaccard and Hausdorff distance than FIRST, Hippodeep and Freesurfer, the Dice accuracy for this patch‐based algorithm may be lower than those reported in the literature due to the higher atrophy level in our test set (Manjón &amp; Coupé, <xref rid="hbm24811-bib-0027" ref-type="ref">2016</xref>). While Hippodeep and FreeSurfer's distributions were centered around 0.76 and 0.74, respectively, they had multiple cases with a Dice lower than 0.65. FIRST failed to run on eight subjects, producing errors without completion (mainly SDS cases) and generated misregistrations for another four, producing outputs with incorrect orientations. Hippodeep's standard deviation was more than 3× higher than the average standard deviation of the other tested algorithms, possibly highlighting the instability of deep networks when presented with different inputs than the training set. Dice, Jaccard similarity coefficients, and Hausdorff distance between ground truth manual labels of the hippocampus and predicted segmentations for the six tested techniques are summarized in Table <xref rid="hbm24811-tbl-0002" ref-type="table">2</xref>. Our model outperformed the other state‐of‐the‐art techniques by 5% or more.</p>
      <fig fig-type="Figure" xml:lang="en" id="hbm24811-fig-0002" orientation="portrait" position="float">
        <label>Figure 2</label>
        <caption>
          <p>Validation of hippocampal segmentation through volume correlations, Hausdorff distances and Dice, Jaccard similarity coefficients on our proposed model, HippMapp3r (blue) and five established techniques: Hippodeep (yellow), FreeSurfer (green), SBHV (red), FIRST (purple), and volBrain (brown). The proposed model produced the best agreement to manual labels among the six tested techniques in all four metrics. Results for all four metrics displayed for both left and right hippocampi. (a) Correlations between the manually segmented volumes and volumes generated through model predictions. Pearson R correlation coefficients and P‐values are shown for each test. Our method produced the highest volume correlation for both hippocampi (<italic>r</italic> = .95, <italic>p</italic> &lt; .000001). (b) Distribution of Dice coefficients, Hausdorff distances, and Jaccard coefficients (in said order) between ground truth manual labels of the hippocampus and predicted segmentations. Ticks on the x‐axis represent individual segmentation cases (colored by tested technique). Distributions are generated using Gaussian kernel density estimation [Color figure can be viewed at <ext-link ext-link-type="uri" xlink:href="http://wileyonlinelibrary.com">http://wileyonlinelibrary.com</ext-link>]</p>
        </caption>
        <graphic id="nlm-graphic-3" xlink:href="HBM-41-291-g002"/>
      </fig>
      <table-wrap id="hbm24811-tbl-0002" xml:lang="en" orientation="portrait" position="float">
        <label>Table 2</label>
        <caption>
          <p>Dice, Jaccard coefficients, Hausdorff distances, and computational time for hippocampal segmentation methods</p>
        </caption>
        <table frame="hsides" rules="groups">
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <thead valign="bottom">
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" valign="bottom" rowspan="1" colspan="1">Hemisphere</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1"/>
              <th align="left" valign="bottom" rowspan="1" colspan="1">HippMapp3r (mean ± std)</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">Hippodeep (mean ± std)</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">FreeSurfer (mean ± std)</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">SBHV (mean ± std)</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">FIRST<xref ref-type="fn" rid="hbm24811-note-0002">a</xref> (mean ± std)</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">VolBrain (mean ± std)</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td rowspan="3" align="left" valign="top" colspan="1">Left</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Dice coefficient</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.870 ± 0.030</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.781 ± 0.165</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.761 ± 0.031</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.832 ± 0.045</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.794 ± 0.028</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.825 ± 0.036</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Jaccard coefficient</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.771 ± 0.047</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0. 689 ± 0.073</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.615 ± 0.040</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.827 ± 0.051</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.645 ± 0.111</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.704 ± 0.050</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Hausdorff distance (mm)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">1.487 ± 0.485</td>
              <td align="left" valign="top" rowspan="1" colspan="1">3.454 ± 6.296</td>
              <td align="left" valign="top" rowspan="1" colspan="1">2.172 ± 0.314</td>
              <td align="left" valign="top" rowspan="1" colspan="1">1.676 ± 0.584</td>
              <td align="left" valign="top" rowspan="1" colspan="1">2.798 ± 3.689</td>
              <td align="left" valign="top" rowspan="1" colspan="1">1.794 ± 0.463</td>
            </tr>
            <tr>
              <td rowspan="3" align="left" valign="top" colspan="1">Right</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Dice coefficient</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.868 ± 0.036</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.775 ± 0.192</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.769 ± 0.037</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.827 ± 0.051</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.788 ± 0.036</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.828 ± 0.035</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Jaccard coefficient</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.768 ± 0.056</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0. 700 ± 0.057</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.625 ± 0.047</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.708 ± 0.074</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.657 ± 0.043</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.708 ± 0.050</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Hausdorff distance (mm)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">1.440 ± 0.047</td>
              <td align="left" valign="top" rowspan="1" colspan="1">2.915 ± 5.281</td>
              <td align="left" valign="top" rowspan="1" colspan="1">1.988 ± 0.364</td>
              <td align="left" valign="top" rowspan="1" colspan="1">1.793 ± 0.594</td>
              <td align="left" valign="top" rowspan="1" colspan="1">2.291 ± 0.489</td>
              <td align="left" valign="top" rowspan="1" colspan="1">1.740 ± 0.407</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1">Approx. compute time</td>
              <td align="left" valign="top" rowspan="1" colspan="1">14 s</td>
              <td align="left" valign="top" rowspan="1" colspan="1">30 s</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Whole brain: 6 hrs, Hipp.: 7 min (12 cores)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">7 hrs</td>
              <td align="left" valign="top" rowspan="1" colspan="1">6 min</td>
              <td align="left" valign="top" rowspan="1" colspan="1">10 min</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot id="hbm24811-ntgp-0002">
          <fn id="hbm24811-note-0002">
            <label>a</label>
            <p>FIRST failed to run (producing errors) on eight subjects and generated outputs with wrong orientation on another four.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>To test whether an ensemble of networks would lead to a significant improvement in segmentation accuracy we ran the top three performing networks from our cross‐validation experiment (only for the small FOV networks with the same initial whole‐brain prediction) on our test data and averaged the resulting probability maps. The combined prediction of the top‐three ensemble did not produce a significant improvement in accuracy and only resulted in an average of 0.3% dice improvement. Hippodeep appeared to substantially under‐segment a few cases, while FIRST and FreeSurfer over‐segmented multiple patients. Examples of the segmentation results for all tested methods on three participants are shown in Figure <xref rid="hbm24811-fig-0003" ref-type="fig">3</xref>. The border of the hippocampus proper is depicted to highlight that the mis‐segmentations by other techniques are not mainly due to differences in border definitions but to under‐ or over‐segmentation errors of the gray matter structures. Figure <xref rid="hbm24811-fig-0004" ref-type="fig">4</xref> depicts more examples of mis‐segmentations by the tested automated algorithms, specifically mis‐segmentation of the hippocampus proper and segmentation of neighboring white matter structures and CS. It should be noted that our algorithm was trained on data from the same multisite studies as the first test set and that SBHV uses subjects from the SDS study as template atlases.</p>
      <fig fig-type="Figure" xml:lang="en" id="hbm24811-fig-0003" orientation="portrait" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Visual comparison of the five tested segmentation methods on three example subjects. Rows a and b display sagittal slices of the left and right hippocampi, respectively, while rows c through e display segmentations in coronal slices. Manually traced ground truth is displayed in red on the far‐left column. An outline of the ground truth is overlaid on each segmentation output. Regions where over segmentation occurred are indicated by blue arrows. Regions where under segmentation occurred are indicated by yellow arrows [Color figure can be viewed at <ext-link ext-link-type="uri" xlink:href="http://wileyonlinelibrary.com">http://wileyonlinelibrary.com</ext-link>]</p>
        </caption>
        <graphic id="nlm-graphic-5" xlink:href="HBM-41-291-g003"/>
      </fig>
      <fig fig-type="Figure" xml:lang="en" id="hbm24811-fig-0004" orientation="portrait" position="float">
        <label>Figure 4</label>
        <caption>
          <p>Visual comparison of the six tested segmentation methods on one example subject. Rows <bold>a</bold> through d display segmentations in coronal slices, while rows e and f display sagittal slices of the left and right hippocampi, respectively. Regions where over segmentation into neighboring white matter occurred are indicated by yellow arrows. Regions where under and over segmentation of hippocampal proper occurred are indicated by blue and red arrows, respectively. Blue arrow “1” highlights missing subiculum (row c.). Blue arrow “2” indicates missing hippocampal head (row d.). Blue arrow “3” indicates under segmented hippocampal body (row d.). Red arrow “4” indicates an over segmented hippocampal head (row d.). Yellow arrows “5” and “6” show examples of over segmented white matter beyond the subiculum (row c.) [Color figure can be viewed at <ext-link ext-link-type="uri" xlink:href="http://wileyonlinelibrary.com">http://wileyonlinelibrary.com</ext-link>]</p>
        </caption>
        <graphic id="nlm-graphic-7" xlink:href="HBM-41-291-g004"/>
      </fig>
      <p>The cases with the highest and lowest Dice coefficients from the test dataset between manual hippocampal segmentations and our prediction (0.92 and 0.80) are presented in Figure <xref rid="hbm24811-fig-0005" ref-type="fig">5</xref>. These cases highlight the quality of our output segmentations even for the case with lowest dice. The worst case demonstrates some of the challenging features for segmentation including hippocampal shrinkage, malrotation, increased ventricular volume and increased CSF surrounding the hippocampus. Additional examples of six test cases are shown in <xref rid="hbm24811-supitem-0001" ref-type="supplementary-material">Figure S1</xref>, to demonstrate the quality of our model predictions and mismatch to manual labels. Our algorithm was two orders of magnitude faster than some of the tested techniques, segmenting the hippocampi in an average of 14 s on a GPU. While our model was on‐par with the other CNN‐based technique in terms of efficiency, it had a notably higher accuracy on the first test set.</p>
      <fig fig-type="Figure" xml:lang="en" id="hbm24811-fig-0005" orientation="portrait" position="float">
        <label>Figure 5</label>
        <caption>
          <p>Hippocampal segmentation cases with the highest (a) and lowest (b) dice coefficients from the test set in coronal and axial views. Blue labels represent predicted segmentations and red represent manual delineations. Yellow arrowheads highlight areas of mis‐segmentations. (b) Outlines of the segmentations are presented to show the underlying image intensities and features, and demonstrate the agreement in segmentation borders. (c) Mis‐segmented voxels highlight the discrepancies between the segmentations. Red voxels are manually labeled voxels not predicted by the model, and light blue voxels are predicted voxels that were not present in the manual labels [Color figure can be viewed at <ext-link ext-link-type="uri" xlink:href="http://wileyonlinelibrary.com">http://wileyonlinelibrary.com</ext-link>]</p>
        </caption>
        <graphic id="nlm-graphic-9" xlink:href="HBM-41-291-g005"/>
      </fig>
      <p>Figure <xref rid="hbm24811-fig-0006" ref-type="fig">6</xref> demonstrates cases in individuals with severe atrophy where our model produced accurate segmentation. Specifically, we depict cases with large cysts, significant ventricular enlargement and small vessel disease. We present additional difficult cases in <xref rid="hbm24811-supitem-0001" ref-type="supplementary-material">Figure S2</xref>, including a subject with developmental malformation and input images where the neck is occupying a large portion of the FOV. These cases are particularly problematic to atlas‐based algorithms relying on registrations to a template, or those employing a registration‐based initialization. While our network is able to segment both skull‐stripped as well as original T1 images including the skull with comparable accuracy, for cases where the brain does not occupy a large portion of the FOV it may be optimal to skull‐strip the input T1 for improved segmentations.</p>
      <fig fig-type="Figure" xml:lang="en" id="hbm24811-fig-0006" orientation="portrait" position="float">
        <label>Figure 6</label>
        <caption>
          <p>Examples of difficult cases to segment where our model produced accurate segmentations. (a) Presence of a large cyst (top left corner) in the temporal lobe causing hippocampal rotation. (b, c) Two individuals with enlarged ventricles and hippocampal shrinkage. (d) An individual with small vessel disease and visible lacunes within the right hippocampus and surrounding white matter [Color figure can be viewed at <ext-link ext-link-type="uri" xlink:href="http://wileyonlinelibrary.com">http://wileyonlinelibrary.com</ext-link>]</p>
        </caption>
        <graphic id="nlm-graphic-11" xlink:href="HBM-41-291-g006"/>
      </fig>
    </sec>
    <sec id="hbm24811-sec-0012">
      <label>3.2</label>
      <title>Outlier rates</title>
      <p>The models were also evaluated for performance, specifically outliers and failure rates, on two additional populations that are difficult to segment. Due to the lack of ground truth, the z‐scores of the surface areas for each method relative to the computed subject‐specific mean were used as a metric for comparison. High or low surface area z‐scores (for each method) suggested that there had either been significant over or under‐segmentation of the hippocampus, respectively. We considered outliers to be values above or below 2 standard deviations from the mean. Table <xref rid="hbm24811-tbl-0003" ref-type="table">3</xref> summarizes the outlier rates (average of both hemispheres) on the two populations across all the methods. volBrain had the greatest number of outliers for the FTD cohort, followed by Hippodeep (Figure <xref rid="hbm24811-fig-0006" ref-type="fig">6</xref>). This population is characterized by marked hippocampal atrophy and shrinkage particularly in the left temporal lobe (where language lateralizes in most participants), which leads to common segmentation errors due to either substantial underestimation of the volume (e.g., Hippodeep's CNN‐based segmentation) or overestimation with atlas‐based methods (FIRST and FreeSurfer) as shown in Figure <xref rid="hbm24811-fig-0007" ref-type="fig">7</xref>b. In the VCI cohort, characterized by an increased burden of WMH, vascular lesions and the presence of strokes, FreeSurfer had the greatest number of outliers and SBHV under‐segmented around 10% of the cases (<xref rid="hbm24811-supitem-0001" ref-type="supplementary-material">Figure S3</xref>). HippMapp3r had the fewest number of outliers in the two populations with a 1% outlier rate (Table <xref rid="hbm24811-tbl-0003" ref-type="table">3</xref>). Example cases where our model did not produce optimal segmentations are presented in <xref rid="hbm24811-supitem-0001" ref-type="supplementary-material">Figure S4</xref> and included instances with very low SNR or when a lesion localized within the hippocampus. volBrain failed on 20 VCI subjects and 11 FTD subjects. Of all completed cases on both cohorts, volBrain had an outlier rate of 21% in the VCI cohort but failed to produce accurate results on the FTD cohort with an outlier rate close to 50%.</p>
      <table-wrap id="hbm24811-tbl-0003" xml:lang="en" orientation="portrait" position="float">
        <label>Table 3</label>
        <caption>
          <p>Outlier rates based on surface area <italic>z</italic>‐scores for all tested methods across three cohorts</p>
        </caption>
        <table frame="hsides" rules="groups">
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <thead valign="bottom">
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" style="border-bottom:solid 1px #000000" valign="bottom" rowspan="1" colspan="1"/>
              <th colspan="6" align="left" style="border-bottom:solid 1px #000000" valign="bottom" rowspan="1">Method</th>
            </tr>
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" valign="bottom" rowspan="1" colspan="1">Cohort</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">HippMapp3r (%)</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">HippoDeep (%)</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">FIRST (%)</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">FreeSurfer (%)</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">SBHV (%)</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">volBrain<xref ref-type="fn" rid="hbm24811-note-0005">a</xref> (%)</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">FTD</td>
              <td align="left" valign="top" rowspan="1" colspan="1">1</td>
              <td align="left" valign="top" rowspan="1" colspan="1">11</td>
              <td align="left" valign="top" rowspan="1" colspan="1">8</td>
              <td align="left" valign="top" rowspan="1" colspan="1">9</td>
              <td align="left" valign="top" rowspan="1" colspan="1">7</td>
              <td align="left" valign="top" rowspan="1" colspan="1">46</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">VCI</td>
              <td align="left" valign="top" rowspan="1" colspan="1">1</td>
              <td align="left" valign="top" rowspan="1" colspan="1">4</td>
              <td align="left" valign="top" rowspan="1" colspan="1">6</td>
              <td align="left" valign="top" rowspan="1" colspan="1">22</td>
              <td align="left" valign="top" rowspan="1" colspan="1">9</td>
              <td align="left" valign="top" rowspan="1" colspan="1">21</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Real adversarial cases</td>
              <td align="left" valign="top" rowspan="1" colspan="1">2</td>
              <td align="left" valign="top" rowspan="1" colspan="1">3</td>
              <td align="left" valign="top" rowspan="1" colspan="1">4</td>
              <td align="left" valign="top" rowspan="1" colspan="1">10</td>
              <td align="left" valign="top" rowspan="1" colspan="1">10</td>
              <td align="left" valign="top" rowspan="1" colspan="1">4</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot id="hbm24811-ntgp-0003">
          <fn id="hbm24811-note-0003">
            <p><italic>Note</italic>: Rates were combined over both hemispheres.</p>
          </fn>
          <fn id="hbm24811-note-0004">
            <p>Abbreviations: FTD, frontotemporal dementia; VCI, vascular cognitive impairment.</p>
          </fn>
          <fn id="hbm24811-note-0005">
            <label>a</label>
            <p>volBrain failed on 20 VCI subjects and 11 FTD subjects.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <fig fig-type="Figure" xml:lang="en" id="hbm24811-fig-0007" orientation="portrait" position="float">
        <label>Figure 7</label>
        <caption>
          <p>Outlier rates in the frontotemporal dementia population. (a) Mean surface area z‐scores for the right hippocampus segmented by HippMapp3r (blue), Hippodeep (red), FIRST (orange), FreeSurfer (green), SBHV (purple), and volBrain (brown) relative to average of segmentation volumes. (b) Visual comparison of segmentation results for two subjects in the clinical case dataset. Each row represents a distinct subject. Areas of over, under and miss‐segmentation indicated by yellow arrows [Color figure can be viewed at <ext-link ext-link-type="uri" xlink:href="http://wileyonlinelibrary.com">http://wileyonlinelibrary.com</ext-link>]</p>
        </caption>
        <graphic id="nlm-graphic-13" xlink:href="HBM-41-291-g007"/>
      </fig>
    </sec>
    <sec id="hbm24811-sec-0013">
      <label>3.3</label>
      <title>Clinical adversarial attacks</title>
      <p>Accuracy of neural networks suffers when unseen features and characteristics from a new dataset are provided as a new test set, such as differences in image resolution, contrast, noise‐level, and FOV. Clinical data are commonly obtained at lower field strengths and during shorter acquisitions than research scans, producing lower resolution and SNR data. We performed further validation experiments on both real corrupt “adversarial” cases (due to poor quality, artifacts or patient movement) and simulated adversarial data. The surface area z‐scores as well as some example segmentation results for three subjects from the real adversarial cases are shown in Figure <xref rid="hbm24811-fig-0008" ref-type="fig">8</xref>. None of the methods exceeded an outlier rate of 10% on these real adversarial cases (Table <xref rid="hbm24811-tbl-0003" ref-type="table">3</xref>). FreeSurfer and SBHV generated the most outliers, while HippMapp3r had the lowest outlier rate (2%). We also present two corrupt scans, not included in the test datasets, where our model produced surprisingly good segmentations given that a substantial portion of the brain was not imaged at scan time (<xref rid="hbm24811-supitem-0001" ref-type="supplementary-material">Figure S5</xref>).</p>
      <fig fig-type="Figure" xml:lang="en" id="hbm24811-fig-0008" orientation="portrait" position="float">
        <label>Figure 8</label>
        <caption>
          <p>Outlier rates on images with poor quality (real adversarial cases). (a) Mean surface area z‐scores for the right hippocampus segmented by HippMapp3r (blue), Hippodeep (red), FIRST (orange), FreeSurfer (green), SBHV (purple), and volBrain (brown) relative to average of segmentation volumes. (b) Visual comparison of segmentation results for two subjects in the clinical case dataset. Each row represents a distinct subject. Areas of over, under and miss‐segmentation indicated by yellow arrows [Color figure can be viewed at <ext-link ext-link-type="uri" xlink:href="http://wileyonlinelibrary.com">http://wileyonlinelibrary.com</ext-link>]</p>
        </caption>
        <graphic id="nlm-graphic-15" xlink:href="HBM-41-291-g008"/>
      </fig>
      <p>The simulation experiments demonstrated that our model is robust in general to such attacks mirroring our results on the real adversarial cases, as shown in Figure <xref rid="hbm24811-fig-0009" ref-type="fig">9</xref>. Our model was particularly robust to downsampling of images to 2× and cropping FOV in the Superior–Inferior plane by 30%. Downsampling to 2× in‐plane and 4× out‐of‐plane resulted in a drop of 5% in Dice coefficient (Figure <xref rid="hbm24811-fig-0009" ref-type="fig">9</xref>a). While our model was robust to the addition of a small amount of speckle or salt‐and‐pepper noise (with a low standard deviation), it proved more sensitive to the addition of a large amount (sigma) of speckle noise producing a Dice coefficient drop of around 14% (Figure <xref rid="hbm24811-fig-0009" ref-type="fig">9</xref>b).</p>
      <fig fig-type="Figure" xml:lang="en" id="hbm24811-fig-0009" orientation="portrait" position="float">
        <label>Figure 9</label>
        <caption>
          <p>Adversarial attacks on the hippocampal model to simulate clinical data with low resolution, SNR and limited FOV, demonstrating the robustness of our model to such attacks. (a) Downsampling of resolution by 2× in all dimensions (first row), and 2× in‐plane and 4× out‐of‐plane (second row). Downsampling by 2× did not affect segmentation accuracy, while 2× in‐plane and 4× out‐of‐plane resulted in a drop of 5% in Dice coefficient. (b) Varying degrees (sigmas) of speckle and salt‐and‐pepper noise to simulate lower SNR. The addition of speckle noise with a large sigma produced a Dice coefficient drop of ~14%. (c) Cropping of FOV in the Superior–Inferior plane by 15% in each side did not significantly affect segmentation accuracy [Color figure can be viewed at <ext-link ext-link-type="uri" xlink:href="http://wileyonlinelibrary.com">http://wileyonlinelibrary.com</ext-link>]</p>
        </caption>
        <graphic id="nlm-graphic-17" xlink:href="HBM-41-291-g009"/>
      </fig>
    </sec>
  </sec>
  <sec id="hbm24811-sec-0014">
    <label>4</label>
    <title>DISCUSSION</title>
    <p>This work presents a hippocampal segmentation algorithm (HippMapp3r) based on an ensemble of 3D deep artificial neural networks. We compared our segmentation results to five state‐of‐the‐art methods and demonstrated that the proposed algorithm is capable of producing accurate and fast hippocampal segmentations across a diverse range of neurodegenerative diseases. On a test set with manual ground truth, our algorithm achieved the highest volume correlation, Dice and Jaccard scores relative to other tested algorithms and had the highest computational speed, segmenting the hippocampus in an average of 14 s per subject. We further highlighted its robustness to atrophies and lesion types not present in the training set by demonstrating its low outlier rate on two additional test populations. We finally validated its robustness against corrupt and challenging scans on both real (with motion artifacts and low SNR) and simulated adversarial cases (through the systematic degradation of input image quality with simulation akin to low‐quality clinical grade MRI). The high accuracy and efficiency of our model highlight its utility as a tool for the analysis of large multisite studies while providing the opportunity for personalized assessments.</p>
    <p>HippMapp3r produced an average Dice value of 0.869 ± 0.033 across both hippocampi (0.869 ± 0.030 left, 0.868 ± 0.036 right), and Pearson's correlation coefficients of 0.95 and 0.93 for the left and right hippocampi, respectively. The high overlap and similarity between the ground truth and the segmentation results indicate HippMapp3r's ability to segment hippocampi that are variant in shape and position across cohorts. Of note, our hippocampal model was trained on multiple segmentation protocols (in an effort to increase our training set) which have slightly different border definitions for manual segmentation than those reported within a single study. We deliberately chose to test our algorithm on images from elderly patients with brain injury, WMH, and stroke, in order to perform a more clinically relevant validation (unlike many other tools reported in the literature that only use healthy young adults for validation). The presence of these cases with large atrophy, smaller volumes, hippocampal rotation and brain lesions in the test dataset may be why we obtained a lower accuracy with the state‐of‐the‐art methods than reported elsewhere. The high variability of image quality, resolutions, acquisition parameters, and scanners in the dataset is another challenging aspect for many segmentation algorithms that were accounted for in this model by using multisite and multi‐scanner studies in the training and testing datasets. Study overlap in the training and testing set may have set our model at an advantage compared to the other methods but was unavoidable due to lack of available datasets with expert ground truth segmentations. Although training for the hippocampus model was performed using data from three different studies, it would be optimal to test it on data with manual ground truth segmentations from other studies not part of the training set.</p>
    <p>For the second test set, HippMapp3r had the fewest number of outliers, indicating it performed most consistently compared to the other methods for these populations. FreeSurfer and First tended to over segment, while SBHV tended to under segment. When observing performance for the different conditions, FreeSurfer performed substantially worse with the VCI cohort. This is possibly because FreeSurfer using an atlas‐based method that cannot incorporate strokes or high WMH burden. Our outlier rates analyses suggest that atlas‐based methods may be susceptible to segmentation errors when tested on populations with substantial hippocampal atrophy and large hemispheric asymmetry in geometric properties, while CNN‐based methods may not generalize well to test data with drastically different intensity statistics or distributions than those of the training data (for example very noisy data). While we used a filtered mean across methods to compute an estimated subject‐specific mean for our outlier rates analyses, this estimate may have been biased in the cases where most of the methods did not produce accurate segmentations.</p>
    <p>Deep CNNs are susceptible to sharp decreases in accuracy when presented with data from different distributions than the training datasets, or with adversarial attacks. We attempted to characterize this through testing on corrupt data that failed quality control (real adversarial cases with motion and other artifacts) and simulation experiments. The low z‐scores standard deviation (divergence from zero) on the real adversarial cases indicates HippMapp3r's consistency in comparison to other methods. Both the large positive deviations from the mean seen in FreeSurfer and FIRST on these cases, and the negative deviations on the part of Hippodeep, reflect the previously discussed findings when the methods were compared to ground truth labels. In the simulation experiments, we found HippMapp3r was particularly robust to the addition of speckle noise and reduced spatial resolutions. However, it was more sensitive to decreasing the FOV and exhibited a large drop in accuracy with very‐noisy inputs. We did not observe any failure cases with the segmentation model on our test set or during the validation experiments.</p>
    <p>It is worth noting that even when accurate, automated methods are used, a certain degree of neuroanatomical expertise is commonly needed to evaluate whether a segmentation is adequate or should be excluded or edited on a subject level. To enable efficient and accurate assessment of the quality of segmentations, our algorithm generates automated quality check outputs similar to those shown in <xref rid="hbm24811-supitem-0001" ref-type="supplementary-material">Figure S6</xref>; as well as automated volumetric reports for further analyses. For group segmentations, we have added an additional function to perform outlier detection based on volumetric and shape metrics (surface area, eccentricity, and elongation), whereby subjects with metrics higher or lower than 2 standard deviations from the mean are considered outliers.</p>
    <p>The two deep neural network algorithms (Hippodeep and HippMapp3r) were by far the more efficient with processing times on the order of seconds on a GPU as compared to several minutes or hours for the three other tools. While the FreeSurfer pipeline has a typically long computational time compared to other software, it should be noted that it also provides segmentations of numerous brain structures, as well as the hippocampal subfields. Similarly, FIRST and volBrain provided segmentations of other subcortical structures in addition to the whole hippocampus. We opted to rely on a two‐network approach, with the first for initialization, as opposed to a registration‐based method for initialization, to avoid incorporating registration errors and for faster predictions (even using a CPU, <xref rid="hbm24811-supitem-0002" ref-type="supplementary-material">Video S1</xref>).</p>
    <p>Hippodeep mainly employed hippocampus segmentations generated by FreeSurfer on a large dataset as ground truth data because manual tracings are tedious and labor‐intensive (Fischl et al., <xref rid="hbm24811-bib-0010" ref-type="ref">2002</xref>). Although trained using FreeSurfer segmentations, this model outperformed FreeSurfer's atlas‐based algorithm by a small margin on our test dataset. Hippodeep had roughly similar computational time as HippMapp3r, but lower accuracy on the test data. This could be possibly due to the architectural differences between the two networks (including the expanding pathway, residual elements, higher number of feature maps and deep supervision in our model). It could also be due to the difference in the training data and the reliance on a bigger number of manually traced ground‐truth datasets instead of a very large number of FreeSurfer segmentations.</p>
    <p>We chose a U‐net architecture as it has been shown as a successful scheme for several biomedical applications (Ronneberger et al., <xref rid="hbm24811-bib-0033" ref-type="ref">2015</xref>), while also implementing a 3D design as this is advisable for volumetric medical data (Kayalibay et al., <xref rid="hbm24811-bib-0022" ref-type="ref">2017</xref>; Milletari et al., <xref rid="hbm24811-bib-0028" ref-type="ref">2016</xref>). In addition, we used residual units to provide smoother gradient flow through the network and skip connections to forward feature maps computed in the contracting pathway to the expanding pathway. One of the main practical limitations of applying 3D CNNs to medical data is that they are expensive to train due to their increased memory demand and thus usually require down‐sampling the raw data. Our two‐network “ensemble” approach avoided downsampling the hippocampal (medial temporal) region in the second pass. We opted for training our model using manually delineated ground truth segmentations by experts as they should produce more accurate results than relying on outputs from other software. Other algorithms may not fully encode the expert anatomical knowledge and may lead to biases in the predictions. We relied on augmentation through flipping and rotation but observed that augmentation through nonlinear warping did not show improvements for the training loss. We adopted the chosen parameters as they have been shown to produce optimal results in previous literature and based on prior experiments; however, further improvements in accuracy might be made if a more extensive exploration of the parameter space is performed. We ran an exploratory experiment for testing whether one could segment hippocampi and other structures straight from acquired scans, without any processing (like bias‐field correction) or intensity standardization. Intensity standardization (converting image intensities to a zero‐mean, 1 standard deviation) is a common preprocessing procedure in machine/deep learning known to help optimizer convergence and gradient flow. We believe that the lack of intensity standardization and the drastically different intensities across the multisite training set is the main factor for this failure in training. The results of this experiment might, however, be dependent on the training data (the histograms and intensity ranges in the set) and its size.</p>
    <p>While the presented model is accurate and efficient, it is not flexible in terms of inputs as it requires a specific sequence (i.e., image contrasts) and thus would not produce accurate segmentations in its absence (e.g., using T2‐weighted images). This is in contrast to more flexible approaches that can predict given a subset of inputs (Havaei, Guizard, Chapados, &amp; Bengio, <xref rid="hbm24811-bib-0014" ref-type="ref">2016</xref>) and thus the subject of future work. HippMapp3r also relied on the input being in a standard orientation and was unable to accurately segment when dealing with other orientations. Our segmentation algorithm does not separate the hippocampus into its different subfields, this is due to the scarcity of high‐resolution sub‐millimeter T1 and T2‐weighted scans and manual subfield delineations in a large dataset for training, as well as lack of histological validation.</p>
    <p>In summary, this work we present an automated whole hippocampal segmentation algorithm based on 3D CNNs that is robust to brain atrophy and lesions associated with aging and neurodegeneration of the human brain. The algorithm was trained on 209 datasets (before augmentation) from different cognitive neurology cohorts (NC, MCI, AD, or TLE), as a means of reflecting a subset of the wide range of elderly adults that undergo brain MRI. These datasets were acquired from multiple studies to obtain good generalizability across resolutions, acquisition parameters, and scanners. Our deep networks improve upon state‐of‐the‐art techniques in terms of both accuracy and efficiency. We observed a large margin in Dice and Jaccard similarity coefficients, and volume correlations between manual and automated segmentations for our model compared to other well‐established methods. Our model was also one or two orders of magnitude faster than some of the tested methods, segmenting the hippocampi in seconds. This combination of efficiency and accuracy against other methods suggests broad utility for large multisite studies, as well as personalized assessments. We have further validated our networks by demonstrating its robustness to realistic clinic adversarial cases including sharp decreases in resolution, SNR, and cropping of FOV, indicating the potential for clinical adoption. The model is made public and accessible for use in the research setting.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supporting information</title>
    <supplementary-material content-type="local-data" id="hbm24811-supitem-0001">
      <caption>
        <p><bold>Figure S1</bold>. Hippocampal segmentations in six individual subjects highlighting the quality of segmentations compared to manual segmentations. Each row represents a distinct subject. (A) Predicted segmentations (blue) overlaid on manual delineations (red). (B) Outlines of the segmentations are presented to show the underlying image intensities and features, and demonstrate the agreement in segmentation borders. (C) Mis‐segmented voxels highlight the discrepancies between the segmentations. Red voxels are manually labeled voxels not predicted by the model, and light blue voxels are predicted voxels that were not present in the manual labels.</p>
        <p><bold>Figure S2.</bold> Additional difficult cases where our model produced accurate segmentations. (A) Coronal views of hippocampal segmentation by our network in a subject with a developmental malformation. (B) Hippocampal segmentation of a subject input T1 where the neck is occupying a large portion of the field of view, making it difficult for atlas‐based and registration techniques.</p>
        <p><bold>Figure S3.</bold> Outlier rates on images in VCI cohort. (A) Mean surface area z‐scores for the right hippocampus segmented by HippMapp3r (blue), Hippodeep (red), FIRST (orange), FreeSurfer (green), SBHV (purple), and volBrain (brown) relative to average of segmentation volumes. (B) Visual comparison of segmentation results for two subjects in the clinical case dataset. Each row represents a distinct subject. Areas of over, under and miss‐segmentation indicated by yellow arrows.</p>
        <p><bold>Figure S4.</bold> Examples of adversarial cases where our model produced segmentation errors due to inferior image quality and the presence of a lesion in the hippocampus.</p>
        <p><bold>Figure S5.</bold> Examples of two cases where a large portion of the brain was not imaged (not included in the test sets) but our model produced surprisingly good results.</p>
        <p><bold>Figure S6.</bold> Example of our automated quality check output for qualitative assessment of segmentations. The model accurately excluded a medial vessel neighboring the left hippocampus, highlighting its robustness to large anatomical variability. (A) Quality check snapshot output generated by our algorithm in the coronal axis. Lighter blue color depicts the left hippocampus. (B) 3D model and 3‐axis views of the same segmentation as viewed in the visualization software ITKsnap.</p>
      </caption>
      <media xlink:href="HBM-41-291-s001.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="hbm24811-supitem-0002">
      <caption>
        <p><bold>Video S1</bold>. Hippocampal segmentation tutorial : <ext-link ext-link-type="uri" xlink:href="https://youtu.be/5eogjQ4f1_k">https://youtu.be/5eogjQ4f1_k</ext-link>.</p>
      </caption>
      <media xlink:href="HBM-41-291-s002.mov">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <sec sec-type="data-availability" id="hbm24811-sec-0015">
    <title>DATA ACCESSIBILITY</title>
    <p>The developed algorithm and trained models (network weights) are publicly available at: <ext-link ext-link-type="uri" xlink:href="https://hippmapp3r.readthedocs.io" specific-use="software is-supplemented-by">https://hippmapp3r.readthedocs.io</ext-link>, under the GNU General Public License v3.0. An example dataset is included for testing purposes. We have developed an easy‐to‐use pipeline with a GUI and thorough documentation for making it accessible to users without programming knowledge.</p>
  </sec>
  <ref-list id="hbm24811-bibl-0001" content-type="cited-references">
    <title>REFERENCES</title>
    <ref id="hbm24811-bib-0001">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0001"><collab collab-type="authors">Anon</collab>
. (<year>2005</year>). <article-title>The human hippocampus, H.M. Duvernoy Third edition, Springer‐Verlag Berlin Heidelberg 2005 (232 pages)</article-title>. <source xml:lang="en">Journal of Neuroradiology</source>, <volume>32</volume>(<issue>4</issue>), <fpage>288</fpage>
<pub-id pub-id-type="doi">10.1016/s0150-9861(05)83157-8</pub-id>
</mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0002">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0002"><string-name><surname>Barnes</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bartlett</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>van de Pol</surname>, <given-names>L. A.</given-names></string-name>, <string-name><surname>Loy</surname>, <given-names>C. T.</given-names></string-name>, <string-name><surname>Scahill</surname>, <given-names>R. I.</given-names></string-name>, <string-name><surname>Frost</surname>, <given-names>C.</given-names></string-name>, … <string-name><surname>Fox</surname>, <given-names>N. C.</given-names></string-name> (<year>2009</year>). <article-title>A meta‐analysis of hippocampal atrophy rates in Alzheimer's disease</article-title>. <source xml:lang="en">Neurobiology of Aging</source>, <volume>30</volume>, <fpage>1711</fpage>–<lpage>1723</lpage>.<pub-id pub-id-type="pmid">18346820</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0003">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0003"><string-name><surname>Bernasconi</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Natsume</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Bernasconi</surname>, <given-names>A.</given-names></string-name> (<year>2005</year>). <article-title>Progression in temporal lobe epilepsy: Differential atrophy in mesial temporal structures</article-title>. <source xml:lang="en">Neurology</source>, <volume>65</volume>, <fpage>223</fpage>–<lpage>228</lpage>.<pub-id pub-id-type="pmid">16043790</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0004">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0004"><string-name><surname>Boccardi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bocchetta</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Morency</surname>, <given-names>F. C.</given-names></string-name>, <string-name><surname>Collins</surname>, <given-names>D. L.</given-names></string-name>, <string-name><surname>Nishikawa</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ganzola</surname>, <given-names>R.</given-names></string-name>, … <collab collab-type="authors">EADC‐ADNI Working Group on The Harmonized Protocol for Manual Hippocampal Segmentation and for the Alzheimer's Disease Neuroimaging Initiative</collab>
. (<year>2015</year>). <article-title>Training labels for hippocampal segmentation based on the EADC‐ADNI harmonized hippocampal protocol</article-title>. <source xml:lang="en">Alzheimers Dement</source>, <volume>11</volume>, <fpage>175</fpage>–<lpage>183</lpage>.<pub-id pub-id-type="pmid">25616957</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0005">
      <mixed-citation publication-type="miscellaneous" id="hbm24811-cit-0005"><string-name><surname>Çiçek</surname>, <given-names>Ö.</given-names></string-name>, <string-name><surname>Abdulkadir</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Lienkamp</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Brox</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Ronneberger</surname>, <given-names>O.</given-names></string-name> (<year>2016</year>). 3D U‐Net: Learning dense volumetric segmentation from sparse annotation. <italic>Lecture Notes in Computer Science</italic> (pp. <fpage>424</fpage>–<lpage>432</lpage>).</mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0006">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0006"><string-name><surname>Courchesne</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Chisum</surname>, <given-names>H. J.</given-names></string-name>, <string-name><surname>Townsend</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Cowles</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Covington</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Egaas</surname>, <given-names>B.</given-names></string-name>, … <string-name><surname>Press</surname>, <given-names>G. A.</given-names></string-name> (<year>2000</year>). <article-title>Normal brain development and aging: Quantitative analysis at in vivo MR imaging in healthy volunteers</article-title>. <source xml:lang="en">Radiology</source>, <volume>216</volume>, <fpage>672</fpage>–<lpage>682</lpage>.<pub-id pub-id-type="pmid">10966694</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0007">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0007"><string-name><surname>Das</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Mechanic‐Hamilton</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Korczykowski</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pluta</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Glynn</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Avants</surname>, <given-names>B. B.</given-names></string-name>, … <string-name><surname>Yushkevich</surname>, <given-names>P. A.</given-names></string-name> (<year>2009</year>). <article-title>Structure specific analysis of the hippocampus in temporal lobe epilepsy</article-title>. <source xml:lang="en">Hippocampus</source>, <volume>19</volume>, <fpage>517</fpage>–<lpage>525</lpage>.<pub-id pub-id-type="pmid">19437496</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0008">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0008"><string-name><surname>Deshpande</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Gao</surname>, <given-names>F. Q.</given-names></string-name>, <string-name><surname>Bakshi</surname>, <given-names>S. N.</given-names></string-name>, <string-name><surname>Leibovitch</surname>, <given-names>F. S.</given-names></string-name>, <string-name><surname>Black</surname>, <given-names>S. E.</given-names></string-name>, &amp; <collab collab-type="authors">Sunnybrook Dementia Study</collab>
. (<year>2004</year>). <article-title>Simple linear and area MR measurements can help distinguish between Alzheimer's disease, frontotemporal dementia, and normal aging: The Sunnybrook Dementia Study</article-title>. <source xml:lang="en">Brain and Cognition</source>, <volume>54</volume>, <fpage>165</fpage>–<lpage>166</lpage>.<pub-id pub-id-type="pmid">15025054</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0009">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0009"><string-name><surname>Dice</surname>, <given-names>L. R.</given-names></string-name> (<year>1945</year>). <article-title>Measures of the amount of ecologic association between species</article-title>. <source xml:lang="en">Ecology</source>, <volume>26</volume>, <fpage>297</fpage>–<lpage>302</lpage>.</mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0010">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0010"><string-name><surname>Fischl</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Salat</surname>, <given-names>D. H.</given-names></string-name>, <string-name><surname>Busa</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Albert</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dieterich</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Haselgrove</surname>, <given-names>C.</given-names></string-name>, … <string-name><surname>Dale</surname>, <given-names>A. M.</given-names></string-name> (<year>2002</year>). <article-title>Whole brain segmentation: Automated labeling of neuroanatomical structures in the human brain</article-title>. <source xml:lang="en">Neuron</source>, <volume>33</volume>, <fpage>341</fpage>–<lpage>355</lpage>.<pub-id pub-id-type="pmid">11832223</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0011">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0011"><string-name><surname>Goubran</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bernhardt</surname>, <given-names>B. C.</given-names></string-name>, <string-name><surname>Cantor‐Rivera</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Lau</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Blinston</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Hammond</surname>, <given-names>R. R.</given-names></string-name>, … <string-name><surname>Khan</surname>, <given-names>A. R.</given-names></string-name> (<year>2016</year>). <article-title>In vivo MRI signatures of hippocampal subfield pathology in intractable epilepsy</article-title>. <source xml:lang="en">Human Brain Mapping</source>, <volume>37</volume>, <fpage>1103</fpage>–<lpage>1119</lpage>.<pub-id pub-id-type="pmid">26679097</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0012">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0012"><string-name><surname>Goubran</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rudko</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Santyr</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Gati</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Szekeres</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Peters</surname>, <given-names>T. M.</given-names></string-name>, &amp; <string-name><surname>Khan</surname>, <given-names>A. R.</given-names></string-name> (<year>2014</year>). <article-title>In vivo normative atlas of the hippocampal subfields using multi‐echo susceptibility imaging at 7 Tesla</article-title>. <source xml:lang="en">Human Brain Mapping</source>, <volume>35</volume>, <fpage>3588</fpage>–<lpage>3601</lpage>.<pub-id pub-id-type="pmid">24339427</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0013">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0013"><string-name><surname>Hasboun</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Chantôme</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Zouaoui</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sahel</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Deladoeuille</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sourour</surname>, <given-names>N.</given-names></string-name>, … <string-name><surname>Dormont</surname>, <given-names>D.</given-names></string-name> (<year>1996</year>). <article-title>MR determination of hippocampal volume: Comparison of three methods</article-title>. <source xml:lang="en">AJNR. American Journal of Neuroradiology</source>, <volume>17</volume>, <fpage>1091</fpage>–<lpage>1098</lpage>.<pub-id pub-id-type="pmid">8791921</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0014">
      <mixed-citation publication-type="miscellaneous" id="hbm24811-cit-0014"><string-name><surname>Havaei</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Guizard</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Chapados</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Bengio</surname><given-names>Y</given-names></string-name> (<year>2016</year>). HeMIS: Hetero‐modal image segmentation. arXiv [cs.CV]. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1607.05194">http://arxiv.org/abs/1607.05194</ext-link>
</mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0015">
      <mixed-citation publication-type="miscellaneous" id="hbm24811-cit-0015"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ren</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Sun</surname>, <given-names>J.</given-names></string-name> (<year>2016</year>). Deep residual learning for image recognition. <italic>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic>
<pub-id pub-id-type="doi">10.1109/cvpr.2016.90</pub-id>
</mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0016">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0016"><string-name><surname>Iglesias</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Augustinack</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Nguyen</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Player</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Player</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Wright</surname>, <given-names>M.</given-names></string-name>, … <collab collab-type="authors">Alzheimer's Disease Neuroimaging Initiative</collab>
. (<year>2015</year>). <article-title>A computational atlas of the hippocampal formation using ex vivo, ultra‐high resolution MRI: Application to adaptive segmentation of in vivo MRI</article-title>. <source xml:lang="en">NeuroImage</source>, <volume>115</volume>, <fpage>117</fpage>–<lpage>137</lpage>.<pub-id pub-id-type="pmid">25936807</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0017">
      <mixed-citation publication-type="book" id="hbm24811-cit-0017"><string-name><surname>Isensee</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Kickingereder</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Wick</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Bendszus</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Maier‐Hein</surname>, <given-names>K. H.</given-names></string-name> (<year>2018</year>). <chapter-title>Brain tumor segmentation and radiomics survival prediction: Contribution to the BRATS 2017 challenge</chapter-title> In <source xml:lang="en">Lecture notes in computer science</source> (pp. <fpage>287</fpage>–<lpage>297</lpage>). <publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Springer Science+Business Media</publisher-name>.</mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0018">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0018"><string-name><surname>Jaccard</surname>, <given-names>P.</given-names></string-name> (<year>1912</year>). <article-title>The distribution of the FLORA in the alpine ZONE.1</article-title>. <source xml:lang="en">The New Phytologist</source>, <volume>11</volume>, <fpage>37</fpage>–<lpage>50</lpage>.</mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0019">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0019"><string-name><surname>Jack</surname>, <given-names>C. R.</given-names>, <suffix>Jr.</suffix></string-name>, <string-name><surname>Petersen</surname>, <given-names>R. C.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>O'Brien</surname>, <given-names>P. C.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>G. E.</given-names></string-name>, <string-name><surname>Ivnik</surname>, <given-names>R. J.</given-names></string-name>, … <string-name><surname>Kokmen</surname>, <given-names>E.</given-names></string-name> (<year>1998</year>). <article-title>Rate of medial temporal lobe atrophy in typical aging and Alzheimer's disease</article-title>. <source xml:lang="en">Neurology</source>, <volume>51</volume>, <fpage>993</fpage>–<lpage>999</lpage>.<pub-id pub-id-type="pmid">9781519</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0020">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0020"><string-name><surname>Jack</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>Petersen</surname>, <given-names>R. C.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>O'Brien</surname>, <given-names>P. C.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>G. E.</given-names></string-name>, <string-name><surname>Ivnik</surname>, <given-names>R. J.</given-names></string-name>, … <string-name><surname>Kokmen</surname>, <given-names>E.</given-names></string-name> (<year>2000</year>). <article-title>Rates of hippocampal atrophy correlate with change in clinical status in aging and AD</article-title>. <source xml:lang="en">Neurology</source>, <volume>55</volume>, <fpage>484</fpage>–<lpage>490</lpage>.<pub-id pub-id-type="pmid">10953178</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0021">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0021"><string-name><surname>Kamnitsas</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Ledig</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Newcombe</surname>, <given-names>V. F. J.</given-names></string-name>, <string-name><surname>Simpson</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Kane</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Menon</surname>, <given-names>D. K.</given-names></string-name>, … <string-name><surname>Glocker</surname>, <given-names>B.</given-names></string-name> (<year>2017</year>). <article-title>Efficient multi‐scale 3D CNN with fully connected CRF for accurate brain lesion segmentation</article-title>. <source xml:lang="en">Medical Image Analysis</source>, <volume>36</volume>, <fpage>61</fpage>–<lpage>78</lpage>.<pub-id pub-id-type="pmid">27865153</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0022">
      <mixed-citation publication-type="miscellaneous" id="hbm24811-cit-0022"><string-name><surname>Kayalibay</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Jensen</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>van der Smagt</surname>, <given-names>P.</given-names></string-name> (<year>2017</year>). CNN‐based segmentation of medical imaging data. arXiv [cs.CV]. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1701.03056">http://arxiv.org/abs/1701.03056</ext-link>
</mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0023">
      <mixed-citation publication-type="miscellaneous" id="hbm24811-cit-0023"><string-name><surname>Kingma</surname>, <given-names>D. P.</given-names></string-name>, &amp; <string-name><surname>Ba</surname>, <given-names>J.</given-names></string-name> (<year>2014</year>). Adam: A method for stochastic optimization. arXiv [cs.LG]. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</ext-link>
</mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0024">
      <mixed-citation publication-type="miscellaneous" id="hbm24811-cit-0024"><string-name><surname>Krizhevsky</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sutskever</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Hinton</surname>, <given-names>G. E.</given-names></string-name> (<year>2012</year>). ImageNet classification with deep convolutional neural networks. <italic>Proceedings of the 25th International Conference on Neural Information Processing Systems – Volume 1</italic>. USA: Curran Associates Inc. NIPS'12 (pp. 1097–1105).</mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0025">
      <mixed-citation publication-type="miscellaneous" id="hbm24811-cit-0025"><string-name><surname>Lee</surname>, <given-names>C.‐Y.</given-names></string-name>, <string-name><surname>Xie</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Gallagher</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Z.</given-names></string-name>, &amp; <string-name><surname>Tu</surname>, <given-names>Z.</given-names></string-name> (<year>2014</year>). Deeply‐supervised nets. arXiv [stat.ML]. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1409.5185">http://arxiv.org/abs/1409.5185</ext-link>
</mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0026">
      <mixed-citation publication-type="miscellaneous" id="hbm24811-cit-0026"><string-name><surname>Long</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Shelhamer</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Darrell</surname>, <given-names>T.</given-names></string-name> (<year>2015</year>). Fully convolutional networks for semantic segmentation. <italic>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic>
<pub-id pub-id-type="doi">10.1109/cvpr.2015.7298965</pub-id>
</mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0027">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0027"><string-name><surname>Manjón</surname>, <given-names>J. V.</given-names></string-name>, &amp; <string-name><surname>Coupé</surname>, <given-names>P.</given-names></string-name> (<year>2016</year>). <article-title>volBrain: An online MRI brain volumetry system</article-title>. <source xml:lang="en">Frontiers in Neuroinformatics</source>, <volume>10</volume>, <fpage>30</fpage>.<pub-id pub-id-type="pmid">27512372</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0028">
      <mixed-citation publication-type="miscellaneous" id="hbm24811-cit-0028"><string-name><surname>Milletari</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Navab</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Ahmadi</surname>, <given-names>S.‐A.</given-names></string-name> (<year>2016</year>). V‐Net: Fully convolutional neural networks for volumetric medical image segmentation. <italic>2016 Fourth International Conference on 3D Vision (3DV)</italic>
<pub-id pub-id-type="doi">10.1109/3dv.2016.79</pub-id>
</mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0029">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0029"><string-name><surname>Nestor</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Gibson</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Gao</surname>, <given-names>F.‐Q.</given-names></string-name>, <string-name><surname>Kiss</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Black</surname>, <given-names>S. E.</given-names></string-name>, &amp; <collab collab-type="authors">Alzheimer's Disease Neuroimaging Initiative</collab>
. (<year>2013</year>). <article-title>A direct morphometric comparison of five labeling protocols for multi‐atlas driven automatic segmentation of the hippocampus in Alzheimer's disease</article-title>. <source xml:lang="en">NeuroImage</source>, <volume>66</volume>, <fpage>50</fpage>–<lpage>70</lpage>.<pub-id pub-id-type="pmid">23142652</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0030">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0030"><string-name><surname>Patenaude</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Kennedy</surname>, <given-names>D. N.</given-names></string-name>, &amp; <string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name> (<year>2011</year>). <article-title>A Bayesian model of shape and appearance for subcortical brain segmentation</article-title>. <source xml:lang="en">NeuroImage</source>, <volume>56</volume>, <fpage>907</fpage>–<lpage>922</lpage>.<pub-id pub-id-type="pmid">21352927</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0031">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0031"><string-name><surname>Pearson</surname>, <given-names>K.</given-names></string-name> (<year>1895</year>). <article-title>Note on regression and inheritance in the case of two parents</article-title>. <source xml:lang="en">Proceedings of the Royal Society of London</source>, <volume>58</volume>, <fpage>240</fpage>–<lpage>242</lpage>.</mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0032">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0032"><string-name><surname>Pluta</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Avants</surname>, <given-names>B. B.</given-names></string-name>, <string-name><surname>Glynn</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Awate</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Gee</surname>, <given-names>J. C.</given-names></string-name>, &amp; <string-name><surname>Detre</surname>, <given-names>J. A.</given-names></string-name> (<year>2009</year>). <article-title>Appearance and incomplete label matching for diffeomorphic template based hippocampus segmentation</article-title>. <source xml:lang="en">Hippocampus</source>, <volume>19</volume>, <fpage>565</fpage>–<lpage>571</lpage>.<pub-id pub-id-type="pmid">19437413</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0033">
      <mixed-citation publication-type="book" id="hbm24811-cit-0033"><string-name><surname>Ronneberger</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Fischer</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Brox</surname>, <given-names>T.</given-names></string-name> (<year>2015</year>). <chapter-title>U‐Net: Convolutional networks for biomedical image segmentation</chapter-title> In <source xml:lang="en">Lecture Notes in Computer Science</source> (pp. <fpage>234</fpage>–<lpage>241</lpage>). <publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Springer Science+Business Media</publisher-name>.</mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0034">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0034"><string-name><surname>Rusinek</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>De Santi</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Frid</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Tsui</surname>, <given-names>W.‐H.</given-names></string-name>, <string-name><surname>Tarshish</surname>, <given-names>C. Y.</given-names></string-name>, <string-name><surname>Convit</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>de Leon</surname>, <given-names>M. J.</given-names></string-name> (<year>2003</year>). <article-title>Regional brain atrophy rate predicts future cognitive decline: 6‐year longitudinal MR imaging study of normal aging</article-title>. <source xml:lang="en">Radiology</source>, <volume>229</volume>, <fpage>691</fpage>–<lpage>696</lpage>.<pub-id pub-id-type="pmid">14657306</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0035">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0035"><string-name><surname>Santyr</surname>, <given-names>B. G.</given-names></string-name>, <string-name><surname>Goubran</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lau</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Kwan</surname>, <given-names>B. Y. M.</given-names></string-name>, <string-name><surname>Salehi</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>D. H.</given-names></string-name>, … <string-name><surname>Khan</surname>, <given-names>A. R.</given-names></string-name> (<year>2017</year>). <article-title>Investigation of hippocampal substructures in focal temporal lobe epilepsy with and without hippocampal sclerosis at 7T</article-title>. <source xml:lang="en">Journal of Magnetic Resonance Imaging</source>, <volume>45</volume>, <fpage>1359</fpage>–<lpage>1370</lpage>.<pub-id pub-id-type="pmid">27564217</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0036">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0036"><string-name><surname>Scahill</surname>, <given-names>R. I.</given-names></string-name>, <string-name><surname>Frost</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Jenkins</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Whitwell</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Rossor</surname>, <given-names>M. N.</given-names></string-name>, &amp; <string-name><surname>Fox</surname>, <given-names>N. C.</given-names></string-name> (<year>2003</year>). <article-title>A longitudinal study of brain volume changes in normal aging using serial registered magnetic resonance imaging</article-title>. <source xml:lang="en">Archives of Neurology</source>, <volume>60</volume>, <fpage>989</fpage>–<lpage>994</lpage>.<pub-id pub-id-type="pmid">12873856</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0037">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0037"><string-name><surname>Schoemaker</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Buss</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Head</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Sandman</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Davis</surname>, <given-names>E. P.</given-names></string-name>, <string-name><surname>Chakravarty</surname>, <given-names>M. M.</given-names></string-name>, … <string-name><surname>Pruessner</surname>, <given-names>J. C.</given-names></string-name> (<year>2016</year>). <article-title>Hippocampus and amygdala volumes from magnetic resonance images in children: Assessing accuracy of FreeSurfer and FSL against manual segmentation</article-title>. <source xml:lang="en">NeuroImage</source>, <volume>129</volume>, <fpage>1</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">26824403</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0038">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0038"><string-name><surname>Steen</surname>, <given-names>R. G.</given-names></string-name>, <string-name><surname>Mull</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>McClure</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Hamer</surname>, <given-names>R. M.</given-names></string-name>, &amp; <string-name><surname>Lieberman</surname>, <given-names>J. A.</given-names></string-name> (<year>2006</year>). <article-title>Brain volume in first‐episode schizophrenia: Systematic review and meta‐analysis of magnetic resonance imaging studies</article-title>. <source xml:lang="en">The British Journal of Psychiatry</source>, <volume>188</volume>, <fpage>510</fpage>–<lpage>518</lpage>.<pub-id pub-id-type="pmid">16738340</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0039">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0039"><string-name><surname>Sullivan</surname>, <given-names>E. V.</given-names></string-name> (<year>2002</year>). <article-title>Differential rates of regional brain change in callosal and ventricular size: A 4‐year longitudinal MRI study of elderly men</article-title>. <source xml:lang="en">Cerebral Cortex</source>, <volume>12</volume>, <fpage>438</fpage>–<lpage>445</lpage>.<pub-id pub-id-type="pmid">11884358</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0040">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0040"><string-name><surname>Thyreau</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Sato</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Fukuda</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Taki</surname>, <given-names>Y.</given-names></string-name> (<year>2018</year>). <article-title>Segmentation of the hippocampus by transferring algorithmic knowledge for large cohort processing</article-title>. <source xml:lang="en">Medical Image Analysis</source>, <volume>43</volume>, <fpage>214</fpage>–<lpage>228</lpage>.<pub-id pub-id-type="pmid">29156419</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0041">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0041"><string-name><surname>Tustison</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Avants</surname>, <given-names>B. B.</given-names></string-name>, <string-name><surname>Cook</surname>, <given-names>P. A.</given-names></string-name>, <string-name><surname>Zheng</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Egan</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Yushkevich</surname>, <given-names>P. A.</given-names></string-name>, &amp; <string-name><surname>Gee</surname>, <given-names>J. C.</given-names></string-name> (<year>2010</year>). <article-title>N4ITK: Improved N3 bias correction</article-title>. <source xml:lang="en">IEEE Transactions on Medical Imaging</source>, <volume>29</volume>, <fpage>1310</fpage>–<lpage>1320</lpage>.<pub-id pub-id-type="pmid">20378467</pub-id></mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0042">
      <mixed-citation publication-type="miscellaneous" id="hbm24811-cit-0042"><string-name><surname>Ulyanov</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Vedaldi</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Lempitsky</surname>, <given-names>V.</given-names></string-name> (<year>2016</year>). Instance normalization: The missing ingredient for fast stylization. arXiv [cs.CV]. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1607.08022">http://arxiv.org/abs/1607.08022</ext-link>
</mixed-citation>
    </ref>
    <ref id="hbm24811-bib-0043">
      <mixed-citation publication-type="journal" id="hbm24811-cit-0043"><string-name><surname>Yushkevich</surname>, <given-names>P. A.</given-names></string-name>, <string-name><surname>Piven</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hazlett</surname>, <given-names>H. C.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>R. G.</given-names></string-name>, <string-name><surname>Ho</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Gee</surname>, <given-names>J. C.</given-names></string-name>, &amp; <string-name><surname>Gerig</surname>, <given-names>G.</given-names></string-name> (<year>2006</year>). <article-title>User‐guided 3D active contour segmentation of anatomical structures: Significantly improved efficiency and reliability</article-title>. <source xml:lang="en">NeuroImage</source>, <volume>31</volume>, <fpage>1116</fpage>–<lpage>1128</lpage>.<pub-id pub-id-type="pmid">16545965</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
