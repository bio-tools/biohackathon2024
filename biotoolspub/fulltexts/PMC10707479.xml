<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?noissn?>
<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr Proc SIGCHI Conf Hum Factor Comput Syst?>
<?submitter-system nihms?>
<?submitter-userid 10917418?>
<?submitter-authority eRA?>
<?submitter-login nilsgehlenborg?>
<?submitter-name Nils Gehlenborg?>
<?domain nihpa?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">101620299</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">41882</journal-id>
    <journal-id journal-id-type="nlm-ta">Proc SIGCHI Conf Hum Factor Comput Syst</journal-id>
    <journal-title-group>
      <journal-title>Proceedings of the SIGCHI conference on human factors in computing systems. CHI Conference</journal-title>
    </journal-title-group>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10707479</article-id>
    <article-id pub-id-type="pmid">38074525</article-id>
    <article-id pub-id-type="doi">10.1145/3544548.3581127</article-id>
    <article-id pub-id-type="manuscript">nihpa1947573</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DRAVA: Aligning Human Concepts with Machine Learning Latent Dimensions for the Visual Exploration of Small Multiples</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Qianwen</given-names>
        </name>
        <aff id="A1">Harvard Medical School, Boston, MA, USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>L‚ÄôYi</surname>
          <given-names>Sehi</given-names>
        </name>
        <aff id="A2">Harvard Medical School, Boston, MA, USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gehlenborg</surname>
          <given-names>Nils</given-names>
        </name>
        <aff id="A3">Harvard Medical School, Boston, MA, USA</aff>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="CR1">
        <email>qianwen_wang@hms.harvard.edu</email>
      </corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>28</day>
      <month>11</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>19</day>
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>08</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <volume>2023</volume>
    <elocation-id>833</elocation-id>
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial International 4.0 License</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P1">Latent vectors extracted by machine learning (ML) are widely used in data exploration (<italic toggle="yes">e.g</italic>., t-SNE) but suffer from a lack of interpretability. While previous studies employed disentangled representation learning (DRL) to enable more interpretable exploration, they often overlooked the potential mismatches between the concepts of humans and the semantic dimensions learned by DRL. To address this issue, we propose Drava, a visual analytics system that supports users in 1) relating the concepts of humans with the semantic dimensions of DRL and identifying mismatches, 2) providing feedback to minimize the mismatches, and 3) obtaining data insights from concept-driven exploration. Drava provides a set of visualizations and interactions based on visual piles to help users understand and refine concepts and conduct concept-driven exploration. Meanwhile, Drava employs a concept adaptor model to fine-tune the semantic dimensions of DRL based on user refinement. The usefulness of Drava is demonstrated through application scenarios and experimental validation.</p>
    </abstract>
    <kwd-group>
      <kwd>Visual exploration</kwd>
      <kwd>XAI</kwd>
      <kwd>Human-AI collaboration</kwd>
      <kwd>latent space</kwd>
      <kwd>small multiples</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>1</label>
    <title>INTRODUCTION</title>
    <p id="P2">Presenting analyzed small multiples (<italic toggle="yes">e.g</italic>., patches of medical images, miniature visualizations of a large genomic sequence) using latent vectors learned by machine learning (ML) models has become a common practice in many visual analytics systems [<xref rid="R6" ref-type="bibr">6</xref>, <xref rid="R10" ref-type="bibr">10</xref>]. A latent vector, usually represented as multi-dimensional quantitative values, is a compact representation of the analyzed data to capture relevant information. For example, a 64√ó64 pixel image can be represented as a 10-dimensional latent vector. Compared with analysis using raw data or human-crafted metrics, latent vectors enable users to organize and explore a large amount of data and conduct analysis tasks, such as finding similar items and identifying outliers, more efficiently.</p>
    <p id="P3">Even though latent vectors can accurately capture patterns extracted from the analyzed data, they cannot be directly interpreted by humans like the original images or texts. For this reason, latent vectors are usually used to represent the similarity between data items, assuming the latent vectors of two similar items are close in a latent space. For example, dimension reduction methods (<italic toggle="yes">e.g</italic>., t-SNE [<xref rid="R58" ref-type="bibr">58</xref>], UMAP [<xref rid="R41" ref-type="bibr">41</xref>]) are widely used to visualize latent vectors in 2D space, showing the similarities and differences among data items. Other prior studies proposed to hierarchically cluster items based on their latent vectors to conduct pattern-driven visual analytics [<xref rid="R6" ref-type="bibr">6</xref>]. However, definitions of ‚Äúsimilar items‚Äù vary depending on analysis tasks, and there is no single definition that can be applied to all scenarios. Even though some prior studies have incorporated user input to learn user‚Äôs perception of similarity [<xref rid="R10" ref-type="bibr">10</xref>, <xref rid="R32" ref-type="bibr">32</xref>] and even to extract human-readable concepts (<italic toggle="yes">e.g</italic>., gender from face images) [<xref rid="R37" ref-type="bibr">37</xref>, <xref rid="R68" ref-type="bibr">68</xref>], visual analytics based on latent vectors still suffers from their limited interpretability.</p>
    <p id="P4">Disentangled representation learning (DRL) [<xref rid="R12" ref-type="bibr">12</xref>, <xref rid="R22" ref-type="bibr">22</xref>] is a promising approach that can provide more explainable latent vectors through unsupervised learning, <italic toggle="yes">i.e</italic>., without human labels. By disentangling features and encoding them as separated dimensions in the latent vectors, DRL can generate latent vectors whose values carry semantics and can reveal human-understandable concepts, <italic toggle="yes">e.g</italic>., the value on one dimension indicates whether a person is smiling or not (<xref rid="F1" ref-type="fig">Figure 1d</xref>). We call such dimensions semantic dimensions. Some recent visualization tools [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R20" ref-type="bibr">20</xref>, <xref rid="R59" ref-type="bibr">59</xref>] have successfully employed DRL in their analysis and demonstrated the effectiveness of DRL. For example, Gou <italic toggle="yes">et al</italic>. [<xref rid="R18" ref-type="bibr">18</xref>] used DRL for traffic light images to summarize images based on human-readable concepts, such as color, brightness, and rotation. These studies usually assumed that the learned semantic dimensions can perfectly capture human concepts, and the concepts can be accurately represented by a set of synthesized images. However, these assumptions do not always hold. Potential mismatches can exist between the semantic latent dimensions learned by ML models and the concepts of humans. As shown in <xref rid="F2" ref-type="fig">Figure 2a</xref>, one latent dimension correlates to the angle of human head according to the synthesized images. But when using this dimension to organize images, our experiment results show that the model confuses ‚Äúangle of the head‚Äù with ‚Äúwhether part of the face is covered‚Äù, <italic toggle="yes">e.g</italic>., covered by a dark shadow or a flower (<xref rid="F2" ref-type="fig">Figure 2b</xref>). Meanwhile, previous studies focus on using DRL to diagnose supervised ML models rather than building an understanding of the data [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R20" ref-type="bibr">20</xref>]. They provide limited discussion about user needs in understanding and utilizing DRL for concept-driven data exploration.</p>
    <p id="P5">This study aims to provide a more interpretable and flexible visual exploration of small multiples by better aligning concepts of human users with the semantic latent vectors generated by ML models. We propose Drava, an interactive system that utilizes <bold>D</bold>isentangled <bold>R</bold>epresentation learning as <bold>A V</bold>isual <bold>A</bold>nalytics approach for concept-driven data exploration. In Drava, a dataset is represented as a set of small multiples [<xref rid="R57" ref-type="bibr">57</xref>], <italic toggle="yes">i.e</italic>., a series of basic charts or graphics that show instances or different slices of the dataset (<xref rid="F1" ref-type="fig">Figure 1</xref>). Hereafter, we call each small multiple as a <italic toggle="yes">data item</italic>. For each data item, DRL learns a multi-dimensional latent vector, certain dimensions of which have semantic meanings. Drava supports an interpretable exploration of these items by supporting users in correlating and aligning the semantic dimensions with human concepts. The interactive visualizations and algorithms in Drava are motivated and guided by a three-step workflow that we propose. Throughout this workflow, users 1) understand ML-learned semantic dimensions and identify their potential mismatches with human concepts, 2) refine and align ML semantic dimensions with human concepts, and 3) generate new knowledge about the analyzed data through concept-driven exploration. Particularly, Drava automatically ranks latent vectors and proposes a concept adaptor that can refine a concept based on human input. Meanwhile, a set of interactions based on visual piles [<xref rid="R33" ref-type="bibr">33</xref>] are provided, enabling users to effectively arrange, summarize, and compare items based on human-readable concepts. We demonstrate the usefulness of Drava through experimental validation and four usage scenarios. Drava is available at <ext-link xlink:href="https://qianwen.info/DRAVA/" ext-link-type="uri">https://qianwen.info/DRAVA/</ext-link>.</p>
  </sec>
  <sec id="S2">
    <label>2</label>
    <title>BACKGROUND: DISENTANGLED REPRESENTATION LEARNING</title>
    <p id="P6">DRL is a promising machine learning method that is able to extract interpretable features without human supervision. Given an input item <bold>x</bold>, the goal of DRL is to learn a vector <bold>z</bold> that captures the features of <bold>x</bold> in a disentangled manner. For example, as shown in <xref rid="F2" ref-type="fig">Figure 2a</xref>, a DRL model learns to represent an image of a human face using <bold>z</bold> and captures the feature ‚Äúthe angle of head‚Äù independently in <italic toggle="yes">z</italic><sub>1</sub> (<italic toggle="yes">i.e</italic>., the second dimension of <bold>z</bold>). Similarly, an area chart can be described as a vector <bold>z</bold> where <italic toggle="yes">z</italic><sub>0</sub> indicates the height of the chart, <italic toggle="yes">z</italic><sub>1</sub> indicates the trend, etc. DRL assumes <bold>x</bold> as a joint distribution of independent and dependent generative factors [<xref rid="R22" ref-type="bibr">22</xref>]. While these independent factors will be captured in separated dimensions of <bold>z</bold> (<italic toggle="yes">i.e</italic>., semantic dimensions), the dependent factors will remain entangled in other dimensions of <bold>z</bold> that are not used for representing the independent factors. In other words, some dimensions of <bold>z</bold> will have semantic meanings while others will not. For a precise mathematical definition of disentangled and entangled dimensions, we refer the readers to [<xref rid="R8" ref-type="bibr">8</xref>, <xref rid="R21" ref-type="bibr">21</xref>, <xref rid="R22" ref-type="bibr">22</xref>].</p>
    <p id="P7">A DRL model learns disentangled representations via two loss terms, a reconstruction term and a regularization term. The reconstruction term evaluates the differences between the input item <bold>x</bold> and the reconstructed item <inline-formula><mml:math id="M1" display="inline"><mml:mrow><mml:mover accent="true"><mml:mstyle mathvariant="bold"><mml:mtext>x</mml:mtext></mml:mstyle><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, encouraging the model to learn <bold>z</bold> that capture the main characteristics of the input item. The regularization term encourages disentanglement of the latent vectors. A DRL model is usually constructed by encouraging disentanglement in standard generative models, such as VAE [<xref rid="R22" ref-type="bibr">22</xref>] and GAN [<xref rid="R12" ref-type="bibr">12</xref>]. The state-of-the-art DRL approaches are largely based on VAE mainly due to their better training stability than GAN-based methods. For example, the loss function of <italic toggle="yes">Œ≤</italic>-VAE is defined as
<disp-formula id="FD1"><label>(1)</label><mml:math id="M2" display="block"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>œï</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>‚à£</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext>log</mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mi>Œ∏</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>‚à£</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>‚àí</mml:mo><mml:mi>Œ≤</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>œï</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>‚à£</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>‚Äñ</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
The first term is a reconstruction loss, and the second term is a regularization for disentanglement. With <italic toggle="yes">Œ≤</italic> &gt; 1, <italic toggle="yes">Œ≤</italic>-VAE encourages disentangled <bold>z</bold> by putting a constraint on the latent bottleneck. Prior studies have proposed various regularization terms for disentanglement. For more details, refer to prior studies [<xref rid="R21" ref-type="bibr">21</xref>, <xref rid="R36" ref-type="bibr">36</xref>]. Given its wide popularity, we use <italic toggle="yes">Œ≤</italic>-VAE in Drava with some modifications (<xref rid="S14" ref-type="sec">subsection 6.1</xref>). The proposed framework can be easily adapted to other VAE-based DRL, such as FactorVAE [<xref rid="R28" ref-type="bibr">28</xref>] or <italic toggle="yes">Œ≤</italic>-TCVAE [<xref rid="R11" ref-type="bibr">11</xref>].</p>
  </sec>
  <sec id="S3">
    <label>3</label>
    <title>RELATED WORK</title>
    <p id="P8">First, since Drava aims to assist data exploration using explainable latent vectors, it is closely related to <bold>visual analytics on latent vectors</bold> and, more broadly, <bold>visual analytics for ML models</bold> whose hidden layers generate latent vectors of the input data.</p>
    <p id="P9">Many visual analytics tools have been proposed to support interactive explorations of latent vectors. Dimensionality reduction techniques, such as t-SNE [<xref rid="R58" ref-type="bibr">58</xref>], UMAP [<xref rid="R41" ref-type="bibr">41</xref>], PCA [<xref rid="R1" ref-type="bibr">1</xref>], and their variants [<xref rid="R35" ref-type="bibr">35</xref>, <xref rid="R66" ref-type="bibr">66</xref>], are widely used to assist the visualization of latent vectors. Most of them focus on analyzing the latent vectors generated by a specific model [<xref rid="R67" ref-type="bibr">67</xref>], such as a convolutional neural network [<xref rid="R25" ref-type="bibr">25</xref>, <xref rid="R34" ref-type="bibr">34</xref>, <xref rid="R46" ref-type="bibr">46</xref>], a graph neural network [<xref rid="R24" ref-type="bibr">24</xref>], and a recurrent neural network [<xref rid="R35" ref-type="bibr">35</xref>, <xref rid="R42" ref-type="bibr">42</xref>, <xref rid="R54" ref-type="bibr">54</xref>]. Other studies aim to provide more generic methods for visually exploring the latent space [<xref rid="R7" ref-type="bibr">7</xref>, <xref rid="R37" ref-type="bibr">37</xref>, <xref rid="R51" ref-type="bibr">51</xref>]. Most relevant to our study is LSC [<xref rid="R37" ref-type="bibr">37</xref>], which provides comprehensive support for mapping and comparing semantic dimensions in the analysis of latent vectors. However, LSC requires users to manually identify semantic dimensions, either by importing data labels or by interactively grouping items.</p>
    <p id="P10">Apart from showing latent vectors, previous studies have combined interactive visual analytics with interactive or explainable ML to introduce interpretability into the analysis of latent vectors [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R23" ref-type="bibr">23</xref>, <xref rid="R68" ref-type="bibr">68</xref>]. Several studies [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R20" ref-type="bibr">20</xref>, <xref rid="R59" ref-type="bibr">59</xref>] used DRL to extract semantic dimensions and associate model performance with human concepts (<italic toggle="yes">e.g</italic>., brightness of images, location of objects). The semantic dimensions learned by DRL are directly used without refinement, mostly because they are low-level concepts that can be easily extracted by ML. Jia <italic toggle="yes">et al</italic>. [<xref rid="R23" ref-type="bibr">23</xref>] proposed a visual explainable active learning approach that asks users questions and uses their answers to learn explainable attributes that can be used to classify images from unseen classes. Zhao <italic toggle="yes">et al</italic>. [<xref rid="R68" ref-type="bibr">68</xref>] proposed a visualization tool where users can explore and label image patches with a certain concept. These labels are used to train a concept extractor network, enabling users to diagnose model predictions using the learned concept.</p>
    <p id="P11">However, these studies mainly focus on understanding the working mechanism of ML models and improving model performances (<italic toggle="yes">i.e</italic>., VIS for ML). How to utilize explainable latent vectors for concept-driven data exploration (<italic toggle="yes">i.e</italic>., XAI for VIS) has not been extensively discussed. Drava is built upon previous visual analytics studies on latent vectors and ML models. Unlike previous studies, Drava focuses on aligning interpretable latent vectors with human concepts to assist concept-driven data exploration.</p>
    <p id="P12">Second, Drava learns the visual representation and supports <bold>the exploration of small multiples</bold> [<xref rid="R57" ref-type="bibr">57</xref>], a series of miniature visualizations that represent different facets, subsets, or instances of a dataset. Current studies in data visual exploration usually present small multiples as points (<italic toggle="yes">e.g</italic>., [<xref rid="R7" ref-type="bibr">7</xref>, <xref rid="R16" ref-type="bibr">16</xref>, <xref rid="R47" ref-type="bibr">47</xref>, <xref rid="R51" ref-type="bibr">51</xref>]), glyphs (<italic toggle="yes">e.g</italic>., [<xref rid="R29" ref-type="bibr">29</xref>, <xref rid="R63" ref-type="bibr">63</xref>]), or images (<italic toggle="yes">e.g</italic>., [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R27" ref-type="bibr">27</xref>, <xref rid="R37" ref-type="bibr">37</xref>]) and place them in a grid, a dimension reduction projection, or a data-driven layout. For example, Sharkzor [<xref rid="R27" ref-type="bibr">27</xref>] enabled users to interactively organize images and their groups while providing visual cues for groups (<italic toggle="yes">e.g</italic>., badges). AxiSketcher [<xref rid="R29" ref-type="bibr">29</xref>] uses glyph representations and offers sketch-based interactions to flexibly arrange data items in the 2D space. Even though these studies provided valuable insights, they provide limited support in inspecting and summarizing a group of small multiples, which are important to reveal and remove the mismatches between human concepts and ML semantic dimensions. Some interaction techniques have been proposed to better organize small multiples and facilitate the exploration, such as interactive piling [<xref rid="R4" ref-type="bibr">4</xref>, <xref rid="R30" ref-type="bibr">30</xref>, <xref rid="R33" ref-type="bibr">33</xref>] and hierarchical clustering [<xref rid="R6" ref-type="bibr">6</xref>, <xref rid="R31" ref-type="bibr">31</xref>]. For example, interactive piling is inspired by physical piles and enables users to effectively group, aggregate, browse, and compare small multiples. However, these interactions are usually designed for specific application scenarios and cannot be directly applied to concept-driven exploration. In Drava, we adapt interactive piling to facilitate the concept-driven exploration of small multiples, especially focusing on the interpretation of semantic dimensions, the mismatch identification between ML semantic dimensions and human concepts, and guidance on refining semantic dimensions.</p>
    <p id="P13">Third, to better guide user exploration and insight generation, researchers have proposed <bold>interactive ML for visual data exploration</bold>, which learns what visual concepts are important to users from user feedback [<xref rid="R5" ref-type="bibr">5</xref>, <xref rid="R10" ref-type="bibr">10</xref>, <xref rid="R15" ref-type="bibr">15</xref>, <xref rid="R32" ref-type="bibr">32</xref>, <xref rid="R61" ref-type="bibr">61</xref>]. For example, Behrisch <italic toggle="yes">et al</italic>. [<xref rid="R5" ref-type="bibr">5</xref>] trained a classifier to interactively capture users‚Äô notion of interestingness when exploring many scatter plots. This classifier is then used to recommend potentially interesting plots and guide the exploration of large multidimensional data. Cai <italic toggle="yes">et al</italic>.[<xref rid="R10" ref-type="bibr">10</xref>] provides an interactive tool that empowers users to refine an ML model by communicating what types of similarities are most important when searching certain medical images. Peax [<xref rid="R32" ref-type="bibr">32</xref>] proposes an efficient and accurate query of a certain visual pattern in sequential data by learning from users‚Äô binary feedback on samples selected through active learning strategy. However, prior studies mainly use interactive ML to assist with similarity queries, <italic toggle="yes">i.e</italic>., modeling the similarity between items and user-selected targets. Despite the helpful guidance that these studies provide in data exploration, they cannot provide a comprehensive overview of the analyzed data.</p>
    <p id="P14">Like these approaches, Drava employs learning from user input to provide more precise exploration guidance. Furthermore, Drava provides semantic dimensions and supports summarization, exploration, and analysis based on different visual concepts.</p>
  </sec>
  <sec id="S4">
    <label>4</label>
    <title>WORKFLOW AND TASKS</title>
    <p id="P15">In this section, we decompose the overall goal of <italic toggle="yes">concept-driven visual exploration using DRL</italic> into three main steps (<xref rid="F3" ref-type="fig">Figure 3</xref>). We discuss the user tasks within each step from two aspects: the characteristics of DRL, as discussed in the DRL literature [<xref rid="R8" ref-type="bibr">8</xref>, <xref rid="R22" ref-type="bibr">22</xref>, <xref rid="R28" ref-type="bibr">28</xref>]; and the user needs in visual data exploration, largely informed by the task summarization work in previous studies [<xref rid="R16" ref-type="bibr">16</xref>, <xref rid="R33" ref-type="bibr">33</xref>, <xref rid="R37" ref-type="bibr">37</xref>]. These user tasks have been well established in previous studies and can be reused to effectively guide the design of Drava. Moreover, reuses in the task analysis can increase the design quality and reduce expenditure, as recommended in [<xref rid="R44" ref-type="bibr">44</xref>, <xref rid="R55" ref-type="bibr">55</xref>, <xref rid="R56" ref-type="bibr">56</xref>].</p>
    <sec id="S5">
      <title><bold>Step 1: Interpret ML Semantic Dimensions</bold>.</title>
      <p id="P16">Since only a subset of the latent dimensions correlates with semantic meanings, users should be assisted to <italic toggle="yes">identify the semantic dimensions efficiently</italic> (<bold>T1.1</bold>). For a specific dimension, users can <italic toggle="yes">interpret its semantic meaning</italic> (<bold>T1.2</bold>) through 1) synthesized images generated by single value traversal of this dimension or 2) data items sorted and grouped by their value in this dimension. A group summary can help users to efficiently understand the semantic meaning of a large number of items, associate it with a human concept, and identify mismatches. Unlike previous studies that group items based on their overall similarities, concept-based analysis requires to group and summarize items based on certain concepts. Therefore, proper aggregations should be provided to <italic toggle="yes">highlight the concept of interest and fade out others</italic> (<bold>T1.3</bold>) when summarizing an item group.</p>
    </sec>
    <sec id="S6">
      <title>Step 2: Align ML Semantic Dimensions with Human Concepts.</title>
      <p id="P17">Once a mismatch is identified, users modify the semantic dimension to better align it with the human‚Äôs definition of concepts. Such <italic toggle="yes">refinement should be user-friendly and conducted upon objects that users are familiar with</italic> (<bold>T2.1</bold>), <italic toggle="yes">e.g</italic>., data items and item groups rather than numerical values of latent dimensions. Meanwhile, <italic toggle="yes">visual cues should be provided to guide and facilitate the user refinement</italic> (<bold>T2.2</bold>), <italic toggle="yes">e.g</italic>., highlight the items that are grouped wrongly due to a concept mismatch.</p>
    </sec>
    <sec id="S7">
      <title>Step 3: Generate New Human Knowledge about the Data.</title>
      <p id="P18">Users <italic toggle="yes">explore the data items based on the identified concepts</italic> (<bold>T3.1</bold>) to generate insights about the analyzed items, including the distribution of items on one or multiple visual concepts, the association between different concepts. Such analysis can be further enhanced by <italic toggle="yes">correlating the concepts with other item metadata</italic> (<bold>T3.2</bold>), such as the spatial information and the item labels.</p>
      <p id="P19">The three steps are interconnected (<italic toggle="yes">i.e</italic>., arrows in <xref rid="F3" ref-type="fig">Figure 3</xref>). For example, users may directly go to Step 3 from Step 1 if they do not observe obvious mismatches. Users can also go back from Step 3 to Step 2 if they find some semantic dimensions fail to support their analysis tasks and require further refinement. Drava provides a set of dedicated interactive visualizations and algorithms that are closely coupled with this three-step workflow.</p>
    </sec>
  </sec>
  <sec id="S8">
    <label>5</label>
    <title>VISUAL INTERFACE</title>
    <p id="P20">The user interface of Drava (<xref rid="F4" ref-type="fig">Figure 4</xref>) consists of a <italic toggle="yes">Concept View</italic>, an <italic toggle="yes">Item Browser</italic>, and an optional <italic toggle="yes">Spatial View</italic>. The interactions related to visual piles are based on the design space proposed by Lekschas <italic toggle="yes">et al</italic>. [<xref rid="R33" ref-type="bibr">33</xref>], selected, modified, and extended to better reflect tasks described in <xref rid="S4" ref-type="sec">section 4</xref>.</p>
    <sec id="S9">
      <label>5.1</label>
      <title>Concept View</title>
      <p id="P21">In the <italic toggle="yes">Concept View</italic> (<xref rid="F4" ref-type="fig">Figure 4a</xref>), each latent dimension is visualized as a histogram and a list of synthesized images. The histogram shows the distribution of all items based on their values on the corresponding latent dimension (<bold>T3.1</bold>). Since the exact values of a latent dimension do not have specific meanings, we use synthesized images rather than numbers as the tick labels of the <italic toggle="yes">x</italic>-axis in the histogram. The synthesized images are generated by the decoder in the DRL model. For one specific latent dimension, the synthesized images are generated using a set of latent vectors whose values only differ on this dimension. These synthesized images illustrate the visual changes associated with the value traversal on the investigated dimension and help users understand its semantics (<bold>T1.2</bold>). Users can filter items based on their values on specific semantic dimensions by clicking on bars of a histogram (<xref rid="F4" ref-type="fig">Figure 4F</xref>).</p>
      <p id="P22">As explained in <xref rid="S2" ref-type="sec">section 2</xref>, only a subset of the latent dimensions are semantic and correlate with human concepts [<xref rid="R8" ref-type="bibr">8</xref>, <xref rid="R28" ref-type="bibr">28</xref>]. Therefore, it is important to provide a mechanism that guides users in the exploration of a potentially large number of dimensions. Drava calculates a salience score for each latent dimension (<xref rid="S16" ref-type="sec">subsection 6.3</xref>), indicating how important a particular latent dimension is for the synthesized images. As shown in <xref rid="F4" ref-type="fig">Figure 4A</xref>, all latent dimensions are ranked based on their salience scores, and the normalized score of each latent dimension is visualized by the width of a gray bar (<bold>T1.1</bold>). Users can change the dimension name based on their interpretation of the associated concept to facilitate the following analysis. Users can also remove irrelevant dimensions and add other customized dimensions from the item metadata.</p>
    </sec>
    <sec id="S10">
      <label>5.2</label>
      <title>Item Browser</title>
      <p id="P23">The <italic toggle="yes">Item Browser</italic> (<xref rid="F4" ref-type="fig">Figure 4b</xref>) layouts all items in a 2D space where users can freely arrange and group items. Arranging items based on their values of certain semantic dimensions enables users to interpret semantic dimensions and understand the item distribution among a certain concept (<bold>T3.1</bold>). A set of synthesized images are added to the <italic toggle="yes">x</italic>‚Äì and/or <italic toggle="yes">y</italic>‚Äìaxis to guide the interpretation of latent semantic dimensions and the exploration of data items (<xref rid="F4" ref-type="fig">Figure 4B</xref>). Since the visual appearance of the synthesized images largely depends on the latent vector, Drava allows users to select an item and use its latent vectors to generate synthesized images. This interaction enables users to further validate the concept associated with a latent dimension and identify possible mismatches (<bold>T1.2</bold>).</p>
      <p id="P24">Since the number of items can be large and the items often overlap with each other, effective grouping and summarizing mechanisms are needed. In Drava, users can either manually group items using a lasso selection or automatically group items based on their proximity in the 2D space. Drava provides various methods for summarizing a group of items and revealing abnormal items inside this group (<bold>T1.3</bold>), as shown in <xref rid="F5" ref-type="fig">Figure 5a</xref>‚Äì<xref rid="F5" ref-type="fig">b</xref>. When items are arranged horizontally (<italic toggle="yes">i.e</italic>., 1D grouping), items will be stacked along the vertical direction, and each item will be visualized as an item preview. Users can select the grouping method in the configure panel based on the characteristics of items and concepts.</p>
      <p id="P25">Labels can also be added to individual items or item groups to incorporate more item metadata into the analysis and investigate their associations with concepts, as shown in <xref rid="F5" ref-type="fig">Figure 5c</xref> (<bold>T3.2</bold>). To investigate more details about an item group, users can browse items by hovering on their item previews (<xref rid="F4" ref-type="fig">Figure 4D</xref>). A pop-up menu, shown upon right-clicking on an item group, enables users to depile this group or browse the items in a separate window.</p>
    </sec>
    <sec id="S11">
      <label>5.3</label>
      <title>Spatial View</title>
      <p id="P26">The <italic toggle="yes">Spatial View</italic> (<xref rid="F4" ref-type="fig">Figure 4c</xref>) is an optional view for data items that have spatial/context information. For example, in <xref rid="F4" ref-type="fig">Figure 4</xref>, each item indicates a region of interest in a huge genomic interaction matrix and is arranged according to its genomic location. Users can zoom and pan to obtain an overview or inspect further details. The <italic toggle="yes">Spatial View</italic> is coordinated with other views to reveal the correlations between concepts and item context (<bold>T3.2</bold>). When users filter items in the <italic toggle="yes">Concept View</italic>, the corresponding items will fade out in the <italic toggle="yes">Spatial View</italic>.</p>
    </sec>
    <sec id="S12">
      <label>5.4</label>
      <title>User Refinement</title>
      <p id="P27">Instead of directly modifying the hard-to-interpret latent values, Drava supports refinement towards groups and items (<bold>T2.1</bold>). For one selected semantic dimension <italic toggle="yes">D</italic><sub><italic toggle="yes">i</italic></sub>, Drava groups items (21 bins by default) based on their values of this dimension <italic toggle="yes">d</italic><sub><italic toggle="yes">i</italic></sub> to represent the gradual changes associated with this dimension. First, this default group assignment may have inappropriate thresholds, <italic toggle="yes">e.g</italic>., assigning similar items into two adjacent groups. Therefore, Drava enables users to merge (<xref rid="F6" ref-type="fig">Figure 6b1</xref>) or split (<xref rid="F6" ref-type="fig">Figure 6b2</xref>) groups to construct more meaningful groups according to one concept. More importantly, due to the imperfection of algorithms, the latent values may not accurately depict the concept for certain items, leading to inappropriate <italic toggle="yes">x</italic> position and group assignment for these items. Users can align the concepts and semantic dimensions by changing the item position (<xref rid="F6" ref-type="fig">Figure 6b1</xref>) and reassigning the group of these items (<xref rid="F6" ref-type="fig">Figure 6b3</xref>).</p>
      <p id="P28">Several mechanisms are provided to assist users in locating abnormal items and groups (<bold>T2.2</bold>), as shown in <xref rid="F6" ref-type="fig">Figure 6a</xref>. First, users can decide whether to merge or split groups by comparing these groups side by side (a1). Second, Drava enables users to identify abnormal items through previews (a2). For example, as shown in <xref rid="F4" ref-type="fig">Figure 4C</xref>, all items are grouped based on the thickness of their diagonal. Users can locate an abnormal item because its preview is darker than others. Users then examine this item through in-place browsing (<xref rid="F4" ref-type="fig">Figure 4D</xref>), extract it using the pop-up menu (<xref rid="F4" ref-type="fig">Figure 4E</xref>), and drag and drop it to a proper group based its diagonal thickness. Apart from identifying abnormal items through previews, users can also browse a group in a separate window and arrange the items using selected metrics (a3). In our experiments, we found certain metric values are useful in identifying abnormal items, including the reconstruction loss, the deviation of the latent value, the item metadata, and the uncertainty score.</p>
      <p id="P29">After user refinement, Drava supports two mechanisms, local and global, to update the items and/or the underlying model (<xref rid="F6" ref-type="fig">Figure 6c</xref>). By default, Drava employs a local updating mechanism, which remembers the user refinement, applies it to items with similar latent vectors, but does not modify the underlying model. Similar items are defined by setting a threshold <italic toggle="yes">Œ∏</italic> to the <italic toggle="yes">L</italic>2 distances of their latent vectors to the that of the refined items. On the contrary, global update initializes and fine-tunes a concept adaptor (<xref rid="S15" ref-type="sec">subsection 6.2</xref>). The values for all other items at this dimension will be updated accordingly by this concept adaptor. The global refinement is triggered by clicking the <italic toggle="yes">update concept</italic> button. Since it is hard for users to label an item with an exact numerical value, global refinement can only be triggered when items are grouped for a certain concept.</p>
    </sec>
  </sec>
  <sec id="S13">
    <label>6</label>
    <title>MODEL SETUP AND IMPLEMENTATION</title>
    <sec id="S14">
      <label>6.1</label>
      <title>Learning Semantic Dimensions using DRL</title>
      <p id="P30">Our DRL model is based on the <italic toggle="yes">Œ≤</italic>-VAE [<xref rid="R22" ref-type="bibr">22</xref>]. The structure of the DRL model is illustrated in <xref rid="F7" ref-type="fig">Figure 7</xref>, encoder (a) and decoder (b). Each convolution block consists of a convolution layer, a batch normalization layer, and a leaky ReLU (Rectified Linear Unit) activation. The decoder architecture is the transpose of the encoder. Following the practice in [<xref rid="R53" ref-type="bibr">53</xref>], we do not include Max Pooling layers by setting <italic toggle="yes">stride</italic> = 2 in the convolution layer. All usage scenarios in this paper use this structure and only vary in 1) the number of convolution and transposed convolution blocks, 2) the kernel size and the number of channels of the convolution and transposed convolution layers, and 3) the output size of the fully connected layer (<italic toggle="yes">i.e</italic>., the number of dimensions for the latent vector).</p>
      <p id="P31">We use the loss function proposed by Burgess <italic toggle="yes">et al</italic>. [<xref rid="R8" ref-type="bibr">8</xref>], which progressively increases the information capacity during the training process. An Adam optimization is used to train the model. Note that we use the mean <italic toggle="yes">Œº</italic> of the normal distribution learned by the encoder rather than the sampled <inline-formula><mml:math id="M3" display="inline"><mml:mrow><mml:mi>z</mml:mi><mml:mo>~</mml:mo><mml:mo>ùí©</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Œº</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>œÉ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as the latent vector for the input data, which enables deterministic latent values for each data item.</p>
      <p id="P32">Even though we implement and evaluate Drava using <italic toggle="yes">Œ≤</italic>-VAE, the proposed framework can be easily adapted to other VAE-based DRL models, such FactorVAE [<xref rid="R28" ref-type="bibr">28</xref>] and <italic toggle="yes">Œ≤</italic>-TCVAE [<xref rid="R11" ref-type="bibr">11</xref>].</p>
    </sec>
    <sec id="S15">
      <label>6.2</label>
      <title>Concept Adaptor</title>
      <p id="P33">The concept adaptor is a lightweight model that modifies semantic dimensions based on user refinements. For each semantic dimension, one concept adaptor will be generated if users use this dimension to arrange items, refine the item groups, and apply a global update. Since it is hard for users to associate the concept with an exact numerical value, the concept adaptor is only used to refine a concept for already grouped items. In other words, the concept adaptor is a multi-class classifier. The concept adaptor uses the feature map generated by the encoder hidden layer as input and predicts the group that the input item should belong to.</p>
      <p id="P34"><xref rid="F7" ref-type="fig">Figure 7c</xref> illustrates the structure of the concept adaptor. The convolution block contains a convolution layer (kernel size =4, stride =2) and a batch normalization. The convolution layer has <italic toggle="yes">n</italic> output channels where <italic toggle="yes">n</italic> equals to the number of item groups. A <italic toggle="yes">n</italic> √ó 1 vector will be obtained after a global max pooling layer and then feed into a softmax function. A cross entropy is used to calculate the loss. An Adam optimization is used to train the model.</p>
      <p id="P35">Once items are grouped based on the values of one dimension, users can initialize a concept adaptor accordingly. The training ends when the validation loss does not decrease. For all the datasets used in <xref rid="S22" ref-type="sec">section 8</xref>, the initialization took less than two minutes on a machine with one Tesla K80 GPU. After users have refined the item groups (<italic toggle="yes">i.e</italic>., change the classification label) for some items, the concept adaptor will be fine-tuned accordingly. During the fine-tuning, we increase the weight of the items that have been refined by the users. For the back-end models, only the concept adaptor is updated with user refinement, while the encoder and decoder are fixed. For the data items, only the values of the specific latent dimension (<italic toggle="yes">i.e</italic>., dimension used as the <italic toggle="yes">x</italic> axis) will be updated by the concept adaptor, while other dimensions will remain the same.</p>
    </sec>
    <sec id="S16">
      <label>6.3</label>
      <title>Dimension Ranking</title>
      <p id="P36">We rank all latent dimensions based on their importance to help users quickly locate semantic dimensions. Inspired by the salience scores used in interpretable ML [<xref rid="R26" ref-type="bibr">26</xref>], we gauge the importance of a latent dimension via the sensitivity of the reconstructed image <inline-formula><mml:math id="M4" display="inline"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> to changes in the magnitude of a latent dimension <italic toggle="yes">z</italic><sub><italic toggle="yes">i</italic></sub>. However, directly using the gradients <inline-formula><mml:math id="M5" display="inline"><mml:mrow><mml:mo>‚àÇ</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>/</mml:mo><mml:mo>‚àÇ</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> has several issues. First, it is a local importance score that is calculated for a particular reconstructed image. Second, it is a vector rather than a scalar value and can be hard to compare across. Third, it counts pixel-level differences that are not necessarily consistent with human perception. To solve these issues, we use a simple but effective method, <italic toggle="yes">i.e</italic>., averaging the importance score across output dimensions and across a set of sampled latent vectors. To mimic human perception of the synthesized images, we use latent vectors of the synthesized images as samples. Instead of using the reconstructed output, we use the feature maps generated by the second last layer of the decoder, aiming to capture high-level features rather than pixel-to-pixel differences.
<disp-formula id="FD2"><mml:math id="M6" display="block"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>m</mml:mi><mml:mo>√ó</mml:mo><mml:mi>n</mml:mi><mml:mo>√ó</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfrac><mml:mstyle><mml:munder><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>‚àÇ</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>‚àÇ</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula>
Where <italic toggle="yes">z</italic><sub><italic toggle="yes">k</italic></sub> is the <italic toggle="yes">k</italic><sub><italic toggle="yes">th</italic></sub> sampled latent vector, <italic toggle="yes">z</italic><sub><italic toggle="yes">k,i</italic></sub> is its value at dimension <italic toggle="yes">i</italic>, <italic toggle="yes">L</italic><sub><italic toggle="yes">m,n</italic></sub> (<italic toggle="yes">z</italic><sub><italic toggle="yes">k</italic></sub>) is the feature map (<italic toggle="yes">m, n</italic>) of the second to last decoder layer.</p>
      <p id="P37">The salience score serves as a useful indicator for semantic dimensions (<xref rid="F8" ref-type="fig">Figure 8</xref>). Theoretically, a dimension with a high salience score is not necessarily equal to a semantic dimension, <italic toggle="yes">e.g</italic>., a dimension is not semantic but significantly influences the output. However, the DRL model will minimize the existence of such dimensions by disentangling features and encoding them as separate dimensions. Ranking all dimensions based on salience scores can help users exclude many latent dimensions that do not contribute to the output and have little semantic meanings (<italic toggle="yes">e.g</italic>., <xref rid="F8" ref-type="fig">Figure 8b</xref>).</p>
    </sec>
    <sec id="S17">
      <label>6.4</label>
      <title>Implementation</title>
      <p id="P38">The implementation of Drava includes a front-end for interactive visualization and a back-end for data storage and the DRL model. The front-end is implemented in TypeScript using React [<xref rid="R17" ref-type="bibr">17</xref>], Piling.js [<xref rid="R33" ref-type="bibr">33</xref>], and Gosling.js [<xref rid="R39" ref-type="bibr">39</xref>]. The visualizations are rendered using SVG, Canvas, and WebGL. The back-end DRL model and concept adaptor are implemented in Python with PyTorch [<xref rid="R45" ref-type="bibr">45</xref>]. The front-end and back-end communicate via a Flask [<xref rid="R19" ref-type="bibr">19</xref>] web server built in Python. Users can easily apply Drava to their own datasets through two YAML configuration files that configure the back-end model training process and the front-end interface, respectively. The source code and documentation are available at <ext-link xlink:href="https://qianwen.info/DRAVA/" ext-link-type="uri">https://qianwen.info/DRAVA/</ext-link>.</p>
    </sec>
  </sec>
  <sec id="S18">
    <label>7</label>
    <title>EXPERIMENTAL VALIDATION</title>
    <p id="P39">In this section, we evaluated the back-end model in Drava from three aspects: 1) the <italic toggle="yes">representativeness</italic> of the latent vector, 2) the <italic toggle="yes">semantic meaning</italic> of individual latent dimensions, and 3) the improvements from <italic toggle="yes">concept fine-tuning</italic>. Previous studies either focused on assessing the disentanglement of latent dimensions [<xref rid="R8" ref-type="bibr">8</xref>, <xref rid="R22" ref-type="bibr">22</xref>, <xref rid="R28" ref-type="bibr">28</xref>] or overlooked the possible mismatches between human concepts and semantic dimensions [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R20" ref-type="bibr">20</xref>, <xref rid="R59" ref-type="bibr">59</xref>]. Therefore, it is important to validate the quality of these semantic latent vectors and their fine-tuning mechanism.</p>
    <sec id="S19">
      <title>Representativeness of the Latent Vector.</title>
      <p id="P40">We used the reconstruction quality to show whether the latent vectors can capture all the important visual features of the input data. <xref rid="F9" ref-type="fig">Figure 9</xref> exemplifies the reconstruction quality of the latent vectors for the four datasets used in the application scenarios (<xref rid="S22" ref-type="sec">section 8</xref>). Instead of the absolute similarity or the realism of the reconstructed images, we focused on evaluating whether the reconstructed images are able to capture important concepts. For the relatively simple <italic toggle="yes">dsprites</italic> shapes dataset (b), the model is able to generate images that are very similar to the input data. For more complex datasets (a, c‚Äìd), even though some details in the input data are missing, the model can reconstruct salient concepts.</p>
    </sec>
    <sec id="S20">
      <title>Semantic Meaning of Individual Latent Dimensions.</title>
      <p id="P41">To evaluate whether a single latent dimension can sufficiently depict a concept, we classified items based on their values on a certain semantic dimension and reported the classification accuracy. Specifically, for <italic toggle="yes">n</italic> classes belonging to a concept, <italic toggle="yes">n</italic> ‚àí 1 thresholds are learned to classify items. For example, the ‚Äúsmiling‚Äù concept has two classes, smiling and not smiling. We first identified a latent dimension <italic toggle="yes">D</italic><sub><italic toggle="yes">i</italic></sub> that is related to the ‚Äúsmiling‚Äù concept. We then classified each item based on whether its value on this dimension <italic toggle="yes">d</italic><sub><italic toggle="yes">i</italic></sub> is larger or smaller than a threshold <italic toggle="yes">thr</italic>, which was chosen to maximize the classification accuracy of all items. We used the <italic toggle="yes">dsprites</italic> and the <italic toggle="yes">CelebA</italic> datasets because they have labels for a diverse set of concepts. The results in <xref rid="T1" ref-type="table">Table 1</xref> demonstrated that the latent dimension value could effectively represent the corresponding concept but also showed space for further improvement.</p>
    </sec>
    <sec id="S21">
      <title>Improvements from Concept Fine-tuning.</title>
      <p id="P42">We evaluated the fine-tuning mechanism of the concept adaptor by comparing the classification accuracy of a specific concept before and after user refinement. This evaluation used the ‚Äúscale‚Äù concept from the <italic toggle="yes">dsprites</italic> dataset and the ‚Äúsmiling‚Äù and ‚Äúbangs‚Äù concepts from the <italic toggle="yes">CelebA</italic> dataset, because they have relatively low accuracy without any human refinement (<xref rid="T1" ref-type="table">Table 1</xref>). We chose an active learning method as the baseline for evaluating the concept adaptor. The baseline had the same architecture as the concept adaptor. We used simulated user feedback to obtain reproducible results in a variety of settings. Following the common practices in evaluating interactive machine learning [<xref rid="R13" ref-type="bibr">13</xref>] and active learning [<xref rid="R49" ref-type="bibr">49</xref>], we simulated user feedback as an oracle (<italic toggle="yes">i.e</italic>., always providing correct labels to the queried items). Both the concept adaptor and the baseline used the same simulation at each iteration but with different initialization. The active learning baseline is initialized with 5% labels. The concept adaptor is initialized with no labels but the same item groups as that in <xref rid="T1" ref-type="table">Table 1</xref>. Such an initialization simulates how users would divide items into several groups for a specific concept based on their latent dimension values. At each iteration, <italic toggle="yes">N</italic> items were refined (for the concept adaptor) or labeled (for the baseline) and models were trained until the validation loss did not decrease, which typically took around 10‚Äì20 epochs and less than 20 seconds. We experimented with three metrics for selecting the <italic toggle="yes">N</italic> items: uncertainty scores of the classification, the standard deviation of the latent dimension value, and differences between the latent dimension value and the classification threshold. We found that refining items with the highest uncertainty score led to the best model performances. Even though we used an oracle to simulate user refinement here, real-world users can easily examine and label these items in Drava by selecting a metric of interest as the <italic toggle="yes">y</italic> axis in <italic toggle="yes">Item Browser</italic>.</p>
      <p id="P43">We ran experiments under three settings: <italic toggle="yes">N</italic> = 1%, 2%, and 5% of the items. A total of 15 iterations were performed for each experiment. The results in <xref rid="F10" ref-type="fig">Figure 10</xref> were obtained by averaging the results of three experiments. First, the increased accuracy indicated that the concept adaptor helped align a concept and a semantic latent dimension. Compared with the baseline, the concept adaptor generated more accurate concepts by leveraging the values of the semantic dimension. Second, the curves of the concept adaptor were more smooth than the baseline, indicating a more stable improvement over iterations. Third, while the concept adaptor and the baseline required the same amount of user effort at each iteration (<italic toggle="yes">i.e</italic>., the same <italic toggle="yes">N</italic> and the same user simulation), the concept adaptor required less user effort at the initialization than the baseline (<italic toggle="yes">i.e</italic>., drawing two or three lasso selections vs. labeling 5% of the items one by one). Fourth, it was not surprising that the difference between the concept adaptor and the baseline model decreased with the increase of <italic toggle="yes">N</italic> and iteration steps. The advantages of the concept adaptor mainly result from using the semantic dimension values. As more and more items are labeled, these semantic dimensions become less useful in describing a concept.</p>
    </sec>
  </sec>
  <sec id="S22">
    <label>8</label>
    <title>APPLICATION SCENARIOS</title>
    <p id="P44">In this section, we present four application scenarios of Drava using one simulated dataset and three real-world datasets. For all four application scenarios, the DRL model is trained on the whole dataset with no labels used. The four application scenarios are conducted under collaboration with domain users, including two postdoctoral researchers on computer vision (P1 and P2, both for <xref rid="S23" ref-type="sec">subsection 8.1</xref> and <xref rid="S27" ref-type="sec">subsection 8.2</xref>), two researchers on genomic analysis (P3 and P4, for <xref rid="S32" ref-type="sec">subsection 8.3</xref>), and a professor on histopathological image analysis (P5, for <xref rid="S37" ref-type="sec">subsection 8.4</xref>). For each application scenario, we first provided a tutorial to introduce the functionalities of Drava. We then demonstrate our analysis and validate our findings with the participants. Participants can freely explore Drava and conduct additional analysis on the provided dataset. We further collected qualitative feedback about Drava from the participants.</p>
    <sec id="S23">
      <label>8.1</label>
      <title>Simple Shapes</title>
      <sec id="S24">
        <title>Data, Model, and Analysis Overview.</title>
        <p id="P45">This scenario uses the <italic toggle="yes">dsprites</italic> dataset [<xref rid="R40" ref-type="bibr">40</xref>], which consists of three types of simple shapes (<italic toggle="yes">i.e</italic>., square, ellipse, heart) with different scales, positions, and orientations. We uniformly sampled 1,000 items. The DRL model has four convolution blocks, each of which has 32 channels, a kernel of size 4, and a stride of 2. The latent vector has 8 dimensions. Even though this is a simple dataset, it can work as a proxy for more complicated datasets, such as the bounding boxes in object detection or the masks for cell segmentation. In this scenario, we explore the distribution of items according to concepts related to position and size, which are identified, validated, and refined by users.</p>
      </sec>
      <sec id="S25">
        <title>Arranging Items based on Concepts of Interest.</title>
        <p id="P46">To start with, we display all items in a 2D space using UMAP, a dimension reduction method that is commonly used for visualizing items with latent vectors. While the UMAP successfully put items with similar shapes and scales close to one another, the shape position information is mostly ignored, as shown in <xref rid="F11" ref-type="fig">Figure 11a</xref>. The position information can be important for some analysis tasks, <italic toggle="yes">e.g</italic>., object detection in autopilot.</p>
        <p id="P47">Based on the synthesized images in the <italic toggle="yes">Concept View</italic>, the position-related information is successfully extracted in two top-ranked dimensions, which we rename to <monospace>dim_x</monospace> and <monospace>dim_y</monospace> (<xref rid="F1" ref-type="fig">Figure 1e</xref>). As shown in <xref rid="F11" ref-type="fig">Figure 11b</xref>, all items are arranged and grouped based on the <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> position of the shape. We choose the <italic toggle="yes">average</italic> method to summarize a group, which enables us to inspect the positions of shapes without browsing individual items one by one.</p>
      </sec>
      <sec id="S26">
        <title>Refine a Semantic Dimension.</title>
        <p id="P48">The scale, <italic toggle="yes">i.e</italic>., size, of the shapes is also a vital piece of information for some analyses and has been successfully extracted in a latent dimension (named as <monospace>dim_size</monospace>). We verify this semantic dimension in the <italic toggle="yes">Item Browser</italic>, setting <monospace>dim_size</monospace> as <italic toggle="yes">x</italic> axis and its deviation <italic toggle="yes">œÉ</italic> as the <italic toggle="yes">y</italic> axis. While all items are sorted based on their size from left to right, we find that items on the left side are all squares (<xref rid="F11" ref-type="fig">Figure 11c1</xref>). We speculate this is because an ellipse or a heart, even with the same scale, is smaller than a square in terms of absolute pixel areas.</p>
        <p id="P49">To obtain a semantic dimension that better matches the analysis purpose and indicates the scale regardless of shape types, we refine <monospace>dim_size</monospace> using the concept adaptor. We set the ‚Äúreconstruction loss‚Äù as the <italic toggle="yes">y</italic> axis to reveal abnormal items (<xref rid="F11" ref-type="fig">Figure 11c2</xref>) and modify the <italic toggle="yes">x</italic> position of these items. We then group the items into three main groups, indicating large, medium, and small scales, respectively. After clicking the <italic toggle="yes">update concept</italic> button, the concept adaptor is initialized based on our grouping. We further refine these groups using the <italic toggle="yes">browse separately</italic> function, examining each group and updating the group mainly by moving items of ellipse or heart shape from the medium group to the large group. After several updates, we click the <italic toggle="yes">update concept</italic> button again. The concept adaptor is fine-tuned based on the refined item groups and updates the grouping of all items. After several iterations, we obtain three groups that more accurately reflect the scale of shapes without the influence of shape types (refer to <xref rid="S18" ref-type="sec">section 7</xref> for quantitative results).</p>
      </sec>
    </sec>
    <sec id="S27">
      <label>8.2</label>
      <title>Celebrity Images</title>
      <sec id="S28">
        <title>Data, Model, and Analysis Overview.</title>
        <p id="P50">This usage scenario uses the celebrity images from the <italic toggle="yes">CelebA</italic> dataset [<xref rid="R38" ref-type="bibr">38</xref>]. The DRL model is trained on the complete dataset, and we randomly sample 1,000 items for the exploration in Drava. The DRL model has five convolution blocks, each of which contains a kernel of size 3, a stride of 2, and 32, 64, 128, 256, and 512 channels, respectively. The latent vector has 20 dimensions. In this scenario, we investigate the quality of the <italic toggle="yes">CelebA</italic> dataset based on the diversity, balance, and association of the concepts in this dataset.</p>
      </sec>
      <sec id="S29">
        <title>Examine Dataset Diversity.</title>
        <p id="P51">Collecting a diverse dataset is important in ML to improve the model performance in real-world deployment and avoid algorithmic discrimination of certain populations [<xref rid="R65" ref-type="bibr">65</xref>]. The concepts extracted by Drava offer an effective approach to investigating the diversity of a dataset.</p>
        <p id="P52">Based on the synthesized images in the <italic toggle="yes">Concept View</italic>, we can affirm that diverse visual concepts exist in the analyzed data items. The analyzed items vary in a number of aspects, including emotional expression, gender, angle, skin color, background color, hair length, and hairstyle. To further verify our interpretation of the semantics of individual dimensions, we can interactively change the latent vector to update the synthesized images and group items based on their latent values at a selected dimension (<xref rid="F1" ref-type="fig">Figure 1d</xref>).</p>
      </sec>
      <sec id="S30">
        <title>Investigate Dataset Balance.</title>
        <p id="P53">We then analyze the item distribution along individual concepts as dataset imbalance can introduce bias during model training and impair model performance. For example, for the ‚Äúskin color‚Äù concept, a dataset with a large number of items with fair skin and only a small number of items with dark skin can lead to an ML model that has poor performance on the latter. As shown in <xref rid="F12" ref-type="fig">Figure 12a</xref>, we arrange and group items based on <monospace>dim_9</monospace>, which captures skin color based on the synthesized images. Through browsing items in these groups, we find only the right several groups include people with dark skin (a1), indicating a relatively small portion. When we browse individual items in each group (a2), we can find that this portion is even smaller since the model considers people with dark skin and people with shadows on their faces as similar. This observation implies an imbalance related to skin color, which may introduce a bias into a model trained on it.</p>
      </sec>
      <sec id="S31">
        <title>Confirm Concept Association.</title>
        <p id="P54">Based on <xref rid="F12" ref-type="fig">Figure 12a</xref>, we suspect a correlation between dark skin and dark background. Such correlations can be treated as causalities by ML models [<xref rid="R68" ref-type="bibr">68</xref>] and need to be avoided. We confirm this suspicion by arranging all items using <monospace>dim_9</monospace> (skin tone) as the <italic toggle="yes">x</italic>-axis and <monospace>dim_16</monospace> (background darkness) as the <italic toggle="yes">y</italic>-axis. The resulting distribution (<xref rid="F12" ref-type="fig">Figure 12b</xref>) dispels our suspicion. Even though the distribution is not uniform, the dataset contains both items that have fair skin and dark background (b1) and items that have dark skin and light background (b2).</p>
      </sec>
    </sec>
    <sec id="S32">
      <label>8.3</label>
      <title>Genomic Interaction Matrix</title>
      <sec id="S33">
        <title>Data, Model, and Analysis Overview.</title>
        <p id="P55">This usage scenario uses a genome interaction matrix for the HFFc6 cell line published by Rao <italic toggle="yes">et al</italic>. [<xref rid="R48" ref-type="bibr">48</xref>]. The matrices describe the chromatin interactions between different genomic locations, which is related to the physical folding of DNA that affects the regulation of gene expression. In a genome interaction matrix, rows and columns represent genomic locations, and the color intensity indicates the interaction probability between a pair of locations. Experts typically examine regions of interest (ROI) that have unique visual patterns and indicate specific biological events. Since the size of the matrix is huge, <italic toggle="yes">i.e</italic>., 3 billion √ó 3 billion for human genomes, this analysis process is often laborious and time-consuming.</p>
        <p id="P56">We generate small multiples for one specific type of ROI called Topologically Associated Domains (TAD), which are visually represented as squares that are presumably organized hierarchically. We first extract TADs from the interaction matrix using OnTAD [<xref rid="R3" ref-type="bibr">3</xref>] and then use the DRL model to generate a latent vector for each TAD. We demonstrate Drava using the 855 TADs extracted from chromosome 5 of the HFFc6 cell line. The DRL model has three convolution blocks with filter sizes of 7, 5, 3 and channel sizes of 32, 64, 128, respectively. The latent vector has 8 dimensions.</p>
        <p id="P57">In this scenario, we investigate different types of TADs by identifying, validating, and refining concepts that correspond to important visual patterns of TADs. Guided by these concepts, we are able to locate different types of TADs and examine the spatial distribution of these TADs on the whole genome.</p>
      </sec>
      <sec id="S34">
        <title>Understand Data through Concepts.</title>
        <p id="P58">The visual appearance of TADs in a heatmap can serve as effective proxies of the underlying data patterns and biological events [<xref rid="R3" ref-type="bibr">3</xref>, <xref rid="R30" ref-type="bibr">30</xref>]. Therefore, by interpreting the visual concepts, we can inspect how the underlying data and the associated biological events vary among the analyzed items. In the <italic toggle="yes">Concept View</italic>, we identified three dimensions of interest. <monospace>Dim_7</monospace> (renamed as <monospace>dim_thick</monospace>) indicates the thickness of the diagonal (<italic toggle="yes">e.g</italic>., an item changing from <xref rid="F13" ref-type="fig">Figure 13a1</xref> to <xref rid="F13" ref-type="fig">a4</xref>), which is related to the resolution of the TAD on the matrix since we resize all TADs into a fixed pixel size for the DRL model. <monospace>Dim_0</monospace> indicates the asymmetry of the nested TAD structure (<italic toggle="yes">e.g</italic>., an item changing from <xref rid="F13" ref-type="fig">Figure 13</xref>a2 to a3). <monospace>Dim_6</monospace> (renamed as <monospace>dim_nest</monospace>) corresponds to whether a TAD data item contains additional nested squares (<italic toggle="yes">i.e</italic>., nested TAD such as a2, a3) or not (<italic toggle="yes">i.e</italic>., single TAD such as a1, a4). Other dimensions are either hard to interpret because there is little variation in the synthesized images or can not be associated with meaningful domain insights. <monospace>Dim_thick</monospace> and <monospace>dim_nest</monospace> are the top two dimensions based on the salience scores, indicating the usefulness of the dimension ranking. The three dimensions (<monospace>dim_thick</monospace>, <monospace>dim_0</monospace>, <monospace>dim_6</monospace>) correspond to important attributes of TADs, as described by An <italic toggle="yes">et al</italic>. [<xref rid="R3" ref-type="bibr">3</xref>].</p>
      </sec>
      <sec id="S35">
        <title>Verify and Refine Concepts.</title>
        <p id="P59">After obtaining a basic understanding of the semantic meaning of each dimension through their synthesized images, we further verify the three concepts one by one through grouping and browsing data items. Interestingly, we find that <monospace>dim_nest</monospace> confuses the thickness of the diagonal with the nested structure of TADs. As shown in <xref rid="F13" ref-type="fig">Figure 13b</xref>, items are grouped based on <monospace>dim_nest</monospace> and use ‚Äúpartial‚Äù to generate item previews. Users can identify items with thick diagonals from the item preview (as annotated by the orange marks) and examine them in detail by hovering over them. This issue can hardly be revealed through the synthesized images (<xref rid="F13" ref-type="fig">Figure 13c1</xref>), which are widely used as the only method to interpret semantic meanings in previous literature [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R59" ref-type="bibr">59</xref>]. This observation shows the importance of further verifying a concept base on data items and the need for user refinement.</p>
        <p id="P60">Since <monospace>dim_thick</monospace> can indicate the TAD size, we use it as the <italic toggle="yes">y</italic> axis to help refine the concept associated with <monospace>dim_nest</monospace>. As shown in <xref rid="F13" ref-type="fig">Figure 13c</xref>, items arranged in different vertical positions based on their diagonal thickness, enabling successful separation of nested TAD (<italic toggle="yes">e.g</italic>., a2, a3) from single TADs with thick diagonal (<italic toggle="yes">e.g</italic>., a4). Users can refine <monospace>dim_nest</monospace> by a lasso selection on all single TADs that have large <monospace>dim_nest</monospace> values and moving them to the left-most position (<italic toggle="yes">e.g</italic>., assigning them a small value for <monospace>dim_nest</monospace>), as shown in <xref rid="F13" ref-type="fig">Figure 13c3</xref>. The refinement is recorded using the local updating mechanism and applied to similar items.</p>
      </sec>
      <sec id="S36">
        <title>Locate items of interest.</title>
        <p id="P61">After the refinement, users can easily locate nested TADs in <xref rid="F13" ref-type="fig">Figure 13C4</xref> through a lasso selection. They can also filter these TADs based on <monospace>dim_thick</monospace> and <monospace>dim_nest</monospace> using their histograms. The nested structure in TADs is important to understand the boundary usage in gene regulation [<xref rid="R3" ref-type="bibr">3</xref>]. For this purpose, these identified items can be further examined in the <italic toggle="yes">Spatial View</italic> (<xref rid="F4" ref-type="fig">Figure 4</xref>), which reveals the genomic locations of these TADs and associated them with other context information (<italic toggle="yes">e.g</italic>., chromatin accessibility).</p>
      </sec>
    </sec>
    <sec id="S37">
      <label>8.4</label>
      <title>Breast Cancer Specimen</title>
      <sec id="S38">
        <title>Data, Model, and Analysis Overview.</title>
        <p id="P62">This usage scenario uses breast histopathology images downloaded from [<xref rid="R43" ref-type="bibr">43</xref>]. This dataset contains 277,524 patches (50 √ó 50 pixels) extracted from stained whole mount slide images of breast cancer specimens from 162 patients scanned at 40x magnification. The DRL model is trained on the whole dataset. In this usage scenario, we explore the 1,745 image patches from one patient. The DRL model has five convolution blocks, each with a kernel of size 3 and 32, 64, 128, 256, and 512 channels, respectively. The latent vector has 12 dimensions. In this scenario, we examine the presence of cancer cells in these items and analyze the performance of a classification model. Specifically, we identify concepts and associate them with domain semantics. We then use these concepts to describe the characteristics of hard-to-classify items.</p>
      </sec>
      <sec id="S39">
        <title>Interpret Visual Concepts and Assign Domain Semantics.</title>
        <p id="P63">We first visualize all the items using UMAP (<xref rid="F1" ref-type="fig">Figure 1a</xref>). However, the UMAP projection is not ideal since it is based on the overall similarities and considers some irrelevant information, such as the position of tissue patches and the orientation of tissue patches.</p>
        <p id="P64">Therefore, we check the <italic toggle="yes">Concept View</italic> to find dimensions that can indicate concepts with domain semantics. Based on the synthesized images, we speculate that <monospace>dim_5</monospace> is related to the density of tissues and <monospace>dim_2</monospace> is related to the color of the stained tissues. Our interpretation of these two dimensions is further confirmed by examining the grouped items in the <italic toggle="yes">Item Browser</italic>. As shown in <xref rid="F1" ref-type="fig">Figure 1b</xref>, when all items are arranged based on dim_5, items on the left side have almost no white space, indicating a high tissue density, while items on the right side have more white spaces, indicating loose tissues or fatty tissues. When all items are arranged based on <monospace>dim_2</monospace>, items on the left side have a more purple hue while items on the right side have a more pink hue. We then rename <monospace>dim_5</monospace> as <monospace>dim_density</monospace> and <monospace>dim_2</monospace> as <monospace>dim_color</monospace>.</p>
        <p id="P65">We arrange all items using <monospace>dim_density</monospace> as the <italic toggle="yes">x</italic> axis and <monospace>dim_color</monospace> as the <italic toggle="yes">y</italic> axis and then add a label for each item from the item metadata to indicate whether this item contains Invasive Ductal Carcinoma (IDC), a subtype of breast cancer cells (<italic toggle="yes">i.e</italic>., orange and blue item labels in <xref rid="F14" ref-type="fig">Figure 14a</xref>). As shown in <xref rid="F14" ref-type="fig">Figure 14a</xref>, there is a strong correlation between the presence of IDC and the two visual concepts mapped on the <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> axes. We group items (<xref rid="F1" ref-type="fig">Figure 1c</xref>) to reduce the visual clutter. Items with purple and dense tissues (<xref rid="F14" ref-type="fig">Figure 14a1</xref>) are more likely to contain IDC (<italic toggle="yes">i.e</italic>., orange labels) while items that are closer to pink (a2) and contain less dense tissue (a3) are less likely to contain IDC (<italic toggle="yes">i.e</italic>., blue labels). This association is further confirmed by a pathologist. Even though the identification of cancer cells needs to consider a variety of factors, the color and the tissue density are strong indicators of the presence of cancer cells. Cancer cells are typically dense, which leads to less white space, and have larger and darker nuclei than normal cells, which leads to more purple color.</p>
      </sec>
      <sec id="S40">
        <title>Identify Hard Examples for IDC Identification.</title>
        <p id="P66">Identifying regions in the whole mount slide image (<italic toggle="yes">i.e</italic>., items in our analysis) with IDC is an important task for pathologists to assign an aggressiveness grade to cancer. Since <monospace>dim_dense</monospace> and <monospace>dim_color</monospace> are related to the identification of cancer cells, we further analyzed how they influence on the prediction of IDC in an ML model. We train an IDC classification model by fine-tuning a ResNet34 model, as described in [<xref rid="R52" ref-type="bibr">52</xref>], and record the model prediction and confidence score for each item.</p>
        <p id="P67">Confident wrong predictions and false negatives are more consequential in real-world deployment [<xref rid="R9" ref-type="bibr">9</xref>], as patients may fail to receive the treatment they need. Therefore, we are especially interested in false-negative prediction with high confidence scores. We import item metadata to the concept view and filter items accordingly, <italic toggle="yes">i.e</italic>., ground truth = positive, prediction = negative, confidence score &gt; 0.8, as shown in <xref rid="F14" ref-type="fig">Figure 14b</xref>. According to the <italic toggle="yes">Item Browser</italic> (<xref rid="F14" ref-type="fig">Figure 14c</xref>), the filtered items are close to each other in the <italic toggle="yes">Item Browser</italic>, containing tissues that are not very dense and have a more purple hue. Since items with cancer cells usually contain dense tissues, this may explain why the classification model makes very confident but wrong predictions. We further examine the original spatial positions of these items in the <italic toggle="yes">Spatial View</italic> (see <xref rid="F14" ref-type="fig">Figure 14d</xref>), where other items are faded out with a semi-transparent white mask. We find the items of interest (<italic toggle="yes">i.e</italic>., non-masked items) are from regions where fatty tissues are surrounded by cancer cells, as shown by the orange boxes. This can explain why these items have many white spaces and only contain a small number of cancer cells. This observation is valuable for understanding and improving this IDC diagnosis model. First, it indicates when and where the IDC prediction model tends to make confident false negative predictions and a double-check from human experts is needed. Second, the training strategy can be modified accordingly (<italic toggle="yes">e.g</italic>., increasing the sample weight of these loose and purple tissues) to improve the model performance.</p>
      </sec>
    </sec>
    <sec id="S41">
      <label>8.5</label>
      <title>User Feedback</title>
      <p id="P68">We collected qualitative user feedback about Drava from the collaborated domain users.</p>
      <p id="P69">Participants commented that Drava provided <italic toggle="yes">‚Äúan attractive addition‚Äù</italic> (P5) to the current analysis methods. They liked the comprehensive user interaction provided by Drava. P4 commented that <italic toggle="yes">‚Äúthe item preview is engaging and useful‚Äù</italic>. Participants (P1, P4, P5) commented that it is not always easy to interpret a semantic dimension using one set of synthesized images. Therefore, the functionalities to generate synthesized images for a given baseline and to summarize item groups for a certain dimension are helpful. All participants agreed that Drava provided helpful guidance in interpreting and refining the ML semantic dimensions.</p>
      <p id="P70">The participants also provided valuable suggestions for further improvements. While some dimensions were reported as <italic toggle="yes">‚Äúeasy to associate with human concepts‚Äù</italic>, participants also complained that some dimensions had unclear semantics and were hard to interpret. This issue might be caused by the entangled concepts (<xref rid="S2" ref-type="sec">section 2</xref>) or the quality of the synthesized images. Instead of manually changing baseline images for the synthesized images (<xref rid="F4" ref-type="fig">Figure 4B</xref>), P3 suggested that Drava should recommend several baseline images to facilitate the interpretation of semantic dimensions. P1 and P2 were concerned about the extent to which their refinements will influence the back-end model. P1 stated that refining item groups without updating the back-end model (<italic toggle="yes">i.e</italic>., a local update) made him <italic toggle="yes">‚Äúfeel safer and in control‚Äù</italic>. Such concerns about automation are consistent with the observations in previous studies [<xref rid="R64" ref-type="bibr">64</xref>]. On the other hand, P1 also agreed that the local update can be inefficient and that updating the back-end model is necessary when analyzing a large number of items. P1 and P2 both provided suggestions for improving the global update mechanism, such as annotating how items change after updating the concept adaptor.</p>
    </sec>
  </sec>
  <sec id="S42">
    <label>9</label>
    <title>DISCUSSION</title>
    <sec id="S43">
      <label>9.1</label>
      <title>The Scope of Drava</title>
      <sec id="S44">
        <title>Dependence on DRL Performance and Data Quality.</title>
        <p id="P71">The concept-driven exploration provided by Drava is based on interpreting, refining, and utilizing semantic dimensions. Therefore, Drava‚Äôs capability depends on what semantic dimensions a DRL model can learn, which highly relies on the DRL model performance and the data quality [<xref rid="R28" ref-type="bibr">28</xref>]. Drava may fail to capture the desired concepts in the semantic dimensions due to the limited capabilities of the model or the low quality of the dataset. We believe that advances in DRL will further empower Drava and provide more opportunities for concept-driven data exploration. Additionally, the concept adaptor in Drava enables users to improve an unsatisfied concept through user refinement. In the worst-case scenario where the desired concepts can not be learned by the DRL model, Drava can serve as a pure interactive active learning tool that learns a concept merely based on user labeling.</p>
      </sec>
      <sec id="S45">
        <title>Visual Complexity of Concepts.</title>
        <p id="P72">Apart from DRL performance and data quality, whether a concept can be identified in Drava is also related to its visual complexity. Here, a visually complex concept indicates an abstract or subjective concept that has diverse visual representations, which makes it hard to visually summarize and interpret the concept via either the synthesized images or interactive piles. For example, in the <italic toggle="yes">CelebA</italic> dataset, some concepts are simple and have clear visual representations (<italic toggle="yes">e.g</italic>., black objects near eyes for a ‚Äúsunglass‚Äù concept), but other concepts are rather complex and involve varying visual presentations (<italic toggle="yes">e.g</italic>., ‚Äúattractive‚Äù can be related to either short or long hair, oval or round face shapes), making it hard to be visually summarized.</p>
      </sec>
      <sec id="S46">
        <title>Format and Characteristics of Data Items.</title>
        <p id="P73">To achieve the concept-driven exploration in Drava, data items need to fulfill two requirements. First, the data items must be visually perceivable for humans. Image datasets naturally fulfill this requirement. For other types of datasets (<italic toggle="yes">e.g</italic>., sequences, matrices), a workaround is to visualize the dataset and use the visualization (or segments of the visualization) as data items. For example, in <xref rid="S32" ref-type="sec">subsection 8.3</xref>, we convert a genome interaction matrix dataset into a heatmap visualization and treat each ROI in the heatmap as a data item. Second, these data items need to have similar appearances and share the same concepts, as shown in <xref rid="S22" ref-type="sec">section 8</xref>. Data items with dramatically different appearances not only make it challenging for the ML model to learn and extract concepts but also results in high cognitive loads for users to identify and validate concepts. For example, Drava can not be applied to the ILSVRC dataset [<xref rid="R50" ref-type="bibr">50</xref>], which contains diverse images depicting 1,000 different object categories.</p>
      </sec>
    </sec>
    <sec id="S47">
      <label>9.2</label>
      <title>Human Factors in Drava</title>
      <p id="P74">Human factors play an important role in human-in-the-loop AI tools [<xref rid="R2" ref-type="bibr">2</xref>, <xref rid="R10" ref-type="bibr">10</xref>, <xref rid="R65" ref-type="bibr">65</xref>]. Here, we discuss two important human factors in Drava, <italic toggle="yes">i.e</italic>., cognitive biases and cognitive load, including their impacts, our design considerations for mitigating the impacts, and the limitations of the current design.</p>
      <sec id="S48">
        <title>Cognitive Bias.</title>
        <p id="P75">ML models do not know what a human concept is. It is the users who associate the concepts of humans with the semantic dimensions of ML. As a result, the interpretation and refinement of the semantic dimensions can be influenced by users‚Äô cognitive biases (<italic toggle="yes">e.g</italic>., confirmation bias, anchoring bias, and availability bias). To facilitate the user interpretation, Drava supports concept validation through various interactions (<italic toggle="yes">e.g</italic>., changing the baseline image, arranging and piling items) rather than merely relying on the observation of a set of synthesized images. We also plan to support hypothesis generation and testing [<xref rid="R60" ref-type="bibr">60</xref>] to further reduce misinterpretation. However, Drava does not have mechanisms that are specifically designed for minimizing cognitive biases. Future studies are needed to systematically investigate the causes of and the solutions for cognitive bias in human‚ÄìAI collaboration.</p>
      </sec>
      <sec id="S49">
        <title>Cognitive Load.</title>
        <p id="P76">While more latent dimensions will potentially enable the model to capture more meaningful concepts, it will also increase the cognitive load of users. Drava alleviates this issue by enabling users to rank dimensions based on their salience scores and remove less relevant dimensions. We have successfully tested Drava in application scenarios with at most 32 latent dimensions. A large number of dimensions (<italic toggle="yes">e.g</italic>., 100) can challenge the cognitive capacity of users and undermine the usability of Drava. Like other hyperparameters in ML, the number of latent dimensions needs to be carefully selected to strike a balance between the representative of the latent dimensions and the cognitive load of the users. Promising directions for reducing the cognitive load include progressively revealing the information [<xref rid="R62" ref-type="bibr">62</xref>] and tracking provenance data [<xref rid="R14" ref-type="bibr">14</xref>].</p>
      </sec>
    </sec>
    <sec id="S50">
      <label>9.3</label>
      <title>Relation to Dimension Reduction Methods.</title>
      <p id="P77">The application scenarios present examples where the item arrangement based on a dimension reduction method (<italic toggle="yes">i.e</italic>., UMAP) fails to fill the analysis needs. Particularly, Drava enables visual exploration and analysis that focuses on the similarity of certain concepts rather than overall similarity. Drava complements the widely used dimension-reduction-based visual exploration tools. Drava is most suitable for analysis scenarios in which data items are similar (<italic toggle="yes">i.e</italic>., share multiple concepts) and the analysis concentrates on specific concepts. Dimension reduction projection (<italic toggle="yes">e.g</italic>., t-SNE, UMAP) is still an effective method for visualizing latent vectors, especially when the items form distinct clusters, and when the analysis focuses on overall similarity among items.</p>
    </sec>
    <sec id="S51">
      <label>9.4</label>
      <title>Scalability of Rendering and Interaction.</title>
      <p id="P78">The rendering scalability of Drava is mainly limited by its rendering engine in the <italic toggle="yes">Item Browser</italic>, which is built upon Piling.js [<xref rid="R33" ref-type="bibr">33</xref>]. The <italic toggle="yes">Item Browser</italic> can handle the rendering of and the interaction with 2,000 items with reasonable performance: the <italic toggle="yes">Item Browser</italic> can be initialized in less than 15 seconds and perform the interaction animation in no less than 50 frames per second on a laptop (MacBook Pro, 2020). Data loading is only performed when users open the tool for the first time. Loading depends on the bandwidth of the internet connection and the size of the dataset. It typically takes less than 30 seconds for the four datasets described in the application scenarios. Drava currently does not provide direct support for visualizing and interacting with more than several thousand items. In the future, we plan to further improve its scalability via item sampling and dynamically adjusting the level of detail.</p>
    </sec>
    <sec id="S52">
      <label>9.5</label>
      <title>Limitations of Evaluation</title>
      <p id="P79">We evaluated Drava on four application scenarios with five domain users. The evaluation demonstrated Drava‚Äôs capability on different types of datasets, domains, and analysis scenarios. At the same time, we admit the limitations resulting from the selection of participants and the setting of the evaluation. In particular, we only selected five participants in a non-random manner. The evaluation was based on self-reported feedback and included limited independent user exploration. While the evaluation revealed valuable insights and feedback, the generalizability of the results should be treated with caution. In the future, we plan to conduct a user study with a larger group of participants. Apart from assessing the usability of Drava, this user study will help validate the proposed workflow (<xref rid="S4" ref-type="sec">section 4</xref>) and understand user behaviors in human‚ÄìAI collaboration.</p>
    </sec>
  </sec>
  <sec id="S53">
    <label>10</label>
    <title>CONCLUSION</title>
    <p id="P80">This paper introduces Drava, a visual analytics system that employs DRL to support the concept-driven exploration of small multiples. Focusing on the ambiguity and imperfection of DRL semantic dimensions, Drava proposes a set of interactive visualizations and algorithms to help users better interpret DRL semantic dimensions, align them with human concepts, and utilize them for visual exploration. The application of Drava for data exploration complements the widely used dimension-reduction-based visual exploration tools, especially for situations where 1) the analyzed items are similar and share multiple visual concepts and 2) the analysis focuses on certain visual concepts rather than the overall similarity. Our application scenarios demonstrate the usefulness of Drava on various datasets and for different analysis purposes. Finally, Drava demonstrates the possibilities of employing XAI techniques to help users better understand data and support visual data exploration across a wide range of domains.</p>
  </sec>
  <sec sec-type="supplementary-material" id="SM1">
    <title>Supplementary Material</title>
    <supplementary-material id="SD1" position="float" content-type="local-data">
      <label>Supplementary Video</label>
      <media xlink:href="NIHMS1947573-supplement-Supplementary_Video.mp4" id="d64e2067" position="anchor" mimetype="video" mime-subtype="mp4"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="S54">
    <title>ACKNOWLEDGMENTS</title>
    <p id="P81">This work was supported by the National Institutes of Health (OT2OD026677, U24CA237617, UM1HG011536, R33CA263666). Q.W. is supported, in part, by Harvard Data Science Initiative Postdoctoral Research Fund.</p>
  </ack>
  <ref-list>
    <title>REFERENCES</title>
    <ref id="R1">
      <label>[1]</label>
      <mixed-citation publication-type="journal"><name><surname>Abdi</surname><given-names>Herv√©</given-names></name> and <name><surname>Williams</surname><given-names>Lynne J</given-names></name>. <year>2010</year>. <article-title>Principal component analysis</article-title>. <source>Wiley interdisciplinary reviews: computational statistics</source>
<volume>2</volume>, <issue>4</issue> (<comment>2010</comment>), <fpage>433</fpage>‚Äì<lpage>459</lpage>.</mixed-citation>
    </ref>
    <ref id="R2">
      <label>[2]</label>
      <mixed-citation publication-type="journal"><name><surname>Amershi</surname><given-names>Saleema</given-names></name>, <name><surname>Cakmak</surname><given-names>Maya</given-names></name>, <name><surname>Knox</surname><given-names>William Bradley</given-names></name>, and <name><surname>Kulesza</surname><given-names>Todd</given-names></name>. <year>2014</year>. <article-title>Power to the people: The role of humans in interactive machine learning</article-title>. <source>AI Magazine</source><volume>35</volume>, <issue>4</issue> (<comment>2014</comment>), <fpage>105</fpage>‚Äì<lpage>120</lpage>.</mixed-citation>
    </ref>
    <ref id="R3">
      <label>[3]</label>
      <mixed-citation publication-type="journal"><name><surname>An</surname><given-names>Lin</given-names></name>, <name><surname>Yang</surname><given-names>Tao</given-names></name>, <name><surname>Yang</surname><given-names>Jiahao</given-names></name>, <name><surname>Nuebler</surname><given-names>Johannes</given-names></name>, <name><surname>Xiang</surname><given-names>Guanjue</given-names></name>, <name><surname>Ross C Hardison</surname><given-names>Qunhua Li</given-names></name>, and <name><surname>Zhang</surname><given-names>Yu</given-names></name>. <year>2019</year>. <article-title>OnTAD: hierarchical domain structure reveals the divergence of activity among TADs and boundaries</article-title>. <source>Genome Biology</source><volume>20</volume>, <issue>1</issue> (<comment>2019</comment>), <fpage>1</fpage>‚Äì<lpage>16</lpage>.<pub-id pub-id-type="pmid">30606230</pub-id>
</mixed-citation>
    </ref>
    <ref id="R4">
      <label>[4]</label>
      <mixed-citation publication-type="book"><name><surname>Bach</surname><given-names>Benjamin</given-names></name>, <name><surname>Nathalie Henry-Riche</surname><given-names>Tim Dwyer</given-names></name>, <name><surname>Madhyastha</surname><given-names>Tara</given-names></name>, <name><surname>Fekete</surname><given-names>J-D</given-names></name>, and <name><surname>Grabowski</surname><given-names>Thomas</given-names></name>. <year>2015</year>. <part-title>Small MultiPiles: Piling time to explore temporal patterns in dynamic networks</part-title>. In <source>Computer Graphics Forum</source>, Vol. <volume>34</volume>. <publisher-name>Wiley Online Library</publisher-name>, <publisher-loc>NJ, USA</publisher-loc>, <fpage>31</fpage>‚Äì<lpage>40</lpage>.</mixed-citation>
    </ref>
    <ref id="R5">
      <label>[5]</label>
      <mixed-citation publication-type="book"><name><surname>Behrisch</surname><given-names>Michael</given-names></name>, <name><surname>Korkmaz</surname><given-names>Fatih</given-names></name>, <name><surname>Shao</surname><given-names>Lin</given-names></name>, and <name><surname>Schreck</surname><given-names>Tobias</given-names></name>. <year>2014</year>. <part-title>Feedback-driven interactive exploration of large multidimensional data supported by visual classifier</part-title>. In <source>2014 IEEE Conference on Visual Analytics Science and Technology (VAST)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>43</fpage>‚Äì<lpage>52</lpage>.</mixed-citation>
    </ref>
    <ref id="R6">
      <label>[6]</label>
      <mixed-citation publication-type="book"><name><surname>Behrisch</surname><given-names>Michael</given-names></name>, <name><surname>Krueger</surname><given-names>Robert</given-names></name>, <name><surname>Lekschas</surname><given-names>Fritz</given-names></name>, <name><surname>Schreck</surname><given-names>Tobias</given-names></name>, <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>, and <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>. <year>2018</year>. <part-title>Visual Pattern-Driven Exploration of Big Data</part-title>. In <source>International Symposium on Big Data Visual and Immersive Analytics (BDVA 18)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>1</fpage>‚Äì<lpage>11</lpage>.</mixed-citation>
    </ref>
    <ref id="R7">
      <label>[7]</label>
      <mixed-citation publication-type="book"><name><surname>Boggust</surname><given-names>Angie</given-names></name>, <name><surname>Carter</surname><given-names>Brandon</given-names></name>, and <name><surname>Satyanarayan</surname><given-names>Arvind</given-names></name>. <year>2022</year>. <part-title>Embedding comparator: Visualizing differences in global structure and local neighborhoods via small multiples</part-title>. In <source>27th International Conference on Intelligent User Interfaces</source>. <publisher-name>ACM</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>746</fpage>‚Äì<lpage>766</lpage>.</mixed-citation>
    </ref>
    <ref id="R8">
      <label>[8]</label>
      <mixed-citation publication-type="book"><name><surname>Christopher P Burgess</surname><given-names>Irina Higgins</given-names></name>, <name><surname>Pal</surname><given-names>Arka</given-names></name>, <name><surname>Matthey</surname><given-names>Loic</given-names></name>, <name><surname>Watters</surname><given-names>Nick</given-names></name>, <name><surname>Desjardins</surname><given-names>Guillaume</given-names></name>, and <name><surname>Lerchner</surname><given-names>Alexander</given-names></name>. <year>2017</year>. <part-title>Understanding disentangling in <italic toggle="yes">beta</italic>-VAE</part-title>. In <source>International Conference on Machine Learning (ICLR)</source>. <publisher-name>International Machine Learning Society</publisher-name>, <publisher-loc>Sydney, Australia</publisher-loc>, <fpage>10</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R9">
      <label>[9]</label>
      <mixed-citation publication-type="journal"><name><surname>Burt</surname><given-names>Tal</given-names></name>, <name><surname>Button</surname><given-names>KS</given-names></name>, <name><surname>Thom</surname><given-names>HHZ</given-names></name>, <name><surname>Noveck</surname><given-names>RJ</given-names></name>, and <name><surname>Munaf√≤</surname><given-names>Marcus R</given-names></name>. <year>2017</year>. <article-title>The Burden of the ‚ÄúFalse-Negatives‚Äù in Clinical Development: Analyses of Current and Alternative Scenarios and Corrective Measures</article-title>. <source>Clinical and Translational Science</source><volume>10</volume>, <issue>6</issue> (<comment>2017</comment>), <fpage>470</fpage>‚Äì<lpage>479</lpage>.<pub-id pub-id-type="pmid">28675646</pub-id>
</mixed-citation>
    </ref>
    <ref id="R10">
      <label>[10]</label>
      <mixed-citation publication-type="book"><name><surname>Carrie J Cai</surname><given-names>Emily Reif</given-names></name>, <name><surname>Hegde</surname><given-names>Narayan</given-names></name>, <name><surname>Hipp</surname><given-names>Jason</given-names></name>, <name><surname>Kim</surname><given-names>Been</given-names></name>, <name><surname>Smilkov</surname><given-names>Daniel</given-names></name>, <name><surname>Wattenberg</surname><given-names>Martin</given-names></name>, <name><surname>Viegas</surname><given-names>Fernanda</given-names></name>, <name><surname>Corrado</surname><given-names>Greg S</given-names></name>, and <name><surname>Stumpe</surname><given-names>Martin C</given-names></name>. <year>2019</year>. <part-title>Human-centered tools for coping with imperfect algorithms during medical decision-making</part-title>. In <source>Proceedings of the 2019 CHI conference on Human Factors in Computing Systems</source>. <publisher-name>ACM</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>1</fpage>‚Äì<lpage>14</lpage>.</mixed-citation>
    </ref>
    <ref id="R11">
      <label>[11]</label>
      <mixed-citation publication-type="journal"><name><surname>Ricky TQ Chen</surname><given-names>Xuechen Li</given-names></name>, <name><surname>Grosse</surname><given-names>Roger B</given-names></name>, and <name><surname>Duvenaud</surname><given-names>David K</given-names></name>. <year>2018</year>. <article-title>Isolating sources of disentanglement in variational autoencoders</article-title>. <source>Advances in Neural Information Processing Systems</source><volume>31</volume> (<comment>2018</comment>), <fpage>11</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R12">
      <label>[12]</label>
      <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>Xi</given-names></name>, <name><surname>Duan</surname><given-names>Yan</given-names></name>, <name><surname>Houthooft</surname><given-names>Rein</given-names></name>, <name><surname>Schulman</surname><given-names>John</given-names></name>, <name><surname>Sutskever</surname><given-names>Ilya</given-names></name>, and <name><surname>Abbeel</surname><given-names>Pieter</given-names></name>. <year>2016</year>. <article-title>Infogan: Interpretable representation learning by information maximizing generative adversarial nets</article-title>. <source>Advances in Neural Information Processing Systems</source><volume>29</volume> (<comment>2016</comment>), <fpage>10</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R13">
      <label>[13]</label>
      <mixed-citation publication-type="journal"><name><surname>Cheng</surname><given-names>Furui</given-names></name>, <name><surname>Mark S Keller</surname><given-names>Huamin Qu</given-names></name>, <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>, and <name><surname>Wang</surname><given-names>Qianwen</given-names></name>. <year>2023</year>. <article-title>Polyphony: an Interactive Transfer Learning Framework for Single-Cell Data Analysis</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>29</volume>, <issue>1</issue> (<comment>2023</comment>), <fpage>591</fpage>‚Äì<lpage>601</lpage>.<pub-id pub-id-type="pmid">36155452</pub-id>
</mixed-citation>
    </ref>
    <ref id="R14">
      <label>[14]</label>
      <mixed-citation publication-type="book"><name><surname>Cutler</surname><given-names>Zach</given-names></name>, <name><surname>Gadhave</surname><given-names>Kiran</given-names></name>, and <name><surname>Lex</surname><given-names>Alexander</given-names></name>. <year>2020</year>. <part-title>Trrack: A library for provenance-tracking in web-based visualizations</part-title>. In <source>2020 IEEE Visualization Conference (VIS)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>116</fpage>‚Äì<lpage>120</lpage>.</mixed-citation>
    </ref>
    <ref id="R15">
      <label>[15]</label>
      <mixed-citation publication-type="book"><name><surname>Frederik L Dennig</surname><given-names>Tom Polk</given-names></name>, <name><surname>Lin</surname><given-names>Zudi</given-names></name>, <name><surname>Schreck</surname><given-names>Tobias</given-names></name>, <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>, and <name><surname>Behrisch</surname><given-names>Michael</given-names></name>. <year>2019</year>. <part-title>FDive: Learning relevance models using pattern-based similarity measures</part-title>. In <source>2019 IEEE Conference on Visual Analytics Science and Technology (VAST)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>69</fpage>‚Äì<lpage>80</lpage>.</mixed-citation>
    </ref>
    <ref id="R16">
      <label>[16]</label>
      <mixed-citation publication-type="book"><name><surname>Eckelt</surname><given-names>K</given-names></name>, <name><surname>Hinterreiter</surname><given-names>A</given-names></name>, <name><surname>Adelberger</surname><given-names>P</given-names></name>, <name><surname>Walchshofer</surname><given-names>C</given-names></name>, <name><surname>Dhanoa</surname><given-names>V</given-names></name>, <name><surname>Humer</surname><given-names>C</given-names></name>, <name><surname>Heckmann</surname><given-names>M</given-names></name>, <name><surname>Steinparz</surname><given-names>C</given-names></name>, and <name><surname>Streit</surname><given-names>M</given-names></name>. <year>2022</year>. <part-title>Visual Exploration of Relationships and Structure in Low-Dimensional Embeddings</part-title>.. In <source>OSF Preprint</source>, Vol. <pub-id pub-id-type="doi">10.31219/osf.io/ujbrs</pub-id>. <publisher-name>Open Society Foundation</publisher-name>, <publisher-loc>SA</publisher-loc>, <fpage>15</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R17">
      <label>[17]</label>
      <mixed-citation publication-type="webpage"><collab>Facebook</collab>. <year>2014</year>. <source>React.js</source>. <comment><ext-link xlink:href="https://github.com/facebook/react" ext-link-type="uri">https://github.com/facebook/react</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R18">
      <label>[18]</label>
      <mixed-citation publication-type="journal"><name><surname>Gou</surname><given-names>Liang</given-names></name>, <name><surname>Zou</surname><given-names>Lincan</given-names></name>, <name><surname>Li</surname><given-names>Nanxiang</given-names></name>, <name><surname>Hofmann</surname><given-names>Michael</given-names></name>, <name><surname>Arvind Kumar Shekar</surname><given-names>Axel Wendt</given-names></name>, and <name><surname>Ren</surname><given-names>Liu</given-names></name>. <year>2020</year>. <article-title>VATLD: a visual analytics system to assess, understand and improve traffic light detection</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>27</volume>, <issue>2</issue> (<comment>2020</comment>), <fpage>261</fpage>‚Äì<lpage>271</lpage>.</mixed-citation>
    </ref>
    <ref id="R19">
      <label>[19]</label>
      <mixed-citation publication-type="book"><name><surname>Grinberg</surname><given-names>Miguel</given-names></name>. <year>2018</year>. <source>Flask web development: developing web applications with python</source>. <publisher-name>O‚ÄôReilly Media, Inc</publisher-name>., <publisher-loc>USA</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="R20">
      <label>[20]</label>
      <mixed-citation publication-type="journal"><name><surname>He</surname><given-names>Wenbin</given-names></name>, <name><surname>Zou</surname><given-names>Lincan</given-names></name>, <name><surname>Arvind Kumar Shekar</surname><given-names>Liang Gou</given-names></name>, and <name><surname>Ren</surname><given-names>Liu</given-names></name>. <year>2021</year>. <article-title>Where Can We Help? A Visual Analytics Approach to Diagnosing and Improving Semantic Segmentation of Movable Objects</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>1</issue> (<comment>2021</comment>), <fpage>1040</fpage>‚Äì<lpage>1050</lpage>.<pub-id pub-id-type="pmid">34587077</pub-id>
</mixed-citation>
    </ref>
    <ref id="R21">
      <label>[21]</label>
      <mixed-citation publication-type="book"><name><surname>Higgins</surname><given-names>Irina</given-names></name>, <name><surname>Amos</surname><given-names>David</given-names></name>, <name><surname>Pfau</surname><given-names>David</given-names></name>, <name><surname>Racaniere</surname><given-names>Sebastien</given-names></name>, <name><surname>Matthey</surname><given-names>Loic</given-names></name>, <name><surname>Rezende</surname><given-names>Danilo</given-names></name>, and <name><surname>Lerchner</surname><given-names>Alexander</given-names></name>. <year>2018</year>. <part-title>Towards a definition of disentangled representations</part-title>. In <source>arXiv preprint</source>, <comment>Vol. arXiv:1812.02230.</comment><publisher-name>Cornell University</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>25</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R22">
      <label>[22]</label>
      <mixed-citation publication-type="book"><name><surname>Higgins</surname><given-names>Irina</given-names></name>, <name><surname>Matthey</surname><given-names>Loic</given-names></name>, <name><surname>Pal</surname><given-names>Arka</given-names></name>, <name><surname>Burgess</surname><given-names>Christopher</given-names></name>, <name><surname>Glorot</surname><given-names>Xavier</given-names></name>, <name><surname>Botvinick</surname><given-names>Matthew</given-names></name>, <name><surname>Mohamed</surname><given-names>Shakir</given-names></name>, and <name><surname>Lerchner</surname><given-names>Alexander</given-names></name>. <year>2016</year>. <part-title>beta-vae: Learning basic visual concepts with a constrained variational framework</part-title>. In <source>International Conference on Machine Learning (ICLR)</source>. <publisher-name>International Machine Learning Society</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>12</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R23">
      <label>[23]</label>
      <mixed-citation publication-type="journal"><name><surname>Jia</surname><given-names>Shichao</given-names></name>, <name><surname>Li</surname><given-names>Zeyu</given-names></name>, <name><surname>Chen</surname><given-names>Nuo</given-names></name>, and <name><surname>Zhang</surname><given-names>Jiawan</given-names></name>. <year>2021</year>. <article-title>Towards visual explainable active learning for zero-shot classification</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>1</issue> (<comment>2021</comment>), <fpage>791</fpage>‚Äì<lpage>801</lpage>.<pub-id pub-id-type="pmid">34587036</pub-id>
</mixed-citation>
    </ref>
    <ref id="R24">
      <label>[24]</label>
      <mixed-citation publication-type="book"><name><surname>Jin</surname><given-names>Zhihua</given-names></name>, <name><surname>Wang</surname><given-names>Yong</given-names></name>, <name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Ming</surname><given-names>Yao</given-names></name>, <name><surname>Ma</surname><given-names>Tengfei</given-names></name>, and <name><surname>Qu</surname><given-names>Huamin</given-names></name>. <year>2020</year>. <part-title>GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks</part-title>. In <source>arXiv preprint</source>, <comment>Vol. arXiv:2011.11048.</comment><publisher-name>Cornell University</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>17</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R25">
      <label>[25]</label>
      <mixed-citation publication-type="journal"><name><surname>Kahng</surname><given-names>Minsuk</given-names></name>, <name><surname>Pierre Y</surname><given-names> Andrews</given-names></name>, <name><surname>Aditya</surname><given-names>Kalro</given-names></name>, and <name><surname>Chau</surname><given-names>Duen Horng</given-names></name>. <year>2017</year>. <article-title>Activis: Visual exploration of industry-scale deep neural network models</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>24</volume>, <issue>1</issue> (<comment>2017</comment>), <fpage>88</fpage>‚Äì<lpage>97</lpage>.<pub-id pub-id-type="pmid">28866557</pub-id>
</mixed-citation>
    </ref>
    <ref id="R26">
      <label>[26]</label>
      <mixed-citation publication-type="book"><name><surname>Kim</surname><given-names>Been</given-names></name>, <name><surname>Wattenberg</surname><given-names>Martin</given-names></name>, <name><surname>Gilmer</surname><given-names>Justin</given-names></name>, <name><surname>Cai</surname><given-names>Carrie</given-names></name>, <name><surname>Wexler</surname><given-names>James</given-names></name>, <name><surname>Viegas</surname><given-names>Fernanda</given-names></name>, <etal/><year>2018</year>. <part-title>Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)</part-title>. In <source>International conference on machine learning</source>. <publisher-name>PMLR</publisher-name>, <publisher-loc>Stockholm, Sweden</publisher-loc>, <fpage>2668</fpage>‚Äì<lpage>2677</lpage>.</mixed-citation>
    </ref>
    <ref id="R27">
      <label>[27]</label>
      <mixed-citation publication-type="journal"><name><surname>Kim</surname><given-names>Hannah</given-names></name>, <name><surname>Choo</surname><given-names>Jaegul</given-names></name>, <name><surname>Park</surname><given-names>Haesun</given-names></name>, and <name><surname>Endert</surname><given-names>Alex</given-names></name>. <year>2015</year>. <article-title>Interaxis: Steering scatterplot axes via observation-level interaction</article-title>. <source>IEEE transactions on visualization and computer graphics</source><volume>22</volume>, <issue>1</issue> (<comment>2015</comment>), <fpage>131</fpage>‚Äì<lpage>140</lpage>.<pub-id pub-id-type="pmid">26357399</pub-id>
</mixed-citation>
    </ref>
    <ref id="R28">
      <label>[28]</label>
      <mixed-citation publication-type="book"><name><surname>Kim</surname><given-names>Hyunjik</given-names></name> and <name><surname>Mnih</surname><given-names>Andriy</given-names></name>. <year>2018</year>. <part-title>Disentangling by factorising</part-title>. In <source>International Conference on Machine Learning</source>. <publisher-name>PMLR</publisher-name>, <publisher-loc>Stockholm, Sweden</publisher-loc>, <fpage>2649</fpage>‚Äì<lpage>2658</lpage>.</mixed-citation>
    </ref>
    <ref id="R29">
      <label>[29]</label>
      <mixed-citation publication-type="journal"><name><surname>Bum Chul Kwon</surname><given-names>Hannah Kim</given-names></name>, <name><surname>Wall</surname><given-names>Emily</given-names></name>, <name><surname>Choo</surname><given-names>Jaegul</given-names></name>, <name><surname>Park</surname><given-names>Haesun</given-names></name>, and <name><surname>Endert</surname><given-names>Alex</given-names></name>. <year>2016</year>. <article-title>Axisketcher: Interactive nonlinear axis mapping of visualizations through user drawings</article-title>. <source>IEEE transactions on visualization and computer graphics</source><volume>23</volume>, <issue>1</issue> (<comment>2016</comment>), <fpage>221</fpage>‚Äì<lpage>230</lpage>.<pub-id pub-id-type="pmid">27514048</pub-id>
</mixed-citation>
    </ref>
    <ref id="R30">
      <label>[30]</label>
      <mixed-citation publication-type="journal"><name><surname>Lekschas</surname><given-names>Fritz</given-names></name>, <name><surname>Bach</surname><given-names>Benjamin</given-names></name>, <name><surname>Kerpedjiev</surname><given-names>Peter</given-names></name>, <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>, and <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>. <year>2017</year>. <article-title>HiPiler: visual exploration of large genome interaction matrices with interactive small multiples</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>24</volume>, <issue>1</issue> (<comment>2017</comment>), <fpage>522</fpage>‚Äì<lpage>531</lpage>.<pub-id pub-id-type="pmid">28866592</pub-id>
</mixed-citation>
    </ref>
    <ref id="R31">
      <label>[31]</label>
      <mixed-citation publication-type="journal"><name><surname>Lekschas</surname><given-names>Fritz</given-names></name>, <name><surname>Behrisch</surname><given-names>Michael</given-names></name>, <name><surname>Bach</surname><given-names>Benjamin</given-names></name>, <name><surname>Kerpedjiev</surname><given-names>Peter</given-names></name>, <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>, and <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>. <year>2020</year>. <article-title>Pattern-Driven Navigation in 2D Multiscale Visualizations with Scalable Insets</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>26</volume>, <issue>1</issue> (<comment>1 1 2020</comment>), <fpage>611</fpage>‚Äì<lpage>621</lpage>.<pub-id pub-id-type="pmid">31442989</pub-id>
</mixed-citation>
    </ref>
    <ref id="R32">
      <label>[32]</label>
      <mixed-citation publication-type="book"><name><surname>Lekschas</surname><given-names>Fritz</given-names></name>, <name><surname>Peterson</surname><given-names>Brant</given-names></name>, <name><surname>Haehn</surname><given-names>Daniel</given-names></name>, <name><surname>Ma</surname><given-names>Eric</given-names></name>, <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>, and <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>. <year>2020</year>. <part-title>Peax: Interactive visual pattern search in sequential data using unsupervised deep representation learning</part-title>. In <source>Computer Graphics Forum</source>, Vol. <volume>39‚Äì3</volume>. <publisher-name>Wiley Online Library</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>167</fpage>‚Äì<lpage>179</lpage>.</mixed-citation>
    </ref>
    <ref id="R33">
      <label>[33]</label>
      <mixed-citation publication-type="journal"><name><surname>Lekschas</surname><given-names>Fritz</given-names></name>, <name><surname>Zhou</surname><given-names>Xinyi</given-names></name>, <name><surname>Chen</surname><given-names>Wei</given-names></name>, <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>, <name><surname>Bach</surname><given-names>Benjamin</given-names></name>, and <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>. <year>2021</year>. <article-title>A Generic Framework and Library for Exploration of Small Multiples through Interactive Piling</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>27</volume>, <fpage>2</fpage> (<comment>1 2 2021</comment>), <fpage>358</fpage>‚Äì<lpage>368</lpage>.</mixed-citation>
    </ref>
    <ref id="R34">
      <label>[34]</label>
      <mixed-citation publication-type="book"><name><surname>Liu</surname><given-names>Mengchen</given-names></name>, <name><surname>Liu</surname><given-names>Shixia</given-names></name>, <name><surname>Su</surname><given-names>Hang</given-names></name>, <name><surname>Cao</surname><given-names>Kelei</given-names></name>, and <name><surname>Zhu</surname><given-names>Jun</given-names></name>. <year>2018</year>. <part-title>Analyzing the noise robustness of deep neural networks</part-title>. In <source>2018 IEEE Conference on Visual Analytics Science and Technology (VAST)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>60</fpage>‚Äì<lpage>71</lpage>.</mixed-citation>
    </ref>
    <ref id="R35">
      <label>[35]</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>Shusen</given-names></name>, <name><surname>Bremer</surname><given-names>Peer-Timo</given-names></name>, <name><surname>Jayaraman J Thiagarajan</surname><given-names>Vivek Srikumar</given-names></name>, <name><surname>Wang</surname><given-names>Bei</given-names></name>, <name><surname>Livnat</surname><given-names>Yarden</given-names></name>, and <name><surname>Pascucci</surname><given-names>Valerio</given-names></name>. <year>2017</year>. <article-title>Visual exploration of semantic relationships in neural word embeddings</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>24</volume>, <issue>1</issue> (<comment>2017</comment>), <fpage>553</fpage>‚Äì<lpage>562</lpage>.<pub-id pub-id-type="pmid">28866574</pub-id>
</mixed-citation>
    </ref>
    <ref id="R36">
      <label>[36]</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>Xiao</given-names></name>, <name><surname>Sanchez</surname><given-names>Pedro</given-names></name>, <name><surname>Thermos</surname><given-names>Spyridon</given-names></name>, <name><surname>O‚ÄôNeil</surname><given-names>Alison Q</given-names></name>, and <name><surname>Tsaftaris</surname><given-names>Sotirios A</given-names></name>. <year>2022</year>. <article-title>Learning disentangled representations in the imaging domain</article-title>. <source>Medical Image Analysis</source><volume>80</volume> (<comment>2022</comment>), <fpage>102516</fpage>.<pub-id pub-id-type="pmid">35751992</pub-id>
</mixed-citation>
    </ref>
    <ref id="R37">
      <label>[37]</label>
      <mixed-citation publication-type="book"><name><surname>Liu</surname><given-names>Yang</given-names></name>, <name><surname>Jun</surname><given-names>Eunice</given-names></name>, <name><surname>Li</surname><given-names>Qisheng</given-names></name>, and <name><surname>Heer</surname><given-names>Jeffrey</given-names></name>. <year>2019</year>. <part-title>Latent space cartography: Visual analysis of vector space embeddings</part-title>. In <source>Computer Graphics Forum</source>, Vol. <volume>38</volume>. <publisher-name>Wiley Online Library</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>67</fpage>‚Äì<lpage>78</lpage>.</mixed-citation>
    </ref>
    <ref id="R38">
      <label>[38]</label>
      <mixed-citation publication-type="book"><name><surname>Liu</surname><given-names>Ziwei</given-names></name>, <name><surname>Luo</surname><given-names>Ping</given-names></name>, <name><surname>Wang</surname><given-names>Xiaogang</given-names></name>, and <name><surname>Tang</surname><given-names>Xiaoou</given-names></name>. <year>2015</year>. <part-title>Deep Learning Face Attributes in the Wild</part-title>. In <source>Proceedings of International Conference on Computer Vision (ICCV)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>3730</fpage>‚Äì<lpage>3738</lpage>.</mixed-citation>
    </ref>
    <ref id="R39">
      <label>[39]</label>
      <mixed-citation publication-type="journal"><name><surname>Sehi L‚ÄôYi</surname><given-names>Qianwen Wang</given-names></name>, <name><surname>Lekschas</surname><given-names>Fritz</given-names></name>, and <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>. <year>2021</year>. <article-title>Gosling: A Grammar-based Toolkit for Scalable and Interactive Genomics Data Visualization</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>1</issue> (<comment>1 10 2021</comment>), <fpage>140</fpage>‚Äì<lpage>150</lpage>.<pub-id pub-id-type="pmid">34596551</pub-id>
</mixed-citation>
    </ref>
    <ref id="R40">
      <label>[40]</label>
      <mixed-citation publication-type="webpage"><name><surname>Matthey</surname><given-names>Loic</given-names></name>, <name><surname>Higgins</surname><given-names>Irina</given-names></name>, <name><surname>Hassabis</surname><given-names>Demis</given-names></name>, and <name><surname>Lerchner</surname><given-names>Alexander</given-names></name>. <year>2017</year>. <source>dSprites: Disentanglement testing Sprites dataset</source>. <comment><ext-link xlink:href="https://github.com/deepmind/dsprites-dataset/" ext-link-type="uri">https://github.com/deepmind/dsprites-dataset/</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R41">
      <label>[41]</label>
      <mixed-citation publication-type="journal"><name><surname>Leland McInnes</surname><given-names>John Healy</given-names></name>, <name><surname>Saul</surname><given-names>Nathaniel</given-names></name>, and <name><surname>Gro√überger</surname><given-names>Lukas</given-names></name>. <year>2018</year>. <article-title>UMAP: Uniform Manifold Approximation and Projection</article-title>. <source>Journal of Open Source Software</source><volume>3</volume>, <issue>29</issue> (<comment>2018</comment>), <fpage>861</fpage>.</mixed-citation>
    </ref>
    <ref id="R42">
      <label>[42]</label>
      <mixed-citation publication-type="book"><name><surname>Ming</surname><given-names>Yao</given-names></name>, <name><surname>Cao</surname><given-names>Shaozu</given-names></name>, <name><surname>Zhang</surname><given-names>Ruixiang</given-names></name>, <name><surname>Li</surname><given-names>Zhen</given-names></name>, <name><surname>Chen</surname><given-names>Yuanzhe</given-names></name>, <name><surname>Song</surname><given-names>Yangqiu</given-names></name>, and <name><surname>Qu</surname><given-names>Huamin</given-names></name>. <year>2017</year>. <part-title>Understanding hidden memories of recurrent neural networks</part-title>. In <source>2017 IEEE Conference on Visual Analytics Science and Technology (VAST)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>13</fpage>‚Äì<lpage>24</lpage>.</mixed-citation>
    </ref>
    <ref id="R43">
      <label>[43]</label>
      <mixed-citation publication-type="webpage"><name><surname>Mooney</surname><given-names>Paul</given-names></name>. <year>2017</year>. <source>Breast Histopathology Images</source>. <comment><ext-link xlink:href="https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images" ext-link-type="uri">https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R44">
      <label>[44]</label>
      <mixed-citation publication-type="journal"><name><surname>Mori</surname><given-names>Giulio</given-names></name>, <name><surname>Patern√≤</surname><given-names>Fabio</given-names></name>, and <name><surname>Santoro</surname><given-names>Carmen</given-names></name>. <year>2002</year>. <article-title>CTTE: support for developing and analyzing task models for interactive system design</article-title>. <source>IEEE Transactions on software engineering</source><volume>28</volume>, <issue>8</issue> (<comment>2002</comment>), <fpage>797</fpage>‚Äì<lpage>813</lpage>.</mixed-citation>
    </ref>
    <ref id="R45">
      <label>[45]</label>
      <mixed-citation publication-type="book"><name><surname>Paszke</surname><given-names>Adam</given-names></name>, <name><surname>Gross</surname><given-names>Sam</given-names></name>, <name><surname>Massa</surname><given-names>Francisco</given-names></name>, <name><surname>Lerer</surname><given-names>Adam</given-names></name>, <name><surname>Bradbury</surname><given-names>James</given-names></name>, <name><surname>Chanan</surname><given-names>Gregory</given-names></name>, <name><surname>Killeen</surname><given-names>Trevor</given-names></name>, <name><surname>Lin</surname><given-names>Zeming</given-names></name>, <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>, <name><surname>Antiga</surname><given-names>Luca</given-names></name>, <name><surname>Desmaison</surname><given-names>Alban</given-names></name>, <name><surname>Kopf</surname><given-names>Andreas</given-names></name>, <name><surname>Yang</surname><given-names>Edward</given-names></name>, <name><surname>Zachary DeVito</surname><given-names>Martin Raison</given-names></name>, <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>, <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>, <name><surname>Steiner</surname><given-names>Benoit</given-names></name>, <name><surname>Fang</surname><given-names>Lu</given-names></name>, <name><surname>Bai</surname><given-names>Junjie</given-names></name>, and <name><surname>Chintala</surname><given-names>Soumith</given-names></name>. <year>2019</year>. <part-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</part-title>. In <source>Advances in Neural Information Processing Systems 32</source>, <name><surname>Wallach</surname><given-names>H</given-names></name>, <name><surname>Larochelle</surname><given-names>H</given-names></name>, <name><surname>Beygelzimer</surname><given-names>A</given-names></name>, <name><surname>d‚ÄôAlch√©-Buc</surname><given-names>F</given-names></name>, <name><surname>Fox</surname><given-names>E</given-names></name>, and <name><surname>Garnett</surname><given-names>R</given-names></name> (Eds.). <publisher-name>Curran Associates, Inc</publisher-name>., <publisher-loc>NY, USA</publisher-loc>, <fpage>8024</fpage>‚Äì<lpage>8035</lpage>.</mixed-citation>
    </ref>
    <ref id="R46">
      <label>[46]</label>
      <mixed-citation publication-type="journal"><name><surname>Pezzotti</surname><given-names>Nicola</given-names></name>, <name><surname>Thomas H√∂llt</surname><given-names>Jan Van Gemert</given-names></name>, <name><surname>Boudewijn PF Lelieveldt</surname><given-names>Elmar Eisemann</given-names></name>, and <name><surname>Vilanova</surname><given-names>Anna</given-names></name>. <year>2017</year>. <article-title>Deepeyes: Progressive visual analytics for designing deep neural networks</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>24</volume>, <issue>1</issue> (<comment>2017</comment>), <fpage>98</fpage>‚Äì<lpage>108</lpage>.<pub-id pub-id-type="pmid">28866543</pub-id>
</mixed-citation>
    </ref>
    <ref id="R47">
      <label>[47]</label>
      <mixed-citation publication-type="book"><name><surname>Pirrung</surname><given-names>Meg</given-names></name>, <name><surname>Hilliard</surname><given-names>Nathan</given-names></name>, <name><surname>Yankov</surname><given-names>Art√´m</given-names></name>, <name><surname>Nancy O‚ÄôBrien</surname><given-names>Paul Weidert</given-names></name>, <name><surname>Corley</surname><given-names>Courtney D</given-names></name>, and <name><surname>Hodas</surname><given-names>Nathan O</given-names></name>. <year>2018</year>. <part-title>Sharkzor: Interactive deep learning for image triage, sort and summary</part-title>. In <source>arXiv preprint</source>, <comment>Vol. arXiv:1802.05316.</comment><publisher-name>Cornell University</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>4</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R48">
      <label>[48]</label>
      <mixed-citation publication-type="journal"><name><surname>Rao</surname><given-names>Suhas SP</given-names></name>, <name><surname>Huntley</surname><given-names>Miriam H</given-names></name>, <name><surname>Durand</surname><given-names>Neva C</given-names></name>, <name><surname>Stamenova</surname><given-names>Elena K</given-names></name>, <name><surname>Bochkov</surname><given-names>Ivan D</given-names></name>, <name><surname>Robinson</surname><given-names>James T</given-names></name>, <name><surname>Sanborn</surname><given-names>Adrian L</given-names></name>, <name><surname>Machol</surname><given-names>Ido</given-names></name>, <name><surname>Omer</surname><given-names>Arina D</given-names></name>, <name><surname>Lander</surname><given-names>Eric S</given-names></name>, <etal/><year>2014</year>. <article-title>A 3D map of the human genome at kilobase resolution reveals principles of chromatin looping</article-title>. <source>Cell</source><volume>159</volume>, <issue>7</issue> (<comment>2014</comment>), <fpage>1665</fpage>‚Äì<lpage>1680</lpage>.<pub-id pub-id-type="pmid">25497547</pub-id>
</mixed-citation>
    </ref>
    <ref id="R49">
      <label>[49]</label>
      <mixed-citation publication-type="journal"><name><surname>Ren</surname><given-names>Pengzhen</given-names></name>, <name><surname>Xiao</surname><given-names>Yun</given-names></name>, <name><surname>Chang</surname><given-names>Xiaojun</given-names></name>, <name><surname>Huang</surname><given-names>Po-Yao</given-names></name>, <name><surname>Li</surname><given-names>Zhihui</given-names></name>, <name><surname>Brij B Gupta</surname><given-names>Xiaojiang Chen</given-names></name>, and <name><surname>Wang</surname><given-names>Xin</given-names></name>. <year>2021</year>. <article-title>A survey of deep active learning</article-title>. <source>Comput. Surveys</source><volume>54</volume>, <issue>9</issue> (<comment>2021</comment>), <fpage>1</fpage>‚Äì<lpage>40</lpage>.</mixed-citation>
    </ref>
    <ref id="R50">
      <label>[50]</label>
      <mixed-citation publication-type="journal"><name><surname>Russakovsky</surname><given-names>Olga</given-names></name>, <name><surname>Deng</surname><given-names>Jia</given-names></name>, <name><surname>Su</surname><given-names>Hao</given-names></name>, <name><surname>Krause</surname><given-names>Jonathan</given-names></name>, <name><surname>Satheesh</surname><given-names>Sanjeev</given-names></name>, <name><surname>Ma</surname><given-names>Sean</given-names></name>, <name><surname>Huang</surname><given-names>Zhiheng</given-names></name>, <name><surname>Karpathy</surname><given-names>Andrej</given-names></name>, <name><surname>Khosla</surname><given-names>Aditya</given-names></name>, <name><surname>Bernstein</surname><given-names>Michael</given-names></name>, <name><surname>Berg</surname><given-names>Alexander C.</given-names></name>, and <name><surname>Fei-Fei</surname><given-names>Li</given-names></name>. <year>2015</year>. <article-title>ImageNet Large Scale Visual Recognition Challenge</article-title>. <source>International Journal of Computer Vision (IJCV)</source><volume>115</volume>, <issue>3</issue> (<comment>2015</comment>), <fpage>211</fpage>‚Äì<lpage>252</lpage>. <pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></mixed-citation>
    </ref>
    <ref id="R51">
      <label>[51]</label>
      <mixed-citation publication-type="book"><name><surname>Smilkov</surname><given-names>Daniel</given-names></name>, <name><surname>Thorat</surname><given-names>Nikhil</given-names></name>, <name><surname>Nicholson</surname><given-names>Charles</given-names></name>, <name><surname>Reif</surname><given-names>Emily</given-names></name>, <name><surname>Vi√©gas</surname><given-names>Fernanda B</given-names></name>, and <name><surname>Martin Wattenberg</surname></name>. <year>2016</year>. <part-title>Embedding projector: Interactive visualization and interpretation of embeddings</part-title>. In <source>arXiv preprint</source>, <comment>Vol. arXiv:1611.05469.</comment><publisher-name>Cornell University</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>4</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R52">
      <label>[52]</label>
      <mixed-citation publication-type="webpage"><name><surname>Sothivelr</surname><given-names>Karthick</given-names></name>. <year>2020</year>. <source>Breast Cancer Classification With PyTorch and Deep Learning</source>. <comment><ext-link xlink:href="https://medium.com/swlh/" ext-link-type="uri">https://medium.com/swlh/</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R53">
      <label>[53]</label>
      <mixed-citation publication-type="book"><name><surname>Jost Tobias Springenberg</surname><given-names>Alexey Dosovitskiy</given-names></name>, <name><surname>Brox</surname><given-names>Thomas</given-names></name>, and <name><surname>Ried-miller</surname><given-names>Martin</given-names></name>. <year>2014</year>. <part-title>Striving for simplicity: The all convolutional net</part-title>. In <source>ICLR (workshop track)</source>. <publisher-name>International Machine Learning Society</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>14</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R54">
      <label>[54]</label>
      <mixed-citation publication-type="journal"><name><surname>Strobelt</surname><given-names>Hendrik</given-names></name>, <name><surname>Gehrmann</surname><given-names>Sebastian</given-names></name>, <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>, and <name><surname>Rush</surname><given-names>Alexander M</given-names></name>. <year>2017</year>. <article-title>Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>24</volume>, <issue>1</issue> (<comment>2017</comment>), <fpage>667</fpage>‚Äì<lpage>676</lpage>.<pub-id pub-id-type="pmid">28866526</pub-id>
</mixed-citation>
    </ref>
    <ref id="R55">
      <label>[55]</label>
      <mixed-citation publication-type="journal"><name><surname>Sutcliffe</surname><given-names>Alistair</given-names></name>. <year>2000</year>. <article-title>On the effective use and reuse of HCI knowledge</article-title>. <source>ACM Transactions on Computer-Human Interaction (TOCHI)</source><volume>7</volume>, <issue>2</issue> (<comment>2000</comment>), <fpage>197</fpage>‚Äì<lpage>221</lpage>.</mixed-citation>
    </ref>
    <ref id="R56">
      <label>[56]</label>
      <mixed-citation publication-type="journal"><name><surname>Sutcliffe</surname><given-names>Alistair G</given-names></name> and <name><surname>Carroll</surname><given-names>John M</given-names></name>. <year>1999</year>. <article-title>Designing claims for reuse in interactive systems design</article-title>. <source>International Journal of Human-Computer Studies</source>
<volume>50</volume>, <issue>3</issue> (<comment>1999</comment>), <fpage>213</fpage>‚Äì<lpage>241</lpage>.</mixed-citation>
    </ref>
    <ref id="R57">
      <label>[57]</label>
      <mixed-citation publication-type="book"><name><surname>Tufte</surname><given-names>Edward R</given-names></name>, <name><surname>Goeler</surname><given-names>Nora Hillman</given-names></name>, and <name><surname>Benson</surname><given-names>Richard</given-names></name>. <year>1990</year>. <source>Envisioning information</source>. Vol. <volume>126</volume>. <publisher-name>Graphics press</publisher-name>, <publisher-loc>Cheshire, CT</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="R58">
      <label>[58]</label>
      <mixed-citation publication-type="journal"><collab>Laurens Van der Maaten and Geoffrey Hinton</collab>. <year>2008</year>. <article-title>Visualizing data using t-SNE</article-title>. <source>Journal of Machine Learning Research</source><volume>9</volume>, <issue>11</issue> (<comment>2008</comment>), <fpage>2579</fpage>‚Äì<lpage>2605</lpage>.</mixed-citation>
    </ref>
    <ref id="R59">
      <label>[59]</label>
      <mixed-citation publication-type="book"><name><surname>Wang</surname><given-names>Junpeng</given-names></name>, <name><surname>Zhang</surname><given-names>Wei</given-names></name>, and <name><surname>Yang</surname><given-names>Hao</given-names></name>. <year>2020</year>. <part-title>SCANViz: Interpreting the symbol-concept association captured by deep neural networks through visual analytics</part-title>. In <source>2020 IEEE Pacific Visualization Symposium (PacificVis)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>51</fpage>‚Äì<lpage>60</lpage>.</mixed-citation>
    </ref>
    <ref id="R60">
      <label>[60]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Alexander</surname><given-names>William</given-names></name>, <name><surname>Pegg</surname><given-names>Jack</given-names></name>, <name><surname>Qu</surname><given-names>Huamin</given-names></name>, and <name><surname>Chen</surname><given-names>Min</given-names></name>. <year>2020</year>. <article-title>HypoML: Visual analysis for hypothesis-based evaluation of machine learning models</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>27</volume>, <issue>2</issue> (<comment>2020</comment>), <fpage>1417</fpage>‚Äì<lpage>1426</lpage>.</mixed-citation>
    </ref>
    <ref id="R61">
      <label>[61]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Chen</surname><given-names>Zhutian</given-names></name>, <name><surname>Wang</surname><given-names>Yong</given-names></name>, and <name><surname>Qu</surname><given-names>Huamin</given-names></name>. <year>2021</year>. <article-title>A Survey on ML4VIS: Applying MachineLearning Advances to Data Visualization</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>12</issue> (<comment>2021</comment>), <fpage>5134</fpage>‚Äì<lpage>5153</lpage>.</mixed-citation>
    </ref>
    <ref id="R62">
      <label>[62]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Li</surname><given-names>Zhen</given-names></name>, <name><surname>Fu</surname><given-names>Siwei</given-names></name>, <name><surname>Cui</surname><given-names>Weiwei</given-names></name>, and <name><surname>Qu</surname><given-names>Huamin</given-names></name>. <year>2019</year>. <article-title>Narvis: Authoring Narrative Slideshows for Introducing Data Visualization Designs</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>25</volume>, <issue>1</issue> (<month>jan</month>
<comment>2019</comment>), <fpage>779</fpage>‚Äì<lpage>788</lpage>.</mixed-citation>
    </ref>
    <ref id="R63">
      <label>[63]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Mazor</surname><given-names>Tali</given-names></name>, <name><surname>Theresa A Harbig</surname><given-names>Ethan Cerami</given-names></name>, and <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>. <year>2021</year>. <article-title>ThreadStates: State-based Visual Analysis of Disease Progression</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>1</issue> (<comment>2021</comment>), <fpage>238</fpage>‚Äì<lpage>247</lpage>.<pub-id pub-id-type="pmid">34587068</pub-id>
</mixed-citation>
    </ref>
    <ref id="R64">
      <label>[64]</label>
      <mixed-citation publication-type="book"><name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Ming</surname><given-names>Yao</given-names></name>, <name><surname>Jin</surname><given-names>Zhihua</given-names></name>, <name><surname>Shen</surname><given-names>Qiaomu</given-names></name>, <name><surname>Liu</surname><given-names>Dongyu</given-names></name>, <name><surname>Micah J Smith</surname><given-names>Kalyan Veeramachaneni</given-names></name>, and <name><surname>Qu</surname><given-names>Huamin</given-names></name>. <year>2019</year>. <part-title>Atmseer: Increasing transparency and controllability in automated machine learning</part-title>. In <source>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</source>. <publisher-name>ACM</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>1</fpage>‚Äì<lpage>12</lpage>.</mixed-citation>
    </ref>
    <ref id="R65">
      <label>[65]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Xu</surname><given-names>Zhenhua</given-names></name>, <name><surname>Chen</surname><given-names>Zhutian</given-names></name>, <name><surname>Wang</surname><given-names>Yong</given-names></name>, <name><surname>Liu</surname><given-names>Shixia</given-names></name>, and <name><surname>Qu</surname><given-names>Huamin</given-names></name>. <year>2020</year>. <article-title>Visual analysis of discrimination in machine learning</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>27</volume>, <issue>2</issue> (<comment>2020</comment>), <fpage>1470</fpage>‚Äì<lpage>1480</lpage>.</mixed-citation>
    </ref>
    <ref id="R66">
      <label>[66]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Yinqiao</given-names></name>, <name><surname>Chen</surname><given-names>Lu</given-names></name>, <name><surname>Jo</surname><given-names>Jaemin</given-names></name>, and <name><surname>Wang</surname><given-names>Yunhai</given-names></name>. <year>2021</year>. <article-title>Joint t-SNE for Comparable Projections of Multiple High-Dimensional Datasets</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>1</issue> (<comment>2021</comment>), <fpage>623</fpage>‚Äì<lpage>632</lpage>.<pub-id pub-id-type="pmid">34587021</pub-id>
</mixed-citation>
    </ref>
    <ref id="R67">
      <label>[67]</label>
      <mixed-citation publication-type="journal"><name><surname>Yuan</surname><given-names>Jun</given-names></name>, <name><surname>Chen</surname><given-names>Changjian</given-names></name>, <name><surname>Yang</surname><given-names>Weikai</given-names></name>, <name><surname>Liu</surname><given-names>Mengchen</given-names></name>, <name><surname>Xia</surname><given-names>Jiazhi</given-names></name>, and <name><surname>Liu</surname><given-names>Shixia</given-names></name>. <year>2021</year>. <article-title>A survey of visual analytics techniques for machine learning</article-title>. <source>Computational Visual Media</source><volume>7</volume>, <issue>1</issue> (<comment>2021</comment>), <fpage>3</fpage>‚Äì<lpage>36</lpage>.</mixed-citation>
    </ref>
    <ref id="R68">
      <label>[68]</label>
      <mixed-citation publication-type="journal"><name><surname>Zhao</surname><given-names>Zhenge</given-names></name>, <name><surname>Xu</surname><given-names>Panpan</given-names></name>, <name><surname>Scheidegger</surname><given-names>Carlos</given-names></name>, and <name><surname>Ren</surname><given-names>Liu</given-names></name>. <year>2021</year>. <article-title>Human-in-the-loop Extraction of Interpretable Concepts in Deep Learning Models</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>1</issue> (<comment>2021</comment>), <fpage>780</fpage>‚Äì<lpage>790</lpage>.<pub-id pub-id-type="pmid">34587066</pub-id>
</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="F1">
    <label>Figure 1:</label>
    <caption>
      <p id="P82">Drava enables concept-driven exploration by aligning semantic latent dimensions with human concepts. (a) UMAP projection of image patches of breast cancer specimens. (b) All image patches are organized and piled up based on the density of tissues. (c) All image patches are grouped into a grid layout according to the tissue density and color. The two visual concepts reveal a strong association of the presentation of invasive ductal carcinomas (IDC), <italic toggle="yes">i.e</italic>., the orange label. (d‚Äìe) More examples.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0001" position="float"/>
  </fig>
  <fig position="float" id="F2">
    <label>Figure 2:</label>
    <caption>
      <p id="P83">Mismatches between semantic latent dimensions and human concepts (red dashed boxes). (a): Synthesized images through value traversal of a latent dimension. (b): Items with the same latent values as the left- and right-most synthesized images.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0002" position="float"/>
  </fig>
  <fig position="float" id="F3">
    <label>Figure 3:</label>
    <caption>
      <p id="P84">A three-step workflow that guides the application of DRL for the concept-driven exploration of small multiples.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0003" position="float"/>
  </fig>
  <fig position="float" id="F4">
    <label>Figure 4:</label>
    <caption>
      <p id="P85">The user interface of Drava. The <italic toggle="yes">Concept View</italic> presents latent dimensions and other metadata as rows. The <italic toggle="yes">Item Browser</italic> enables user exploration of items based on selected concepts from the <italic toggle="yes">Concept View</italic> and can be controlled through the Configuration panel on the right. The <italic toggle="yes">Spatial View</italic> provides context information of the items when applicable.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0004" position="float"/>
  </fig>
  <fig position="float" id="F5">
    <label>Figure 5:</label>
    <caption>
      <p id="P86">Drava provides various grouping and labeling methods to help users interpret semantic dimension. (a‚Äìb) Different approaches to pile up items based on their characteristics. (c) Labels can be added to both items and item groups.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0005" position="float"/>
  </fig>
  <fig position="float" id="F6">
    <label>Figure 6:</label>
    <caption>
      <p id="P87">Overview of the interactions and mechanisms that Drava provides for identifying concept mismatches (a), refining items and groups (b), and updating items and underlying models (c).</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0006" position="float"/>
  </fig>
  <fig position="float" id="F7">
    <label>Figure 7:</label>
    <caption>
      <p id="P88">Architecture of the encoder, the decoder, and the concept adaptor.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0007" position="float"/>
  </fig>
  <fig position="float" id="F8">
    <label>Figure 8:</label>
    <caption>
      <p id="P89">Dimensions (rows) with different salience scores (a‚Äìb). The visual changes are clearer when changing the values of the dimensions with the highest salience scores (a) compared to the dimensions with the lowest scores (b).</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0008" position="float"/>
  </fig>
  <fig position="float" id="F9">
    <label>Figure 9:</label>
    <caption>
      <p id="P90">Examples of reconstructed outputs on four different datasets (a‚Äìd). The first row shows the original inputs and the second row represents the reconstructed outputs.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0009" position="float"/>
  </fig>
  <fig position="float" id="F10">
    <label>Figure 10:</label>
    <caption>
      <p id="P91">We compared our concept adaptor with active learning (baseline) on three different concepts (<italic toggle="yes">i.e</italic>., scale, smiling, and bangs) under three conditions (<italic toggle="yes">i.e</italic>., <italic toggle="yes">N</italic> = 1%, 2%, 5%). Each line graph shows the accuracy over 15 iterations. The concept adaptor (yellow) overall showed higher accuracy.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0010" position="float"/>
  </fig>
  <fig position="float" id="F11">
    <label>Figure 11:</label>
    <caption>
      <p id="P92">The application scenario using the simple shape data. (a) A UMAP projection puts images together even though the positions of shapes are different. (b) Users can arrange images based on shape positions. (c) Images are arranged based on the scales of shape, but the left-most side is mostly squares (c1), indicating the need for user refinement.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0011" position="float"/>
  </fig>
  <fig position="float" id="F12">
    <label>Figure 12:</label>
    <caption>
      <p id="P93">The application scenario using celebrity images. (a) Items are grouped based on a visual concept that is related to skin color. (b) Items are then rearranged by adding another visual concept that is related to the background darkness as the <italic toggle="yes">y</italic> axis. The dataset contains both items that have fair skin and dark background (b1) and items that have dark skin and light background (b2).</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0012" position="float"/>
  </fig>
  <fig position="float" id="F13">
    <label>Figure 13:</label>
    <caption>
      <p id="P94">The application scenario using a genomic interaction matrix. (a1‚Äìa4) Four representative items vary on three concepts: the thickness of the diagonal (a1 and a4), the presence of nested squares (a1 and a2), and the asymmetric structure of the nested squares (a2 and a3). (b) A group has both the items with nested squares and the items with thick diagonals (orange marks). Each item is displayed upon mouse hovering. (c) Arranging items using <monospace>dim_nest</monospace> as <italic toggle="yes">x</italic> axis and <monospace>dim_thick</monospace> as <italic toggle="yes">y</italic> axis clearly separates these two concepts.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0013" position="float"/>
  </fig>
  <fig position="float" id="F14">
    <label>Figure 14:</label>
    <caption>
      <p id="P95">The application scenario using the breast histopathology images. (a) Arranging image patches from breast cancer specimens based on concepts learned by Drava shows a strong association between the presence of IDC (the color of item labels) and the two visual concepts, <italic toggle="yes">i.e</italic>., the tissue density (a2 vs. a3, the <italic toggle="yes">x</italic> axis) and the tissue color (a1 vs. a2, the <italic toggle="yes">y</italic> axis). We further (b) filter these items and (c) display them in a grid layout to identify confident false-positive predictions without visual clutter. (d) The spatial view enables us to locate the items in the original whole-mount slide image.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0014" position="float"/>
  </fig>
  <table-wrap position="float" id="T1">
    <label>Table 1:</label>
    <caption>
      <p id="P96">Semantic meaning of individual latent dimensions. We compare Drava (<italic toggle="yes">i.e</italic>., using the value of one latent dimension to classify the corresponding concept) with random guesses on five concepts from two datasets. The results show that the latent dimension value could effectively indicate the corresponding concept.</p>
    </caption>
    <table frame="box" rules="all">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-left: hidden" rowspan="1" colspan="1">dataset</th>
          <th colspan="3" align="center" valign="middle" rowspan="1">dsprites</th>
          <th colspan="2" align="center" valign="middle" style="border-right: hidden" rowspan="1">CelebA</th>
        </tr>
        <tr>
          <th align="center" valign="middle" style="border-left: hidden" rowspan="1" colspan="1">concept</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">pos_x</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">pos_y</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">scale</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">smiling</th>
          <th align="center" valign="middle" style="border-right: hidden" rowspan="1" colspan="1">bangs</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" style="border-left: hidden" rowspan="1" colspan="1">random guess</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.333</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.333</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.333</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.5</td>
          <td align="center" valign="middle" style="border-right: hidden" rowspan="1" colspan="1">0.5</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-left: hidden" rowspan="1" colspan="1">Drava (no human refine)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.87</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.93</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.62</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.70</td>
          <td align="center" valign="middle" style="border-right: hidden" rowspan="1" colspan="1">0.77</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <boxed-text id="BX1" position="float">
    <caption>
      <title>CCS CONCEPTS</title>
    </caption>
    <list list-type="bullet" id="L2">
      <list-item>
        <p id="P97">Human-centered computing ‚Üí Interactive systems and tools</p>
      </list-item>
      <list-item>
        <p id="P98">Visual analytics</p>
      </list-item>
      <list-item>
        <p id="P99">Information visualization</p>
      </list-item>
    </list>
  </boxed-text>
</floats-group>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?noissn?>
<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr Proc SIGCHI Conf Hum Factor Comput Syst?>
<?submitter-system nihms?>
<?submitter-userid 10917418?>
<?submitter-authority eRA?>
<?submitter-login nilsgehlenborg?>
<?submitter-name Nils Gehlenborg?>
<?domain nihpa?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">101620299</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">41882</journal-id>
    <journal-id journal-id-type="nlm-ta">Proc SIGCHI Conf Hum Factor Comput Syst</journal-id>
    <journal-title-group>
      <journal-title>Proceedings of the SIGCHI conference on human factors in computing systems. CHI Conference</journal-title>
    </journal-title-group>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10707479</article-id>
    <article-id pub-id-type="pmid">38074525</article-id>
    <article-id pub-id-type="doi">10.1145/3544548.3581127</article-id>
    <article-id pub-id-type="manuscript">nihpa1947573</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DRAVA: Aligning Human Concepts with Machine Learning Latent Dimensions for the Visual Exploration of Small Multiples</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Qianwen</given-names>
        </name>
        <aff id="A1">Harvard Medical School, Boston, MA, USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>L‚ÄôYi</surname>
          <given-names>Sehi</given-names>
        </name>
        <aff id="A2">Harvard Medical School, Boston, MA, USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gehlenborg</surname>
          <given-names>Nils</given-names>
        </name>
        <aff id="A3">Harvard Medical School, Boston, MA, USA</aff>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="CR1">
        <email>qianwen_wang@hms.harvard.edu</email>
      </corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>28</day>
      <month>11</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>19</day>
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>08</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <volume>2023</volume>
    <elocation-id>833</elocation-id>
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial International 4.0 License</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P1">Latent vectors extracted by machine learning (ML) are widely used in data exploration (<italic toggle="yes">e.g</italic>., t-SNE) but suffer from a lack of interpretability. While previous studies employed disentangled representation learning (DRL) to enable more interpretable exploration, they often overlooked the potential mismatches between the concepts of humans and the semantic dimensions learned by DRL. To address this issue, we propose Drava, a visual analytics system that supports users in 1) relating the concepts of humans with the semantic dimensions of DRL and identifying mismatches, 2) providing feedback to minimize the mismatches, and 3) obtaining data insights from concept-driven exploration. Drava provides a set of visualizations and interactions based on visual piles to help users understand and refine concepts and conduct concept-driven exploration. Meanwhile, Drava employs a concept adaptor model to fine-tune the semantic dimensions of DRL based on user refinement. The usefulness of Drava is demonstrated through application scenarios and experimental validation.</p>
    </abstract>
    <kwd-group>
      <kwd>Visual exploration</kwd>
      <kwd>XAI</kwd>
      <kwd>Human-AI collaboration</kwd>
      <kwd>latent space</kwd>
      <kwd>small multiples</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>1</label>
    <title>INTRODUCTION</title>
    <p id="P2">Presenting analyzed small multiples (<italic toggle="yes">e.g</italic>., patches of medical images, miniature visualizations of a large genomic sequence) using latent vectors learned by machine learning (ML) models has become a common practice in many visual analytics systems [<xref rid="R6" ref-type="bibr">6</xref>, <xref rid="R10" ref-type="bibr">10</xref>]. A latent vector, usually represented as multi-dimensional quantitative values, is a compact representation of the analyzed data to capture relevant information. For example, a 64√ó64 pixel image can be represented as a 10-dimensional latent vector. Compared with analysis using raw data or human-crafted metrics, latent vectors enable users to organize and explore a large amount of data and conduct analysis tasks, such as finding similar items and identifying outliers, more efficiently.</p>
    <p id="P3">Even though latent vectors can accurately capture patterns extracted from the analyzed data, they cannot be directly interpreted by humans like the original images or texts. For this reason, latent vectors are usually used to represent the similarity between data items, assuming the latent vectors of two similar items are close in a latent space. For example, dimension reduction methods (<italic toggle="yes">e.g</italic>., t-SNE [<xref rid="R58" ref-type="bibr">58</xref>], UMAP [<xref rid="R41" ref-type="bibr">41</xref>]) are widely used to visualize latent vectors in 2D space, showing the similarities and differences among data items. Other prior studies proposed to hierarchically cluster items based on their latent vectors to conduct pattern-driven visual analytics [<xref rid="R6" ref-type="bibr">6</xref>]. However, definitions of ‚Äúsimilar items‚Äù vary depending on analysis tasks, and there is no single definition that can be applied to all scenarios. Even though some prior studies have incorporated user input to learn user‚Äôs perception of similarity [<xref rid="R10" ref-type="bibr">10</xref>, <xref rid="R32" ref-type="bibr">32</xref>] and even to extract human-readable concepts (<italic toggle="yes">e.g</italic>., gender from face images) [<xref rid="R37" ref-type="bibr">37</xref>, <xref rid="R68" ref-type="bibr">68</xref>], visual analytics based on latent vectors still suffers from their limited interpretability.</p>
    <p id="P4">Disentangled representation learning (DRL) [<xref rid="R12" ref-type="bibr">12</xref>, <xref rid="R22" ref-type="bibr">22</xref>] is a promising approach that can provide more explainable latent vectors through unsupervised learning, <italic toggle="yes">i.e</italic>., without human labels. By disentangling features and encoding them as separated dimensions in the latent vectors, DRL can generate latent vectors whose values carry semantics and can reveal human-understandable concepts, <italic toggle="yes">e.g</italic>., the value on one dimension indicates whether a person is smiling or not (<xref rid="F1" ref-type="fig">Figure 1d</xref>). We call such dimensions semantic dimensions. Some recent visualization tools [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R20" ref-type="bibr">20</xref>, <xref rid="R59" ref-type="bibr">59</xref>] have successfully employed DRL in their analysis and demonstrated the effectiveness of DRL. For example, Gou <italic toggle="yes">et al</italic>. [<xref rid="R18" ref-type="bibr">18</xref>] used DRL for traffic light images to summarize images based on human-readable concepts, such as color, brightness, and rotation. These studies usually assumed that the learned semantic dimensions can perfectly capture human concepts, and the concepts can be accurately represented by a set of synthesized images. However, these assumptions do not always hold. Potential mismatches can exist between the semantic latent dimensions learned by ML models and the concepts of humans. As shown in <xref rid="F2" ref-type="fig">Figure 2a</xref>, one latent dimension correlates to the angle of human head according to the synthesized images. But when using this dimension to organize images, our experiment results show that the model confuses ‚Äúangle of the head‚Äù with ‚Äúwhether part of the face is covered‚Äù, <italic toggle="yes">e.g</italic>., covered by a dark shadow or a flower (<xref rid="F2" ref-type="fig">Figure 2b</xref>). Meanwhile, previous studies focus on using DRL to diagnose supervised ML models rather than building an understanding of the data [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R20" ref-type="bibr">20</xref>]. They provide limited discussion about user needs in understanding and utilizing DRL for concept-driven data exploration.</p>
    <p id="P5">This study aims to provide a more interpretable and flexible visual exploration of small multiples by better aligning concepts of human users with the semantic latent vectors generated by ML models. We propose Drava, an interactive system that utilizes <bold>D</bold>isentangled <bold>R</bold>epresentation learning as <bold>A V</bold>isual <bold>A</bold>nalytics approach for concept-driven data exploration. In Drava, a dataset is represented as a set of small multiples [<xref rid="R57" ref-type="bibr">57</xref>], <italic toggle="yes">i.e</italic>., a series of basic charts or graphics that show instances or different slices of the dataset (<xref rid="F1" ref-type="fig">Figure 1</xref>). Hereafter, we call each small multiple as a <italic toggle="yes">data item</italic>. For each data item, DRL learns a multi-dimensional latent vector, certain dimensions of which have semantic meanings. Drava supports an interpretable exploration of these items by supporting users in correlating and aligning the semantic dimensions with human concepts. The interactive visualizations and algorithms in Drava are motivated and guided by a three-step workflow that we propose. Throughout this workflow, users 1) understand ML-learned semantic dimensions and identify their potential mismatches with human concepts, 2) refine and align ML semantic dimensions with human concepts, and 3) generate new knowledge about the analyzed data through concept-driven exploration. Particularly, Drava automatically ranks latent vectors and proposes a concept adaptor that can refine a concept based on human input. Meanwhile, a set of interactions based on visual piles [<xref rid="R33" ref-type="bibr">33</xref>] are provided, enabling users to effectively arrange, summarize, and compare items based on human-readable concepts. We demonstrate the usefulness of Drava through experimental validation and four usage scenarios. Drava is available at <ext-link xlink:href="https://qianwen.info/DRAVA/" ext-link-type="uri">https://qianwen.info/DRAVA/</ext-link>.</p>
  </sec>
  <sec id="S2">
    <label>2</label>
    <title>BACKGROUND: DISENTANGLED REPRESENTATION LEARNING</title>
    <p id="P6">DRL is a promising machine learning method that is able to extract interpretable features without human supervision. Given an input item <bold>x</bold>, the goal of DRL is to learn a vector <bold>z</bold> that captures the features of <bold>x</bold> in a disentangled manner. For example, as shown in <xref rid="F2" ref-type="fig">Figure 2a</xref>, a DRL model learns to represent an image of a human face using <bold>z</bold> and captures the feature ‚Äúthe angle of head‚Äù independently in <italic toggle="yes">z</italic><sub>1</sub> (<italic toggle="yes">i.e</italic>., the second dimension of <bold>z</bold>). Similarly, an area chart can be described as a vector <bold>z</bold> where <italic toggle="yes">z</italic><sub>0</sub> indicates the height of the chart, <italic toggle="yes">z</italic><sub>1</sub> indicates the trend, etc. DRL assumes <bold>x</bold> as a joint distribution of independent and dependent generative factors [<xref rid="R22" ref-type="bibr">22</xref>]. While these independent factors will be captured in separated dimensions of <bold>z</bold> (<italic toggle="yes">i.e</italic>., semantic dimensions), the dependent factors will remain entangled in other dimensions of <bold>z</bold> that are not used for representing the independent factors. In other words, some dimensions of <bold>z</bold> will have semantic meanings while others will not. For a precise mathematical definition of disentangled and entangled dimensions, we refer the readers to [<xref rid="R8" ref-type="bibr">8</xref>, <xref rid="R21" ref-type="bibr">21</xref>, <xref rid="R22" ref-type="bibr">22</xref>].</p>
    <p id="P7">A DRL model learns disentangled representations via two loss terms, a reconstruction term and a regularization term. The reconstruction term evaluates the differences between the input item <bold>x</bold> and the reconstructed item <inline-formula><mml:math id="M1" display="inline"><mml:mrow><mml:mover accent="true"><mml:mstyle mathvariant="bold"><mml:mtext>x</mml:mtext></mml:mstyle><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, encouraging the model to learn <bold>z</bold> that capture the main characteristics of the input item. The regularization term encourages disentanglement of the latent vectors. A DRL model is usually constructed by encouraging disentanglement in standard generative models, such as VAE [<xref rid="R22" ref-type="bibr">22</xref>] and GAN [<xref rid="R12" ref-type="bibr">12</xref>]. The state-of-the-art DRL approaches are largely based on VAE mainly due to their better training stability than GAN-based methods. For example, the loss function of <italic toggle="yes">Œ≤</italic>-VAE is defined as
<disp-formula id="FD1"><label>(1)</label><mml:math id="M2" display="block"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>œï</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>‚à£</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext>log</mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mi>Œ∏</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>‚à£</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>‚àí</mml:mo><mml:mi>Œ≤</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>œï</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>‚à£</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>‚Äñ</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
The first term is a reconstruction loss, and the second term is a regularization for disentanglement. With <italic toggle="yes">Œ≤</italic> &gt; 1, <italic toggle="yes">Œ≤</italic>-VAE encourages disentangled <bold>z</bold> by putting a constraint on the latent bottleneck. Prior studies have proposed various regularization terms for disentanglement. For more details, refer to prior studies [<xref rid="R21" ref-type="bibr">21</xref>, <xref rid="R36" ref-type="bibr">36</xref>]. Given its wide popularity, we use <italic toggle="yes">Œ≤</italic>-VAE in Drava with some modifications (<xref rid="S14" ref-type="sec">subsection 6.1</xref>). The proposed framework can be easily adapted to other VAE-based DRL, such as FactorVAE [<xref rid="R28" ref-type="bibr">28</xref>] or <italic toggle="yes">Œ≤</italic>-TCVAE [<xref rid="R11" ref-type="bibr">11</xref>].</p>
  </sec>
  <sec id="S3">
    <label>3</label>
    <title>RELATED WORK</title>
    <p id="P8">First, since Drava aims to assist data exploration using explainable latent vectors, it is closely related to <bold>visual analytics on latent vectors</bold> and, more broadly, <bold>visual analytics for ML models</bold> whose hidden layers generate latent vectors of the input data.</p>
    <p id="P9">Many visual analytics tools have been proposed to support interactive explorations of latent vectors. Dimensionality reduction techniques, such as t-SNE [<xref rid="R58" ref-type="bibr">58</xref>], UMAP [<xref rid="R41" ref-type="bibr">41</xref>], PCA [<xref rid="R1" ref-type="bibr">1</xref>], and their variants [<xref rid="R35" ref-type="bibr">35</xref>, <xref rid="R66" ref-type="bibr">66</xref>], are widely used to assist the visualization of latent vectors. Most of them focus on analyzing the latent vectors generated by a specific model [<xref rid="R67" ref-type="bibr">67</xref>], such as a convolutional neural network [<xref rid="R25" ref-type="bibr">25</xref>, <xref rid="R34" ref-type="bibr">34</xref>, <xref rid="R46" ref-type="bibr">46</xref>], a graph neural network [<xref rid="R24" ref-type="bibr">24</xref>], and a recurrent neural network [<xref rid="R35" ref-type="bibr">35</xref>, <xref rid="R42" ref-type="bibr">42</xref>, <xref rid="R54" ref-type="bibr">54</xref>]. Other studies aim to provide more generic methods for visually exploring the latent space [<xref rid="R7" ref-type="bibr">7</xref>, <xref rid="R37" ref-type="bibr">37</xref>, <xref rid="R51" ref-type="bibr">51</xref>]. Most relevant to our study is LSC [<xref rid="R37" ref-type="bibr">37</xref>], which provides comprehensive support for mapping and comparing semantic dimensions in the analysis of latent vectors. However, LSC requires users to manually identify semantic dimensions, either by importing data labels or by interactively grouping items.</p>
    <p id="P10">Apart from showing latent vectors, previous studies have combined interactive visual analytics with interactive or explainable ML to introduce interpretability into the analysis of latent vectors [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R23" ref-type="bibr">23</xref>, <xref rid="R68" ref-type="bibr">68</xref>]. Several studies [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R20" ref-type="bibr">20</xref>, <xref rid="R59" ref-type="bibr">59</xref>] used DRL to extract semantic dimensions and associate model performance with human concepts (<italic toggle="yes">e.g</italic>., brightness of images, location of objects). The semantic dimensions learned by DRL are directly used without refinement, mostly because they are low-level concepts that can be easily extracted by ML. Jia <italic toggle="yes">et al</italic>. [<xref rid="R23" ref-type="bibr">23</xref>] proposed a visual explainable active learning approach that asks users questions and uses their answers to learn explainable attributes that can be used to classify images from unseen classes. Zhao <italic toggle="yes">et al</italic>. [<xref rid="R68" ref-type="bibr">68</xref>] proposed a visualization tool where users can explore and label image patches with a certain concept. These labels are used to train a concept extractor network, enabling users to diagnose model predictions using the learned concept.</p>
    <p id="P11">However, these studies mainly focus on understanding the working mechanism of ML models and improving model performances (<italic toggle="yes">i.e</italic>., VIS for ML). How to utilize explainable latent vectors for concept-driven data exploration (<italic toggle="yes">i.e</italic>., XAI for VIS) has not been extensively discussed. Drava is built upon previous visual analytics studies on latent vectors and ML models. Unlike previous studies, Drava focuses on aligning interpretable latent vectors with human concepts to assist concept-driven data exploration.</p>
    <p id="P12">Second, Drava learns the visual representation and supports <bold>the exploration of small multiples</bold> [<xref rid="R57" ref-type="bibr">57</xref>], a series of miniature visualizations that represent different facets, subsets, or instances of a dataset. Current studies in data visual exploration usually present small multiples as points (<italic toggle="yes">e.g</italic>., [<xref rid="R7" ref-type="bibr">7</xref>, <xref rid="R16" ref-type="bibr">16</xref>, <xref rid="R47" ref-type="bibr">47</xref>, <xref rid="R51" ref-type="bibr">51</xref>]), glyphs (<italic toggle="yes">e.g</italic>., [<xref rid="R29" ref-type="bibr">29</xref>, <xref rid="R63" ref-type="bibr">63</xref>]), or images (<italic toggle="yes">e.g</italic>., [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R27" ref-type="bibr">27</xref>, <xref rid="R37" ref-type="bibr">37</xref>]) and place them in a grid, a dimension reduction projection, or a data-driven layout. For example, Sharkzor [<xref rid="R27" ref-type="bibr">27</xref>] enabled users to interactively organize images and their groups while providing visual cues for groups (<italic toggle="yes">e.g</italic>., badges). AxiSketcher [<xref rid="R29" ref-type="bibr">29</xref>] uses glyph representations and offers sketch-based interactions to flexibly arrange data items in the 2D space. Even though these studies provided valuable insights, they provide limited support in inspecting and summarizing a group of small multiples, which are important to reveal and remove the mismatches between human concepts and ML semantic dimensions. Some interaction techniques have been proposed to better organize small multiples and facilitate the exploration, such as interactive piling [<xref rid="R4" ref-type="bibr">4</xref>, <xref rid="R30" ref-type="bibr">30</xref>, <xref rid="R33" ref-type="bibr">33</xref>] and hierarchical clustering [<xref rid="R6" ref-type="bibr">6</xref>, <xref rid="R31" ref-type="bibr">31</xref>]. For example, interactive piling is inspired by physical piles and enables users to effectively group, aggregate, browse, and compare small multiples. However, these interactions are usually designed for specific application scenarios and cannot be directly applied to concept-driven exploration. In Drava, we adapt interactive piling to facilitate the concept-driven exploration of small multiples, especially focusing on the interpretation of semantic dimensions, the mismatch identification between ML semantic dimensions and human concepts, and guidance on refining semantic dimensions.</p>
    <p id="P13">Third, to better guide user exploration and insight generation, researchers have proposed <bold>interactive ML for visual data exploration</bold>, which learns what visual concepts are important to users from user feedback [<xref rid="R5" ref-type="bibr">5</xref>, <xref rid="R10" ref-type="bibr">10</xref>, <xref rid="R15" ref-type="bibr">15</xref>, <xref rid="R32" ref-type="bibr">32</xref>, <xref rid="R61" ref-type="bibr">61</xref>]. For example, Behrisch <italic toggle="yes">et al</italic>. [<xref rid="R5" ref-type="bibr">5</xref>] trained a classifier to interactively capture users‚Äô notion of interestingness when exploring many scatter plots. This classifier is then used to recommend potentially interesting plots and guide the exploration of large multidimensional data. Cai <italic toggle="yes">et al</italic>.[<xref rid="R10" ref-type="bibr">10</xref>] provides an interactive tool that empowers users to refine an ML model by communicating what types of similarities are most important when searching certain medical images. Peax [<xref rid="R32" ref-type="bibr">32</xref>] proposes an efficient and accurate query of a certain visual pattern in sequential data by learning from users‚Äô binary feedback on samples selected through active learning strategy. However, prior studies mainly use interactive ML to assist with similarity queries, <italic toggle="yes">i.e</italic>., modeling the similarity between items and user-selected targets. Despite the helpful guidance that these studies provide in data exploration, they cannot provide a comprehensive overview of the analyzed data.</p>
    <p id="P14">Like these approaches, Drava employs learning from user input to provide more precise exploration guidance. Furthermore, Drava provides semantic dimensions and supports summarization, exploration, and analysis based on different visual concepts.</p>
  </sec>
  <sec id="S4">
    <label>4</label>
    <title>WORKFLOW AND TASKS</title>
    <p id="P15">In this section, we decompose the overall goal of <italic toggle="yes">concept-driven visual exploration using DRL</italic> into three main steps (<xref rid="F3" ref-type="fig">Figure 3</xref>). We discuss the user tasks within each step from two aspects: the characteristics of DRL, as discussed in the DRL literature [<xref rid="R8" ref-type="bibr">8</xref>, <xref rid="R22" ref-type="bibr">22</xref>, <xref rid="R28" ref-type="bibr">28</xref>]; and the user needs in visual data exploration, largely informed by the task summarization work in previous studies [<xref rid="R16" ref-type="bibr">16</xref>, <xref rid="R33" ref-type="bibr">33</xref>, <xref rid="R37" ref-type="bibr">37</xref>]. These user tasks have been well established in previous studies and can be reused to effectively guide the design of Drava. Moreover, reuses in the task analysis can increase the design quality and reduce expenditure, as recommended in [<xref rid="R44" ref-type="bibr">44</xref>, <xref rid="R55" ref-type="bibr">55</xref>, <xref rid="R56" ref-type="bibr">56</xref>].</p>
    <sec id="S5">
      <title><bold>Step 1: Interpret ML Semantic Dimensions</bold>.</title>
      <p id="P16">Since only a subset of the latent dimensions correlates with semantic meanings, users should be assisted to <italic toggle="yes">identify the semantic dimensions efficiently</italic> (<bold>T1.1</bold>). For a specific dimension, users can <italic toggle="yes">interpret its semantic meaning</italic> (<bold>T1.2</bold>) through 1) synthesized images generated by single value traversal of this dimension or 2) data items sorted and grouped by their value in this dimension. A group summary can help users to efficiently understand the semantic meaning of a large number of items, associate it with a human concept, and identify mismatches. Unlike previous studies that group items based on their overall similarities, concept-based analysis requires to group and summarize items based on certain concepts. Therefore, proper aggregations should be provided to <italic toggle="yes">highlight the concept of interest and fade out others</italic> (<bold>T1.3</bold>) when summarizing an item group.</p>
    </sec>
    <sec id="S6">
      <title>Step 2: Align ML Semantic Dimensions with Human Concepts.</title>
      <p id="P17">Once a mismatch is identified, users modify the semantic dimension to better align it with the human‚Äôs definition of concepts. Such <italic toggle="yes">refinement should be user-friendly and conducted upon objects that users are familiar with</italic> (<bold>T2.1</bold>), <italic toggle="yes">e.g</italic>., data items and item groups rather than numerical values of latent dimensions. Meanwhile, <italic toggle="yes">visual cues should be provided to guide and facilitate the user refinement</italic> (<bold>T2.2</bold>), <italic toggle="yes">e.g</italic>., highlight the items that are grouped wrongly due to a concept mismatch.</p>
    </sec>
    <sec id="S7">
      <title>Step 3: Generate New Human Knowledge about the Data.</title>
      <p id="P18">Users <italic toggle="yes">explore the data items based on the identified concepts</italic> (<bold>T3.1</bold>) to generate insights about the analyzed items, including the distribution of items on one or multiple visual concepts, the association between different concepts. Such analysis can be further enhanced by <italic toggle="yes">correlating the concepts with other item metadata</italic> (<bold>T3.2</bold>), such as the spatial information and the item labels.</p>
      <p id="P19">The three steps are interconnected (<italic toggle="yes">i.e</italic>., arrows in <xref rid="F3" ref-type="fig">Figure 3</xref>). For example, users may directly go to Step 3 from Step 1 if they do not observe obvious mismatches. Users can also go back from Step 3 to Step 2 if they find some semantic dimensions fail to support their analysis tasks and require further refinement. Drava provides a set of dedicated interactive visualizations and algorithms that are closely coupled with this three-step workflow.</p>
    </sec>
  </sec>
  <sec id="S8">
    <label>5</label>
    <title>VISUAL INTERFACE</title>
    <p id="P20">The user interface of Drava (<xref rid="F4" ref-type="fig">Figure 4</xref>) consists of a <italic toggle="yes">Concept View</italic>, an <italic toggle="yes">Item Browser</italic>, and an optional <italic toggle="yes">Spatial View</italic>. The interactions related to visual piles are based on the design space proposed by Lekschas <italic toggle="yes">et al</italic>. [<xref rid="R33" ref-type="bibr">33</xref>], selected, modified, and extended to better reflect tasks described in <xref rid="S4" ref-type="sec">section 4</xref>.</p>
    <sec id="S9">
      <label>5.1</label>
      <title>Concept View</title>
      <p id="P21">In the <italic toggle="yes">Concept View</italic> (<xref rid="F4" ref-type="fig">Figure 4a</xref>), each latent dimension is visualized as a histogram and a list of synthesized images. The histogram shows the distribution of all items based on their values on the corresponding latent dimension (<bold>T3.1</bold>). Since the exact values of a latent dimension do not have specific meanings, we use synthesized images rather than numbers as the tick labels of the <italic toggle="yes">x</italic>-axis in the histogram. The synthesized images are generated by the decoder in the DRL model. For one specific latent dimension, the synthesized images are generated using a set of latent vectors whose values only differ on this dimension. These synthesized images illustrate the visual changes associated with the value traversal on the investigated dimension and help users understand its semantics (<bold>T1.2</bold>). Users can filter items based on their values on specific semantic dimensions by clicking on bars of a histogram (<xref rid="F4" ref-type="fig">Figure 4F</xref>).</p>
      <p id="P22">As explained in <xref rid="S2" ref-type="sec">section 2</xref>, only a subset of the latent dimensions are semantic and correlate with human concepts [<xref rid="R8" ref-type="bibr">8</xref>, <xref rid="R28" ref-type="bibr">28</xref>]. Therefore, it is important to provide a mechanism that guides users in the exploration of a potentially large number of dimensions. Drava calculates a salience score for each latent dimension (<xref rid="S16" ref-type="sec">subsection 6.3</xref>), indicating how important a particular latent dimension is for the synthesized images. As shown in <xref rid="F4" ref-type="fig">Figure 4A</xref>, all latent dimensions are ranked based on their salience scores, and the normalized score of each latent dimension is visualized by the width of a gray bar (<bold>T1.1</bold>). Users can change the dimension name based on their interpretation of the associated concept to facilitate the following analysis. Users can also remove irrelevant dimensions and add other customized dimensions from the item metadata.</p>
    </sec>
    <sec id="S10">
      <label>5.2</label>
      <title>Item Browser</title>
      <p id="P23">The <italic toggle="yes">Item Browser</italic> (<xref rid="F4" ref-type="fig">Figure 4b</xref>) layouts all items in a 2D space where users can freely arrange and group items. Arranging items based on their values of certain semantic dimensions enables users to interpret semantic dimensions and understand the item distribution among a certain concept (<bold>T3.1</bold>). A set of synthesized images are added to the <italic toggle="yes">x</italic>‚Äì and/or <italic toggle="yes">y</italic>‚Äìaxis to guide the interpretation of latent semantic dimensions and the exploration of data items (<xref rid="F4" ref-type="fig">Figure 4B</xref>). Since the visual appearance of the synthesized images largely depends on the latent vector, Drava allows users to select an item and use its latent vectors to generate synthesized images. This interaction enables users to further validate the concept associated with a latent dimension and identify possible mismatches (<bold>T1.2</bold>).</p>
      <p id="P24">Since the number of items can be large and the items often overlap with each other, effective grouping and summarizing mechanisms are needed. In Drava, users can either manually group items using a lasso selection or automatically group items based on their proximity in the 2D space. Drava provides various methods for summarizing a group of items and revealing abnormal items inside this group (<bold>T1.3</bold>), as shown in <xref rid="F5" ref-type="fig">Figure 5a</xref>‚Äì<xref rid="F5" ref-type="fig">b</xref>. When items are arranged horizontally (<italic toggle="yes">i.e</italic>., 1D grouping), items will be stacked along the vertical direction, and each item will be visualized as an item preview. Users can select the grouping method in the configure panel based on the characteristics of items and concepts.</p>
      <p id="P25">Labels can also be added to individual items or item groups to incorporate more item metadata into the analysis and investigate their associations with concepts, as shown in <xref rid="F5" ref-type="fig">Figure 5c</xref> (<bold>T3.2</bold>). To investigate more details about an item group, users can browse items by hovering on their item previews (<xref rid="F4" ref-type="fig">Figure 4D</xref>). A pop-up menu, shown upon right-clicking on an item group, enables users to depile this group or browse the items in a separate window.</p>
    </sec>
    <sec id="S11">
      <label>5.3</label>
      <title>Spatial View</title>
      <p id="P26">The <italic toggle="yes">Spatial View</italic> (<xref rid="F4" ref-type="fig">Figure 4c</xref>) is an optional view for data items that have spatial/context information. For example, in <xref rid="F4" ref-type="fig">Figure 4</xref>, each item indicates a region of interest in a huge genomic interaction matrix and is arranged according to its genomic location. Users can zoom and pan to obtain an overview or inspect further details. The <italic toggle="yes">Spatial View</italic> is coordinated with other views to reveal the correlations between concepts and item context (<bold>T3.2</bold>). When users filter items in the <italic toggle="yes">Concept View</italic>, the corresponding items will fade out in the <italic toggle="yes">Spatial View</italic>.</p>
    </sec>
    <sec id="S12">
      <label>5.4</label>
      <title>User Refinement</title>
      <p id="P27">Instead of directly modifying the hard-to-interpret latent values, Drava supports refinement towards groups and items (<bold>T2.1</bold>). For one selected semantic dimension <italic toggle="yes">D</italic><sub><italic toggle="yes">i</italic></sub>, Drava groups items (21 bins by default) based on their values of this dimension <italic toggle="yes">d</italic><sub><italic toggle="yes">i</italic></sub> to represent the gradual changes associated with this dimension. First, this default group assignment may have inappropriate thresholds, <italic toggle="yes">e.g</italic>., assigning similar items into two adjacent groups. Therefore, Drava enables users to merge (<xref rid="F6" ref-type="fig">Figure 6b1</xref>) or split (<xref rid="F6" ref-type="fig">Figure 6b2</xref>) groups to construct more meaningful groups according to one concept. More importantly, due to the imperfection of algorithms, the latent values may not accurately depict the concept for certain items, leading to inappropriate <italic toggle="yes">x</italic> position and group assignment for these items. Users can align the concepts and semantic dimensions by changing the item position (<xref rid="F6" ref-type="fig">Figure 6b1</xref>) and reassigning the group of these items (<xref rid="F6" ref-type="fig">Figure 6b3</xref>).</p>
      <p id="P28">Several mechanisms are provided to assist users in locating abnormal items and groups (<bold>T2.2</bold>), as shown in <xref rid="F6" ref-type="fig">Figure 6a</xref>. First, users can decide whether to merge or split groups by comparing these groups side by side (a1). Second, Drava enables users to identify abnormal items through previews (a2). For example, as shown in <xref rid="F4" ref-type="fig">Figure 4C</xref>, all items are grouped based on the thickness of their diagonal. Users can locate an abnormal item because its preview is darker than others. Users then examine this item through in-place browsing (<xref rid="F4" ref-type="fig">Figure 4D</xref>), extract it using the pop-up menu (<xref rid="F4" ref-type="fig">Figure 4E</xref>), and drag and drop it to a proper group based its diagonal thickness. Apart from identifying abnormal items through previews, users can also browse a group in a separate window and arrange the items using selected metrics (a3). In our experiments, we found certain metric values are useful in identifying abnormal items, including the reconstruction loss, the deviation of the latent value, the item metadata, and the uncertainty score.</p>
      <p id="P29">After user refinement, Drava supports two mechanisms, local and global, to update the items and/or the underlying model (<xref rid="F6" ref-type="fig">Figure 6c</xref>). By default, Drava employs a local updating mechanism, which remembers the user refinement, applies it to items with similar latent vectors, but does not modify the underlying model. Similar items are defined by setting a threshold <italic toggle="yes">Œ∏</italic> to the <italic toggle="yes">L</italic>2 distances of their latent vectors to the that of the refined items. On the contrary, global update initializes and fine-tunes a concept adaptor (<xref rid="S15" ref-type="sec">subsection 6.2</xref>). The values for all other items at this dimension will be updated accordingly by this concept adaptor. The global refinement is triggered by clicking the <italic toggle="yes">update concept</italic> button. Since it is hard for users to label an item with an exact numerical value, global refinement can only be triggered when items are grouped for a certain concept.</p>
    </sec>
  </sec>
  <sec id="S13">
    <label>6</label>
    <title>MODEL SETUP AND IMPLEMENTATION</title>
    <sec id="S14">
      <label>6.1</label>
      <title>Learning Semantic Dimensions using DRL</title>
      <p id="P30">Our DRL model is based on the <italic toggle="yes">Œ≤</italic>-VAE [<xref rid="R22" ref-type="bibr">22</xref>]. The structure of the DRL model is illustrated in <xref rid="F7" ref-type="fig">Figure 7</xref>, encoder (a) and decoder (b). Each convolution block consists of a convolution layer, a batch normalization layer, and a leaky ReLU (Rectified Linear Unit) activation. The decoder architecture is the transpose of the encoder. Following the practice in [<xref rid="R53" ref-type="bibr">53</xref>], we do not include Max Pooling layers by setting <italic toggle="yes">stride</italic> = 2 in the convolution layer. All usage scenarios in this paper use this structure and only vary in 1) the number of convolution and transposed convolution blocks, 2) the kernel size and the number of channels of the convolution and transposed convolution layers, and 3) the output size of the fully connected layer (<italic toggle="yes">i.e</italic>., the number of dimensions for the latent vector).</p>
      <p id="P31">We use the loss function proposed by Burgess <italic toggle="yes">et al</italic>. [<xref rid="R8" ref-type="bibr">8</xref>], which progressively increases the information capacity during the training process. An Adam optimization is used to train the model. Note that we use the mean <italic toggle="yes">Œº</italic> of the normal distribution learned by the encoder rather than the sampled <inline-formula><mml:math id="M3" display="inline"><mml:mrow><mml:mi>z</mml:mi><mml:mo>~</mml:mo><mml:mo>ùí©</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Œº</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>œÉ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as the latent vector for the input data, which enables deterministic latent values for each data item.</p>
      <p id="P32">Even though we implement and evaluate Drava using <italic toggle="yes">Œ≤</italic>-VAE, the proposed framework can be easily adapted to other VAE-based DRL models, such FactorVAE [<xref rid="R28" ref-type="bibr">28</xref>] and <italic toggle="yes">Œ≤</italic>-TCVAE [<xref rid="R11" ref-type="bibr">11</xref>].</p>
    </sec>
    <sec id="S15">
      <label>6.2</label>
      <title>Concept Adaptor</title>
      <p id="P33">The concept adaptor is a lightweight model that modifies semantic dimensions based on user refinements. For each semantic dimension, one concept adaptor will be generated if users use this dimension to arrange items, refine the item groups, and apply a global update. Since it is hard for users to associate the concept with an exact numerical value, the concept adaptor is only used to refine a concept for already grouped items. In other words, the concept adaptor is a multi-class classifier. The concept adaptor uses the feature map generated by the encoder hidden layer as input and predicts the group that the input item should belong to.</p>
      <p id="P34"><xref rid="F7" ref-type="fig">Figure 7c</xref> illustrates the structure of the concept adaptor. The convolution block contains a convolution layer (kernel size =4, stride =2) and a batch normalization. The convolution layer has <italic toggle="yes">n</italic> output channels where <italic toggle="yes">n</italic> equals to the number of item groups. A <italic toggle="yes">n</italic> √ó 1 vector will be obtained after a global max pooling layer and then feed into a softmax function. A cross entropy is used to calculate the loss. An Adam optimization is used to train the model.</p>
      <p id="P35">Once items are grouped based on the values of one dimension, users can initialize a concept adaptor accordingly. The training ends when the validation loss does not decrease. For all the datasets used in <xref rid="S22" ref-type="sec">section 8</xref>, the initialization took less than two minutes on a machine with one Tesla K80 GPU. After users have refined the item groups (<italic toggle="yes">i.e</italic>., change the classification label) for some items, the concept adaptor will be fine-tuned accordingly. During the fine-tuning, we increase the weight of the items that have been refined by the users. For the back-end models, only the concept adaptor is updated with user refinement, while the encoder and decoder are fixed. For the data items, only the values of the specific latent dimension (<italic toggle="yes">i.e</italic>., dimension used as the <italic toggle="yes">x</italic> axis) will be updated by the concept adaptor, while other dimensions will remain the same.</p>
    </sec>
    <sec id="S16">
      <label>6.3</label>
      <title>Dimension Ranking</title>
      <p id="P36">We rank all latent dimensions based on their importance to help users quickly locate semantic dimensions. Inspired by the salience scores used in interpretable ML [<xref rid="R26" ref-type="bibr">26</xref>], we gauge the importance of a latent dimension via the sensitivity of the reconstructed image <inline-formula><mml:math id="M4" display="inline"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> to changes in the magnitude of a latent dimension <italic toggle="yes">z</italic><sub><italic toggle="yes">i</italic></sub>. However, directly using the gradients <inline-formula><mml:math id="M5" display="inline"><mml:mrow><mml:mo>‚àÇ</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>/</mml:mo><mml:mo>‚àÇ</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> has several issues. First, it is a local importance score that is calculated for a particular reconstructed image. Second, it is a vector rather than a scalar value and can be hard to compare across. Third, it counts pixel-level differences that are not necessarily consistent with human perception. To solve these issues, we use a simple but effective method, <italic toggle="yes">i.e</italic>., averaging the importance score across output dimensions and across a set of sampled latent vectors. To mimic human perception of the synthesized images, we use latent vectors of the synthesized images as samples. Instead of using the reconstructed output, we use the feature maps generated by the second last layer of the decoder, aiming to capture high-level features rather than pixel-to-pixel differences.
<disp-formula id="FD2"><mml:math id="M6" display="block"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>m</mml:mi><mml:mo>√ó</mml:mo><mml:mi>n</mml:mi><mml:mo>√ó</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfrac><mml:mstyle><mml:munder><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>‚àÇ</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>‚àÇ</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula>
Where <italic toggle="yes">z</italic><sub><italic toggle="yes">k</italic></sub> is the <italic toggle="yes">k</italic><sub><italic toggle="yes">th</italic></sub> sampled latent vector, <italic toggle="yes">z</italic><sub><italic toggle="yes">k,i</italic></sub> is its value at dimension <italic toggle="yes">i</italic>, <italic toggle="yes">L</italic><sub><italic toggle="yes">m,n</italic></sub> (<italic toggle="yes">z</italic><sub><italic toggle="yes">k</italic></sub>) is the feature map (<italic toggle="yes">m, n</italic>) of the second to last decoder layer.</p>
      <p id="P37">The salience score serves as a useful indicator for semantic dimensions (<xref rid="F8" ref-type="fig">Figure 8</xref>). Theoretically, a dimension with a high salience score is not necessarily equal to a semantic dimension, <italic toggle="yes">e.g</italic>., a dimension is not semantic but significantly influences the output. However, the DRL model will minimize the existence of such dimensions by disentangling features and encoding them as separate dimensions. Ranking all dimensions based on salience scores can help users exclude many latent dimensions that do not contribute to the output and have little semantic meanings (<italic toggle="yes">e.g</italic>., <xref rid="F8" ref-type="fig">Figure 8b</xref>).</p>
    </sec>
    <sec id="S17">
      <label>6.4</label>
      <title>Implementation</title>
      <p id="P38">The implementation of Drava includes a front-end for interactive visualization and a back-end for data storage and the DRL model. The front-end is implemented in TypeScript using React [<xref rid="R17" ref-type="bibr">17</xref>], Piling.js [<xref rid="R33" ref-type="bibr">33</xref>], and Gosling.js [<xref rid="R39" ref-type="bibr">39</xref>]. The visualizations are rendered using SVG, Canvas, and WebGL. The back-end DRL model and concept adaptor are implemented in Python with PyTorch [<xref rid="R45" ref-type="bibr">45</xref>]. The front-end and back-end communicate via a Flask [<xref rid="R19" ref-type="bibr">19</xref>] web server built in Python. Users can easily apply Drava to their own datasets through two YAML configuration files that configure the back-end model training process and the front-end interface, respectively. The source code and documentation are available at <ext-link xlink:href="https://qianwen.info/DRAVA/" ext-link-type="uri">https://qianwen.info/DRAVA/</ext-link>.</p>
    </sec>
  </sec>
  <sec id="S18">
    <label>7</label>
    <title>EXPERIMENTAL VALIDATION</title>
    <p id="P39">In this section, we evaluated the back-end model in Drava from three aspects: 1) the <italic toggle="yes">representativeness</italic> of the latent vector, 2) the <italic toggle="yes">semantic meaning</italic> of individual latent dimensions, and 3) the improvements from <italic toggle="yes">concept fine-tuning</italic>. Previous studies either focused on assessing the disentanglement of latent dimensions [<xref rid="R8" ref-type="bibr">8</xref>, <xref rid="R22" ref-type="bibr">22</xref>, <xref rid="R28" ref-type="bibr">28</xref>] or overlooked the possible mismatches between human concepts and semantic dimensions [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R20" ref-type="bibr">20</xref>, <xref rid="R59" ref-type="bibr">59</xref>]. Therefore, it is important to validate the quality of these semantic latent vectors and their fine-tuning mechanism.</p>
    <sec id="S19">
      <title>Representativeness of the Latent Vector.</title>
      <p id="P40">We used the reconstruction quality to show whether the latent vectors can capture all the important visual features of the input data. <xref rid="F9" ref-type="fig">Figure 9</xref> exemplifies the reconstruction quality of the latent vectors for the four datasets used in the application scenarios (<xref rid="S22" ref-type="sec">section 8</xref>). Instead of the absolute similarity or the realism of the reconstructed images, we focused on evaluating whether the reconstructed images are able to capture important concepts. For the relatively simple <italic toggle="yes">dsprites</italic> shapes dataset (b), the model is able to generate images that are very similar to the input data. For more complex datasets (a, c‚Äìd), even though some details in the input data are missing, the model can reconstruct salient concepts.</p>
    </sec>
    <sec id="S20">
      <title>Semantic Meaning of Individual Latent Dimensions.</title>
      <p id="P41">To evaluate whether a single latent dimension can sufficiently depict a concept, we classified items based on their values on a certain semantic dimension and reported the classification accuracy. Specifically, for <italic toggle="yes">n</italic> classes belonging to a concept, <italic toggle="yes">n</italic> ‚àí 1 thresholds are learned to classify items. For example, the ‚Äúsmiling‚Äù concept has two classes, smiling and not smiling. We first identified a latent dimension <italic toggle="yes">D</italic><sub><italic toggle="yes">i</italic></sub> that is related to the ‚Äúsmiling‚Äù concept. We then classified each item based on whether its value on this dimension <italic toggle="yes">d</italic><sub><italic toggle="yes">i</italic></sub> is larger or smaller than a threshold <italic toggle="yes">thr</italic>, which was chosen to maximize the classification accuracy of all items. We used the <italic toggle="yes">dsprites</italic> and the <italic toggle="yes">CelebA</italic> datasets because they have labels for a diverse set of concepts. The results in <xref rid="T1" ref-type="table">Table 1</xref> demonstrated that the latent dimension value could effectively represent the corresponding concept but also showed space for further improvement.</p>
    </sec>
    <sec id="S21">
      <title>Improvements from Concept Fine-tuning.</title>
      <p id="P42">We evaluated the fine-tuning mechanism of the concept adaptor by comparing the classification accuracy of a specific concept before and after user refinement. This evaluation used the ‚Äúscale‚Äù concept from the <italic toggle="yes">dsprites</italic> dataset and the ‚Äúsmiling‚Äù and ‚Äúbangs‚Äù concepts from the <italic toggle="yes">CelebA</italic> dataset, because they have relatively low accuracy without any human refinement (<xref rid="T1" ref-type="table">Table 1</xref>). We chose an active learning method as the baseline for evaluating the concept adaptor. The baseline had the same architecture as the concept adaptor. We used simulated user feedback to obtain reproducible results in a variety of settings. Following the common practices in evaluating interactive machine learning [<xref rid="R13" ref-type="bibr">13</xref>] and active learning [<xref rid="R49" ref-type="bibr">49</xref>], we simulated user feedback as an oracle (<italic toggle="yes">i.e</italic>., always providing correct labels to the queried items). Both the concept adaptor and the baseline used the same simulation at each iteration but with different initialization. The active learning baseline is initialized with 5% labels. The concept adaptor is initialized with no labels but the same item groups as that in <xref rid="T1" ref-type="table">Table 1</xref>. Such an initialization simulates how users would divide items into several groups for a specific concept based on their latent dimension values. At each iteration, <italic toggle="yes">N</italic> items were refined (for the concept adaptor) or labeled (for the baseline) and models were trained until the validation loss did not decrease, which typically took around 10‚Äì20 epochs and less than 20 seconds. We experimented with three metrics for selecting the <italic toggle="yes">N</italic> items: uncertainty scores of the classification, the standard deviation of the latent dimension value, and differences between the latent dimension value and the classification threshold. We found that refining items with the highest uncertainty score led to the best model performances. Even though we used an oracle to simulate user refinement here, real-world users can easily examine and label these items in Drava by selecting a metric of interest as the <italic toggle="yes">y</italic> axis in <italic toggle="yes">Item Browser</italic>.</p>
      <p id="P43">We ran experiments under three settings: <italic toggle="yes">N</italic> = 1%, 2%, and 5% of the items. A total of 15 iterations were performed for each experiment. The results in <xref rid="F10" ref-type="fig">Figure 10</xref> were obtained by averaging the results of three experiments. First, the increased accuracy indicated that the concept adaptor helped align a concept and a semantic latent dimension. Compared with the baseline, the concept adaptor generated more accurate concepts by leveraging the values of the semantic dimension. Second, the curves of the concept adaptor were more smooth than the baseline, indicating a more stable improvement over iterations. Third, while the concept adaptor and the baseline required the same amount of user effort at each iteration (<italic toggle="yes">i.e</italic>., the same <italic toggle="yes">N</italic> and the same user simulation), the concept adaptor required less user effort at the initialization than the baseline (<italic toggle="yes">i.e</italic>., drawing two or three lasso selections vs. labeling 5% of the items one by one). Fourth, it was not surprising that the difference between the concept adaptor and the baseline model decreased with the increase of <italic toggle="yes">N</italic> and iteration steps. The advantages of the concept adaptor mainly result from using the semantic dimension values. As more and more items are labeled, these semantic dimensions become less useful in describing a concept.</p>
    </sec>
  </sec>
  <sec id="S22">
    <label>8</label>
    <title>APPLICATION SCENARIOS</title>
    <p id="P44">In this section, we present four application scenarios of Drava using one simulated dataset and three real-world datasets. For all four application scenarios, the DRL model is trained on the whole dataset with no labels used. The four application scenarios are conducted under collaboration with domain users, including two postdoctoral researchers on computer vision (P1 and P2, both for <xref rid="S23" ref-type="sec">subsection 8.1</xref> and <xref rid="S27" ref-type="sec">subsection 8.2</xref>), two researchers on genomic analysis (P3 and P4, for <xref rid="S32" ref-type="sec">subsection 8.3</xref>), and a professor on histopathological image analysis (P5, for <xref rid="S37" ref-type="sec">subsection 8.4</xref>). For each application scenario, we first provided a tutorial to introduce the functionalities of Drava. We then demonstrate our analysis and validate our findings with the participants. Participants can freely explore Drava and conduct additional analysis on the provided dataset. We further collected qualitative feedback about Drava from the participants.</p>
    <sec id="S23">
      <label>8.1</label>
      <title>Simple Shapes</title>
      <sec id="S24">
        <title>Data, Model, and Analysis Overview.</title>
        <p id="P45">This scenario uses the <italic toggle="yes">dsprites</italic> dataset [<xref rid="R40" ref-type="bibr">40</xref>], which consists of three types of simple shapes (<italic toggle="yes">i.e</italic>., square, ellipse, heart) with different scales, positions, and orientations. We uniformly sampled 1,000 items. The DRL model has four convolution blocks, each of which has 32 channels, a kernel of size 4, and a stride of 2. The latent vector has 8 dimensions. Even though this is a simple dataset, it can work as a proxy for more complicated datasets, such as the bounding boxes in object detection or the masks for cell segmentation. In this scenario, we explore the distribution of items according to concepts related to position and size, which are identified, validated, and refined by users.</p>
      </sec>
      <sec id="S25">
        <title>Arranging Items based on Concepts of Interest.</title>
        <p id="P46">To start with, we display all items in a 2D space using UMAP, a dimension reduction method that is commonly used for visualizing items with latent vectors. While the UMAP successfully put items with similar shapes and scales close to one another, the shape position information is mostly ignored, as shown in <xref rid="F11" ref-type="fig">Figure 11a</xref>. The position information can be important for some analysis tasks, <italic toggle="yes">e.g</italic>., object detection in autopilot.</p>
        <p id="P47">Based on the synthesized images in the <italic toggle="yes">Concept View</italic>, the position-related information is successfully extracted in two top-ranked dimensions, which we rename to <monospace>dim_x</monospace> and <monospace>dim_y</monospace> (<xref rid="F1" ref-type="fig">Figure 1e</xref>). As shown in <xref rid="F11" ref-type="fig">Figure 11b</xref>, all items are arranged and grouped based on the <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> position of the shape. We choose the <italic toggle="yes">average</italic> method to summarize a group, which enables us to inspect the positions of shapes without browsing individual items one by one.</p>
      </sec>
      <sec id="S26">
        <title>Refine a Semantic Dimension.</title>
        <p id="P48">The scale, <italic toggle="yes">i.e</italic>., size, of the shapes is also a vital piece of information for some analyses and has been successfully extracted in a latent dimension (named as <monospace>dim_size</monospace>). We verify this semantic dimension in the <italic toggle="yes">Item Browser</italic>, setting <monospace>dim_size</monospace> as <italic toggle="yes">x</italic> axis and its deviation <italic toggle="yes">œÉ</italic> as the <italic toggle="yes">y</italic> axis. While all items are sorted based on their size from left to right, we find that items on the left side are all squares (<xref rid="F11" ref-type="fig">Figure 11c1</xref>). We speculate this is because an ellipse or a heart, even with the same scale, is smaller than a square in terms of absolute pixel areas.</p>
        <p id="P49">To obtain a semantic dimension that better matches the analysis purpose and indicates the scale regardless of shape types, we refine <monospace>dim_size</monospace> using the concept adaptor. We set the ‚Äúreconstruction loss‚Äù as the <italic toggle="yes">y</italic> axis to reveal abnormal items (<xref rid="F11" ref-type="fig">Figure 11c2</xref>) and modify the <italic toggle="yes">x</italic> position of these items. We then group the items into three main groups, indicating large, medium, and small scales, respectively. After clicking the <italic toggle="yes">update concept</italic> button, the concept adaptor is initialized based on our grouping. We further refine these groups using the <italic toggle="yes">browse separately</italic> function, examining each group and updating the group mainly by moving items of ellipse or heart shape from the medium group to the large group. After several updates, we click the <italic toggle="yes">update concept</italic> button again. The concept adaptor is fine-tuned based on the refined item groups and updates the grouping of all items. After several iterations, we obtain three groups that more accurately reflect the scale of shapes without the influence of shape types (refer to <xref rid="S18" ref-type="sec">section 7</xref> for quantitative results).</p>
      </sec>
    </sec>
    <sec id="S27">
      <label>8.2</label>
      <title>Celebrity Images</title>
      <sec id="S28">
        <title>Data, Model, and Analysis Overview.</title>
        <p id="P50">This usage scenario uses the celebrity images from the <italic toggle="yes">CelebA</italic> dataset [<xref rid="R38" ref-type="bibr">38</xref>]. The DRL model is trained on the complete dataset, and we randomly sample 1,000 items for the exploration in Drava. The DRL model has five convolution blocks, each of which contains a kernel of size 3, a stride of 2, and 32, 64, 128, 256, and 512 channels, respectively. The latent vector has 20 dimensions. In this scenario, we investigate the quality of the <italic toggle="yes">CelebA</italic> dataset based on the diversity, balance, and association of the concepts in this dataset.</p>
      </sec>
      <sec id="S29">
        <title>Examine Dataset Diversity.</title>
        <p id="P51">Collecting a diverse dataset is important in ML to improve the model performance in real-world deployment and avoid algorithmic discrimination of certain populations [<xref rid="R65" ref-type="bibr">65</xref>]. The concepts extracted by Drava offer an effective approach to investigating the diversity of a dataset.</p>
        <p id="P52">Based on the synthesized images in the <italic toggle="yes">Concept View</italic>, we can affirm that diverse visual concepts exist in the analyzed data items. The analyzed items vary in a number of aspects, including emotional expression, gender, angle, skin color, background color, hair length, and hairstyle. To further verify our interpretation of the semantics of individual dimensions, we can interactively change the latent vector to update the synthesized images and group items based on their latent values at a selected dimension (<xref rid="F1" ref-type="fig">Figure 1d</xref>).</p>
      </sec>
      <sec id="S30">
        <title>Investigate Dataset Balance.</title>
        <p id="P53">We then analyze the item distribution along individual concepts as dataset imbalance can introduce bias during model training and impair model performance. For example, for the ‚Äúskin color‚Äù concept, a dataset with a large number of items with fair skin and only a small number of items with dark skin can lead to an ML model that has poor performance on the latter. As shown in <xref rid="F12" ref-type="fig">Figure 12a</xref>, we arrange and group items based on <monospace>dim_9</monospace>, which captures skin color based on the synthesized images. Through browsing items in these groups, we find only the right several groups include people with dark skin (a1), indicating a relatively small portion. When we browse individual items in each group (a2), we can find that this portion is even smaller since the model considers people with dark skin and people with shadows on their faces as similar. This observation implies an imbalance related to skin color, which may introduce a bias into a model trained on it.</p>
      </sec>
      <sec id="S31">
        <title>Confirm Concept Association.</title>
        <p id="P54">Based on <xref rid="F12" ref-type="fig">Figure 12a</xref>, we suspect a correlation between dark skin and dark background. Such correlations can be treated as causalities by ML models [<xref rid="R68" ref-type="bibr">68</xref>] and need to be avoided. We confirm this suspicion by arranging all items using <monospace>dim_9</monospace> (skin tone) as the <italic toggle="yes">x</italic>-axis and <monospace>dim_16</monospace> (background darkness) as the <italic toggle="yes">y</italic>-axis. The resulting distribution (<xref rid="F12" ref-type="fig">Figure 12b</xref>) dispels our suspicion. Even though the distribution is not uniform, the dataset contains both items that have fair skin and dark background (b1) and items that have dark skin and light background (b2).</p>
      </sec>
    </sec>
    <sec id="S32">
      <label>8.3</label>
      <title>Genomic Interaction Matrix</title>
      <sec id="S33">
        <title>Data, Model, and Analysis Overview.</title>
        <p id="P55">This usage scenario uses a genome interaction matrix for the HFFc6 cell line published by Rao <italic toggle="yes">et al</italic>. [<xref rid="R48" ref-type="bibr">48</xref>]. The matrices describe the chromatin interactions between different genomic locations, which is related to the physical folding of DNA that affects the regulation of gene expression. In a genome interaction matrix, rows and columns represent genomic locations, and the color intensity indicates the interaction probability between a pair of locations. Experts typically examine regions of interest (ROI) that have unique visual patterns and indicate specific biological events. Since the size of the matrix is huge, <italic toggle="yes">i.e</italic>., 3 billion √ó 3 billion for human genomes, this analysis process is often laborious and time-consuming.</p>
        <p id="P56">We generate small multiples for one specific type of ROI called Topologically Associated Domains (TAD), which are visually represented as squares that are presumably organized hierarchically. We first extract TADs from the interaction matrix using OnTAD [<xref rid="R3" ref-type="bibr">3</xref>] and then use the DRL model to generate a latent vector for each TAD. We demonstrate Drava using the 855 TADs extracted from chromosome 5 of the HFFc6 cell line. The DRL model has three convolution blocks with filter sizes of 7, 5, 3 and channel sizes of 32, 64, 128, respectively. The latent vector has 8 dimensions.</p>
        <p id="P57">In this scenario, we investigate different types of TADs by identifying, validating, and refining concepts that correspond to important visual patterns of TADs. Guided by these concepts, we are able to locate different types of TADs and examine the spatial distribution of these TADs on the whole genome.</p>
      </sec>
      <sec id="S34">
        <title>Understand Data through Concepts.</title>
        <p id="P58">The visual appearance of TADs in a heatmap can serve as effective proxies of the underlying data patterns and biological events [<xref rid="R3" ref-type="bibr">3</xref>, <xref rid="R30" ref-type="bibr">30</xref>]. Therefore, by interpreting the visual concepts, we can inspect how the underlying data and the associated biological events vary among the analyzed items. In the <italic toggle="yes">Concept View</italic>, we identified three dimensions of interest. <monospace>Dim_7</monospace> (renamed as <monospace>dim_thick</monospace>) indicates the thickness of the diagonal (<italic toggle="yes">e.g</italic>., an item changing from <xref rid="F13" ref-type="fig">Figure 13a1</xref> to <xref rid="F13" ref-type="fig">a4</xref>), which is related to the resolution of the TAD on the matrix since we resize all TADs into a fixed pixel size for the DRL model. <monospace>Dim_0</monospace> indicates the asymmetry of the nested TAD structure (<italic toggle="yes">e.g</italic>., an item changing from <xref rid="F13" ref-type="fig">Figure 13</xref>a2 to a3). <monospace>Dim_6</monospace> (renamed as <monospace>dim_nest</monospace>) corresponds to whether a TAD data item contains additional nested squares (<italic toggle="yes">i.e</italic>., nested TAD such as a2, a3) or not (<italic toggle="yes">i.e</italic>., single TAD such as a1, a4). Other dimensions are either hard to interpret because there is little variation in the synthesized images or can not be associated with meaningful domain insights. <monospace>Dim_thick</monospace> and <monospace>dim_nest</monospace> are the top two dimensions based on the salience scores, indicating the usefulness of the dimension ranking. The three dimensions (<monospace>dim_thick</monospace>, <monospace>dim_0</monospace>, <monospace>dim_6</monospace>) correspond to important attributes of TADs, as described by An <italic toggle="yes">et al</italic>. [<xref rid="R3" ref-type="bibr">3</xref>].</p>
      </sec>
      <sec id="S35">
        <title>Verify and Refine Concepts.</title>
        <p id="P59">After obtaining a basic understanding of the semantic meaning of each dimension through their synthesized images, we further verify the three concepts one by one through grouping and browsing data items. Interestingly, we find that <monospace>dim_nest</monospace> confuses the thickness of the diagonal with the nested structure of TADs. As shown in <xref rid="F13" ref-type="fig">Figure 13b</xref>, items are grouped based on <monospace>dim_nest</monospace> and use ‚Äúpartial‚Äù to generate item previews. Users can identify items with thick diagonals from the item preview (as annotated by the orange marks) and examine them in detail by hovering over them. This issue can hardly be revealed through the synthesized images (<xref rid="F13" ref-type="fig">Figure 13c1</xref>), which are widely used as the only method to interpret semantic meanings in previous literature [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R59" ref-type="bibr">59</xref>]. This observation shows the importance of further verifying a concept base on data items and the need for user refinement.</p>
        <p id="P60">Since <monospace>dim_thick</monospace> can indicate the TAD size, we use it as the <italic toggle="yes">y</italic> axis to help refine the concept associated with <monospace>dim_nest</monospace>. As shown in <xref rid="F13" ref-type="fig">Figure 13c</xref>, items arranged in different vertical positions based on their diagonal thickness, enabling successful separation of nested TAD (<italic toggle="yes">e.g</italic>., a2, a3) from single TADs with thick diagonal (<italic toggle="yes">e.g</italic>., a4). Users can refine <monospace>dim_nest</monospace> by a lasso selection on all single TADs that have large <monospace>dim_nest</monospace> values and moving them to the left-most position (<italic toggle="yes">e.g</italic>., assigning them a small value for <monospace>dim_nest</monospace>), as shown in <xref rid="F13" ref-type="fig">Figure 13c3</xref>. The refinement is recorded using the local updating mechanism and applied to similar items.</p>
      </sec>
      <sec id="S36">
        <title>Locate items of interest.</title>
        <p id="P61">After the refinement, users can easily locate nested TADs in <xref rid="F13" ref-type="fig">Figure 13C4</xref> through a lasso selection. They can also filter these TADs based on <monospace>dim_thick</monospace> and <monospace>dim_nest</monospace> using their histograms. The nested structure in TADs is important to understand the boundary usage in gene regulation [<xref rid="R3" ref-type="bibr">3</xref>]. For this purpose, these identified items can be further examined in the <italic toggle="yes">Spatial View</italic> (<xref rid="F4" ref-type="fig">Figure 4</xref>), which reveals the genomic locations of these TADs and associated them with other context information (<italic toggle="yes">e.g</italic>., chromatin accessibility).</p>
      </sec>
    </sec>
    <sec id="S37">
      <label>8.4</label>
      <title>Breast Cancer Specimen</title>
      <sec id="S38">
        <title>Data, Model, and Analysis Overview.</title>
        <p id="P62">This usage scenario uses breast histopathology images downloaded from [<xref rid="R43" ref-type="bibr">43</xref>]. This dataset contains 277,524 patches (50 √ó 50 pixels) extracted from stained whole mount slide images of breast cancer specimens from 162 patients scanned at 40x magnification. The DRL model is trained on the whole dataset. In this usage scenario, we explore the 1,745 image patches from one patient. The DRL model has five convolution blocks, each with a kernel of size 3 and 32, 64, 128, 256, and 512 channels, respectively. The latent vector has 12 dimensions. In this scenario, we examine the presence of cancer cells in these items and analyze the performance of a classification model. Specifically, we identify concepts and associate them with domain semantics. We then use these concepts to describe the characteristics of hard-to-classify items.</p>
      </sec>
      <sec id="S39">
        <title>Interpret Visual Concepts and Assign Domain Semantics.</title>
        <p id="P63">We first visualize all the items using UMAP (<xref rid="F1" ref-type="fig">Figure 1a</xref>). However, the UMAP projection is not ideal since it is based on the overall similarities and considers some irrelevant information, such as the position of tissue patches and the orientation of tissue patches.</p>
        <p id="P64">Therefore, we check the <italic toggle="yes">Concept View</italic> to find dimensions that can indicate concepts with domain semantics. Based on the synthesized images, we speculate that <monospace>dim_5</monospace> is related to the density of tissues and <monospace>dim_2</monospace> is related to the color of the stained tissues. Our interpretation of these two dimensions is further confirmed by examining the grouped items in the <italic toggle="yes">Item Browser</italic>. As shown in <xref rid="F1" ref-type="fig">Figure 1b</xref>, when all items are arranged based on dim_5, items on the left side have almost no white space, indicating a high tissue density, while items on the right side have more white spaces, indicating loose tissues or fatty tissues. When all items are arranged based on <monospace>dim_2</monospace>, items on the left side have a more purple hue while items on the right side have a more pink hue. We then rename <monospace>dim_5</monospace> as <monospace>dim_density</monospace> and <monospace>dim_2</monospace> as <monospace>dim_color</monospace>.</p>
        <p id="P65">We arrange all items using <monospace>dim_density</monospace> as the <italic toggle="yes">x</italic> axis and <monospace>dim_color</monospace> as the <italic toggle="yes">y</italic> axis and then add a label for each item from the item metadata to indicate whether this item contains Invasive Ductal Carcinoma (IDC), a subtype of breast cancer cells (<italic toggle="yes">i.e</italic>., orange and blue item labels in <xref rid="F14" ref-type="fig">Figure 14a</xref>). As shown in <xref rid="F14" ref-type="fig">Figure 14a</xref>, there is a strong correlation between the presence of IDC and the two visual concepts mapped on the <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> axes. We group items (<xref rid="F1" ref-type="fig">Figure 1c</xref>) to reduce the visual clutter. Items with purple and dense tissues (<xref rid="F14" ref-type="fig">Figure 14a1</xref>) are more likely to contain IDC (<italic toggle="yes">i.e</italic>., orange labels) while items that are closer to pink (a2) and contain less dense tissue (a3) are less likely to contain IDC (<italic toggle="yes">i.e</italic>., blue labels). This association is further confirmed by a pathologist. Even though the identification of cancer cells needs to consider a variety of factors, the color and the tissue density are strong indicators of the presence of cancer cells. Cancer cells are typically dense, which leads to less white space, and have larger and darker nuclei than normal cells, which leads to more purple color.</p>
      </sec>
      <sec id="S40">
        <title>Identify Hard Examples for IDC Identification.</title>
        <p id="P66">Identifying regions in the whole mount slide image (<italic toggle="yes">i.e</italic>., items in our analysis) with IDC is an important task for pathologists to assign an aggressiveness grade to cancer. Since <monospace>dim_dense</monospace> and <monospace>dim_color</monospace> are related to the identification of cancer cells, we further analyzed how they influence on the prediction of IDC in an ML model. We train an IDC classification model by fine-tuning a ResNet34 model, as described in [<xref rid="R52" ref-type="bibr">52</xref>], and record the model prediction and confidence score for each item.</p>
        <p id="P67">Confident wrong predictions and false negatives are more consequential in real-world deployment [<xref rid="R9" ref-type="bibr">9</xref>], as patients may fail to receive the treatment they need. Therefore, we are especially interested in false-negative prediction with high confidence scores. We import item metadata to the concept view and filter items accordingly, <italic toggle="yes">i.e</italic>., ground truth = positive, prediction = negative, confidence score &gt; 0.8, as shown in <xref rid="F14" ref-type="fig">Figure 14b</xref>. According to the <italic toggle="yes">Item Browser</italic> (<xref rid="F14" ref-type="fig">Figure 14c</xref>), the filtered items are close to each other in the <italic toggle="yes">Item Browser</italic>, containing tissues that are not very dense and have a more purple hue. Since items with cancer cells usually contain dense tissues, this may explain why the classification model makes very confident but wrong predictions. We further examine the original spatial positions of these items in the <italic toggle="yes">Spatial View</italic> (see <xref rid="F14" ref-type="fig">Figure 14d</xref>), where other items are faded out with a semi-transparent white mask. We find the items of interest (<italic toggle="yes">i.e</italic>., non-masked items) are from regions where fatty tissues are surrounded by cancer cells, as shown by the orange boxes. This can explain why these items have many white spaces and only contain a small number of cancer cells. This observation is valuable for understanding and improving this IDC diagnosis model. First, it indicates when and where the IDC prediction model tends to make confident false negative predictions and a double-check from human experts is needed. Second, the training strategy can be modified accordingly (<italic toggle="yes">e.g</italic>., increasing the sample weight of these loose and purple tissues) to improve the model performance.</p>
      </sec>
    </sec>
    <sec id="S41">
      <label>8.5</label>
      <title>User Feedback</title>
      <p id="P68">We collected qualitative user feedback about Drava from the collaborated domain users.</p>
      <p id="P69">Participants commented that Drava provided <italic toggle="yes">‚Äúan attractive addition‚Äù</italic> (P5) to the current analysis methods. They liked the comprehensive user interaction provided by Drava. P4 commented that <italic toggle="yes">‚Äúthe item preview is engaging and useful‚Äù</italic>. Participants (P1, P4, P5) commented that it is not always easy to interpret a semantic dimension using one set of synthesized images. Therefore, the functionalities to generate synthesized images for a given baseline and to summarize item groups for a certain dimension are helpful. All participants agreed that Drava provided helpful guidance in interpreting and refining the ML semantic dimensions.</p>
      <p id="P70">The participants also provided valuable suggestions for further improvements. While some dimensions were reported as <italic toggle="yes">‚Äúeasy to associate with human concepts‚Äù</italic>, participants also complained that some dimensions had unclear semantics and were hard to interpret. This issue might be caused by the entangled concepts (<xref rid="S2" ref-type="sec">section 2</xref>) or the quality of the synthesized images. Instead of manually changing baseline images for the synthesized images (<xref rid="F4" ref-type="fig">Figure 4B</xref>), P3 suggested that Drava should recommend several baseline images to facilitate the interpretation of semantic dimensions. P1 and P2 were concerned about the extent to which their refinements will influence the back-end model. P1 stated that refining item groups without updating the back-end model (<italic toggle="yes">i.e</italic>., a local update) made him <italic toggle="yes">‚Äúfeel safer and in control‚Äù</italic>. Such concerns about automation are consistent with the observations in previous studies [<xref rid="R64" ref-type="bibr">64</xref>]. On the other hand, P1 also agreed that the local update can be inefficient and that updating the back-end model is necessary when analyzing a large number of items. P1 and P2 both provided suggestions for improving the global update mechanism, such as annotating how items change after updating the concept adaptor.</p>
    </sec>
  </sec>
  <sec id="S42">
    <label>9</label>
    <title>DISCUSSION</title>
    <sec id="S43">
      <label>9.1</label>
      <title>The Scope of Drava</title>
      <sec id="S44">
        <title>Dependence on DRL Performance and Data Quality.</title>
        <p id="P71">The concept-driven exploration provided by Drava is based on interpreting, refining, and utilizing semantic dimensions. Therefore, Drava‚Äôs capability depends on what semantic dimensions a DRL model can learn, which highly relies on the DRL model performance and the data quality [<xref rid="R28" ref-type="bibr">28</xref>]. Drava may fail to capture the desired concepts in the semantic dimensions due to the limited capabilities of the model or the low quality of the dataset. We believe that advances in DRL will further empower Drava and provide more opportunities for concept-driven data exploration. Additionally, the concept adaptor in Drava enables users to improve an unsatisfied concept through user refinement. In the worst-case scenario where the desired concepts can not be learned by the DRL model, Drava can serve as a pure interactive active learning tool that learns a concept merely based on user labeling.</p>
      </sec>
      <sec id="S45">
        <title>Visual Complexity of Concepts.</title>
        <p id="P72">Apart from DRL performance and data quality, whether a concept can be identified in Drava is also related to its visual complexity. Here, a visually complex concept indicates an abstract or subjective concept that has diverse visual representations, which makes it hard to visually summarize and interpret the concept via either the synthesized images or interactive piles. For example, in the <italic toggle="yes">CelebA</italic> dataset, some concepts are simple and have clear visual representations (<italic toggle="yes">e.g</italic>., black objects near eyes for a ‚Äúsunglass‚Äù concept), but other concepts are rather complex and involve varying visual presentations (<italic toggle="yes">e.g</italic>., ‚Äúattractive‚Äù can be related to either short or long hair, oval or round face shapes), making it hard to be visually summarized.</p>
      </sec>
      <sec id="S46">
        <title>Format and Characteristics of Data Items.</title>
        <p id="P73">To achieve the concept-driven exploration in Drava, data items need to fulfill two requirements. First, the data items must be visually perceivable for humans. Image datasets naturally fulfill this requirement. For other types of datasets (<italic toggle="yes">e.g</italic>., sequences, matrices), a workaround is to visualize the dataset and use the visualization (or segments of the visualization) as data items. For example, in <xref rid="S32" ref-type="sec">subsection 8.3</xref>, we convert a genome interaction matrix dataset into a heatmap visualization and treat each ROI in the heatmap as a data item. Second, these data items need to have similar appearances and share the same concepts, as shown in <xref rid="S22" ref-type="sec">section 8</xref>. Data items with dramatically different appearances not only make it challenging for the ML model to learn and extract concepts but also results in high cognitive loads for users to identify and validate concepts. For example, Drava can not be applied to the ILSVRC dataset [<xref rid="R50" ref-type="bibr">50</xref>], which contains diverse images depicting 1,000 different object categories.</p>
      </sec>
    </sec>
    <sec id="S47">
      <label>9.2</label>
      <title>Human Factors in Drava</title>
      <p id="P74">Human factors play an important role in human-in-the-loop AI tools [<xref rid="R2" ref-type="bibr">2</xref>, <xref rid="R10" ref-type="bibr">10</xref>, <xref rid="R65" ref-type="bibr">65</xref>]. Here, we discuss two important human factors in Drava, <italic toggle="yes">i.e</italic>., cognitive biases and cognitive load, including their impacts, our design considerations for mitigating the impacts, and the limitations of the current design.</p>
      <sec id="S48">
        <title>Cognitive Bias.</title>
        <p id="P75">ML models do not know what a human concept is. It is the users who associate the concepts of humans with the semantic dimensions of ML. As a result, the interpretation and refinement of the semantic dimensions can be influenced by users‚Äô cognitive biases (<italic toggle="yes">e.g</italic>., confirmation bias, anchoring bias, and availability bias). To facilitate the user interpretation, Drava supports concept validation through various interactions (<italic toggle="yes">e.g</italic>., changing the baseline image, arranging and piling items) rather than merely relying on the observation of a set of synthesized images. We also plan to support hypothesis generation and testing [<xref rid="R60" ref-type="bibr">60</xref>] to further reduce misinterpretation. However, Drava does not have mechanisms that are specifically designed for minimizing cognitive biases. Future studies are needed to systematically investigate the causes of and the solutions for cognitive bias in human‚ÄìAI collaboration.</p>
      </sec>
      <sec id="S49">
        <title>Cognitive Load.</title>
        <p id="P76">While more latent dimensions will potentially enable the model to capture more meaningful concepts, it will also increase the cognitive load of users. Drava alleviates this issue by enabling users to rank dimensions based on their salience scores and remove less relevant dimensions. We have successfully tested Drava in application scenarios with at most 32 latent dimensions. A large number of dimensions (<italic toggle="yes">e.g</italic>., 100) can challenge the cognitive capacity of users and undermine the usability of Drava. Like other hyperparameters in ML, the number of latent dimensions needs to be carefully selected to strike a balance between the representative of the latent dimensions and the cognitive load of the users. Promising directions for reducing the cognitive load include progressively revealing the information [<xref rid="R62" ref-type="bibr">62</xref>] and tracking provenance data [<xref rid="R14" ref-type="bibr">14</xref>].</p>
      </sec>
    </sec>
    <sec id="S50">
      <label>9.3</label>
      <title>Relation to Dimension Reduction Methods.</title>
      <p id="P77">The application scenarios present examples where the item arrangement based on a dimension reduction method (<italic toggle="yes">i.e</italic>., UMAP) fails to fill the analysis needs. Particularly, Drava enables visual exploration and analysis that focuses on the similarity of certain concepts rather than overall similarity. Drava complements the widely used dimension-reduction-based visual exploration tools. Drava is most suitable for analysis scenarios in which data items are similar (<italic toggle="yes">i.e</italic>., share multiple concepts) and the analysis concentrates on specific concepts. Dimension reduction projection (<italic toggle="yes">e.g</italic>., t-SNE, UMAP) is still an effective method for visualizing latent vectors, especially when the items form distinct clusters, and when the analysis focuses on overall similarity among items.</p>
    </sec>
    <sec id="S51">
      <label>9.4</label>
      <title>Scalability of Rendering and Interaction.</title>
      <p id="P78">The rendering scalability of Drava is mainly limited by its rendering engine in the <italic toggle="yes">Item Browser</italic>, which is built upon Piling.js [<xref rid="R33" ref-type="bibr">33</xref>]. The <italic toggle="yes">Item Browser</italic> can handle the rendering of and the interaction with 2,000 items with reasonable performance: the <italic toggle="yes">Item Browser</italic> can be initialized in less than 15 seconds and perform the interaction animation in no less than 50 frames per second on a laptop (MacBook Pro, 2020). Data loading is only performed when users open the tool for the first time. Loading depends on the bandwidth of the internet connection and the size of the dataset. It typically takes less than 30 seconds for the four datasets described in the application scenarios. Drava currently does not provide direct support for visualizing and interacting with more than several thousand items. In the future, we plan to further improve its scalability via item sampling and dynamically adjusting the level of detail.</p>
    </sec>
    <sec id="S52">
      <label>9.5</label>
      <title>Limitations of Evaluation</title>
      <p id="P79">We evaluated Drava on four application scenarios with five domain users. The evaluation demonstrated Drava‚Äôs capability on different types of datasets, domains, and analysis scenarios. At the same time, we admit the limitations resulting from the selection of participants and the setting of the evaluation. In particular, we only selected five participants in a non-random manner. The evaluation was based on self-reported feedback and included limited independent user exploration. While the evaluation revealed valuable insights and feedback, the generalizability of the results should be treated with caution. In the future, we plan to conduct a user study with a larger group of participants. Apart from assessing the usability of Drava, this user study will help validate the proposed workflow (<xref rid="S4" ref-type="sec">section 4</xref>) and understand user behaviors in human‚ÄìAI collaboration.</p>
    </sec>
  </sec>
  <sec id="S53">
    <label>10</label>
    <title>CONCLUSION</title>
    <p id="P80">This paper introduces Drava, a visual analytics system that employs DRL to support the concept-driven exploration of small multiples. Focusing on the ambiguity and imperfection of DRL semantic dimensions, Drava proposes a set of interactive visualizations and algorithms to help users better interpret DRL semantic dimensions, align them with human concepts, and utilize them for visual exploration. The application of Drava for data exploration complements the widely used dimension-reduction-based visual exploration tools, especially for situations where 1) the analyzed items are similar and share multiple visual concepts and 2) the analysis focuses on certain visual concepts rather than the overall similarity. Our application scenarios demonstrate the usefulness of Drava on various datasets and for different analysis purposes. Finally, Drava demonstrates the possibilities of employing XAI techniques to help users better understand data and support visual data exploration across a wide range of domains.</p>
  </sec>
  <sec sec-type="supplementary-material" id="SM1">
    <title>Supplementary Material</title>
    <supplementary-material id="SD1" position="float" content-type="local-data">
      <label>Supplementary Video</label>
      <media xlink:href="NIHMS1947573-supplement-Supplementary_Video.mp4" id="d64e2067" position="anchor" mimetype="video" mime-subtype="mp4"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="S54">
    <title>ACKNOWLEDGMENTS</title>
    <p id="P81">This work was supported by the National Institutes of Health (OT2OD026677, U24CA237617, UM1HG011536, R33CA263666). Q.W. is supported, in part, by Harvard Data Science Initiative Postdoctoral Research Fund.</p>
  </ack>
  <ref-list>
    <title>REFERENCES</title>
    <ref id="R1">
      <label>[1]</label>
      <mixed-citation publication-type="journal"><name><surname>Abdi</surname><given-names>Herv√©</given-names></name> and <name><surname>Williams</surname><given-names>Lynne J</given-names></name>. <year>2010</year>. <article-title>Principal component analysis</article-title>. <source>Wiley interdisciplinary reviews: computational statistics</source>
<volume>2</volume>, <issue>4</issue> (<comment>2010</comment>), <fpage>433</fpage>‚Äì<lpage>459</lpage>.</mixed-citation>
    </ref>
    <ref id="R2">
      <label>[2]</label>
      <mixed-citation publication-type="journal"><name><surname>Amershi</surname><given-names>Saleema</given-names></name>, <name><surname>Cakmak</surname><given-names>Maya</given-names></name>, <name><surname>Knox</surname><given-names>William Bradley</given-names></name>, and <name><surname>Kulesza</surname><given-names>Todd</given-names></name>. <year>2014</year>. <article-title>Power to the people: The role of humans in interactive machine learning</article-title>. <source>AI Magazine</source><volume>35</volume>, <issue>4</issue> (<comment>2014</comment>), <fpage>105</fpage>‚Äì<lpage>120</lpage>.</mixed-citation>
    </ref>
    <ref id="R3">
      <label>[3]</label>
      <mixed-citation publication-type="journal"><name><surname>An</surname><given-names>Lin</given-names></name>, <name><surname>Yang</surname><given-names>Tao</given-names></name>, <name><surname>Yang</surname><given-names>Jiahao</given-names></name>, <name><surname>Nuebler</surname><given-names>Johannes</given-names></name>, <name><surname>Xiang</surname><given-names>Guanjue</given-names></name>, <name><surname>Ross C Hardison</surname><given-names>Qunhua Li</given-names></name>, and <name><surname>Zhang</surname><given-names>Yu</given-names></name>. <year>2019</year>. <article-title>OnTAD: hierarchical domain structure reveals the divergence of activity among TADs and boundaries</article-title>. <source>Genome Biology</source><volume>20</volume>, <issue>1</issue> (<comment>2019</comment>), <fpage>1</fpage>‚Äì<lpage>16</lpage>.<pub-id pub-id-type="pmid">30606230</pub-id>
</mixed-citation>
    </ref>
    <ref id="R4">
      <label>[4]</label>
      <mixed-citation publication-type="book"><name><surname>Bach</surname><given-names>Benjamin</given-names></name>, <name><surname>Nathalie Henry-Riche</surname><given-names>Tim Dwyer</given-names></name>, <name><surname>Madhyastha</surname><given-names>Tara</given-names></name>, <name><surname>Fekete</surname><given-names>J-D</given-names></name>, and <name><surname>Grabowski</surname><given-names>Thomas</given-names></name>. <year>2015</year>. <part-title>Small MultiPiles: Piling time to explore temporal patterns in dynamic networks</part-title>. In <source>Computer Graphics Forum</source>, Vol. <volume>34</volume>. <publisher-name>Wiley Online Library</publisher-name>, <publisher-loc>NJ, USA</publisher-loc>, <fpage>31</fpage>‚Äì<lpage>40</lpage>.</mixed-citation>
    </ref>
    <ref id="R5">
      <label>[5]</label>
      <mixed-citation publication-type="book"><name><surname>Behrisch</surname><given-names>Michael</given-names></name>, <name><surname>Korkmaz</surname><given-names>Fatih</given-names></name>, <name><surname>Shao</surname><given-names>Lin</given-names></name>, and <name><surname>Schreck</surname><given-names>Tobias</given-names></name>. <year>2014</year>. <part-title>Feedback-driven interactive exploration of large multidimensional data supported by visual classifier</part-title>. In <source>2014 IEEE Conference on Visual Analytics Science and Technology (VAST)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>43</fpage>‚Äì<lpage>52</lpage>.</mixed-citation>
    </ref>
    <ref id="R6">
      <label>[6]</label>
      <mixed-citation publication-type="book"><name><surname>Behrisch</surname><given-names>Michael</given-names></name>, <name><surname>Krueger</surname><given-names>Robert</given-names></name>, <name><surname>Lekschas</surname><given-names>Fritz</given-names></name>, <name><surname>Schreck</surname><given-names>Tobias</given-names></name>, <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>, and <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>. <year>2018</year>. <part-title>Visual Pattern-Driven Exploration of Big Data</part-title>. In <source>International Symposium on Big Data Visual and Immersive Analytics (BDVA 18)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>1</fpage>‚Äì<lpage>11</lpage>.</mixed-citation>
    </ref>
    <ref id="R7">
      <label>[7]</label>
      <mixed-citation publication-type="book"><name><surname>Boggust</surname><given-names>Angie</given-names></name>, <name><surname>Carter</surname><given-names>Brandon</given-names></name>, and <name><surname>Satyanarayan</surname><given-names>Arvind</given-names></name>. <year>2022</year>. <part-title>Embedding comparator: Visualizing differences in global structure and local neighborhoods via small multiples</part-title>. In <source>27th International Conference on Intelligent User Interfaces</source>. <publisher-name>ACM</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>746</fpage>‚Äì<lpage>766</lpage>.</mixed-citation>
    </ref>
    <ref id="R8">
      <label>[8]</label>
      <mixed-citation publication-type="book"><name><surname>Christopher P Burgess</surname><given-names>Irina Higgins</given-names></name>, <name><surname>Pal</surname><given-names>Arka</given-names></name>, <name><surname>Matthey</surname><given-names>Loic</given-names></name>, <name><surname>Watters</surname><given-names>Nick</given-names></name>, <name><surname>Desjardins</surname><given-names>Guillaume</given-names></name>, and <name><surname>Lerchner</surname><given-names>Alexander</given-names></name>. <year>2017</year>. <part-title>Understanding disentangling in <italic toggle="yes">beta</italic>-VAE</part-title>. In <source>International Conference on Machine Learning (ICLR)</source>. <publisher-name>International Machine Learning Society</publisher-name>, <publisher-loc>Sydney, Australia</publisher-loc>, <fpage>10</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R9">
      <label>[9]</label>
      <mixed-citation publication-type="journal"><name><surname>Burt</surname><given-names>Tal</given-names></name>, <name><surname>Button</surname><given-names>KS</given-names></name>, <name><surname>Thom</surname><given-names>HHZ</given-names></name>, <name><surname>Noveck</surname><given-names>RJ</given-names></name>, and <name><surname>Munaf√≤</surname><given-names>Marcus R</given-names></name>. <year>2017</year>. <article-title>The Burden of the ‚ÄúFalse-Negatives‚Äù in Clinical Development: Analyses of Current and Alternative Scenarios and Corrective Measures</article-title>. <source>Clinical and Translational Science</source><volume>10</volume>, <issue>6</issue> (<comment>2017</comment>), <fpage>470</fpage>‚Äì<lpage>479</lpage>.<pub-id pub-id-type="pmid">28675646</pub-id>
</mixed-citation>
    </ref>
    <ref id="R10">
      <label>[10]</label>
      <mixed-citation publication-type="book"><name><surname>Carrie J Cai</surname><given-names>Emily Reif</given-names></name>, <name><surname>Hegde</surname><given-names>Narayan</given-names></name>, <name><surname>Hipp</surname><given-names>Jason</given-names></name>, <name><surname>Kim</surname><given-names>Been</given-names></name>, <name><surname>Smilkov</surname><given-names>Daniel</given-names></name>, <name><surname>Wattenberg</surname><given-names>Martin</given-names></name>, <name><surname>Viegas</surname><given-names>Fernanda</given-names></name>, <name><surname>Corrado</surname><given-names>Greg S</given-names></name>, and <name><surname>Stumpe</surname><given-names>Martin C</given-names></name>. <year>2019</year>. <part-title>Human-centered tools for coping with imperfect algorithms during medical decision-making</part-title>. In <source>Proceedings of the 2019 CHI conference on Human Factors in Computing Systems</source>. <publisher-name>ACM</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>1</fpage>‚Äì<lpage>14</lpage>.</mixed-citation>
    </ref>
    <ref id="R11">
      <label>[11]</label>
      <mixed-citation publication-type="journal"><name><surname>Ricky TQ Chen</surname><given-names>Xuechen Li</given-names></name>, <name><surname>Grosse</surname><given-names>Roger B</given-names></name>, and <name><surname>Duvenaud</surname><given-names>David K</given-names></name>. <year>2018</year>. <article-title>Isolating sources of disentanglement in variational autoencoders</article-title>. <source>Advances in Neural Information Processing Systems</source><volume>31</volume> (<comment>2018</comment>), <fpage>11</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R12">
      <label>[12]</label>
      <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>Xi</given-names></name>, <name><surname>Duan</surname><given-names>Yan</given-names></name>, <name><surname>Houthooft</surname><given-names>Rein</given-names></name>, <name><surname>Schulman</surname><given-names>John</given-names></name>, <name><surname>Sutskever</surname><given-names>Ilya</given-names></name>, and <name><surname>Abbeel</surname><given-names>Pieter</given-names></name>. <year>2016</year>. <article-title>Infogan: Interpretable representation learning by information maximizing generative adversarial nets</article-title>. <source>Advances in Neural Information Processing Systems</source><volume>29</volume> (<comment>2016</comment>), <fpage>10</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R13">
      <label>[13]</label>
      <mixed-citation publication-type="journal"><name><surname>Cheng</surname><given-names>Furui</given-names></name>, <name><surname>Mark S Keller</surname><given-names>Huamin Qu</given-names></name>, <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>, and <name><surname>Wang</surname><given-names>Qianwen</given-names></name>. <year>2023</year>. <article-title>Polyphony: an Interactive Transfer Learning Framework for Single-Cell Data Analysis</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>29</volume>, <issue>1</issue> (<comment>2023</comment>), <fpage>591</fpage>‚Äì<lpage>601</lpage>.<pub-id pub-id-type="pmid">36155452</pub-id>
</mixed-citation>
    </ref>
    <ref id="R14">
      <label>[14]</label>
      <mixed-citation publication-type="book"><name><surname>Cutler</surname><given-names>Zach</given-names></name>, <name><surname>Gadhave</surname><given-names>Kiran</given-names></name>, and <name><surname>Lex</surname><given-names>Alexander</given-names></name>. <year>2020</year>. <part-title>Trrack: A library for provenance-tracking in web-based visualizations</part-title>. In <source>2020 IEEE Visualization Conference (VIS)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>116</fpage>‚Äì<lpage>120</lpage>.</mixed-citation>
    </ref>
    <ref id="R15">
      <label>[15]</label>
      <mixed-citation publication-type="book"><name><surname>Frederik L Dennig</surname><given-names>Tom Polk</given-names></name>, <name><surname>Lin</surname><given-names>Zudi</given-names></name>, <name><surname>Schreck</surname><given-names>Tobias</given-names></name>, <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>, and <name><surname>Behrisch</surname><given-names>Michael</given-names></name>. <year>2019</year>. <part-title>FDive: Learning relevance models using pattern-based similarity measures</part-title>. In <source>2019 IEEE Conference on Visual Analytics Science and Technology (VAST)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>69</fpage>‚Äì<lpage>80</lpage>.</mixed-citation>
    </ref>
    <ref id="R16">
      <label>[16]</label>
      <mixed-citation publication-type="book"><name><surname>Eckelt</surname><given-names>K</given-names></name>, <name><surname>Hinterreiter</surname><given-names>A</given-names></name>, <name><surname>Adelberger</surname><given-names>P</given-names></name>, <name><surname>Walchshofer</surname><given-names>C</given-names></name>, <name><surname>Dhanoa</surname><given-names>V</given-names></name>, <name><surname>Humer</surname><given-names>C</given-names></name>, <name><surname>Heckmann</surname><given-names>M</given-names></name>, <name><surname>Steinparz</surname><given-names>C</given-names></name>, and <name><surname>Streit</surname><given-names>M</given-names></name>. <year>2022</year>. <part-title>Visual Exploration of Relationships and Structure in Low-Dimensional Embeddings</part-title>.. In <source>OSF Preprint</source>, Vol. <pub-id pub-id-type="doi">10.31219/osf.io/ujbrs</pub-id>. <publisher-name>Open Society Foundation</publisher-name>, <publisher-loc>SA</publisher-loc>, <fpage>15</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R17">
      <label>[17]</label>
      <mixed-citation publication-type="webpage"><collab>Facebook</collab>. <year>2014</year>. <source>React.js</source>. <comment><ext-link xlink:href="https://github.com/facebook/react" ext-link-type="uri">https://github.com/facebook/react</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R18">
      <label>[18]</label>
      <mixed-citation publication-type="journal"><name><surname>Gou</surname><given-names>Liang</given-names></name>, <name><surname>Zou</surname><given-names>Lincan</given-names></name>, <name><surname>Li</surname><given-names>Nanxiang</given-names></name>, <name><surname>Hofmann</surname><given-names>Michael</given-names></name>, <name><surname>Arvind Kumar Shekar</surname><given-names>Axel Wendt</given-names></name>, and <name><surname>Ren</surname><given-names>Liu</given-names></name>. <year>2020</year>. <article-title>VATLD: a visual analytics system to assess, understand and improve traffic light detection</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>27</volume>, <issue>2</issue> (<comment>2020</comment>), <fpage>261</fpage>‚Äì<lpage>271</lpage>.</mixed-citation>
    </ref>
    <ref id="R19">
      <label>[19]</label>
      <mixed-citation publication-type="book"><name><surname>Grinberg</surname><given-names>Miguel</given-names></name>. <year>2018</year>. <source>Flask web development: developing web applications with python</source>. <publisher-name>O‚ÄôReilly Media, Inc</publisher-name>., <publisher-loc>USA</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="R20">
      <label>[20]</label>
      <mixed-citation publication-type="journal"><name><surname>He</surname><given-names>Wenbin</given-names></name>, <name><surname>Zou</surname><given-names>Lincan</given-names></name>, <name><surname>Arvind Kumar Shekar</surname><given-names>Liang Gou</given-names></name>, and <name><surname>Ren</surname><given-names>Liu</given-names></name>. <year>2021</year>. <article-title>Where Can We Help? A Visual Analytics Approach to Diagnosing and Improving Semantic Segmentation of Movable Objects</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>1</issue> (<comment>2021</comment>), <fpage>1040</fpage>‚Äì<lpage>1050</lpage>.<pub-id pub-id-type="pmid">34587077</pub-id>
</mixed-citation>
    </ref>
    <ref id="R21">
      <label>[21]</label>
      <mixed-citation publication-type="book"><name><surname>Higgins</surname><given-names>Irina</given-names></name>, <name><surname>Amos</surname><given-names>David</given-names></name>, <name><surname>Pfau</surname><given-names>David</given-names></name>, <name><surname>Racaniere</surname><given-names>Sebastien</given-names></name>, <name><surname>Matthey</surname><given-names>Loic</given-names></name>, <name><surname>Rezende</surname><given-names>Danilo</given-names></name>, and <name><surname>Lerchner</surname><given-names>Alexander</given-names></name>. <year>2018</year>. <part-title>Towards a definition of disentangled representations</part-title>. In <source>arXiv preprint</source>, <comment>Vol. arXiv:1812.02230.</comment><publisher-name>Cornell University</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>25</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R22">
      <label>[22]</label>
      <mixed-citation publication-type="book"><name><surname>Higgins</surname><given-names>Irina</given-names></name>, <name><surname>Matthey</surname><given-names>Loic</given-names></name>, <name><surname>Pal</surname><given-names>Arka</given-names></name>, <name><surname>Burgess</surname><given-names>Christopher</given-names></name>, <name><surname>Glorot</surname><given-names>Xavier</given-names></name>, <name><surname>Botvinick</surname><given-names>Matthew</given-names></name>, <name><surname>Mohamed</surname><given-names>Shakir</given-names></name>, and <name><surname>Lerchner</surname><given-names>Alexander</given-names></name>. <year>2016</year>. <part-title>beta-vae: Learning basic visual concepts with a constrained variational framework</part-title>. In <source>International Conference on Machine Learning (ICLR)</source>. <publisher-name>International Machine Learning Society</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>12</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R23">
      <label>[23]</label>
      <mixed-citation publication-type="journal"><name><surname>Jia</surname><given-names>Shichao</given-names></name>, <name><surname>Li</surname><given-names>Zeyu</given-names></name>, <name><surname>Chen</surname><given-names>Nuo</given-names></name>, and <name><surname>Zhang</surname><given-names>Jiawan</given-names></name>. <year>2021</year>. <article-title>Towards visual explainable active learning for zero-shot classification</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>1</issue> (<comment>2021</comment>), <fpage>791</fpage>‚Äì<lpage>801</lpage>.<pub-id pub-id-type="pmid">34587036</pub-id>
</mixed-citation>
    </ref>
    <ref id="R24">
      <label>[24]</label>
      <mixed-citation publication-type="book"><name><surname>Jin</surname><given-names>Zhihua</given-names></name>, <name><surname>Wang</surname><given-names>Yong</given-names></name>, <name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Ming</surname><given-names>Yao</given-names></name>, <name><surname>Ma</surname><given-names>Tengfei</given-names></name>, and <name><surname>Qu</surname><given-names>Huamin</given-names></name>. <year>2020</year>. <part-title>GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks</part-title>. In <source>arXiv preprint</source>, <comment>Vol. arXiv:2011.11048.</comment><publisher-name>Cornell University</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>17</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R25">
      <label>[25]</label>
      <mixed-citation publication-type="journal"><name><surname>Kahng</surname><given-names>Minsuk</given-names></name>, <name><surname>Pierre Y</surname><given-names> Andrews</given-names></name>, <name><surname>Aditya</surname><given-names>Kalro</given-names></name>, and <name><surname>Chau</surname><given-names>Duen Horng</given-names></name>. <year>2017</year>. <article-title>Activis: Visual exploration of industry-scale deep neural network models</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>24</volume>, <issue>1</issue> (<comment>2017</comment>), <fpage>88</fpage>‚Äì<lpage>97</lpage>.<pub-id pub-id-type="pmid">28866557</pub-id>
</mixed-citation>
    </ref>
    <ref id="R26">
      <label>[26]</label>
      <mixed-citation publication-type="book"><name><surname>Kim</surname><given-names>Been</given-names></name>, <name><surname>Wattenberg</surname><given-names>Martin</given-names></name>, <name><surname>Gilmer</surname><given-names>Justin</given-names></name>, <name><surname>Cai</surname><given-names>Carrie</given-names></name>, <name><surname>Wexler</surname><given-names>James</given-names></name>, <name><surname>Viegas</surname><given-names>Fernanda</given-names></name>, <etal/><year>2018</year>. <part-title>Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)</part-title>. In <source>International conference on machine learning</source>. <publisher-name>PMLR</publisher-name>, <publisher-loc>Stockholm, Sweden</publisher-loc>, <fpage>2668</fpage>‚Äì<lpage>2677</lpage>.</mixed-citation>
    </ref>
    <ref id="R27">
      <label>[27]</label>
      <mixed-citation publication-type="journal"><name><surname>Kim</surname><given-names>Hannah</given-names></name>, <name><surname>Choo</surname><given-names>Jaegul</given-names></name>, <name><surname>Park</surname><given-names>Haesun</given-names></name>, and <name><surname>Endert</surname><given-names>Alex</given-names></name>. <year>2015</year>. <article-title>Interaxis: Steering scatterplot axes via observation-level interaction</article-title>. <source>IEEE transactions on visualization and computer graphics</source><volume>22</volume>, <issue>1</issue> (<comment>2015</comment>), <fpage>131</fpage>‚Äì<lpage>140</lpage>.<pub-id pub-id-type="pmid">26357399</pub-id>
</mixed-citation>
    </ref>
    <ref id="R28">
      <label>[28]</label>
      <mixed-citation publication-type="book"><name><surname>Kim</surname><given-names>Hyunjik</given-names></name> and <name><surname>Mnih</surname><given-names>Andriy</given-names></name>. <year>2018</year>. <part-title>Disentangling by factorising</part-title>. In <source>International Conference on Machine Learning</source>. <publisher-name>PMLR</publisher-name>, <publisher-loc>Stockholm, Sweden</publisher-loc>, <fpage>2649</fpage>‚Äì<lpage>2658</lpage>.</mixed-citation>
    </ref>
    <ref id="R29">
      <label>[29]</label>
      <mixed-citation publication-type="journal"><name><surname>Bum Chul Kwon</surname><given-names>Hannah Kim</given-names></name>, <name><surname>Wall</surname><given-names>Emily</given-names></name>, <name><surname>Choo</surname><given-names>Jaegul</given-names></name>, <name><surname>Park</surname><given-names>Haesun</given-names></name>, and <name><surname>Endert</surname><given-names>Alex</given-names></name>. <year>2016</year>. <article-title>Axisketcher: Interactive nonlinear axis mapping of visualizations through user drawings</article-title>. <source>IEEE transactions on visualization and computer graphics</source><volume>23</volume>, <issue>1</issue> (<comment>2016</comment>), <fpage>221</fpage>‚Äì<lpage>230</lpage>.<pub-id pub-id-type="pmid">27514048</pub-id>
</mixed-citation>
    </ref>
    <ref id="R30">
      <label>[30]</label>
      <mixed-citation publication-type="journal"><name><surname>Lekschas</surname><given-names>Fritz</given-names></name>, <name><surname>Bach</surname><given-names>Benjamin</given-names></name>, <name><surname>Kerpedjiev</surname><given-names>Peter</given-names></name>, <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>, and <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>. <year>2017</year>. <article-title>HiPiler: visual exploration of large genome interaction matrices with interactive small multiples</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>24</volume>, <issue>1</issue> (<comment>2017</comment>), <fpage>522</fpage>‚Äì<lpage>531</lpage>.<pub-id pub-id-type="pmid">28866592</pub-id>
</mixed-citation>
    </ref>
    <ref id="R31">
      <label>[31]</label>
      <mixed-citation publication-type="journal"><name><surname>Lekschas</surname><given-names>Fritz</given-names></name>, <name><surname>Behrisch</surname><given-names>Michael</given-names></name>, <name><surname>Bach</surname><given-names>Benjamin</given-names></name>, <name><surname>Kerpedjiev</surname><given-names>Peter</given-names></name>, <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>, and <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>. <year>2020</year>. <article-title>Pattern-Driven Navigation in 2D Multiscale Visualizations with Scalable Insets</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>26</volume>, <issue>1</issue> (<comment>1 1 2020</comment>), <fpage>611</fpage>‚Äì<lpage>621</lpage>.<pub-id pub-id-type="pmid">31442989</pub-id>
</mixed-citation>
    </ref>
    <ref id="R32">
      <label>[32]</label>
      <mixed-citation publication-type="book"><name><surname>Lekschas</surname><given-names>Fritz</given-names></name>, <name><surname>Peterson</surname><given-names>Brant</given-names></name>, <name><surname>Haehn</surname><given-names>Daniel</given-names></name>, <name><surname>Ma</surname><given-names>Eric</given-names></name>, <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>, and <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>. <year>2020</year>. <part-title>Peax: Interactive visual pattern search in sequential data using unsupervised deep representation learning</part-title>. In <source>Computer Graphics Forum</source>, Vol. <volume>39‚Äì3</volume>. <publisher-name>Wiley Online Library</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>167</fpage>‚Äì<lpage>179</lpage>.</mixed-citation>
    </ref>
    <ref id="R33">
      <label>[33]</label>
      <mixed-citation publication-type="journal"><name><surname>Lekschas</surname><given-names>Fritz</given-names></name>, <name><surname>Zhou</surname><given-names>Xinyi</given-names></name>, <name><surname>Chen</surname><given-names>Wei</given-names></name>, <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>, <name><surname>Bach</surname><given-names>Benjamin</given-names></name>, and <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>. <year>2021</year>. <article-title>A Generic Framework and Library for Exploration of Small Multiples through Interactive Piling</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>27</volume>, <fpage>2</fpage> (<comment>1 2 2021</comment>), <fpage>358</fpage>‚Äì<lpage>368</lpage>.</mixed-citation>
    </ref>
    <ref id="R34">
      <label>[34]</label>
      <mixed-citation publication-type="book"><name><surname>Liu</surname><given-names>Mengchen</given-names></name>, <name><surname>Liu</surname><given-names>Shixia</given-names></name>, <name><surname>Su</surname><given-names>Hang</given-names></name>, <name><surname>Cao</surname><given-names>Kelei</given-names></name>, and <name><surname>Zhu</surname><given-names>Jun</given-names></name>. <year>2018</year>. <part-title>Analyzing the noise robustness of deep neural networks</part-title>. In <source>2018 IEEE Conference on Visual Analytics Science and Technology (VAST)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>60</fpage>‚Äì<lpage>71</lpage>.</mixed-citation>
    </ref>
    <ref id="R35">
      <label>[35]</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>Shusen</given-names></name>, <name><surname>Bremer</surname><given-names>Peer-Timo</given-names></name>, <name><surname>Jayaraman J Thiagarajan</surname><given-names>Vivek Srikumar</given-names></name>, <name><surname>Wang</surname><given-names>Bei</given-names></name>, <name><surname>Livnat</surname><given-names>Yarden</given-names></name>, and <name><surname>Pascucci</surname><given-names>Valerio</given-names></name>. <year>2017</year>. <article-title>Visual exploration of semantic relationships in neural word embeddings</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>24</volume>, <issue>1</issue> (<comment>2017</comment>), <fpage>553</fpage>‚Äì<lpage>562</lpage>.<pub-id pub-id-type="pmid">28866574</pub-id>
</mixed-citation>
    </ref>
    <ref id="R36">
      <label>[36]</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>Xiao</given-names></name>, <name><surname>Sanchez</surname><given-names>Pedro</given-names></name>, <name><surname>Thermos</surname><given-names>Spyridon</given-names></name>, <name><surname>O‚ÄôNeil</surname><given-names>Alison Q</given-names></name>, and <name><surname>Tsaftaris</surname><given-names>Sotirios A</given-names></name>. <year>2022</year>. <article-title>Learning disentangled representations in the imaging domain</article-title>. <source>Medical Image Analysis</source><volume>80</volume> (<comment>2022</comment>), <fpage>102516</fpage>.<pub-id pub-id-type="pmid">35751992</pub-id>
</mixed-citation>
    </ref>
    <ref id="R37">
      <label>[37]</label>
      <mixed-citation publication-type="book"><name><surname>Liu</surname><given-names>Yang</given-names></name>, <name><surname>Jun</surname><given-names>Eunice</given-names></name>, <name><surname>Li</surname><given-names>Qisheng</given-names></name>, and <name><surname>Heer</surname><given-names>Jeffrey</given-names></name>. <year>2019</year>. <part-title>Latent space cartography: Visual analysis of vector space embeddings</part-title>. In <source>Computer Graphics Forum</source>, Vol. <volume>38</volume>. <publisher-name>Wiley Online Library</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>67</fpage>‚Äì<lpage>78</lpage>.</mixed-citation>
    </ref>
    <ref id="R38">
      <label>[38]</label>
      <mixed-citation publication-type="book"><name><surname>Liu</surname><given-names>Ziwei</given-names></name>, <name><surname>Luo</surname><given-names>Ping</given-names></name>, <name><surname>Wang</surname><given-names>Xiaogang</given-names></name>, and <name><surname>Tang</surname><given-names>Xiaoou</given-names></name>. <year>2015</year>. <part-title>Deep Learning Face Attributes in the Wild</part-title>. In <source>Proceedings of International Conference on Computer Vision (ICCV)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>3730</fpage>‚Äì<lpage>3738</lpage>.</mixed-citation>
    </ref>
    <ref id="R39">
      <label>[39]</label>
      <mixed-citation publication-type="journal"><name><surname>Sehi L‚ÄôYi</surname><given-names>Qianwen Wang</given-names></name>, <name><surname>Lekschas</surname><given-names>Fritz</given-names></name>, and <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>. <year>2021</year>. <article-title>Gosling: A Grammar-based Toolkit for Scalable and Interactive Genomics Data Visualization</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>1</issue> (<comment>1 10 2021</comment>), <fpage>140</fpage>‚Äì<lpage>150</lpage>.<pub-id pub-id-type="pmid">34596551</pub-id>
</mixed-citation>
    </ref>
    <ref id="R40">
      <label>[40]</label>
      <mixed-citation publication-type="webpage"><name><surname>Matthey</surname><given-names>Loic</given-names></name>, <name><surname>Higgins</surname><given-names>Irina</given-names></name>, <name><surname>Hassabis</surname><given-names>Demis</given-names></name>, and <name><surname>Lerchner</surname><given-names>Alexander</given-names></name>. <year>2017</year>. <source>dSprites: Disentanglement testing Sprites dataset</source>. <comment><ext-link xlink:href="https://github.com/deepmind/dsprites-dataset/" ext-link-type="uri">https://github.com/deepmind/dsprites-dataset/</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R41">
      <label>[41]</label>
      <mixed-citation publication-type="journal"><name><surname>Leland McInnes</surname><given-names>John Healy</given-names></name>, <name><surname>Saul</surname><given-names>Nathaniel</given-names></name>, and <name><surname>Gro√überger</surname><given-names>Lukas</given-names></name>. <year>2018</year>. <article-title>UMAP: Uniform Manifold Approximation and Projection</article-title>. <source>Journal of Open Source Software</source><volume>3</volume>, <issue>29</issue> (<comment>2018</comment>), <fpage>861</fpage>.</mixed-citation>
    </ref>
    <ref id="R42">
      <label>[42]</label>
      <mixed-citation publication-type="book"><name><surname>Ming</surname><given-names>Yao</given-names></name>, <name><surname>Cao</surname><given-names>Shaozu</given-names></name>, <name><surname>Zhang</surname><given-names>Ruixiang</given-names></name>, <name><surname>Li</surname><given-names>Zhen</given-names></name>, <name><surname>Chen</surname><given-names>Yuanzhe</given-names></name>, <name><surname>Song</surname><given-names>Yangqiu</given-names></name>, and <name><surname>Qu</surname><given-names>Huamin</given-names></name>. <year>2017</year>. <part-title>Understanding hidden memories of recurrent neural networks</part-title>. In <source>2017 IEEE Conference on Visual Analytics Science and Technology (VAST)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>13</fpage>‚Äì<lpage>24</lpage>.</mixed-citation>
    </ref>
    <ref id="R43">
      <label>[43]</label>
      <mixed-citation publication-type="webpage"><name><surname>Mooney</surname><given-names>Paul</given-names></name>. <year>2017</year>. <source>Breast Histopathology Images</source>. <comment><ext-link xlink:href="https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images" ext-link-type="uri">https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R44">
      <label>[44]</label>
      <mixed-citation publication-type="journal"><name><surname>Mori</surname><given-names>Giulio</given-names></name>, <name><surname>Patern√≤</surname><given-names>Fabio</given-names></name>, and <name><surname>Santoro</surname><given-names>Carmen</given-names></name>. <year>2002</year>. <article-title>CTTE: support for developing and analyzing task models for interactive system design</article-title>. <source>IEEE Transactions on software engineering</source><volume>28</volume>, <issue>8</issue> (<comment>2002</comment>), <fpage>797</fpage>‚Äì<lpage>813</lpage>.</mixed-citation>
    </ref>
    <ref id="R45">
      <label>[45]</label>
      <mixed-citation publication-type="book"><name><surname>Paszke</surname><given-names>Adam</given-names></name>, <name><surname>Gross</surname><given-names>Sam</given-names></name>, <name><surname>Massa</surname><given-names>Francisco</given-names></name>, <name><surname>Lerer</surname><given-names>Adam</given-names></name>, <name><surname>Bradbury</surname><given-names>James</given-names></name>, <name><surname>Chanan</surname><given-names>Gregory</given-names></name>, <name><surname>Killeen</surname><given-names>Trevor</given-names></name>, <name><surname>Lin</surname><given-names>Zeming</given-names></name>, <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>, <name><surname>Antiga</surname><given-names>Luca</given-names></name>, <name><surname>Desmaison</surname><given-names>Alban</given-names></name>, <name><surname>Kopf</surname><given-names>Andreas</given-names></name>, <name><surname>Yang</surname><given-names>Edward</given-names></name>, <name><surname>Zachary DeVito</surname><given-names>Martin Raison</given-names></name>, <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>, <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>, <name><surname>Steiner</surname><given-names>Benoit</given-names></name>, <name><surname>Fang</surname><given-names>Lu</given-names></name>, <name><surname>Bai</surname><given-names>Junjie</given-names></name>, and <name><surname>Chintala</surname><given-names>Soumith</given-names></name>. <year>2019</year>. <part-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</part-title>. In <source>Advances in Neural Information Processing Systems 32</source>, <name><surname>Wallach</surname><given-names>H</given-names></name>, <name><surname>Larochelle</surname><given-names>H</given-names></name>, <name><surname>Beygelzimer</surname><given-names>A</given-names></name>, <name><surname>d‚ÄôAlch√©-Buc</surname><given-names>F</given-names></name>, <name><surname>Fox</surname><given-names>E</given-names></name>, and <name><surname>Garnett</surname><given-names>R</given-names></name> (Eds.). <publisher-name>Curran Associates, Inc</publisher-name>., <publisher-loc>NY, USA</publisher-loc>, <fpage>8024</fpage>‚Äì<lpage>8035</lpage>.</mixed-citation>
    </ref>
    <ref id="R46">
      <label>[46]</label>
      <mixed-citation publication-type="journal"><name><surname>Pezzotti</surname><given-names>Nicola</given-names></name>, <name><surname>Thomas H√∂llt</surname><given-names>Jan Van Gemert</given-names></name>, <name><surname>Boudewijn PF Lelieveldt</surname><given-names>Elmar Eisemann</given-names></name>, and <name><surname>Vilanova</surname><given-names>Anna</given-names></name>. <year>2017</year>. <article-title>Deepeyes: Progressive visual analytics for designing deep neural networks</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>24</volume>, <issue>1</issue> (<comment>2017</comment>), <fpage>98</fpage>‚Äì<lpage>108</lpage>.<pub-id pub-id-type="pmid">28866543</pub-id>
</mixed-citation>
    </ref>
    <ref id="R47">
      <label>[47]</label>
      <mixed-citation publication-type="book"><name><surname>Pirrung</surname><given-names>Meg</given-names></name>, <name><surname>Hilliard</surname><given-names>Nathan</given-names></name>, <name><surname>Yankov</surname><given-names>Art√´m</given-names></name>, <name><surname>Nancy O‚ÄôBrien</surname><given-names>Paul Weidert</given-names></name>, <name><surname>Corley</surname><given-names>Courtney D</given-names></name>, and <name><surname>Hodas</surname><given-names>Nathan O</given-names></name>. <year>2018</year>. <part-title>Sharkzor: Interactive deep learning for image triage, sort and summary</part-title>. In <source>arXiv preprint</source>, <comment>Vol. arXiv:1802.05316.</comment><publisher-name>Cornell University</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>4</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R48">
      <label>[48]</label>
      <mixed-citation publication-type="journal"><name><surname>Rao</surname><given-names>Suhas SP</given-names></name>, <name><surname>Huntley</surname><given-names>Miriam H</given-names></name>, <name><surname>Durand</surname><given-names>Neva C</given-names></name>, <name><surname>Stamenova</surname><given-names>Elena K</given-names></name>, <name><surname>Bochkov</surname><given-names>Ivan D</given-names></name>, <name><surname>Robinson</surname><given-names>James T</given-names></name>, <name><surname>Sanborn</surname><given-names>Adrian L</given-names></name>, <name><surname>Machol</surname><given-names>Ido</given-names></name>, <name><surname>Omer</surname><given-names>Arina D</given-names></name>, <name><surname>Lander</surname><given-names>Eric S</given-names></name>, <etal/><year>2014</year>. <article-title>A 3D map of the human genome at kilobase resolution reveals principles of chromatin looping</article-title>. <source>Cell</source><volume>159</volume>, <issue>7</issue> (<comment>2014</comment>), <fpage>1665</fpage>‚Äì<lpage>1680</lpage>.<pub-id pub-id-type="pmid">25497547</pub-id>
</mixed-citation>
    </ref>
    <ref id="R49">
      <label>[49]</label>
      <mixed-citation publication-type="journal"><name><surname>Ren</surname><given-names>Pengzhen</given-names></name>, <name><surname>Xiao</surname><given-names>Yun</given-names></name>, <name><surname>Chang</surname><given-names>Xiaojun</given-names></name>, <name><surname>Huang</surname><given-names>Po-Yao</given-names></name>, <name><surname>Li</surname><given-names>Zhihui</given-names></name>, <name><surname>Brij B Gupta</surname><given-names>Xiaojiang Chen</given-names></name>, and <name><surname>Wang</surname><given-names>Xin</given-names></name>. <year>2021</year>. <article-title>A survey of deep active learning</article-title>. <source>Comput. Surveys</source><volume>54</volume>, <issue>9</issue> (<comment>2021</comment>), <fpage>1</fpage>‚Äì<lpage>40</lpage>.</mixed-citation>
    </ref>
    <ref id="R50">
      <label>[50]</label>
      <mixed-citation publication-type="journal"><name><surname>Russakovsky</surname><given-names>Olga</given-names></name>, <name><surname>Deng</surname><given-names>Jia</given-names></name>, <name><surname>Su</surname><given-names>Hao</given-names></name>, <name><surname>Krause</surname><given-names>Jonathan</given-names></name>, <name><surname>Satheesh</surname><given-names>Sanjeev</given-names></name>, <name><surname>Ma</surname><given-names>Sean</given-names></name>, <name><surname>Huang</surname><given-names>Zhiheng</given-names></name>, <name><surname>Karpathy</surname><given-names>Andrej</given-names></name>, <name><surname>Khosla</surname><given-names>Aditya</given-names></name>, <name><surname>Bernstein</surname><given-names>Michael</given-names></name>, <name><surname>Berg</surname><given-names>Alexander C.</given-names></name>, and <name><surname>Fei-Fei</surname><given-names>Li</given-names></name>. <year>2015</year>. <article-title>ImageNet Large Scale Visual Recognition Challenge</article-title>. <source>International Journal of Computer Vision (IJCV)</source><volume>115</volume>, <issue>3</issue> (<comment>2015</comment>), <fpage>211</fpage>‚Äì<lpage>252</lpage>. <pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></mixed-citation>
    </ref>
    <ref id="R51">
      <label>[51]</label>
      <mixed-citation publication-type="book"><name><surname>Smilkov</surname><given-names>Daniel</given-names></name>, <name><surname>Thorat</surname><given-names>Nikhil</given-names></name>, <name><surname>Nicholson</surname><given-names>Charles</given-names></name>, <name><surname>Reif</surname><given-names>Emily</given-names></name>, <name><surname>Vi√©gas</surname><given-names>Fernanda B</given-names></name>, and <name><surname>Martin Wattenberg</surname></name>. <year>2016</year>. <part-title>Embedding projector: Interactive visualization and interpretation of embeddings</part-title>. In <source>arXiv preprint</source>, <comment>Vol. arXiv:1611.05469.</comment><publisher-name>Cornell University</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>4</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R52">
      <label>[52]</label>
      <mixed-citation publication-type="webpage"><name><surname>Sothivelr</surname><given-names>Karthick</given-names></name>. <year>2020</year>. <source>Breast Cancer Classification With PyTorch and Deep Learning</source>. <comment><ext-link xlink:href="https://medium.com/swlh/" ext-link-type="uri">https://medium.com/swlh/</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R53">
      <label>[53]</label>
      <mixed-citation publication-type="book"><name><surname>Jost Tobias Springenberg</surname><given-names>Alexey Dosovitskiy</given-names></name>, <name><surname>Brox</surname><given-names>Thomas</given-names></name>, and <name><surname>Ried-miller</surname><given-names>Martin</given-names></name>. <year>2014</year>. <part-title>Striving for simplicity: The all convolutional net</part-title>. In <source>ICLR (workshop track)</source>. <publisher-name>International Machine Learning Society</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>14</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R54">
      <label>[54]</label>
      <mixed-citation publication-type="journal"><name><surname>Strobelt</surname><given-names>Hendrik</given-names></name>, <name><surname>Gehrmann</surname><given-names>Sebastian</given-names></name>, <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>, and <name><surname>Rush</surname><given-names>Alexander M</given-names></name>. <year>2017</year>. <article-title>Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>24</volume>, <issue>1</issue> (<comment>2017</comment>), <fpage>667</fpage>‚Äì<lpage>676</lpage>.<pub-id pub-id-type="pmid">28866526</pub-id>
</mixed-citation>
    </ref>
    <ref id="R55">
      <label>[55]</label>
      <mixed-citation publication-type="journal"><name><surname>Sutcliffe</surname><given-names>Alistair</given-names></name>. <year>2000</year>. <article-title>On the effective use and reuse of HCI knowledge</article-title>. <source>ACM Transactions on Computer-Human Interaction (TOCHI)</source><volume>7</volume>, <issue>2</issue> (<comment>2000</comment>), <fpage>197</fpage>‚Äì<lpage>221</lpage>.</mixed-citation>
    </ref>
    <ref id="R56">
      <label>[56]</label>
      <mixed-citation publication-type="journal"><name><surname>Sutcliffe</surname><given-names>Alistair G</given-names></name> and <name><surname>Carroll</surname><given-names>John M</given-names></name>. <year>1999</year>. <article-title>Designing claims for reuse in interactive systems design</article-title>. <source>International Journal of Human-Computer Studies</source>
<volume>50</volume>, <issue>3</issue> (<comment>1999</comment>), <fpage>213</fpage>‚Äì<lpage>241</lpage>.</mixed-citation>
    </ref>
    <ref id="R57">
      <label>[57]</label>
      <mixed-citation publication-type="book"><name><surname>Tufte</surname><given-names>Edward R</given-names></name>, <name><surname>Goeler</surname><given-names>Nora Hillman</given-names></name>, and <name><surname>Benson</surname><given-names>Richard</given-names></name>. <year>1990</year>. <source>Envisioning information</source>. Vol. <volume>126</volume>. <publisher-name>Graphics press</publisher-name>, <publisher-loc>Cheshire, CT</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="R58">
      <label>[58]</label>
      <mixed-citation publication-type="journal"><collab>Laurens Van der Maaten and Geoffrey Hinton</collab>. <year>2008</year>. <article-title>Visualizing data using t-SNE</article-title>. <source>Journal of Machine Learning Research</source><volume>9</volume>, <issue>11</issue> (<comment>2008</comment>), <fpage>2579</fpage>‚Äì<lpage>2605</lpage>.</mixed-citation>
    </ref>
    <ref id="R59">
      <label>[59]</label>
      <mixed-citation publication-type="book"><name><surname>Wang</surname><given-names>Junpeng</given-names></name>, <name><surname>Zhang</surname><given-names>Wei</given-names></name>, and <name><surname>Yang</surname><given-names>Hao</given-names></name>. <year>2020</year>. <part-title>SCANViz: Interpreting the symbol-concept association captured by deep neural networks through visual analytics</part-title>. In <source>2020 IEEE Pacific Visualization Symposium (PacificVis)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>51</fpage>‚Äì<lpage>60</lpage>.</mixed-citation>
    </ref>
    <ref id="R60">
      <label>[60]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Alexander</surname><given-names>William</given-names></name>, <name><surname>Pegg</surname><given-names>Jack</given-names></name>, <name><surname>Qu</surname><given-names>Huamin</given-names></name>, and <name><surname>Chen</surname><given-names>Min</given-names></name>. <year>2020</year>. <article-title>HypoML: Visual analysis for hypothesis-based evaluation of machine learning models</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>27</volume>, <issue>2</issue> (<comment>2020</comment>), <fpage>1417</fpage>‚Äì<lpage>1426</lpage>.</mixed-citation>
    </ref>
    <ref id="R61">
      <label>[61]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Chen</surname><given-names>Zhutian</given-names></name>, <name><surname>Wang</surname><given-names>Yong</given-names></name>, and <name><surname>Qu</surname><given-names>Huamin</given-names></name>. <year>2021</year>. <article-title>A Survey on ML4VIS: Applying MachineLearning Advances to Data Visualization</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>12</issue> (<comment>2021</comment>), <fpage>5134</fpage>‚Äì<lpage>5153</lpage>.</mixed-citation>
    </ref>
    <ref id="R62">
      <label>[62]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Li</surname><given-names>Zhen</given-names></name>, <name><surname>Fu</surname><given-names>Siwei</given-names></name>, <name><surname>Cui</surname><given-names>Weiwei</given-names></name>, and <name><surname>Qu</surname><given-names>Huamin</given-names></name>. <year>2019</year>. <article-title>Narvis: Authoring Narrative Slideshows for Introducing Data Visualization Designs</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>25</volume>, <issue>1</issue> (<month>jan</month>
<comment>2019</comment>), <fpage>779</fpage>‚Äì<lpage>788</lpage>.</mixed-citation>
    </ref>
    <ref id="R63">
      <label>[63]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Mazor</surname><given-names>Tali</given-names></name>, <name><surname>Theresa A Harbig</surname><given-names>Ethan Cerami</given-names></name>, and <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>. <year>2021</year>. <article-title>ThreadStates: State-based Visual Analysis of Disease Progression</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>1</issue> (<comment>2021</comment>), <fpage>238</fpage>‚Äì<lpage>247</lpage>.<pub-id pub-id-type="pmid">34587068</pub-id>
</mixed-citation>
    </ref>
    <ref id="R64">
      <label>[64]</label>
      <mixed-citation publication-type="book"><name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Ming</surname><given-names>Yao</given-names></name>, <name><surname>Jin</surname><given-names>Zhihua</given-names></name>, <name><surname>Shen</surname><given-names>Qiaomu</given-names></name>, <name><surname>Liu</surname><given-names>Dongyu</given-names></name>, <name><surname>Micah J Smith</surname><given-names>Kalyan Veeramachaneni</given-names></name>, and <name><surname>Qu</surname><given-names>Huamin</given-names></name>. <year>2019</year>. <part-title>Atmseer: Increasing transparency and controllability in automated machine learning</part-title>. In <source>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</source>. <publisher-name>ACM</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>1</fpage>‚Äì<lpage>12</lpage>.</mixed-citation>
    </ref>
    <ref id="R65">
      <label>[65]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Xu</surname><given-names>Zhenhua</given-names></name>, <name><surname>Chen</surname><given-names>Zhutian</given-names></name>, <name><surname>Wang</surname><given-names>Yong</given-names></name>, <name><surname>Liu</surname><given-names>Shixia</given-names></name>, and <name><surname>Qu</surname><given-names>Huamin</given-names></name>. <year>2020</year>. <article-title>Visual analysis of discrimination in machine learning</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>27</volume>, <issue>2</issue> (<comment>2020</comment>), <fpage>1470</fpage>‚Äì<lpage>1480</lpage>.</mixed-citation>
    </ref>
    <ref id="R66">
      <label>[66]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Yinqiao</given-names></name>, <name><surname>Chen</surname><given-names>Lu</given-names></name>, <name><surname>Jo</surname><given-names>Jaemin</given-names></name>, and <name><surname>Wang</surname><given-names>Yunhai</given-names></name>. <year>2021</year>. <article-title>Joint t-SNE for Comparable Projections of Multiple High-Dimensional Datasets</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>1</issue> (<comment>2021</comment>), <fpage>623</fpage>‚Äì<lpage>632</lpage>.<pub-id pub-id-type="pmid">34587021</pub-id>
</mixed-citation>
    </ref>
    <ref id="R67">
      <label>[67]</label>
      <mixed-citation publication-type="journal"><name><surname>Yuan</surname><given-names>Jun</given-names></name>, <name><surname>Chen</surname><given-names>Changjian</given-names></name>, <name><surname>Yang</surname><given-names>Weikai</given-names></name>, <name><surname>Liu</surname><given-names>Mengchen</given-names></name>, <name><surname>Xia</surname><given-names>Jiazhi</given-names></name>, and <name><surname>Liu</surname><given-names>Shixia</given-names></name>. <year>2021</year>. <article-title>A survey of visual analytics techniques for machine learning</article-title>. <source>Computational Visual Media</source><volume>7</volume>, <issue>1</issue> (<comment>2021</comment>), <fpage>3</fpage>‚Äì<lpage>36</lpage>.</mixed-citation>
    </ref>
    <ref id="R68">
      <label>[68]</label>
      <mixed-citation publication-type="journal"><name><surname>Zhao</surname><given-names>Zhenge</given-names></name>, <name><surname>Xu</surname><given-names>Panpan</given-names></name>, <name><surname>Scheidegger</surname><given-names>Carlos</given-names></name>, and <name><surname>Ren</surname><given-names>Liu</given-names></name>. <year>2021</year>. <article-title>Human-in-the-loop Extraction of Interpretable Concepts in Deep Learning Models</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>1</issue> (<comment>2021</comment>), <fpage>780</fpage>‚Äì<lpage>790</lpage>.<pub-id pub-id-type="pmid">34587066</pub-id>
</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="F1">
    <label>Figure 1:</label>
    <caption>
      <p id="P82">Drava enables concept-driven exploration by aligning semantic latent dimensions with human concepts. (a) UMAP projection of image patches of breast cancer specimens. (b) All image patches are organized and piled up based on the density of tissues. (c) All image patches are grouped into a grid layout according to the tissue density and color. The two visual concepts reveal a strong association of the presentation of invasive ductal carcinomas (IDC), <italic toggle="yes">i.e</italic>., the orange label. (d‚Äìe) More examples.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0001" position="float"/>
  </fig>
  <fig position="float" id="F2">
    <label>Figure 2:</label>
    <caption>
      <p id="P83">Mismatches between semantic latent dimensions and human concepts (red dashed boxes). (a): Synthesized images through value traversal of a latent dimension. (b): Items with the same latent values as the left- and right-most synthesized images.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0002" position="float"/>
  </fig>
  <fig position="float" id="F3">
    <label>Figure 3:</label>
    <caption>
      <p id="P84">A three-step workflow that guides the application of DRL for the concept-driven exploration of small multiples.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0003" position="float"/>
  </fig>
  <fig position="float" id="F4">
    <label>Figure 4:</label>
    <caption>
      <p id="P85">The user interface of Drava. The <italic toggle="yes">Concept View</italic> presents latent dimensions and other metadata as rows. The <italic toggle="yes">Item Browser</italic> enables user exploration of items based on selected concepts from the <italic toggle="yes">Concept View</italic> and can be controlled through the Configuration panel on the right. The <italic toggle="yes">Spatial View</italic> provides context information of the items when applicable.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0004" position="float"/>
  </fig>
  <fig position="float" id="F5">
    <label>Figure 5:</label>
    <caption>
      <p id="P86">Drava provides various grouping and labeling methods to help users interpret semantic dimension. (a‚Äìb) Different approaches to pile up items based on their characteristics. (c) Labels can be added to both items and item groups.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0005" position="float"/>
  </fig>
  <fig position="float" id="F6">
    <label>Figure 6:</label>
    <caption>
      <p id="P87">Overview of the interactions and mechanisms that Drava provides for identifying concept mismatches (a), refining items and groups (b), and updating items and underlying models (c).</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0006" position="float"/>
  </fig>
  <fig position="float" id="F7">
    <label>Figure 7:</label>
    <caption>
      <p id="P88">Architecture of the encoder, the decoder, and the concept adaptor.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0007" position="float"/>
  </fig>
  <fig position="float" id="F8">
    <label>Figure 8:</label>
    <caption>
      <p id="P89">Dimensions (rows) with different salience scores (a‚Äìb). The visual changes are clearer when changing the values of the dimensions with the highest salience scores (a) compared to the dimensions with the lowest scores (b).</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0008" position="float"/>
  </fig>
  <fig position="float" id="F9">
    <label>Figure 9:</label>
    <caption>
      <p id="P90">Examples of reconstructed outputs on four different datasets (a‚Äìd). The first row shows the original inputs and the second row represents the reconstructed outputs.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0009" position="float"/>
  </fig>
  <fig position="float" id="F10">
    <label>Figure 10:</label>
    <caption>
      <p id="P91">We compared our concept adaptor with active learning (baseline) on three different concepts (<italic toggle="yes">i.e</italic>., scale, smiling, and bangs) under three conditions (<italic toggle="yes">i.e</italic>., <italic toggle="yes">N</italic> = 1%, 2%, 5%). Each line graph shows the accuracy over 15 iterations. The concept adaptor (yellow) overall showed higher accuracy.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0010" position="float"/>
  </fig>
  <fig position="float" id="F11">
    <label>Figure 11:</label>
    <caption>
      <p id="P92">The application scenario using the simple shape data. (a) A UMAP projection puts images together even though the positions of shapes are different. (b) Users can arrange images based on shape positions. (c) Images are arranged based on the scales of shape, but the left-most side is mostly squares (c1), indicating the need for user refinement.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0011" position="float"/>
  </fig>
  <fig position="float" id="F12">
    <label>Figure 12:</label>
    <caption>
      <p id="P93">The application scenario using celebrity images. (a) Items are grouped based on a visual concept that is related to skin color. (b) Items are then rearranged by adding another visual concept that is related to the background darkness as the <italic toggle="yes">y</italic> axis. The dataset contains both items that have fair skin and dark background (b1) and items that have dark skin and light background (b2).</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0012" position="float"/>
  </fig>
  <fig position="float" id="F13">
    <label>Figure 13:</label>
    <caption>
      <p id="P94">The application scenario using a genomic interaction matrix. (a1‚Äìa4) Four representative items vary on three concepts: the thickness of the diagonal (a1 and a4), the presence of nested squares (a1 and a2), and the asymmetric structure of the nested squares (a2 and a3). (b) A group has both the items with nested squares and the items with thick diagonals (orange marks). Each item is displayed upon mouse hovering. (c) Arranging items using <monospace>dim_nest</monospace> as <italic toggle="yes">x</italic> axis and <monospace>dim_thick</monospace> as <italic toggle="yes">y</italic> axis clearly separates these two concepts.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0013" position="float"/>
  </fig>
  <fig position="float" id="F14">
    <label>Figure 14:</label>
    <caption>
      <p id="P95">The application scenario using the breast histopathology images. (a) Arranging image patches from breast cancer specimens based on concepts learned by Drava shows a strong association between the presence of IDC (the color of item labels) and the two visual concepts, <italic toggle="yes">i.e</italic>., the tissue density (a2 vs. a3, the <italic toggle="yes">x</italic> axis) and the tissue color (a1 vs. a2, the <italic toggle="yes">y</italic> axis). We further (b) filter these items and (c) display them in a grid layout to identify confident false-positive predictions without visual clutter. (d) The spatial view enables us to locate the items in the original whole-mount slide image.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0014" position="float"/>
  </fig>
  <table-wrap position="float" id="T1">
    <label>Table 1:</label>
    <caption>
      <p id="P96">Semantic meaning of individual latent dimensions. We compare Drava (<italic toggle="yes">i.e</italic>., using the value of one latent dimension to classify the corresponding concept) with random guesses on five concepts from two datasets. The results show that the latent dimension value could effectively indicate the corresponding concept.</p>
    </caption>
    <table frame="box" rules="all">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-left: hidden" rowspan="1" colspan="1">dataset</th>
          <th colspan="3" align="center" valign="middle" rowspan="1">dsprites</th>
          <th colspan="2" align="center" valign="middle" style="border-right: hidden" rowspan="1">CelebA</th>
        </tr>
        <tr>
          <th align="center" valign="middle" style="border-left: hidden" rowspan="1" colspan="1">concept</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">pos_x</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">pos_y</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">scale</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">smiling</th>
          <th align="center" valign="middle" style="border-right: hidden" rowspan="1" colspan="1">bangs</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" style="border-left: hidden" rowspan="1" colspan="1">random guess</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.333</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.333</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.333</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.5</td>
          <td align="center" valign="middle" style="border-right: hidden" rowspan="1" colspan="1">0.5</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-left: hidden" rowspan="1" colspan="1">Drava (no human refine)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.87</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.93</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.62</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.70</td>
          <td align="center" valign="middle" style="border-right: hidden" rowspan="1" colspan="1">0.77</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <boxed-text id="BX1" position="float">
    <caption>
      <title>CCS CONCEPTS</title>
    </caption>
    <list list-type="bullet" id="L2">
      <list-item>
        <p id="P97">Human-centered computing ‚Üí Interactive systems and tools</p>
      </list-item>
      <list-item>
        <p id="P98">Visual analytics</p>
      </list-item>
      <list-item>
        <p id="P99">Information visualization</p>
      </list-item>
    </list>
  </boxed-text>
</floats-group>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?noissn?>
<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr Proc SIGCHI Conf Hum Factor Comput Syst?>
<?submitter-system nihms?>
<?submitter-userid 10917418?>
<?submitter-authority eRA?>
<?submitter-login nilsgehlenborg?>
<?submitter-name Nils Gehlenborg?>
<?domain nihpa?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">101620299</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">41882</journal-id>
    <journal-id journal-id-type="nlm-ta">Proc SIGCHI Conf Hum Factor Comput Syst</journal-id>
    <journal-title-group>
      <journal-title>Proceedings of the SIGCHI conference on human factors in computing systems. CHI Conference</journal-title>
    </journal-title-group>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10707479</article-id>
    <article-id pub-id-type="pmid">38074525</article-id>
    <article-id pub-id-type="doi">10.1145/3544548.3581127</article-id>
    <article-id pub-id-type="manuscript">nihpa1947573</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DRAVA: Aligning Human Concepts with Machine Learning Latent Dimensions for the Visual Exploration of Small Multiples</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Qianwen</given-names>
        </name>
        <aff id="A1">Harvard Medical School, Boston, MA, USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>L‚ÄôYi</surname>
          <given-names>Sehi</given-names>
        </name>
        <aff id="A2">Harvard Medical School, Boston, MA, USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gehlenborg</surname>
          <given-names>Nils</given-names>
        </name>
        <aff id="A3">Harvard Medical School, Boston, MA, USA</aff>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="CR1">
        <email>qianwen_wang@hms.harvard.edu</email>
      </corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>28</day>
      <month>11</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>19</day>
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>08</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <volume>2023</volume>
    <elocation-id>833</elocation-id>
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial International 4.0 License</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P1">Latent vectors extracted by machine learning (ML) are widely used in data exploration (<italic toggle="yes">e.g</italic>., t-SNE) but suffer from a lack of interpretability. While previous studies employed disentangled representation learning (DRL) to enable more interpretable exploration, they often overlooked the potential mismatches between the concepts of humans and the semantic dimensions learned by DRL. To address this issue, we propose Drava, a visual analytics system that supports users in 1) relating the concepts of humans with the semantic dimensions of DRL and identifying mismatches, 2) providing feedback to minimize the mismatches, and 3) obtaining data insights from concept-driven exploration. Drava provides a set of visualizations and interactions based on visual piles to help users understand and refine concepts and conduct concept-driven exploration. Meanwhile, Drava employs a concept adaptor model to fine-tune the semantic dimensions of DRL based on user refinement. The usefulness of Drava is demonstrated through application scenarios and experimental validation.</p>
    </abstract>
    <kwd-group>
      <kwd>Visual exploration</kwd>
      <kwd>XAI</kwd>
      <kwd>Human-AI collaboration</kwd>
      <kwd>latent space</kwd>
      <kwd>small multiples</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>1</label>
    <title>INTRODUCTION</title>
    <p id="P2">Presenting analyzed small multiples (<italic toggle="yes">e.g</italic>., patches of medical images, miniature visualizations of a large genomic sequence) using latent vectors learned by machine learning (ML) models has become a common practice in many visual analytics systems [<xref rid="R6" ref-type="bibr">6</xref>, <xref rid="R10" ref-type="bibr">10</xref>]. A latent vector, usually represented as multi-dimensional quantitative values, is a compact representation of the analyzed data to capture relevant information. For example, a 64√ó64 pixel image can be represented as a 10-dimensional latent vector. Compared with analysis using raw data or human-crafted metrics, latent vectors enable users to organize and explore a large amount of data and conduct analysis tasks, such as finding similar items and identifying outliers, more efficiently.</p>
    <p id="P3">Even though latent vectors can accurately capture patterns extracted from the analyzed data, they cannot be directly interpreted by humans like the original images or texts. For this reason, latent vectors are usually used to represent the similarity between data items, assuming the latent vectors of two similar items are close in a latent space. For example, dimension reduction methods (<italic toggle="yes">e.g</italic>., t-SNE [<xref rid="R58" ref-type="bibr">58</xref>], UMAP [<xref rid="R41" ref-type="bibr">41</xref>]) are widely used to visualize latent vectors in 2D space, showing the similarities and differences among data items. Other prior studies proposed to hierarchically cluster items based on their latent vectors to conduct pattern-driven visual analytics [<xref rid="R6" ref-type="bibr">6</xref>]. However, definitions of ‚Äúsimilar items‚Äù vary depending on analysis tasks, and there is no single definition that can be applied to all scenarios. Even though some prior studies have incorporated user input to learn user‚Äôs perception of similarity [<xref rid="R10" ref-type="bibr">10</xref>, <xref rid="R32" ref-type="bibr">32</xref>] and even to extract human-readable concepts (<italic toggle="yes">e.g</italic>., gender from face images) [<xref rid="R37" ref-type="bibr">37</xref>, <xref rid="R68" ref-type="bibr">68</xref>], visual analytics based on latent vectors still suffers from their limited interpretability.</p>
    <p id="P4">Disentangled representation learning (DRL) [<xref rid="R12" ref-type="bibr">12</xref>, <xref rid="R22" ref-type="bibr">22</xref>] is a promising approach that can provide more explainable latent vectors through unsupervised learning, <italic toggle="yes">i.e</italic>., without human labels. By disentangling features and encoding them as separated dimensions in the latent vectors, DRL can generate latent vectors whose values carry semantics and can reveal human-understandable concepts, <italic toggle="yes">e.g</italic>., the value on one dimension indicates whether a person is smiling or not (<xref rid="F1" ref-type="fig">Figure 1d</xref>). We call such dimensions semantic dimensions. Some recent visualization tools [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R20" ref-type="bibr">20</xref>, <xref rid="R59" ref-type="bibr">59</xref>] have successfully employed DRL in their analysis and demonstrated the effectiveness of DRL. For example, Gou <italic toggle="yes">et al</italic>. [<xref rid="R18" ref-type="bibr">18</xref>] used DRL for traffic light images to summarize images based on human-readable concepts, such as color, brightness, and rotation. These studies usually assumed that the learned semantic dimensions can perfectly capture human concepts, and the concepts can be accurately represented by a set of synthesized images. However, these assumptions do not always hold. Potential mismatches can exist between the semantic latent dimensions learned by ML models and the concepts of humans. As shown in <xref rid="F2" ref-type="fig">Figure 2a</xref>, one latent dimension correlates to the angle of human head according to the synthesized images. But when using this dimension to organize images, our experiment results show that the model confuses ‚Äúangle of the head‚Äù with ‚Äúwhether part of the face is covered‚Äù, <italic toggle="yes">e.g</italic>., covered by a dark shadow or a flower (<xref rid="F2" ref-type="fig">Figure 2b</xref>). Meanwhile, previous studies focus on using DRL to diagnose supervised ML models rather than building an understanding of the data [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R20" ref-type="bibr">20</xref>]. They provide limited discussion about user needs in understanding and utilizing DRL for concept-driven data exploration.</p>
    <p id="P5">This study aims to provide a more interpretable and flexible visual exploration of small multiples by better aligning concepts of human users with the semantic latent vectors generated by ML models. We propose Drava, an interactive system that utilizes <bold>D</bold>isentangled <bold>R</bold>epresentation learning as <bold>A V</bold>isual <bold>A</bold>nalytics approach for concept-driven data exploration. In Drava, a dataset is represented as a set of small multiples [<xref rid="R57" ref-type="bibr">57</xref>], <italic toggle="yes">i.e</italic>., a series of basic charts or graphics that show instances or different slices of the dataset (<xref rid="F1" ref-type="fig">Figure 1</xref>). Hereafter, we call each small multiple as a <italic toggle="yes">data item</italic>. For each data item, DRL learns a multi-dimensional latent vector, certain dimensions of which have semantic meanings. Drava supports an interpretable exploration of these items by supporting users in correlating and aligning the semantic dimensions with human concepts. The interactive visualizations and algorithms in Drava are motivated and guided by a three-step workflow that we propose. Throughout this workflow, users 1) understand ML-learned semantic dimensions and identify their potential mismatches with human concepts, 2) refine and align ML semantic dimensions with human concepts, and 3) generate new knowledge about the analyzed data through concept-driven exploration. Particularly, Drava automatically ranks latent vectors and proposes a concept adaptor that can refine a concept based on human input. Meanwhile, a set of interactions based on visual piles [<xref rid="R33" ref-type="bibr">33</xref>] are provided, enabling users to effectively arrange, summarize, and compare items based on human-readable concepts. We demonstrate the usefulness of Drava through experimental validation and four usage scenarios. Drava is available at <ext-link xlink:href="https://qianwen.info/DRAVA/" ext-link-type="uri">https://qianwen.info/DRAVA/</ext-link>.</p>
  </sec>
  <sec id="S2">
    <label>2</label>
    <title>BACKGROUND: DISENTANGLED REPRESENTATION LEARNING</title>
    <p id="P6">DRL is a promising machine learning method that is able to extract interpretable features without human supervision. Given an input item <bold>x</bold>, the goal of DRL is to learn a vector <bold>z</bold> that captures the features of <bold>x</bold> in a disentangled manner. For example, as shown in <xref rid="F2" ref-type="fig">Figure 2a</xref>, a DRL model learns to represent an image of a human face using <bold>z</bold> and captures the feature ‚Äúthe angle of head‚Äù independently in <italic toggle="yes">z</italic><sub>1</sub> (<italic toggle="yes">i.e</italic>., the second dimension of <bold>z</bold>). Similarly, an area chart can be described as a vector <bold>z</bold> where <italic toggle="yes">z</italic><sub>0</sub> indicates the height of the chart, <italic toggle="yes">z</italic><sub>1</sub> indicates the trend, etc. DRL assumes <bold>x</bold> as a joint distribution of independent and dependent generative factors [<xref rid="R22" ref-type="bibr">22</xref>]. While these independent factors will be captured in separated dimensions of <bold>z</bold> (<italic toggle="yes">i.e</italic>., semantic dimensions), the dependent factors will remain entangled in other dimensions of <bold>z</bold> that are not used for representing the independent factors. In other words, some dimensions of <bold>z</bold> will have semantic meanings while others will not. For a precise mathematical definition of disentangled and entangled dimensions, we refer the readers to [<xref rid="R8" ref-type="bibr">8</xref>, <xref rid="R21" ref-type="bibr">21</xref>, <xref rid="R22" ref-type="bibr">22</xref>].</p>
    <p id="P7">A DRL model learns disentangled representations via two loss terms, a reconstruction term and a regularization term. The reconstruction term evaluates the differences between the input item <bold>x</bold> and the reconstructed item <inline-formula><mml:math id="M1" display="inline"><mml:mrow><mml:mover accent="true"><mml:mstyle mathvariant="bold"><mml:mtext>x</mml:mtext></mml:mstyle><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, encouraging the model to learn <bold>z</bold> that capture the main characteristics of the input item. The regularization term encourages disentanglement of the latent vectors. A DRL model is usually constructed by encouraging disentanglement in standard generative models, such as VAE [<xref rid="R22" ref-type="bibr">22</xref>] and GAN [<xref rid="R12" ref-type="bibr">12</xref>]. The state-of-the-art DRL approaches are largely based on VAE mainly due to their better training stability than GAN-based methods. For example, the loss function of <italic toggle="yes">Œ≤</italic>-VAE is defined as
<disp-formula id="FD1"><label>(1)</label><mml:math id="M2" display="block"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>œï</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>‚à£</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext>log</mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mi>Œ∏</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>‚à£</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>‚àí</mml:mo><mml:mi>Œ≤</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>œï</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>‚à£</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>‚Äñ</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
The first term is a reconstruction loss, and the second term is a regularization for disentanglement. With <italic toggle="yes">Œ≤</italic> &gt; 1, <italic toggle="yes">Œ≤</italic>-VAE encourages disentangled <bold>z</bold> by putting a constraint on the latent bottleneck. Prior studies have proposed various regularization terms for disentanglement. For more details, refer to prior studies [<xref rid="R21" ref-type="bibr">21</xref>, <xref rid="R36" ref-type="bibr">36</xref>]. Given its wide popularity, we use <italic toggle="yes">Œ≤</italic>-VAE in Drava with some modifications (<xref rid="S14" ref-type="sec">subsection 6.1</xref>). The proposed framework can be easily adapted to other VAE-based DRL, such as FactorVAE [<xref rid="R28" ref-type="bibr">28</xref>] or <italic toggle="yes">Œ≤</italic>-TCVAE [<xref rid="R11" ref-type="bibr">11</xref>].</p>
  </sec>
  <sec id="S3">
    <label>3</label>
    <title>RELATED WORK</title>
    <p id="P8">First, since Drava aims to assist data exploration using explainable latent vectors, it is closely related to <bold>visual analytics on latent vectors</bold> and, more broadly, <bold>visual analytics for ML models</bold> whose hidden layers generate latent vectors of the input data.</p>
    <p id="P9">Many visual analytics tools have been proposed to support interactive explorations of latent vectors. Dimensionality reduction techniques, such as t-SNE [<xref rid="R58" ref-type="bibr">58</xref>], UMAP [<xref rid="R41" ref-type="bibr">41</xref>], PCA [<xref rid="R1" ref-type="bibr">1</xref>], and their variants [<xref rid="R35" ref-type="bibr">35</xref>, <xref rid="R66" ref-type="bibr">66</xref>], are widely used to assist the visualization of latent vectors. Most of them focus on analyzing the latent vectors generated by a specific model [<xref rid="R67" ref-type="bibr">67</xref>], such as a convolutional neural network [<xref rid="R25" ref-type="bibr">25</xref>, <xref rid="R34" ref-type="bibr">34</xref>, <xref rid="R46" ref-type="bibr">46</xref>], a graph neural network [<xref rid="R24" ref-type="bibr">24</xref>], and a recurrent neural network [<xref rid="R35" ref-type="bibr">35</xref>, <xref rid="R42" ref-type="bibr">42</xref>, <xref rid="R54" ref-type="bibr">54</xref>]. Other studies aim to provide more generic methods for visually exploring the latent space [<xref rid="R7" ref-type="bibr">7</xref>, <xref rid="R37" ref-type="bibr">37</xref>, <xref rid="R51" ref-type="bibr">51</xref>]. Most relevant to our study is LSC [<xref rid="R37" ref-type="bibr">37</xref>], which provides comprehensive support for mapping and comparing semantic dimensions in the analysis of latent vectors. However, LSC requires users to manually identify semantic dimensions, either by importing data labels or by interactively grouping items.</p>
    <p id="P10">Apart from showing latent vectors, previous studies have combined interactive visual analytics with interactive or explainable ML to introduce interpretability into the analysis of latent vectors [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R23" ref-type="bibr">23</xref>, <xref rid="R68" ref-type="bibr">68</xref>]. Several studies [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R20" ref-type="bibr">20</xref>, <xref rid="R59" ref-type="bibr">59</xref>] used DRL to extract semantic dimensions and associate model performance with human concepts (<italic toggle="yes">e.g</italic>., brightness of images, location of objects). The semantic dimensions learned by DRL are directly used without refinement, mostly because they are low-level concepts that can be easily extracted by ML. Jia <italic toggle="yes">et al</italic>. [<xref rid="R23" ref-type="bibr">23</xref>] proposed a visual explainable active learning approach that asks users questions and uses their answers to learn explainable attributes that can be used to classify images from unseen classes. Zhao <italic toggle="yes">et al</italic>. [<xref rid="R68" ref-type="bibr">68</xref>] proposed a visualization tool where users can explore and label image patches with a certain concept. These labels are used to train a concept extractor network, enabling users to diagnose model predictions using the learned concept.</p>
    <p id="P11">However, these studies mainly focus on understanding the working mechanism of ML models and improving model performances (<italic toggle="yes">i.e</italic>., VIS for ML). How to utilize explainable latent vectors for concept-driven data exploration (<italic toggle="yes">i.e</italic>., XAI for VIS) has not been extensively discussed. Drava is built upon previous visual analytics studies on latent vectors and ML models. Unlike previous studies, Drava focuses on aligning interpretable latent vectors with human concepts to assist concept-driven data exploration.</p>
    <p id="P12">Second, Drava learns the visual representation and supports <bold>the exploration of small multiples</bold> [<xref rid="R57" ref-type="bibr">57</xref>], a series of miniature visualizations that represent different facets, subsets, or instances of a dataset. Current studies in data visual exploration usually present small multiples as points (<italic toggle="yes">e.g</italic>., [<xref rid="R7" ref-type="bibr">7</xref>, <xref rid="R16" ref-type="bibr">16</xref>, <xref rid="R47" ref-type="bibr">47</xref>, <xref rid="R51" ref-type="bibr">51</xref>]), glyphs (<italic toggle="yes">e.g</italic>., [<xref rid="R29" ref-type="bibr">29</xref>, <xref rid="R63" ref-type="bibr">63</xref>]), or images (<italic toggle="yes">e.g</italic>., [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R27" ref-type="bibr">27</xref>, <xref rid="R37" ref-type="bibr">37</xref>]) and place them in a grid, a dimension reduction projection, or a data-driven layout. For example, Sharkzor [<xref rid="R27" ref-type="bibr">27</xref>] enabled users to interactively organize images and their groups while providing visual cues for groups (<italic toggle="yes">e.g</italic>., badges). AxiSketcher [<xref rid="R29" ref-type="bibr">29</xref>] uses glyph representations and offers sketch-based interactions to flexibly arrange data items in the 2D space. Even though these studies provided valuable insights, they provide limited support in inspecting and summarizing a group of small multiples, which are important to reveal and remove the mismatches between human concepts and ML semantic dimensions. Some interaction techniques have been proposed to better organize small multiples and facilitate the exploration, such as interactive piling [<xref rid="R4" ref-type="bibr">4</xref>, <xref rid="R30" ref-type="bibr">30</xref>, <xref rid="R33" ref-type="bibr">33</xref>] and hierarchical clustering [<xref rid="R6" ref-type="bibr">6</xref>, <xref rid="R31" ref-type="bibr">31</xref>]. For example, interactive piling is inspired by physical piles and enables users to effectively group, aggregate, browse, and compare small multiples. However, these interactions are usually designed for specific application scenarios and cannot be directly applied to concept-driven exploration. In Drava, we adapt interactive piling to facilitate the concept-driven exploration of small multiples, especially focusing on the interpretation of semantic dimensions, the mismatch identification between ML semantic dimensions and human concepts, and guidance on refining semantic dimensions.</p>
    <p id="P13">Third, to better guide user exploration and insight generation, researchers have proposed <bold>interactive ML for visual data exploration</bold>, which learns what visual concepts are important to users from user feedback [<xref rid="R5" ref-type="bibr">5</xref>, <xref rid="R10" ref-type="bibr">10</xref>, <xref rid="R15" ref-type="bibr">15</xref>, <xref rid="R32" ref-type="bibr">32</xref>, <xref rid="R61" ref-type="bibr">61</xref>]. For example, Behrisch <italic toggle="yes">et al</italic>. [<xref rid="R5" ref-type="bibr">5</xref>] trained a classifier to interactively capture users‚Äô notion of interestingness when exploring many scatter plots. This classifier is then used to recommend potentially interesting plots and guide the exploration of large multidimensional data. Cai <italic toggle="yes">et al</italic>.[<xref rid="R10" ref-type="bibr">10</xref>] provides an interactive tool that empowers users to refine an ML model by communicating what types of similarities are most important when searching certain medical images. Peax [<xref rid="R32" ref-type="bibr">32</xref>] proposes an efficient and accurate query of a certain visual pattern in sequential data by learning from users‚Äô binary feedback on samples selected through active learning strategy. However, prior studies mainly use interactive ML to assist with similarity queries, <italic toggle="yes">i.e</italic>., modeling the similarity between items and user-selected targets. Despite the helpful guidance that these studies provide in data exploration, they cannot provide a comprehensive overview of the analyzed data.</p>
    <p id="P14">Like these approaches, Drava employs learning from user input to provide more precise exploration guidance. Furthermore, Drava provides semantic dimensions and supports summarization, exploration, and analysis based on different visual concepts.</p>
  </sec>
  <sec id="S4">
    <label>4</label>
    <title>WORKFLOW AND TASKS</title>
    <p id="P15">In this section, we decompose the overall goal of <italic toggle="yes">concept-driven visual exploration using DRL</italic> into three main steps (<xref rid="F3" ref-type="fig">Figure 3</xref>). We discuss the user tasks within each step from two aspects: the characteristics of DRL, as discussed in the DRL literature [<xref rid="R8" ref-type="bibr">8</xref>, <xref rid="R22" ref-type="bibr">22</xref>, <xref rid="R28" ref-type="bibr">28</xref>]; and the user needs in visual data exploration, largely informed by the task summarization work in previous studies [<xref rid="R16" ref-type="bibr">16</xref>, <xref rid="R33" ref-type="bibr">33</xref>, <xref rid="R37" ref-type="bibr">37</xref>]. These user tasks have been well established in previous studies and can be reused to effectively guide the design of Drava. Moreover, reuses in the task analysis can increase the design quality and reduce expenditure, as recommended in [<xref rid="R44" ref-type="bibr">44</xref>, <xref rid="R55" ref-type="bibr">55</xref>, <xref rid="R56" ref-type="bibr">56</xref>].</p>
    <sec id="S5">
      <title><bold>Step 1: Interpret ML Semantic Dimensions</bold>.</title>
      <p id="P16">Since only a subset of the latent dimensions correlates with semantic meanings, users should be assisted to <italic toggle="yes">identify the semantic dimensions efficiently</italic> (<bold>T1.1</bold>). For a specific dimension, users can <italic toggle="yes">interpret its semantic meaning</italic> (<bold>T1.2</bold>) through 1) synthesized images generated by single value traversal of this dimension or 2) data items sorted and grouped by their value in this dimension. A group summary can help users to efficiently understand the semantic meaning of a large number of items, associate it with a human concept, and identify mismatches. Unlike previous studies that group items based on their overall similarities, concept-based analysis requires to group and summarize items based on certain concepts. Therefore, proper aggregations should be provided to <italic toggle="yes">highlight the concept of interest and fade out others</italic> (<bold>T1.3</bold>) when summarizing an item group.</p>
    </sec>
    <sec id="S6">
      <title>Step 2: Align ML Semantic Dimensions with Human Concepts.</title>
      <p id="P17">Once a mismatch is identified, users modify the semantic dimension to better align it with the human‚Äôs definition of concepts. Such <italic toggle="yes">refinement should be user-friendly and conducted upon objects that users are familiar with</italic> (<bold>T2.1</bold>), <italic toggle="yes">e.g</italic>., data items and item groups rather than numerical values of latent dimensions. Meanwhile, <italic toggle="yes">visual cues should be provided to guide and facilitate the user refinement</italic> (<bold>T2.2</bold>), <italic toggle="yes">e.g</italic>., highlight the items that are grouped wrongly due to a concept mismatch.</p>
    </sec>
    <sec id="S7">
      <title>Step 3: Generate New Human Knowledge about the Data.</title>
      <p id="P18">Users <italic toggle="yes">explore the data items based on the identified concepts</italic> (<bold>T3.1</bold>) to generate insights about the analyzed items, including the distribution of items on one or multiple visual concepts, the association between different concepts. Such analysis can be further enhanced by <italic toggle="yes">correlating the concepts with other item metadata</italic> (<bold>T3.2</bold>), such as the spatial information and the item labels.</p>
      <p id="P19">The three steps are interconnected (<italic toggle="yes">i.e</italic>., arrows in <xref rid="F3" ref-type="fig">Figure 3</xref>). For example, users may directly go to Step 3 from Step 1 if they do not observe obvious mismatches. Users can also go back from Step 3 to Step 2 if they find some semantic dimensions fail to support their analysis tasks and require further refinement. Drava provides a set of dedicated interactive visualizations and algorithms that are closely coupled with this three-step workflow.</p>
    </sec>
  </sec>
  <sec id="S8">
    <label>5</label>
    <title>VISUAL INTERFACE</title>
    <p id="P20">The user interface of Drava (<xref rid="F4" ref-type="fig">Figure 4</xref>) consists of a <italic toggle="yes">Concept View</italic>, an <italic toggle="yes">Item Browser</italic>, and an optional <italic toggle="yes">Spatial View</italic>. The interactions related to visual piles are based on the design space proposed by Lekschas <italic toggle="yes">et al</italic>. [<xref rid="R33" ref-type="bibr">33</xref>], selected, modified, and extended to better reflect tasks described in <xref rid="S4" ref-type="sec">section 4</xref>.</p>
    <sec id="S9">
      <label>5.1</label>
      <title>Concept View</title>
      <p id="P21">In the <italic toggle="yes">Concept View</italic> (<xref rid="F4" ref-type="fig">Figure 4a</xref>), each latent dimension is visualized as a histogram and a list of synthesized images. The histogram shows the distribution of all items based on their values on the corresponding latent dimension (<bold>T3.1</bold>). Since the exact values of a latent dimension do not have specific meanings, we use synthesized images rather than numbers as the tick labels of the <italic toggle="yes">x</italic>-axis in the histogram. The synthesized images are generated by the decoder in the DRL model. For one specific latent dimension, the synthesized images are generated using a set of latent vectors whose values only differ on this dimension. These synthesized images illustrate the visual changes associated with the value traversal on the investigated dimension and help users understand its semantics (<bold>T1.2</bold>). Users can filter items based on their values on specific semantic dimensions by clicking on bars of a histogram (<xref rid="F4" ref-type="fig">Figure 4F</xref>).</p>
      <p id="P22">As explained in <xref rid="S2" ref-type="sec">section 2</xref>, only a subset of the latent dimensions are semantic and correlate with human concepts [<xref rid="R8" ref-type="bibr">8</xref>, <xref rid="R28" ref-type="bibr">28</xref>]. Therefore, it is important to provide a mechanism that guides users in the exploration of a potentially large number of dimensions. Drava calculates a salience score for each latent dimension (<xref rid="S16" ref-type="sec">subsection 6.3</xref>), indicating how important a particular latent dimension is for the synthesized images. As shown in <xref rid="F4" ref-type="fig">Figure 4A</xref>, all latent dimensions are ranked based on their salience scores, and the normalized score of each latent dimension is visualized by the width of a gray bar (<bold>T1.1</bold>). Users can change the dimension name based on their interpretation of the associated concept to facilitate the following analysis. Users can also remove irrelevant dimensions and add other customized dimensions from the item metadata.</p>
    </sec>
    <sec id="S10">
      <label>5.2</label>
      <title>Item Browser</title>
      <p id="P23">The <italic toggle="yes">Item Browser</italic> (<xref rid="F4" ref-type="fig">Figure 4b</xref>) layouts all items in a 2D space where users can freely arrange and group items. Arranging items based on their values of certain semantic dimensions enables users to interpret semantic dimensions and understand the item distribution among a certain concept (<bold>T3.1</bold>). A set of synthesized images are added to the <italic toggle="yes">x</italic>‚Äì and/or <italic toggle="yes">y</italic>‚Äìaxis to guide the interpretation of latent semantic dimensions and the exploration of data items (<xref rid="F4" ref-type="fig">Figure 4B</xref>). Since the visual appearance of the synthesized images largely depends on the latent vector, Drava allows users to select an item and use its latent vectors to generate synthesized images. This interaction enables users to further validate the concept associated with a latent dimension and identify possible mismatches (<bold>T1.2</bold>).</p>
      <p id="P24">Since the number of items can be large and the items often overlap with each other, effective grouping and summarizing mechanisms are needed. In Drava, users can either manually group items using a lasso selection or automatically group items based on their proximity in the 2D space. Drava provides various methods for summarizing a group of items and revealing abnormal items inside this group (<bold>T1.3</bold>), as shown in <xref rid="F5" ref-type="fig">Figure 5a</xref>‚Äì<xref rid="F5" ref-type="fig">b</xref>. When items are arranged horizontally (<italic toggle="yes">i.e</italic>., 1D grouping), items will be stacked along the vertical direction, and each item will be visualized as an item preview. Users can select the grouping method in the configure panel based on the characteristics of items and concepts.</p>
      <p id="P25">Labels can also be added to individual items or item groups to incorporate more item metadata into the analysis and investigate their associations with concepts, as shown in <xref rid="F5" ref-type="fig">Figure 5c</xref> (<bold>T3.2</bold>). To investigate more details about an item group, users can browse items by hovering on their item previews (<xref rid="F4" ref-type="fig">Figure 4D</xref>). A pop-up menu, shown upon right-clicking on an item group, enables users to depile this group or browse the items in a separate window.</p>
    </sec>
    <sec id="S11">
      <label>5.3</label>
      <title>Spatial View</title>
      <p id="P26">The <italic toggle="yes">Spatial View</italic> (<xref rid="F4" ref-type="fig">Figure 4c</xref>) is an optional view for data items that have spatial/context information. For example, in <xref rid="F4" ref-type="fig">Figure 4</xref>, each item indicates a region of interest in a huge genomic interaction matrix and is arranged according to its genomic location. Users can zoom and pan to obtain an overview or inspect further details. The <italic toggle="yes">Spatial View</italic> is coordinated with other views to reveal the correlations between concepts and item context (<bold>T3.2</bold>). When users filter items in the <italic toggle="yes">Concept View</italic>, the corresponding items will fade out in the <italic toggle="yes">Spatial View</italic>.</p>
    </sec>
    <sec id="S12">
      <label>5.4</label>
      <title>User Refinement</title>
      <p id="P27">Instead of directly modifying the hard-to-interpret latent values, Drava supports refinement towards groups and items (<bold>T2.1</bold>). For one selected semantic dimension <italic toggle="yes">D</italic><sub><italic toggle="yes">i</italic></sub>, Drava groups items (21 bins by default) based on their values of this dimension <italic toggle="yes">d</italic><sub><italic toggle="yes">i</italic></sub> to represent the gradual changes associated with this dimension. First, this default group assignment may have inappropriate thresholds, <italic toggle="yes">e.g</italic>., assigning similar items into two adjacent groups. Therefore, Drava enables users to merge (<xref rid="F6" ref-type="fig">Figure 6b1</xref>) or split (<xref rid="F6" ref-type="fig">Figure 6b2</xref>) groups to construct more meaningful groups according to one concept. More importantly, due to the imperfection of algorithms, the latent values may not accurately depict the concept for certain items, leading to inappropriate <italic toggle="yes">x</italic> position and group assignment for these items. Users can align the concepts and semantic dimensions by changing the item position (<xref rid="F6" ref-type="fig">Figure 6b1</xref>) and reassigning the group of these items (<xref rid="F6" ref-type="fig">Figure 6b3</xref>).</p>
      <p id="P28">Several mechanisms are provided to assist users in locating abnormal items and groups (<bold>T2.2</bold>), as shown in <xref rid="F6" ref-type="fig">Figure 6a</xref>. First, users can decide whether to merge or split groups by comparing these groups side by side (a1). Second, Drava enables users to identify abnormal items through previews (a2). For example, as shown in <xref rid="F4" ref-type="fig">Figure 4C</xref>, all items are grouped based on the thickness of their diagonal. Users can locate an abnormal item because its preview is darker than others. Users then examine this item through in-place browsing (<xref rid="F4" ref-type="fig">Figure 4D</xref>), extract it using the pop-up menu (<xref rid="F4" ref-type="fig">Figure 4E</xref>), and drag and drop it to a proper group based its diagonal thickness. Apart from identifying abnormal items through previews, users can also browse a group in a separate window and arrange the items using selected metrics (a3). In our experiments, we found certain metric values are useful in identifying abnormal items, including the reconstruction loss, the deviation of the latent value, the item metadata, and the uncertainty score.</p>
      <p id="P29">After user refinement, Drava supports two mechanisms, local and global, to update the items and/or the underlying model (<xref rid="F6" ref-type="fig">Figure 6c</xref>). By default, Drava employs a local updating mechanism, which remembers the user refinement, applies it to items with similar latent vectors, but does not modify the underlying model. Similar items are defined by setting a threshold <italic toggle="yes">Œ∏</italic> to the <italic toggle="yes">L</italic>2 distances of their latent vectors to the that of the refined items. On the contrary, global update initializes and fine-tunes a concept adaptor (<xref rid="S15" ref-type="sec">subsection 6.2</xref>). The values for all other items at this dimension will be updated accordingly by this concept adaptor. The global refinement is triggered by clicking the <italic toggle="yes">update concept</italic> button. Since it is hard for users to label an item with an exact numerical value, global refinement can only be triggered when items are grouped for a certain concept.</p>
    </sec>
  </sec>
  <sec id="S13">
    <label>6</label>
    <title>MODEL SETUP AND IMPLEMENTATION</title>
    <sec id="S14">
      <label>6.1</label>
      <title>Learning Semantic Dimensions using DRL</title>
      <p id="P30">Our DRL model is based on the <italic toggle="yes">Œ≤</italic>-VAE [<xref rid="R22" ref-type="bibr">22</xref>]. The structure of the DRL model is illustrated in <xref rid="F7" ref-type="fig">Figure 7</xref>, encoder (a) and decoder (b). Each convolution block consists of a convolution layer, a batch normalization layer, and a leaky ReLU (Rectified Linear Unit) activation. The decoder architecture is the transpose of the encoder. Following the practice in [<xref rid="R53" ref-type="bibr">53</xref>], we do not include Max Pooling layers by setting <italic toggle="yes">stride</italic> = 2 in the convolution layer. All usage scenarios in this paper use this structure and only vary in 1) the number of convolution and transposed convolution blocks, 2) the kernel size and the number of channels of the convolution and transposed convolution layers, and 3) the output size of the fully connected layer (<italic toggle="yes">i.e</italic>., the number of dimensions for the latent vector).</p>
      <p id="P31">We use the loss function proposed by Burgess <italic toggle="yes">et al</italic>. [<xref rid="R8" ref-type="bibr">8</xref>], which progressively increases the information capacity during the training process. An Adam optimization is used to train the model. Note that we use the mean <italic toggle="yes">Œº</italic> of the normal distribution learned by the encoder rather than the sampled <inline-formula><mml:math id="M3" display="inline"><mml:mrow><mml:mi>z</mml:mi><mml:mo>~</mml:mo><mml:mo>ùí©</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Œº</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>œÉ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as the latent vector for the input data, which enables deterministic latent values for each data item.</p>
      <p id="P32">Even though we implement and evaluate Drava using <italic toggle="yes">Œ≤</italic>-VAE, the proposed framework can be easily adapted to other VAE-based DRL models, such FactorVAE [<xref rid="R28" ref-type="bibr">28</xref>] and <italic toggle="yes">Œ≤</italic>-TCVAE [<xref rid="R11" ref-type="bibr">11</xref>].</p>
    </sec>
    <sec id="S15">
      <label>6.2</label>
      <title>Concept Adaptor</title>
      <p id="P33">The concept adaptor is a lightweight model that modifies semantic dimensions based on user refinements. For each semantic dimension, one concept adaptor will be generated if users use this dimension to arrange items, refine the item groups, and apply a global update. Since it is hard for users to associate the concept with an exact numerical value, the concept adaptor is only used to refine a concept for already grouped items. In other words, the concept adaptor is a multi-class classifier. The concept adaptor uses the feature map generated by the encoder hidden layer as input and predicts the group that the input item should belong to.</p>
      <p id="P34"><xref rid="F7" ref-type="fig">Figure 7c</xref> illustrates the structure of the concept adaptor. The convolution block contains a convolution layer (kernel size =4, stride =2) and a batch normalization. The convolution layer has <italic toggle="yes">n</italic> output channels where <italic toggle="yes">n</italic> equals to the number of item groups. A <italic toggle="yes">n</italic> √ó 1 vector will be obtained after a global max pooling layer and then feed into a softmax function. A cross entropy is used to calculate the loss. An Adam optimization is used to train the model.</p>
      <p id="P35">Once items are grouped based on the values of one dimension, users can initialize a concept adaptor accordingly. The training ends when the validation loss does not decrease. For all the datasets used in <xref rid="S22" ref-type="sec">section 8</xref>, the initialization took less than two minutes on a machine with one Tesla K80 GPU. After users have refined the item groups (<italic toggle="yes">i.e</italic>., change the classification label) for some items, the concept adaptor will be fine-tuned accordingly. During the fine-tuning, we increase the weight of the items that have been refined by the users. For the back-end models, only the concept adaptor is updated with user refinement, while the encoder and decoder are fixed. For the data items, only the values of the specific latent dimension (<italic toggle="yes">i.e</italic>., dimension used as the <italic toggle="yes">x</italic> axis) will be updated by the concept adaptor, while other dimensions will remain the same.</p>
    </sec>
    <sec id="S16">
      <label>6.3</label>
      <title>Dimension Ranking</title>
      <p id="P36">We rank all latent dimensions based on their importance to help users quickly locate semantic dimensions. Inspired by the salience scores used in interpretable ML [<xref rid="R26" ref-type="bibr">26</xref>], we gauge the importance of a latent dimension via the sensitivity of the reconstructed image <inline-formula><mml:math id="M4" display="inline"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> to changes in the magnitude of a latent dimension <italic toggle="yes">z</italic><sub><italic toggle="yes">i</italic></sub>. However, directly using the gradients <inline-formula><mml:math id="M5" display="inline"><mml:mrow><mml:mo>‚àÇ</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>/</mml:mo><mml:mo>‚àÇ</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> has several issues. First, it is a local importance score that is calculated for a particular reconstructed image. Second, it is a vector rather than a scalar value and can be hard to compare across. Third, it counts pixel-level differences that are not necessarily consistent with human perception. To solve these issues, we use a simple but effective method, <italic toggle="yes">i.e</italic>., averaging the importance score across output dimensions and across a set of sampled latent vectors. To mimic human perception of the synthesized images, we use latent vectors of the synthesized images as samples. Instead of using the reconstructed output, we use the feature maps generated by the second last layer of the decoder, aiming to capture high-level features rather than pixel-to-pixel differences.
<disp-formula id="FD2"><mml:math id="M6" display="block"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>m</mml:mi><mml:mo>√ó</mml:mo><mml:mi>n</mml:mi><mml:mo>√ó</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfrac><mml:mstyle><mml:munder><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>‚àÇ</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>‚àÇ</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula>
Where <italic toggle="yes">z</italic><sub><italic toggle="yes">k</italic></sub> is the <italic toggle="yes">k</italic><sub><italic toggle="yes">th</italic></sub> sampled latent vector, <italic toggle="yes">z</italic><sub><italic toggle="yes">k,i</italic></sub> is its value at dimension <italic toggle="yes">i</italic>, <italic toggle="yes">L</italic><sub><italic toggle="yes">m,n</italic></sub> (<italic toggle="yes">z</italic><sub><italic toggle="yes">k</italic></sub>) is the feature map (<italic toggle="yes">m, n</italic>) of the second to last decoder layer.</p>
      <p id="P37">The salience score serves as a useful indicator for semantic dimensions (<xref rid="F8" ref-type="fig">Figure 8</xref>). Theoretically, a dimension with a high salience score is not necessarily equal to a semantic dimension, <italic toggle="yes">e.g</italic>., a dimension is not semantic but significantly influences the output. However, the DRL model will minimize the existence of such dimensions by disentangling features and encoding them as separate dimensions. Ranking all dimensions based on salience scores can help users exclude many latent dimensions that do not contribute to the output and have little semantic meanings (<italic toggle="yes">e.g</italic>., <xref rid="F8" ref-type="fig">Figure 8b</xref>).</p>
    </sec>
    <sec id="S17">
      <label>6.4</label>
      <title>Implementation</title>
      <p id="P38">The implementation of Drava includes a front-end for interactive visualization and a back-end for data storage and the DRL model. The front-end is implemented in TypeScript using React [<xref rid="R17" ref-type="bibr">17</xref>], Piling.js [<xref rid="R33" ref-type="bibr">33</xref>], and Gosling.js [<xref rid="R39" ref-type="bibr">39</xref>]. The visualizations are rendered using SVG, Canvas, and WebGL. The back-end DRL model and concept adaptor are implemented in Python with PyTorch [<xref rid="R45" ref-type="bibr">45</xref>]. The front-end and back-end communicate via a Flask [<xref rid="R19" ref-type="bibr">19</xref>] web server built in Python. Users can easily apply Drava to their own datasets through two YAML configuration files that configure the back-end model training process and the front-end interface, respectively. The source code and documentation are available at <ext-link xlink:href="https://qianwen.info/DRAVA/" ext-link-type="uri">https://qianwen.info/DRAVA/</ext-link>.</p>
    </sec>
  </sec>
  <sec id="S18">
    <label>7</label>
    <title>EXPERIMENTAL VALIDATION</title>
    <p id="P39">In this section, we evaluated the back-end model in Drava from three aspects: 1) the <italic toggle="yes">representativeness</italic> of the latent vector, 2) the <italic toggle="yes">semantic meaning</italic> of individual latent dimensions, and 3) the improvements from <italic toggle="yes">concept fine-tuning</italic>. Previous studies either focused on assessing the disentanglement of latent dimensions [<xref rid="R8" ref-type="bibr">8</xref>, <xref rid="R22" ref-type="bibr">22</xref>, <xref rid="R28" ref-type="bibr">28</xref>] or overlooked the possible mismatches between human concepts and semantic dimensions [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R20" ref-type="bibr">20</xref>, <xref rid="R59" ref-type="bibr">59</xref>]. Therefore, it is important to validate the quality of these semantic latent vectors and their fine-tuning mechanism.</p>
    <sec id="S19">
      <title>Representativeness of the Latent Vector.</title>
      <p id="P40">We used the reconstruction quality to show whether the latent vectors can capture all the important visual features of the input data. <xref rid="F9" ref-type="fig">Figure 9</xref> exemplifies the reconstruction quality of the latent vectors for the four datasets used in the application scenarios (<xref rid="S22" ref-type="sec">section 8</xref>). Instead of the absolute similarity or the realism of the reconstructed images, we focused on evaluating whether the reconstructed images are able to capture important concepts. For the relatively simple <italic toggle="yes">dsprites</italic> shapes dataset (b), the model is able to generate images that are very similar to the input data. For more complex datasets (a, c‚Äìd), even though some details in the input data are missing, the model can reconstruct salient concepts.</p>
    </sec>
    <sec id="S20">
      <title>Semantic Meaning of Individual Latent Dimensions.</title>
      <p id="P41">To evaluate whether a single latent dimension can sufficiently depict a concept, we classified items based on their values on a certain semantic dimension and reported the classification accuracy. Specifically, for <italic toggle="yes">n</italic> classes belonging to a concept, <italic toggle="yes">n</italic> ‚àí 1 thresholds are learned to classify items. For example, the ‚Äúsmiling‚Äù concept has two classes, smiling and not smiling. We first identified a latent dimension <italic toggle="yes">D</italic><sub><italic toggle="yes">i</italic></sub> that is related to the ‚Äúsmiling‚Äù concept. We then classified each item based on whether its value on this dimension <italic toggle="yes">d</italic><sub><italic toggle="yes">i</italic></sub> is larger or smaller than a threshold <italic toggle="yes">thr</italic>, which was chosen to maximize the classification accuracy of all items. We used the <italic toggle="yes">dsprites</italic> and the <italic toggle="yes">CelebA</italic> datasets because they have labels for a diverse set of concepts. The results in <xref rid="T1" ref-type="table">Table 1</xref> demonstrated that the latent dimension value could effectively represent the corresponding concept but also showed space for further improvement.</p>
    </sec>
    <sec id="S21">
      <title>Improvements from Concept Fine-tuning.</title>
      <p id="P42">We evaluated the fine-tuning mechanism of the concept adaptor by comparing the classification accuracy of a specific concept before and after user refinement. This evaluation used the ‚Äúscale‚Äù concept from the <italic toggle="yes">dsprites</italic> dataset and the ‚Äúsmiling‚Äù and ‚Äúbangs‚Äù concepts from the <italic toggle="yes">CelebA</italic> dataset, because they have relatively low accuracy without any human refinement (<xref rid="T1" ref-type="table">Table 1</xref>). We chose an active learning method as the baseline for evaluating the concept adaptor. The baseline had the same architecture as the concept adaptor. We used simulated user feedback to obtain reproducible results in a variety of settings. Following the common practices in evaluating interactive machine learning [<xref rid="R13" ref-type="bibr">13</xref>] and active learning [<xref rid="R49" ref-type="bibr">49</xref>], we simulated user feedback as an oracle (<italic toggle="yes">i.e</italic>., always providing correct labels to the queried items). Both the concept adaptor and the baseline used the same simulation at each iteration but with different initialization. The active learning baseline is initialized with 5% labels. The concept adaptor is initialized with no labels but the same item groups as that in <xref rid="T1" ref-type="table">Table 1</xref>. Such an initialization simulates how users would divide items into several groups for a specific concept based on their latent dimension values. At each iteration, <italic toggle="yes">N</italic> items were refined (for the concept adaptor) or labeled (for the baseline) and models were trained until the validation loss did not decrease, which typically took around 10‚Äì20 epochs and less than 20 seconds. We experimented with three metrics for selecting the <italic toggle="yes">N</italic> items: uncertainty scores of the classification, the standard deviation of the latent dimension value, and differences between the latent dimension value and the classification threshold. We found that refining items with the highest uncertainty score led to the best model performances. Even though we used an oracle to simulate user refinement here, real-world users can easily examine and label these items in Drava by selecting a metric of interest as the <italic toggle="yes">y</italic> axis in <italic toggle="yes">Item Browser</italic>.</p>
      <p id="P43">We ran experiments under three settings: <italic toggle="yes">N</italic> = 1%, 2%, and 5% of the items. A total of 15 iterations were performed for each experiment. The results in <xref rid="F10" ref-type="fig">Figure 10</xref> were obtained by averaging the results of three experiments. First, the increased accuracy indicated that the concept adaptor helped align a concept and a semantic latent dimension. Compared with the baseline, the concept adaptor generated more accurate concepts by leveraging the values of the semantic dimension. Second, the curves of the concept adaptor were more smooth than the baseline, indicating a more stable improvement over iterations. Third, while the concept adaptor and the baseline required the same amount of user effort at each iteration (<italic toggle="yes">i.e</italic>., the same <italic toggle="yes">N</italic> and the same user simulation), the concept adaptor required less user effort at the initialization than the baseline (<italic toggle="yes">i.e</italic>., drawing two or three lasso selections vs. labeling 5% of the items one by one). Fourth, it was not surprising that the difference between the concept adaptor and the baseline model decreased with the increase of <italic toggle="yes">N</italic> and iteration steps. The advantages of the concept adaptor mainly result from using the semantic dimension values. As more and more items are labeled, these semantic dimensions become less useful in describing a concept.</p>
    </sec>
  </sec>
  <sec id="S22">
    <label>8</label>
    <title>APPLICATION SCENARIOS</title>
    <p id="P44">In this section, we present four application scenarios of Drava using one simulated dataset and three real-world datasets. For all four application scenarios, the DRL model is trained on the whole dataset with no labels used. The four application scenarios are conducted under collaboration with domain users, including two postdoctoral researchers on computer vision (P1 and P2, both for <xref rid="S23" ref-type="sec">subsection 8.1</xref> and <xref rid="S27" ref-type="sec">subsection 8.2</xref>), two researchers on genomic analysis (P3 and P4, for <xref rid="S32" ref-type="sec">subsection 8.3</xref>), and a professor on histopathological image analysis (P5, for <xref rid="S37" ref-type="sec">subsection 8.4</xref>). For each application scenario, we first provided a tutorial to introduce the functionalities of Drava. We then demonstrate our analysis and validate our findings with the participants. Participants can freely explore Drava and conduct additional analysis on the provided dataset. We further collected qualitative feedback about Drava from the participants.</p>
    <sec id="S23">
      <label>8.1</label>
      <title>Simple Shapes</title>
      <sec id="S24">
        <title>Data, Model, and Analysis Overview.</title>
        <p id="P45">This scenario uses the <italic toggle="yes">dsprites</italic> dataset [<xref rid="R40" ref-type="bibr">40</xref>], which consists of three types of simple shapes (<italic toggle="yes">i.e</italic>., square, ellipse, heart) with different scales, positions, and orientations. We uniformly sampled 1,000 items. The DRL model has four convolution blocks, each of which has 32 channels, a kernel of size 4, and a stride of 2. The latent vector has 8 dimensions. Even though this is a simple dataset, it can work as a proxy for more complicated datasets, such as the bounding boxes in object detection or the masks for cell segmentation. In this scenario, we explore the distribution of items according to concepts related to position and size, which are identified, validated, and refined by users.</p>
      </sec>
      <sec id="S25">
        <title>Arranging Items based on Concepts of Interest.</title>
        <p id="P46">To start with, we display all items in a 2D space using UMAP, a dimension reduction method that is commonly used for visualizing items with latent vectors. While the UMAP successfully put items with similar shapes and scales close to one another, the shape position information is mostly ignored, as shown in <xref rid="F11" ref-type="fig">Figure 11a</xref>. The position information can be important for some analysis tasks, <italic toggle="yes">e.g</italic>., object detection in autopilot.</p>
        <p id="P47">Based on the synthesized images in the <italic toggle="yes">Concept View</italic>, the position-related information is successfully extracted in two top-ranked dimensions, which we rename to <monospace>dim_x</monospace> and <monospace>dim_y</monospace> (<xref rid="F1" ref-type="fig">Figure 1e</xref>). As shown in <xref rid="F11" ref-type="fig">Figure 11b</xref>, all items are arranged and grouped based on the <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> position of the shape. We choose the <italic toggle="yes">average</italic> method to summarize a group, which enables us to inspect the positions of shapes without browsing individual items one by one.</p>
      </sec>
      <sec id="S26">
        <title>Refine a Semantic Dimension.</title>
        <p id="P48">The scale, <italic toggle="yes">i.e</italic>., size, of the shapes is also a vital piece of information for some analyses and has been successfully extracted in a latent dimension (named as <monospace>dim_size</monospace>). We verify this semantic dimension in the <italic toggle="yes">Item Browser</italic>, setting <monospace>dim_size</monospace> as <italic toggle="yes">x</italic> axis and its deviation <italic toggle="yes">œÉ</italic> as the <italic toggle="yes">y</italic> axis. While all items are sorted based on their size from left to right, we find that items on the left side are all squares (<xref rid="F11" ref-type="fig">Figure 11c1</xref>). We speculate this is because an ellipse or a heart, even with the same scale, is smaller than a square in terms of absolute pixel areas.</p>
        <p id="P49">To obtain a semantic dimension that better matches the analysis purpose and indicates the scale regardless of shape types, we refine <monospace>dim_size</monospace> using the concept adaptor. We set the ‚Äúreconstruction loss‚Äù as the <italic toggle="yes">y</italic> axis to reveal abnormal items (<xref rid="F11" ref-type="fig">Figure 11c2</xref>) and modify the <italic toggle="yes">x</italic> position of these items. We then group the items into three main groups, indicating large, medium, and small scales, respectively. After clicking the <italic toggle="yes">update concept</italic> button, the concept adaptor is initialized based on our grouping. We further refine these groups using the <italic toggle="yes">browse separately</italic> function, examining each group and updating the group mainly by moving items of ellipse or heart shape from the medium group to the large group. After several updates, we click the <italic toggle="yes">update concept</italic> button again. The concept adaptor is fine-tuned based on the refined item groups and updates the grouping of all items. After several iterations, we obtain three groups that more accurately reflect the scale of shapes without the influence of shape types (refer to <xref rid="S18" ref-type="sec">section 7</xref> for quantitative results).</p>
      </sec>
    </sec>
    <sec id="S27">
      <label>8.2</label>
      <title>Celebrity Images</title>
      <sec id="S28">
        <title>Data, Model, and Analysis Overview.</title>
        <p id="P50">This usage scenario uses the celebrity images from the <italic toggle="yes">CelebA</italic> dataset [<xref rid="R38" ref-type="bibr">38</xref>]. The DRL model is trained on the complete dataset, and we randomly sample 1,000 items for the exploration in Drava. The DRL model has five convolution blocks, each of which contains a kernel of size 3, a stride of 2, and 32, 64, 128, 256, and 512 channels, respectively. The latent vector has 20 dimensions. In this scenario, we investigate the quality of the <italic toggle="yes">CelebA</italic> dataset based on the diversity, balance, and association of the concepts in this dataset.</p>
      </sec>
      <sec id="S29">
        <title>Examine Dataset Diversity.</title>
        <p id="P51">Collecting a diverse dataset is important in ML to improve the model performance in real-world deployment and avoid algorithmic discrimination of certain populations [<xref rid="R65" ref-type="bibr">65</xref>]. The concepts extracted by Drava offer an effective approach to investigating the diversity of a dataset.</p>
        <p id="P52">Based on the synthesized images in the <italic toggle="yes">Concept View</italic>, we can affirm that diverse visual concepts exist in the analyzed data items. The analyzed items vary in a number of aspects, including emotional expression, gender, angle, skin color, background color, hair length, and hairstyle. To further verify our interpretation of the semantics of individual dimensions, we can interactively change the latent vector to update the synthesized images and group items based on their latent values at a selected dimension (<xref rid="F1" ref-type="fig">Figure 1d</xref>).</p>
      </sec>
      <sec id="S30">
        <title>Investigate Dataset Balance.</title>
        <p id="P53">We then analyze the item distribution along individual concepts as dataset imbalance can introduce bias during model training and impair model performance. For example, for the ‚Äúskin color‚Äù concept, a dataset with a large number of items with fair skin and only a small number of items with dark skin can lead to an ML model that has poor performance on the latter. As shown in <xref rid="F12" ref-type="fig">Figure 12a</xref>, we arrange and group items based on <monospace>dim_9</monospace>, which captures skin color based on the synthesized images. Through browsing items in these groups, we find only the right several groups include people with dark skin (a1), indicating a relatively small portion. When we browse individual items in each group (a2), we can find that this portion is even smaller since the model considers people with dark skin and people with shadows on their faces as similar. This observation implies an imbalance related to skin color, which may introduce a bias into a model trained on it.</p>
      </sec>
      <sec id="S31">
        <title>Confirm Concept Association.</title>
        <p id="P54">Based on <xref rid="F12" ref-type="fig">Figure 12a</xref>, we suspect a correlation between dark skin and dark background. Such correlations can be treated as causalities by ML models [<xref rid="R68" ref-type="bibr">68</xref>] and need to be avoided. We confirm this suspicion by arranging all items using <monospace>dim_9</monospace> (skin tone) as the <italic toggle="yes">x</italic>-axis and <monospace>dim_16</monospace> (background darkness) as the <italic toggle="yes">y</italic>-axis. The resulting distribution (<xref rid="F12" ref-type="fig">Figure 12b</xref>) dispels our suspicion. Even though the distribution is not uniform, the dataset contains both items that have fair skin and dark background (b1) and items that have dark skin and light background (b2).</p>
      </sec>
    </sec>
    <sec id="S32">
      <label>8.3</label>
      <title>Genomic Interaction Matrix</title>
      <sec id="S33">
        <title>Data, Model, and Analysis Overview.</title>
        <p id="P55">This usage scenario uses a genome interaction matrix for the HFFc6 cell line published by Rao <italic toggle="yes">et al</italic>. [<xref rid="R48" ref-type="bibr">48</xref>]. The matrices describe the chromatin interactions between different genomic locations, which is related to the physical folding of DNA that affects the regulation of gene expression. In a genome interaction matrix, rows and columns represent genomic locations, and the color intensity indicates the interaction probability between a pair of locations. Experts typically examine regions of interest (ROI) that have unique visual patterns and indicate specific biological events. Since the size of the matrix is huge, <italic toggle="yes">i.e</italic>., 3 billion √ó 3 billion for human genomes, this analysis process is often laborious and time-consuming.</p>
        <p id="P56">We generate small multiples for one specific type of ROI called Topologically Associated Domains (TAD), which are visually represented as squares that are presumably organized hierarchically. We first extract TADs from the interaction matrix using OnTAD [<xref rid="R3" ref-type="bibr">3</xref>] and then use the DRL model to generate a latent vector for each TAD. We demonstrate Drava using the 855 TADs extracted from chromosome 5 of the HFFc6 cell line. The DRL model has three convolution blocks with filter sizes of 7, 5, 3 and channel sizes of 32, 64, 128, respectively. The latent vector has 8 dimensions.</p>
        <p id="P57">In this scenario, we investigate different types of TADs by identifying, validating, and refining concepts that correspond to important visual patterns of TADs. Guided by these concepts, we are able to locate different types of TADs and examine the spatial distribution of these TADs on the whole genome.</p>
      </sec>
      <sec id="S34">
        <title>Understand Data through Concepts.</title>
        <p id="P58">The visual appearance of TADs in a heatmap can serve as effective proxies of the underlying data patterns and biological events [<xref rid="R3" ref-type="bibr">3</xref>, <xref rid="R30" ref-type="bibr">30</xref>]. Therefore, by interpreting the visual concepts, we can inspect how the underlying data and the associated biological events vary among the analyzed items. In the <italic toggle="yes">Concept View</italic>, we identified three dimensions of interest. <monospace>Dim_7</monospace> (renamed as <monospace>dim_thick</monospace>) indicates the thickness of the diagonal (<italic toggle="yes">e.g</italic>., an item changing from <xref rid="F13" ref-type="fig">Figure 13a1</xref> to <xref rid="F13" ref-type="fig">a4</xref>), which is related to the resolution of the TAD on the matrix since we resize all TADs into a fixed pixel size for the DRL model. <monospace>Dim_0</monospace> indicates the asymmetry of the nested TAD structure (<italic toggle="yes">e.g</italic>., an item changing from <xref rid="F13" ref-type="fig">Figure 13</xref>a2 to a3). <monospace>Dim_6</monospace> (renamed as <monospace>dim_nest</monospace>) corresponds to whether a TAD data item contains additional nested squares (<italic toggle="yes">i.e</italic>., nested TAD such as a2, a3) or not (<italic toggle="yes">i.e</italic>., single TAD such as a1, a4). Other dimensions are either hard to interpret because there is little variation in the synthesized images or can not be associated with meaningful domain insights. <monospace>Dim_thick</monospace> and <monospace>dim_nest</monospace> are the top two dimensions based on the salience scores, indicating the usefulness of the dimension ranking. The three dimensions (<monospace>dim_thick</monospace>, <monospace>dim_0</monospace>, <monospace>dim_6</monospace>) correspond to important attributes of TADs, as described by An <italic toggle="yes">et al</italic>. [<xref rid="R3" ref-type="bibr">3</xref>].</p>
      </sec>
      <sec id="S35">
        <title>Verify and Refine Concepts.</title>
        <p id="P59">After obtaining a basic understanding of the semantic meaning of each dimension through their synthesized images, we further verify the three concepts one by one through grouping and browsing data items. Interestingly, we find that <monospace>dim_nest</monospace> confuses the thickness of the diagonal with the nested structure of TADs. As shown in <xref rid="F13" ref-type="fig">Figure 13b</xref>, items are grouped based on <monospace>dim_nest</monospace> and use ‚Äúpartial‚Äù to generate item previews. Users can identify items with thick diagonals from the item preview (as annotated by the orange marks) and examine them in detail by hovering over them. This issue can hardly be revealed through the synthesized images (<xref rid="F13" ref-type="fig">Figure 13c1</xref>), which are widely used as the only method to interpret semantic meanings in previous literature [<xref rid="R18" ref-type="bibr">18</xref>, <xref rid="R59" ref-type="bibr">59</xref>]. This observation shows the importance of further verifying a concept base on data items and the need for user refinement.</p>
        <p id="P60">Since <monospace>dim_thick</monospace> can indicate the TAD size, we use it as the <italic toggle="yes">y</italic> axis to help refine the concept associated with <monospace>dim_nest</monospace>. As shown in <xref rid="F13" ref-type="fig">Figure 13c</xref>, items arranged in different vertical positions based on their diagonal thickness, enabling successful separation of nested TAD (<italic toggle="yes">e.g</italic>., a2, a3) from single TADs with thick diagonal (<italic toggle="yes">e.g</italic>., a4). Users can refine <monospace>dim_nest</monospace> by a lasso selection on all single TADs that have large <monospace>dim_nest</monospace> values and moving them to the left-most position (<italic toggle="yes">e.g</italic>., assigning them a small value for <monospace>dim_nest</monospace>), as shown in <xref rid="F13" ref-type="fig">Figure 13c3</xref>. The refinement is recorded using the local updating mechanism and applied to similar items.</p>
      </sec>
      <sec id="S36">
        <title>Locate items of interest.</title>
        <p id="P61">After the refinement, users can easily locate nested TADs in <xref rid="F13" ref-type="fig">Figure 13C4</xref> through a lasso selection. They can also filter these TADs based on <monospace>dim_thick</monospace> and <monospace>dim_nest</monospace> using their histograms. The nested structure in TADs is important to understand the boundary usage in gene regulation [<xref rid="R3" ref-type="bibr">3</xref>]. For this purpose, these identified items can be further examined in the <italic toggle="yes">Spatial View</italic> (<xref rid="F4" ref-type="fig">Figure 4</xref>), which reveals the genomic locations of these TADs and associated them with other context information (<italic toggle="yes">e.g</italic>., chromatin accessibility).</p>
      </sec>
    </sec>
    <sec id="S37">
      <label>8.4</label>
      <title>Breast Cancer Specimen</title>
      <sec id="S38">
        <title>Data, Model, and Analysis Overview.</title>
        <p id="P62">This usage scenario uses breast histopathology images downloaded from [<xref rid="R43" ref-type="bibr">43</xref>]. This dataset contains 277,524 patches (50 √ó 50 pixels) extracted from stained whole mount slide images of breast cancer specimens from 162 patients scanned at 40x magnification. The DRL model is trained on the whole dataset. In this usage scenario, we explore the 1,745 image patches from one patient. The DRL model has five convolution blocks, each with a kernel of size 3 and 32, 64, 128, 256, and 512 channels, respectively. The latent vector has 12 dimensions. In this scenario, we examine the presence of cancer cells in these items and analyze the performance of a classification model. Specifically, we identify concepts and associate them with domain semantics. We then use these concepts to describe the characteristics of hard-to-classify items.</p>
      </sec>
      <sec id="S39">
        <title>Interpret Visual Concepts and Assign Domain Semantics.</title>
        <p id="P63">We first visualize all the items using UMAP (<xref rid="F1" ref-type="fig">Figure 1a</xref>). However, the UMAP projection is not ideal since it is based on the overall similarities and considers some irrelevant information, such as the position of tissue patches and the orientation of tissue patches.</p>
        <p id="P64">Therefore, we check the <italic toggle="yes">Concept View</italic> to find dimensions that can indicate concepts with domain semantics. Based on the synthesized images, we speculate that <monospace>dim_5</monospace> is related to the density of tissues and <monospace>dim_2</monospace> is related to the color of the stained tissues. Our interpretation of these two dimensions is further confirmed by examining the grouped items in the <italic toggle="yes">Item Browser</italic>. As shown in <xref rid="F1" ref-type="fig">Figure 1b</xref>, when all items are arranged based on dim_5, items on the left side have almost no white space, indicating a high tissue density, while items on the right side have more white spaces, indicating loose tissues or fatty tissues. When all items are arranged based on <monospace>dim_2</monospace>, items on the left side have a more purple hue while items on the right side have a more pink hue. We then rename <monospace>dim_5</monospace> as <monospace>dim_density</monospace> and <monospace>dim_2</monospace> as <monospace>dim_color</monospace>.</p>
        <p id="P65">We arrange all items using <monospace>dim_density</monospace> as the <italic toggle="yes">x</italic> axis and <monospace>dim_color</monospace> as the <italic toggle="yes">y</italic> axis and then add a label for each item from the item metadata to indicate whether this item contains Invasive Ductal Carcinoma (IDC), a subtype of breast cancer cells (<italic toggle="yes">i.e</italic>., orange and blue item labels in <xref rid="F14" ref-type="fig">Figure 14a</xref>). As shown in <xref rid="F14" ref-type="fig">Figure 14a</xref>, there is a strong correlation between the presence of IDC and the two visual concepts mapped on the <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> axes. We group items (<xref rid="F1" ref-type="fig">Figure 1c</xref>) to reduce the visual clutter. Items with purple and dense tissues (<xref rid="F14" ref-type="fig">Figure 14a1</xref>) are more likely to contain IDC (<italic toggle="yes">i.e</italic>., orange labels) while items that are closer to pink (a2) and contain less dense tissue (a3) are less likely to contain IDC (<italic toggle="yes">i.e</italic>., blue labels). This association is further confirmed by a pathologist. Even though the identification of cancer cells needs to consider a variety of factors, the color and the tissue density are strong indicators of the presence of cancer cells. Cancer cells are typically dense, which leads to less white space, and have larger and darker nuclei than normal cells, which leads to more purple color.</p>
      </sec>
      <sec id="S40">
        <title>Identify Hard Examples for IDC Identification.</title>
        <p id="P66">Identifying regions in the whole mount slide image (<italic toggle="yes">i.e</italic>., items in our analysis) with IDC is an important task for pathologists to assign an aggressiveness grade to cancer. Since <monospace>dim_dense</monospace> and <monospace>dim_color</monospace> are related to the identification of cancer cells, we further analyzed how they influence on the prediction of IDC in an ML model. We train an IDC classification model by fine-tuning a ResNet34 model, as described in [<xref rid="R52" ref-type="bibr">52</xref>], and record the model prediction and confidence score for each item.</p>
        <p id="P67">Confident wrong predictions and false negatives are more consequential in real-world deployment [<xref rid="R9" ref-type="bibr">9</xref>], as patients may fail to receive the treatment they need. Therefore, we are especially interested in false-negative prediction with high confidence scores. We import item metadata to the concept view and filter items accordingly, <italic toggle="yes">i.e</italic>., ground truth = positive, prediction = negative, confidence score &gt; 0.8, as shown in <xref rid="F14" ref-type="fig">Figure 14b</xref>. According to the <italic toggle="yes">Item Browser</italic> (<xref rid="F14" ref-type="fig">Figure 14c</xref>), the filtered items are close to each other in the <italic toggle="yes">Item Browser</italic>, containing tissues that are not very dense and have a more purple hue. Since items with cancer cells usually contain dense tissues, this may explain why the classification model makes very confident but wrong predictions. We further examine the original spatial positions of these items in the <italic toggle="yes">Spatial View</italic> (see <xref rid="F14" ref-type="fig">Figure 14d</xref>), where other items are faded out with a semi-transparent white mask. We find the items of interest (<italic toggle="yes">i.e</italic>., non-masked items) are from regions where fatty tissues are surrounded by cancer cells, as shown by the orange boxes. This can explain why these items have many white spaces and only contain a small number of cancer cells. This observation is valuable for understanding and improving this IDC diagnosis model. First, it indicates when and where the IDC prediction model tends to make confident false negative predictions and a double-check from human experts is needed. Second, the training strategy can be modified accordingly (<italic toggle="yes">e.g</italic>., increasing the sample weight of these loose and purple tissues) to improve the model performance.</p>
      </sec>
    </sec>
    <sec id="S41">
      <label>8.5</label>
      <title>User Feedback</title>
      <p id="P68">We collected qualitative user feedback about Drava from the collaborated domain users.</p>
      <p id="P69">Participants commented that Drava provided <italic toggle="yes">‚Äúan attractive addition‚Äù</italic> (P5) to the current analysis methods. They liked the comprehensive user interaction provided by Drava. P4 commented that <italic toggle="yes">‚Äúthe item preview is engaging and useful‚Äù</italic>. Participants (P1, P4, P5) commented that it is not always easy to interpret a semantic dimension using one set of synthesized images. Therefore, the functionalities to generate synthesized images for a given baseline and to summarize item groups for a certain dimension are helpful. All participants agreed that Drava provided helpful guidance in interpreting and refining the ML semantic dimensions.</p>
      <p id="P70">The participants also provided valuable suggestions for further improvements. While some dimensions were reported as <italic toggle="yes">‚Äúeasy to associate with human concepts‚Äù</italic>, participants also complained that some dimensions had unclear semantics and were hard to interpret. This issue might be caused by the entangled concepts (<xref rid="S2" ref-type="sec">section 2</xref>) or the quality of the synthesized images. Instead of manually changing baseline images for the synthesized images (<xref rid="F4" ref-type="fig">Figure 4B</xref>), P3 suggested that Drava should recommend several baseline images to facilitate the interpretation of semantic dimensions. P1 and P2 were concerned about the extent to which their refinements will influence the back-end model. P1 stated that refining item groups without updating the back-end model (<italic toggle="yes">i.e</italic>., a local update) made him <italic toggle="yes">‚Äúfeel safer and in control‚Äù</italic>. Such concerns about automation are consistent with the observations in previous studies [<xref rid="R64" ref-type="bibr">64</xref>]. On the other hand, P1 also agreed that the local update can be inefficient and that updating the back-end model is necessary when analyzing a large number of items. P1 and P2 both provided suggestions for improving the global update mechanism, such as annotating how items change after updating the concept adaptor.</p>
    </sec>
  </sec>
  <sec id="S42">
    <label>9</label>
    <title>DISCUSSION</title>
    <sec id="S43">
      <label>9.1</label>
      <title>The Scope of Drava</title>
      <sec id="S44">
        <title>Dependence on DRL Performance and Data Quality.</title>
        <p id="P71">The concept-driven exploration provided by Drava is based on interpreting, refining, and utilizing semantic dimensions. Therefore, Drava‚Äôs capability depends on what semantic dimensions a DRL model can learn, which highly relies on the DRL model performance and the data quality [<xref rid="R28" ref-type="bibr">28</xref>]. Drava may fail to capture the desired concepts in the semantic dimensions due to the limited capabilities of the model or the low quality of the dataset. We believe that advances in DRL will further empower Drava and provide more opportunities for concept-driven data exploration. Additionally, the concept adaptor in Drava enables users to improve an unsatisfied concept through user refinement. In the worst-case scenario where the desired concepts can not be learned by the DRL model, Drava can serve as a pure interactive active learning tool that learns a concept merely based on user labeling.</p>
      </sec>
      <sec id="S45">
        <title>Visual Complexity of Concepts.</title>
        <p id="P72">Apart from DRL performance and data quality, whether a concept can be identified in Drava is also related to its visual complexity. Here, a visually complex concept indicates an abstract or subjective concept that has diverse visual representations, which makes it hard to visually summarize and interpret the concept via either the synthesized images or interactive piles. For example, in the <italic toggle="yes">CelebA</italic> dataset, some concepts are simple and have clear visual representations (<italic toggle="yes">e.g</italic>., black objects near eyes for a ‚Äúsunglass‚Äù concept), but other concepts are rather complex and involve varying visual presentations (<italic toggle="yes">e.g</italic>., ‚Äúattractive‚Äù can be related to either short or long hair, oval or round face shapes), making it hard to be visually summarized.</p>
      </sec>
      <sec id="S46">
        <title>Format and Characteristics of Data Items.</title>
        <p id="P73">To achieve the concept-driven exploration in Drava, data items need to fulfill two requirements. First, the data items must be visually perceivable for humans. Image datasets naturally fulfill this requirement. For other types of datasets (<italic toggle="yes">e.g</italic>., sequences, matrices), a workaround is to visualize the dataset and use the visualization (or segments of the visualization) as data items. For example, in <xref rid="S32" ref-type="sec">subsection 8.3</xref>, we convert a genome interaction matrix dataset into a heatmap visualization and treat each ROI in the heatmap as a data item. Second, these data items need to have similar appearances and share the same concepts, as shown in <xref rid="S22" ref-type="sec">section 8</xref>. Data items with dramatically different appearances not only make it challenging for the ML model to learn and extract concepts but also results in high cognitive loads for users to identify and validate concepts. For example, Drava can not be applied to the ILSVRC dataset [<xref rid="R50" ref-type="bibr">50</xref>], which contains diverse images depicting 1,000 different object categories.</p>
      </sec>
    </sec>
    <sec id="S47">
      <label>9.2</label>
      <title>Human Factors in Drava</title>
      <p id="P74">Human factors play an important role in human-in-the-loop AI tools [<xref rid="R2" ref-type="bibr">2</xref>, <xref rid="R10" ref-type="bibr">10</xref>, <xref rid="R65" ref-type="bibr">65</xref>]. Here, we discuss two important human factors in Drava, <italic toggle="yes">i.e</italic>., cognitive biases and cognitive load, including their impacts, our design considerations for mitigating the impacts, and the limitations of the current design.</p>
      <sec id="S48">
        <title>Cognitive Bias.</title>
        <p id="P75">ML models do not know what a human concept is. It is the users who associate the concepts of humans with the semantic dimensions of ML. As a result, the interpretation and refinement of the semantic dimensions can be influenced by users‚Äô cognitive biases (<italic toggle="yes">e.g</italic>., confirmation bias, anchoring bias, and availability bias). To facilitate the user interpretation, Drava supports concept validation through various interactions (<italic toggle="yes">e.g</italic>., changing the baseline image, arranging and piling items) rather than merely relying on the observation of a set of synthesized images. We also plan to support hypothesis generation and testing [<xref rid="R60" ref-type="bibr">60</xref>] to further reduce misinterpretation. However, Drava does not have mechanisms that are specifically designed for minimizing cognitive biases. Future studies are needed to systematically investigate the causes of and the solutions for cognitive bias in human‚ÄìAI collaboration.</p>
      </sec>
      <sec id="S49">
        <title>Cognitive Load.</title>
        <p id="P76">While more latent dimensions will potentially enable the model to capture more meaningful concepts, it will also increase the cognitive load of users. Drava alleviates this issue by enabling users to rank dimensions based on their salience scores and remove less relevant dimensions. We have successfully tested Drava in application scenarios with at most 32 latent dimensions. A large number of dimensions (<italic toggle="yes">e.g</italic>., 100) can challenge the cognitive capacity of users and undermine the usability of Drava. Like other hyperparameters in ML, the number of latent dimensions needs to be carefully selected to strike a balance between the representative of the latent dimensions and the cognitive load of the users. Promising directions for reducing the cognitive load include progressively revealing the information [<xref rid="R62" ref-type="bibr">62</xref>] and tracking provenance data [<xref rid="R14" ref-type="bibr">14</xref>].</p>
      </sec>
    </sec>
    <sec id="S50">
      <label>9.3</label>
      <title>Relation to Dimension Reduction Methods.</title>
      <p id="P77">The application scenarios present examples where the item arrangement based on a dimension reduction method (<italic toggle="yes">i.e</italic>., UMAP) fails to fill the analysis needs. Particularly, Drava enables visual exploration and analysis that focuses on the similarity of certain concepts rather than overall similarity. Drava complements the widely used dimension-reduction-based visual exploration tools. Drava is most suitable for analysis scenarios in which data items are similar (<italic toggle="yes">i.e</italic>., share multiple concepts) and the analysis concentrates on specific concepts. Dimension reduction projection (<italic toggle="yes">e.g</italic>., t-SNE, UMAP) is still an effective method for visualizing latent vectors, especially when the items form distinct clusters, and when the analysis focuses on overall similarity among items.</p>
    </sec>
    <sec id="S51">
      <label>9.4</label>
      <title>Scalability of Rendering and Interaction.</title>
      <p id="P78">The rendering scalability of Drava is mainly limited by its rendering engine in the <italic toggle="yes">Item Browser</italic>, which is built upon Piling.js [<xref rid="R33" ref-type="bibr">33</xref>]. The <italic toggle="yes">Item Browser</italic> can handle the rendering of and the interaction with 2,000 items with reasonable performance: the <italic toggle="yes">Item Browser</italic> can be initialized in less than 15 seconds and perform the interaction animation in no less than 50 frames per second on a laptop (MacBook Pro, 2020). Data loading is only performed when users open the tool for the first time. Loading depends on the bandwidth of the internet connection and the size of the dataset. It typically takes less than 30 seconds for the four datasets described in the application scenarios. Drava currently does not provide direct support for visualizing and interacting with more than several thousand items. In the future, we plan to further improve its scalability via item sampling and dynamically adjusting the level of detail.</p>
    </sec>
    <sec id="S52">
      <label>9.5</label>
      <title>Limitations of Evaluation</title>
      <p id="P79">We evaluated Drava on four application scenarios with five domain users. The evaluation demonstrated Drava‚Äôs capability on different types of datasets, domains, and analysis scenarios. At the same time, we admit the limitations resulting from the selection of participants and the setting of the evaluation. In particular, we only selected five participants in a non-random manner. The evaluation was based on self-reported feedback and included limited independent user exploration. While the evaluation revealed valuable insights and feedback, the generalizability of the results should be treated with caution. In the future, we plan to conduct a user study with a larger group of participants. Apart from assessing the usability of Drava, this user study will help validate the proposed workflow (<xref rid="S4" ref-type="sec">section 4</xref>) and understand user behaviors in human‚ÄìAI collaboration.</p>
    </sec>
  </sec>
  <sec id="S53">
    <label>10</label>
    <title>CONCLUSION</title>
    <p id="P80">This paper introduces Drava, a visual analytics system that employs DRL to support the concept-driven exploration of small multiples. Focusing on the ambiguity and imperfection of DRL semantic dimensions, Drava proposes a set of interactive visualizations and algorithms to help users better interpret DRL semantic dimensions, align them with human concepts, and utilize them for visual exploration. The application of Drava for data exploration complements the widely used dimension-reduction-based visual exploration tools, especially for situations where 1) the analyzed items are similar and share multiple visual concepts and 2) the analysis focuses on certain visual concepts rather than the overall similarity. Our application scenarios demonstrate the usefulness of Drava on various datasets and for different analysis purposes. Finally, Drava demonstrates the possibilities of employing XAI techniques to help users better understand data and support visual data exploration across a wide range of domains.</p>
  </sec>
  <sec sec-type="supplementary-material" id="SM1">
    <title>Supplementary Material</title>
    <supplementary-material id="SD1" position="float" content-type="local-data">
      <label>Supplementary Video</label>
      <media xlink:href="NIHMS1947573-supplement-Supplementary_Video.mp4" id="d64e2067" position="anchor" mimetype="video" mime-subtype="mp4"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="S54">
    <title>ACKNOWLEDGMENTS</title>
    <p id="P81">This work was supported by the National Institutes of Health (OT2OD026677, U24CA237617, UM1HG011536, R33CA263666). Q.W. is supported, in part, by Harvard Data Science Initiative Postdoctoral Research Fund.</p>
  </ack>
  <ref-list>
    <title>REFERENCES</title>
    <ref id="R1">
      <label>[1]</label>
      <mixed-citation publication-type="journal"><name><surname>Abdi</surname><given-names>Herv√©</given-names></name> and <name><surname>Williams</surname><given-names>Lynne J</given-names></name>. <year>2010</year>. <article-title>Principal component analysis</article-title>. <source>Wiley interdisciplinary reviews: computational statistics</source>
<volume>2</volume>, <issue>4</issue> (<comment>2010</comment>), <fpage>433</fpage>‚Äì<lpage>459</lpage>.</mixed-citation>
    </ref>
    <ref id="R2">
      <label>[2]</label>
      <mixed-citation publication-type="journal"><name><surname>Amershi</surname><given-names>Saleema</given-names></name>, <name><surname>Cakmak</surname><given-names>Maya</given-names></name>, <name><surname>Knox</surname><given-names>William Bradley</given-names></name>, and <name><surname>Kulesza</surname><given-names>Todd</given-names></name>. <year>2014</year>. <article-title>Power to the people: The role of humans in interactive machine learning</article-title>. <source>AI Magazine</source><volume>35</volume>, <issue>4</issue> (<comment>2014</comment>), <fpage>105</fpage>‚Äì<lpage>120</lpage>.</mixed-citation>
    </ref>
    <ref id="R3">
      <label>[3]</label>
      <mixed-citation publication-type="journal"><name><surname>An</surname><given-names>Lin</given-names></name>, <name><surname>Yang</surname><given-names>Tao</given-names></name>, <name><surname>Yang</surname><given-names>Jiahao</given-names></name>, <name><surname>Nuebler</surname><given-names>Johannes</given-names></name>, <name><surname>Xiang</surname><given-names>Guanjue</given-names></name>, <name><surname>Ross C Hardison</surname><given-names>Qunhua Li</given-names></name>, and <name><surname>Zhang</surname><given-names>Yu</given-names></name>. <year>2019</year>. <article-title>OnTAD: hierarchical domain structure reveals the divergence of activity among TADs and boundaries</article-title>. <source>Genome Biology</source><volume>20</volume>, <issue>1</issue> (<comment>2019</comment>), <fpage>1</fpage>‚Äì<lpage>16</lpage>.<pub-id pub-id-type="pmid">30606230</pub-id>
</mixed-citation>
    </ref>
    <ref id="R4">
      <label>[4]</label>
      <mixed-citation publication-type="book"><name><surname>Bach</surname><given-names>Benjamin</given-names></name>, <name><surname>Nathalie Henry-Riche</surname><given-names>Tim Dwyer</given-names></name>, <name><surname>Madhyastha</surname><given-names>Tara</given-names></name>, <name><surname>Fekete</surname><given-names>J-D</given-names></name>, and <name><surname>Grabowski</surname><given-names>Thomas</given-names></name>. <year>2015</year>. <part-title>Small MultiPiles: Piling time to explore temporal patterns in dynamic networks</part-title>. In <source>Computer Graphics Forum</source>, Vol. <volume>34</volume>. <publisher-name>Wiley Online Library</publisher-name>, <publisher-loc>NJ, USA</publisher-loc>, <fpage>31</fpage>‚Äì<lpage>40</lpage>.</mixed-citation>
    </ref>
    <ref id="R5">
      <label>[5]</label>
      <mixed-citation publication-type="book"><name><surname>Behrisch</surname><given-names>Michael</given-names></name>, <name><surname>Korkmaz</surname><given-names>Fatih</given-names></name>, <name><surname>Shao</surname><given-names>Lin</given-names></name>, and <name><surname>Schreck</surname><given-names>Tobias</given-names></name>. <year>2014</year>. <part-title>Feedback-driven interactive exploration of large multidimensional data supported by visual classifier</part-title>. In <source>2014 IEEE Conference on Visual Analytics Science and Technology (VAST)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>43</fpage>‚Äì<lpage>52</lpage>.</mixed-citation>
    </ref>
    <ref id="R6">
      <label>[6]</label>
      <mixed-citation publication-type="book"><name><surname>Behrisch</surname><given-names>Michael</given-names></name>, <name><surname>Krueger</surname><given-names>Robert</given-names></name>, <name><surname>Lekschas</surname><given-names>Fritz</given-names></name>, <name><surname>Schreck</surname><given-names>Tobias</given-names></name>, <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>, and <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>. <year>2018</year>. <part-title>Visual Pattern-Driven Exploration of Big Data</part-title>. In <source>International Symposium on Big Data Visual and Immersive Analytics (BDVA 18)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>1</fpage>‚Äì<lpage>11</lpage>.</mixed-citation>
    </ref>
    <ref id="R7">
      <label>[7]</label>
      <mixed-citation publication-type="book"><name><surname>Boggust</surname><given-names>Angie</given-names></name>, <name><surname>Carter</surname><given-names>Brandon</given-names></name>, and <name><surname>Satyanarayan</surname><given-names>Arvind</given-names></name>. <year>2022</year>. <part-title>Embedding comparator: Visualizing differences in global structure and local neighborhoods via small multiples</part-title>. In <source>27th International Conference on Intelligent User Interfaces</source>. <publisher-name>ACM</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>746</fpage>‚Äì<lpage>766</lpage>.</mixed-citation>
    </ref>
    <ref id="R8">
      <label>[8]</label>
      <mixed-citation publication-type="book"><name><surname>Christopher P Burgess</surname><given-names>Irina Higgins</given-names></name>, <name><surname>Pal</surname><given-names>Arka</given-names></name>, <name><surname>Matthey</surname><given-names>Loic</given-names></name>, <name><surname>Watters</surname><given-names>Nick</given-names></name>, <name><surname>Desjardins</surname><given-names>Guillaume</given-names></name>, and <name><surname>Lerchner</surname><given-names>Alexander</given-names></name>. <year>2017</year>. <part-title>Understanding disentangling in <italic toggle="yes">beta</italic>-VAE</part-title>. In <source>International Conference on Machine Learning (ICLR)</source>. <publisher-name>International Machine Learning Society</publisher-name>, <publisher-loc>Sydney, Australia</publisher-loc>, <fpage>10</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R9">
      <label>[9]</label>
      <mixed-citation publication-type="journal"><name><surname>Burt</surname><given-names>Tal</given-names></name>, <name><surname>Button</surname><given-names>KS</given-names></name>, <name><surname>Thom</surname><given-names>HHZ</given-names></name>, <name><surname>Noveck</surname><given-names>RJ</given-names></name>, and <name><surname>Munaf√≤</surname><given-names>Marcus R</given-names></name>. <year>2017</year>. <article-title>The Burden of the ‚ÄúFalse-Negatives‚Äù in Clinical Development: Analyses of Current and Alternative Scenarios and Corrective Measures</article-title>. <source>Clinical and Translational Science</source><volume>10</volume>, <issue>6</issue> (<comment>2017</comment>), <fpage>470</fpage>‚Äì<lpage>479</lpage>.<pub-id pub-id-type="pmid">28675646</pub-id>
</mixed-citation>
    </ref>
    <ref id="R10">
      <label>[10]</label>
      <mixed-citation publication-type="book"><name><surname>Carrie J Cai</surname><given-names>Emily Reif</given-names></name>, <name><surname>Hegde</surname><given-names>Narayan</given-names></name>, <name><surname>Hipp</surname><given-names>Jason</given-names></name>, <name><surname>Kim</surname><given-names>Been</given-names></name>, <name><surname>Smilkov</surname><given-names>Daniel</given-names></name>, <name><surname>Wattenberg</surname><given-names>Martin</given-names></name>, <name><surname>Viegas</surname><given-names>Fernanda</given-names></name>, <name><surname>Corrado</surname><given-names>Greg S</given-names></name>, and <name><surname>Stumpe</surname><given-names>Martin C</given-names></name>. <year>2019</year>. <part-title>Human-centered tools for coping with imperfect algorithms during medical decision-making</part-title>. In <source>Proceedings of the 2019 CHI conference on Human Factors in Computing Systems</source>. <publisher-name>ACM</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>1</fpage>‚Äì<lpage>14</lpage>.</mixed-citation>
    </ref>
    <ref id="R11">
      <label>[11]</label>
      <mixed-citation publication-type="journal"><name><surname>Ricky TQ Chen</surname><given-names>Xuechen Li</given-names></name>, <name><surname>Grosse</surname><given-names>Roger B</given-names></name>, and <name><surname>Duvenaud</surname><given-names>David K</given-names></name>. <year>2018</year>. <article-title>Isolating sources of disentanglement in variational autoencoders</article-title>. <source>Advances in Neural Information Processing Systems</source><volume>31</volume> (<comment>2018</comment>), <fpage>11</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R12">
      <label>[12]</label>
      <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>Xi</given-names></name>, <name><surname>Duan</surname><given-names>Yan</given-names></name>, <name><surname>Houthooft</surname><given-names>Rein</given-names></name>, <name><surname>Schulman</surname><given-names>John</given-names></name>, <name><surname>Sutskever</surname><given-names>Ilya</given-names></name>, and <name><surname>Abbeel</surname><given-names>Pieter</given-names></name>. <year>2016</year>. <article-title>Infogan: Interpretable representation learning by information maximizing generative adversarial nets</article-title>. <source>Advances in Neural Information Processing Systems</source><volume>29</volume> (<comment>2016</comment>), <fpage>10</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R13">
      <label>[13]</label>
      <mixed-citation publication-type="journal"><name><surname>Cheng</surname><given-names>Furui</given-names></name>, <name><surname>Mark S Keller</surname><given-names>Huamin Qu</given-names></name>, <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>, and <name><surname>Wang</surname><given-names>Qianwen</given-names></name>. <year>2023</year>. <article-title>Polyphony: an Interactive Transfer Learning Framework for Single-Cell Data Analysis</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>29</volume>, <issue>1</issue> (<comment>2023</comment>), <fpage>591</fpage>‚Äì<lpage>601</lpage>.<pub-id pub-id-type="pmid">36155452</pub-id>
</mixed-citation>
    </ref>
    <ref id="R14">
      <label>[14]</label>
      <mixed-citation publication-type="book"><name><surname>Cutler</surname><given-names>Zach</given-names></name>, <name><surname>Gadhave</surname><given-names>Kiran</given-names></name>, and <name><surname>Lex</surname><given-names>Alexander</given-names></name>. <year>2020</year>. <part-title>Trrack: A library for provenance-tracking in web-based visualizations</part-title>. In <source>2020 IEEE Visualization Conference (VIS)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>116</fpage>‚Äì<lpage>120</lpage>.</mixed-citation>
    </ref>
    <ref id="R15">
      <label>[15]</label>
      <mixed-citation publication-type="book"><name><surname>Frederik L Dennig</surname><given-names>Tom Polk</given-names></name>, <name><surname>Lin</surname><given-names>Zudi</given-names></name>, <name><surname>Schreck</surname><given-names>Tobias</given-names></name>, <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>, and <name><surname>Behrisch</surname><given-names>Michael</given-names></name>. <year>2019</year>. <part-title>FDive: Learning relevance models using pattern-based similarity measures</part-title>. In <source>2019 IEEE Conference on Visual Analytics Science and Technology (VAST)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>69</fpage>‚Äì<lpage>80</lpage>.</mixed-citation>
    </ref>
    <ref id="R16">
      <label>[16]</label>
      <mixed-citation publication-type="book"><name><surname>Eckelt</surname><given-names>K</given-names></name>, <name><surname>Hinterreiter</surname><given-names>A</given-names></name>, <name><surname>Adelberger</surname><given-names>P</given-names></name>, <name><surname>Walchshofer</surname><given-names>C</given-names></name>, <name><surname>Dhanoa</surname><given-names>V</given-names></name>, <name><surname>Humer</surname><given-names>C</given-names></name>, <name><surname>Heckmann</surname><given-names>M</given-names></name>, <name><surname>Steinparz</surname><given-names>C</given-names></name>, and <name><surname>Streit</surname><given-names>M</given-names></name>. <year>2022</year>. <part-title>Visual Exploration of Relationships and Structure in Low-Dimensional Embeddings</part-title>.. In <source>OSF Preprint</source>, Vol. <pub-id pub-id-type="doi">10.31219/osf.io/ujbrs</pub-id>. <publisher-name>Open Society Foundation</publisher-name>, <publisher-loc>SA</publisher-loc>, <fpage>15</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R17">
      <label>[17]</label>
      <mixed-citation publication-type="webpage"><collab>Facebook</collab>. <year>2014</year>. <source>React.js</source>. <comment><ext-link xlink:href="https://github.com/facebook/react" ext-link-type="uri">https://github.com/facebook/react</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R18">
      <label>[18]</label>
      <mixed-citation publication-type="journal"><name><surname>Gou</surname><given-names>Liang</given-names></name>, <name><surname>Zou</surname><given-names>Lincan</given-names></name>, <name><surname>Li</surname><given-names>Nanxiang</given-names></name>, <name><surname>Hofmann</surname><given-names>Michael</given-names></name>, <name><surname>Arvind Kumar Shekar</surname><given-names>Axel Wendt</given-names></name>, and <name><surname>Ren</surname><given-names>Liu</given-names></name>. <year>2020</year>. <article-title>VATLD: a visual analytics system to assess, understand and improve traffic light detection</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>27</volume>, <issue>2</issue> (<comment>2020</comment>), <fpage>261</fpage>‚Äì<lpage>271</lpage>.</mixed-citation>
    </ref>
    <ref id="R19">
      <label>[19]</label>
      <mixed-citation publication-type="book"><name><surname>Grinberg</surname><given-names>Miguel</given-names></name>. <year>2018</year>. <source>Flask web development: developing web applications with python</source>. <publisher-name>O‚ÄôReilly Media, Inc</publisher-name>., <publisher-loc>USA</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="R20">
      <label>[20]</label>
      <mixed-citation publication-type="journal"><name><surname>He</surname><given-names>Wenbin</given-names></name>, <name><surname>Zou</surname><given-names>Lincan</given-names></name>, <name><surname>Arvind Kumar Shekar</surname><given-names>Liang Gou</given-names></name>, and <name><surname>Ren</surname><given-names>Liu</given-names></name>. <year>2021</year>. <article-title>Where Can We Help? A Visual Analytics Approach to Diagnosing and Improving Semantic Segmentation of Movable Objects</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>1</issue> (<comment>2021</comment>), <fpage>1040</fpage>‚Äì<lpage>1050</lpage>.<pub-id pub-id-type="pmid">34587077</pub-id>
</mixed-citation>
    </ref>
    <ref id="R21">
      <label>[21]</label>
      <mixed-citation publication-type="book"><name><surname>Higgins</surname><given-names>Irina</given-names></name>, <name><surname>Amos</surname><given-names>David</given-names></name>, <name><surname>Pfau</surname><given-names>David</given-names></name>, <name><surname>Racaniere</surname><given-names>Sebastien</given-names></name>, <name><surname>Matthey</surname><given-names>Loic</given-names></name>, <name><surname>Rezende</surname><given-names>Danilo</given-names></name>, and <name><surname>Lerchner</surname><given-names>Alexander</given-names></name>. <year>2018</year>. <part-title>Towards a definition of disentangled representations</part-title>. In <source>arXiv preprint</source>, <comment>Vol. arXiv:1812.02230.</comment><publisher-name>Cornell University</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>25</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R22">
      <label>[22]</label>
      <mixed-citation publication-type="book"><name><surname>Higgins</surname><given-names>Irina</given-names></name>, <name><surname>Matthey</surname><given-names>Loic</given-names></name>, <name><surname>Pal</surname><given-names>Arka</given-names></name>, <name><surname>Burgess</surname><given-names>Christopher</given-names></name>, <name><surname>Glorot</surname><given-names>Xavier</given-names></name>, <name><surname>Botvinick</surname><given-names>Matthew</given-names></name>, <name><surname>Mohamed</surname><given-names>Shakir</given-names></name>, and <name><surname>Lerchner</surname><given-names>Alexander</given-names></name>. <year>2016</year>. <part-title>beta-vae: Learning basic visual concepts with a constrained variational framework</part-title>. In <source>International Conference on Machine Learning (ICLR)</source>. <publisher-name>International Machine Learning Society</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>12</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R23">
      <label>[23]</label>
      <mixed-citation publication-type="journal"><name><surname>Jia</surname><given-names>Shichao</given-names></name>, <name><surname>Li</surname><given-names>Zeyu</given-names></name>, <name><surname>Chen</surname><given-names>Nuo</given-names></name>, and <name><surname>Zhang</surname><given-names>Jiawan</given-names></name>. <year>2021</year>. <article-title>Towards visual explainable active learning for zero-shot classification</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>1</issue> (<comment>2021</comment>), <fpage>791</fpage>‚Äì<lpage>801</lpage>.<pub-id pub-id-type="pmid">34587036</pub-id>
</mixed-citation>
    </ref>
    <ref id="R24">
      <label>[24]</label>
      <mixed-citation publication-type="book"><name><surname>Jin</surname><given-names>Zhihua</given-names></name>, <name><surname>Wang</surname><given-names>Yong</given-names></name>, <name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Ming</surname><given-names>Yao</given-names></name>, <name><surname>Ma</surname><given-names>Tengfei</given-names></name>, and <name><surname>Qu</surname><given-names>Huamin</given-names></name>. <year>2020</year>. <part-title>GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks</part-title>. In <source>arXiv preprint</source>, <comment>Vol. arXiv:2011.11048.</comment><publisher-name>Cornell University</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>17</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R25">
      <label>[25]</label>
      <mixed-citation publication-type="journal"><name><surname>Kahng</surname><given-names>Minsuk</given-names></name>, <name><surname>Pierre Y</surname><given-names> Andrews</given-names></name>, <name><surname>Aditya</surname><given-names>Kalro</given-names></name>, and <name><surname>Chau</surname><given-names>Duen Horng</given-names></name>. <year>2017</year>. <article-title>Activis: Visual exploration of industry-scale deep neural network models</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>24</volume>, <issue>1</issue> (<comment>2017</comment>), <fpage>88</fpage>‚Äì<lpage>97</lpage>.<pub-id pub-id-type="pmid">28866557</pub-id>
</mixed-citation>
    </ref>
    <ref id="R26">
      <label>[26]</label>
      <mixed-citation publication-type="book"><name><surname>Kim</surname><given-names>Been</given-names></name>, <name><surname>Wattenberg</surname><given-names>Martin</given-names></name>, <name><surname>Gilmer</surname><given-names>Justin</given-names></name>, <name><surname>Cai</surname><given-names>Carrie</given-names></name>, <name><surname>Wexler</surname><given-names>James</given-names></name>, <name><surname>Viegas</surname><given-names>Fernanda</given-names></name>, <etal/><year>2018</year>. <part-title>Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)</part-title>. In <source>International conference on machine learning</source>. <publisher-name>PMLR</publisher-name>, <publisher-loc>Stockholm, Sweden</publisher-loc>, <fpage>2668</fpage>‚Äì<lpage>2677</lpage>.</mixed-citation>
    </ref>
    <ref id="R27">
      <label>[27]</label>
      <mixed-citation publication-type="journal"><name><surname>Kim</surname><given-names>Hannah</given-names></name>, <name><surname>Choo</surname><given-names>Jaegul</given-names></name>, <name><surname>Park</surname><given-names>Haesun</given-names></name>, and <name><surname>Endert</surname><given-names>Alex</given-names></name>. <year>2015</year>. <article-title>Interaxis: Steering scatterplot axes via observation-level interaction</article-title>. <source>IEEE transactions on visualization and computer graphics</source><volume>22</volume>, <issue>1</issue> (<comment>2015</comment>), <fpage>131</fpage>‚Äì<lpage>140</lpage>.<pub-id pub-id-type="pmid">26357399</pub-id>
</mixed-citation>
    </ref>
    <ref id="R28">
      <label>[28]</label>
      <mixed-citation publication-type="book"><name><surname>Kim</surname><given-names>Hyunjik</given-names></name> and <name><surname>Mnih</surname><given-names>Andriy</given-names></name>. <year>2018</year>. <part-title>Disentangling by factorising</part-title>. In <source>International Conference on Machine Learning</source>. <publisher-name>PMLR</publisher-name>, <publisher-loc>Stockholm, Sweden</publisher-loc>, <fpage>2649</fpage>‚Äì<lpage>2658</lpage>.</mixed-citation>
    </ref>
    <ref id="R29">
      <label>[29]</label>
      <mixed-citation publication-type="journal"><name><surname>Bum Chul Kwon</surname><given-names>Hannah Kim</given-names></name>, <name><surname>Wall</surname><given-names>Emily</given-names></name>, <name><surname>Choo</surname><given-names>Jaegul</given-names></name>, <name><surname>Park</surname><given-names>Haesun</given-names></name>, and <name><surname>Endert</surname><given-names>Alex</given-names></name>. <year>2016</year>. <article-title>Axisketcher: Interactive nonlinear axis mapping of visualizations through user drawings</article-title>. <source>IEEE transactions on visualization and computer graphics</source><volume>23</volume>, <issue>1</issue> (<comment>2016</comment>), <fpage>221</fpage>‚Äì<lpage>230</lpage>.<pub-id pub-id-type="pmid">27514048</pub-id>
</mixed-citation>
    </ref>
    <ref id="R30">
      <label>[30]</label>
      <mixed-citation publication-type="journal"><name><surname>Lekschas</surname><given-names>Fritz</given-names></name>, <name><surname>Bach</surname><given-names>Benjamin</given-names></name>, <name><surname>Kerpedjiev</surname><given-names>Peter</given-names></name>, <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>, and <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>. <year>2017</year>. <article-title>HiPiler: visual exploration of large genome interaction matrices with interactive small multiples</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>24</volume>, <issue>1</issue> (<comment>2017</comment>), <fpage>522</fpage>‚Äì<lpage>531</lpage>.<pub-id pub-id-type="pmid">28866592</pub-id>
</mixed-citation>
    </ref>
    <ref id="R31">
      <label>[31]</label>
      <mixed-citation publication-type="journal"><name><surname>Lekschas</surname><given-names>Fritz</given-names></name>, <name><surname>Behrisch</surname><given-names>Michael</given-names></name>, <name><surname>Bach</surname><given-names>Benjamin</given-names></name>, <name><surname>Kerpedjiev</surname><given-names>Peter</given-names></name>, <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>, and <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>. <year>2020</year>. <article-title>Pattern-Driven Navigation in 2D Multiscale Visualizations with Scalable Insets</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>26</volume>, <issue>1</issue> (<comment>1 1 2020</comment>), <fpage>611</fpage>‚Äì<lpage>621</lpage>.<pub-id pub-id-type="pmid">31442989</pub-id>
</mixed-citation>
    </ref>
    <ref id="R32">
      <label>[32]</label>
      <mixed-citation publication-type="book"><name><surname>Lekschas</surname><given-names>Fritz</given-names></name>, <name><surname>Peterson</surname><given-names>Brant</given-names></name>, <name><surname>Haehn</surname><given-names>Daniel</given-names></name>, <name><surname>Ma</surname><given-names>Eric</given-names></name>, <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>, and <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>. <year>2020</year>. <part-title>Peax: Interactive visual pattern search in sequential data using unsupervised deep representation learning</part-title>. In <source>Computer Graphics Forum</source>, Vol. <volume>39‚Äì3</volume>. <publisher-name>Wiley Online Library</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>167</fpage>‚Äì<lpage>179</lpage>.</mixed-citation>
    </ref>
    <ref id="R33">
      <label>[33]</label>
      <mixed-citation publication-type="journal"><name><surname>Lekschas</surname><given-names>Fritz</given-names></name>, <name><surname>Zhou</surname><given-names>Xinyi</given-names></name>, <name><surname>Chen</surname><given-names>Wei</given-names></name>, <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>, <name><surname>Bach</surname><given-names>Benjamin</given-names></name>, and <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>. <year>2021</year>. <article-title>A Generic Framework and Library for Exploration of Small Multiples through Interactive Piling</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>27</volume>, <fpage>2</fpage> (<comment>1 2 2021</comment>), <fpage>358</fpage>‚Äì<lpage>368</lpage>.</mixed-citation>
    </ref>
    <ref id="R34">
      <label>[34]</label>
      <mixed-citation publication-type="book"><name><surname>Liu</surname><given-names>Mengchen</given-names></name>, <name><surname>Liu</surname><given-names>Shixia</given-names></name>, <name><surname>Su</surname><given-names>Hang</given-names></name>, <name><surname>Cao</surname><given-names>Kelei</given-names></name>, and <name><surname>Zhu</surname><given-names>Jun</given-names></name>. <year>2018</year>. <part-title>Analyzing the noise robustness of deep neural networks</part-title>. In <source>2018 IEEE Conference on Visual Analytics Science and Technology (VAST)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>60</fpage>‚Äì<lpage>71</lpage>.</mixed-citation>
    </ref>
    <ref id="R35">
      <label>[35]</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>Shusen</given-names></name>, <name><surname>Bremer</surname><given-names>Peer-Timo</given-names></name>, <name><surname>Jayaraman J Thiagarajan</surname><given-names>Vivek Srikumar</given-names></name>, <name><surname>Wang</surname><given-names>Bei</given-names></name>, <name><surname>Livnat</surname><given-names>Yarden</given-names></name>, and <name><surname>Pascucci</surname><given-names>Valerio</given-names></name>. <year>2017</year>. <article-title>Visual exploration of semantic relationships in neural word embeddings</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>24</volume>, <issue>1</issue> (<comment>2017</comment>), <fpage>553</fpage>‚Äì<lpage>562</lpage>.<pub-id pub-id-type="pmid">28866574</pub-id>
</mixed-citation>
    </ref>
    <ref id="R36">
      <label>[36]</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>Xiao</given-names></name>, <name><surname>Sanchez</surname><given-names>Pedro</given-names></name>, <name><surname>Thermos</surname><given-names>Spyridon</given-names></name>, <name><surname>O‚ÄôNeil</surname><given-names>Alison Q</given-names></name>, and <name><surname>Tsaftaris</surname><given-names>Sotirios A</given-names></name>. <year>2022</year>. <article-title>Learning disentangled representations in the imaging domain</article-title>. <source>Medical Image Analysis</source><volume>80</volume> (<comment>2022</comment>), <fpage>102516</fpage>.<pub-id pub-id-type="pmid">35751992</pub-id>
</mixed-citation>
    </ref>
    <ref id="R37">
      <label>[37]</label>
      <mixed-citation publication-type="book"><name><surname>Liu</surname><given-names>Yang</given-names></name>, <name><surname>Jun</surname><given-names>Eunice</given-names></name>, <name><surname>Li</surname><given-names>Qisheng</given-names></name>, and <name><surname>Heer</surname><given-names>Jeffrey</given-names></name>. <year>2019</year>. <part-title>Latent space cartography: Visual analysis of vector space embeddings</part-title>. In <source>Computer Graphics Forum</source>, Vol. <volume>38</volume>. <publisher-name>Wiley Online Library</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>67</fpage>‚Äì<lpage>78</lpage>.</mixed-citation>
    </ref>
    <ref id="R38">
      <label>[38]</label>
      <mixed-citation publication-type="book"><name><surname>Liu</surname><given-names>Ziwei</given-names></name>, <name><surname>Luo</surname><given-names>Ping</given-names></name>, <name><surname>Wang</surname><given-names>Xiaogang</given-names></name>, and <name><surname>Tang</surname><given-names>Xiaoou</given-names></name>. <year>2015</year>. <part-title>Deep Learning Face Attributes in the Wild</part-title>. In <source>Proceedings of International Conference on Computer Vision (ICCV)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>3730</fpage>‚Äì<lpage>3738</lpage>.</mixed-citation>
    </ref>
    <ref id="R39">
      <label>[39]</label>
      <mixed-citation publication-type="journal"><name><surname>Sehi L‚ÄôYi</surname><given-names>Qianwen Wang</given-names></name>, <name><surname>Lekschas</surname><given-names>Fritz</given-names></name>, and <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>. <year>2021</year>. <article-title>Gosling: A Grammar-based Toolkit for Scalable and Interactive Genomics Data Visualization</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>1</issue> (<comment>1 10 2021</comment>), <fpage>140</fpage>‚Äì<lpage>150</lpage>.<pub-id pub-id-type="pmid">34596551</pub-id>
</mixed-citation>
    </ref>
    <ref id="R40">
      <label>[40]</label>
      <mixed-citation publication-type="webpage"><name><surname>Matthey</surname><given-names>Loic</given-names></name>, <name><surname>Higgins</surname><given-names>Irina</given-names></name>, <name><surname>Hassabis</surname><given-names>Demis</given-names></name>, and <name><surname>Lerchner</surname><given-names>Alexander</given-names></name>. <year>2017</year>. <source>dSprites: Disentanglement testing Sprites dataset</source>. <comment><ext-link xlink:href="https://github.com/deepmind/dsprites-dataset/" ext-link-type="uri">https://github.com/deepmind/dsprites-dataset/</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R41">
      <label>[41]</label>
      <mixed-citation publication-type="journal"><name><surname>Leland McInnes</surname><given-names>John Healy</given-names></name>, <name><surname>Saul</surname><given-names>Nathaniel</given-names></name>, and <name><surname>Gro√überger</surname><given-names>Lukas</given-names></name>. <year>2018</year>. <article-title>UMAP: Uniform Manifold Approximation and Projection</article-title>. <source>Journal of Open Source Software</source><volume>3</volume>, <issue>29</issue> (<comment>2018</comment>), <fpage>861</fpage>.</mixed-citation>
    </ref>
    <ref id="R42">
      <label>[42]</label>
      <mixed-citation publication-type="book"><name><surname>Ming</surname><given-names>Yao</given-names></name>, <name><surname>Cao</surname><given-names>Shaozu</given-names></name>, <name><surname>Zhang</surname><given-names>Ruixiang</given-names></name>, <name><surname>Li</surname><given-names>Zhen</given-names></name>, <name><surname>Chen</surname><given-names>Yuanzhe</given-names></name>, <name><surname>Song</surname><given-names>Yangqiu</given-names></name>, and <name><surname>Qu</surname><given-names>Huamin</given-names></name>. <year>2017</year>. <part-title>Understanding hidden memories of recurrent neural networks</part-title>. In <source>2017 IEEE Conference on Visual Analytics Science and Technology (VAST)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>13</fpage>‚Äì<lpage>24</lpage>.</mixed-citation>
    </ref>
    <ref id="R43">
      <label>[43]</label>
      <mixed-citation publication-type="webpage"><name><surname>Mooney</surname><given-names>Paul</given-names></name>. <year>2017</year>. <source>Breast Histopathology Images</source>. <comment><ext-link xlink:href="https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images" ext-link-type="uri">https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R44">
      <label>[44]</label>
      <mixed-citation publication-type="journal"><name><surname>Mori</surname><given-names>Giulio</given-names></name>, <name><surname>Patern√≤</surname><given-names>Fabio</given-names></name>, and <name><surname>Santoro</surname><given-names>Carmen</given-names></name>. <year>2002</year>. <article-title>CTTE: support for developing and analyzing task models for interactive system design</article-title>. <source>IEEE Transactions on software engineering</source><volume>28</volume>, <issue>8</issue> (<comment>2002</comment>), <fpage>797</fpage>‚Äì<lpage>813</lpage>.</mixed-citation>
    </ref>
    <ref id="R45">
      <label>[45]</label>
      <mixed-citation publication-type="book"><name><surname>Paszke</surname><given-names>Adam</given-names></name>, <name><surname>Gross</surname><given-names>Sam</given-names></name>, <name><surname>Massa</surname><given-names>Francisco</given-names></name>, <name><surname>Lerer</surname><given-names>Adam</given-names></name>, <name><surname>Bradbury</surname><given-names>James</given-names></name>, <name><surname>Chanan</surname><given-names>Gregory</given-names></name>, <name><surname>Killeen</surname><given-names>Trevor</given-names></name>, <name><surname>Lin</surname><given-names>Zeming</given-names></name>, <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>, <name><surname>Antiga</surname><given-names>Luca</given-names></name>, <name><surname>Desmaison</surname><given-names>Alban</given-names></name>, <name><surname>Kopf</surname><given-names>Andreas</given-names></name>, <name><surname>Yang</surname><given-names>Edward</given-names></name>, <name><surname>Zachary DeVito</surname><given-names>Martin Raison</given-names></name>, <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>, <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>, <name><surname>Steiner</surname><given-names>Benoit</given-names></name>, <name><surname>Fang</surname><given-names>Lu</given-names></name>, <name><surname>Bai</surname><given-names>Junjie</given-names></name>, and <name><surname>Chintala</surname><given-names>Soumith</given-names></name>. <year>2019</year>. <part-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</part-title>. In <source>Advances in Neural Information Processing Systems 32</source>, <name><surname>Wallach</surname><given-names>H</given-names></name>, <name><surname>Larochelle</surname><given-names>H</given-names></name>, <name><surname>Beygelzimer</surname><given-names>A</given-names></name>, <name><surname>d‚ÄôAlch√©-Buc</surname><given-names>F</given-names></name>, <name><surname>Fox</surname><given-names>E</given-names></name>, and <name><surname>Garnett</surname><given-names>R</given-names></name> (Eds.). <publisher-name>Curran Associates, Inc</publisher-name>., <publisher-loc>NY, USA</publisher-loc>, <fpage>8024</fpage>‚Äì<lpage>8035</lpage>.</mixed-citation>
    </ref>
    <ref id="R46">
      <label>[46]</label>
      <mixed-citation publication-type="journal"><name><surname>Pezzotti</surname><given-names>Nicola</given-names></name>, <name><surname>Thomas H√∂llt</surname><given-names>Jan Van Gemert</given-names></name>, <name><surname>Boudewijn PF Lelieveldt</surname><given-names>Elmar Eisemann</given-names></name>, and <name><surname>Vilanova</surname><given-names>Anna</given-names></name>. <year>2017</year>. <article-title>Deepeyes: Progressive visual analytics for designing deep neural networks</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>24</volume>, <issue>1</issue> (<comment>2017</comment>), <fpage>98</fpage>‚Äì<lpage>108</lpage>.<pub-id pub-id-type="pmid">28866543</pub-id>
</mixed-citation>
    </ref>
    <ref id="R47">
      <label>[47]</label>
      <mixed-citation publication-type="book"><name><surname>Pirrung</surname><given-names>Meg</given-names></name>, <name><surname>Hilliard</surname><given-names>Nathan</given-names></name>, <name><surname>Yankov</surname><given-names>Art√´m</given-names></name>, <name><surname>Nancy O‚ÄôBrien</surname><given-names>Paul Weidert</given-names></name>, <name><surname>Corley</surname><given-names>Courtney D</given-names></name>, and <name><surname>Hodas</surname><given-names>Nathan O</given-names></name>. <year>2018</year>. <part-title>Sharkzor: Interactive deep learning for image triage, sort and summary</part-title>. In <source>arXiv preprint</source>, <comment>Vol. arXiv:1802.05316.</comment><publisher-name>Cornell University</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>4</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R48">
      <label>[48]</label>
      <mixed-citation publication-type="journal"><name><surname>Rao</surname><given-names>Suhas SP</given-names></name>, <name><surname>Huntley</surname><given-names>Miriam H</given-names></name>, <name><surname>Durand</surname><given-names>Neva C</given-names></name>, <name><surname>Stamenova</surname><given-names>Elena K</given-names></name>, <name><surname>Bochkov</surname><given-names>Ivan D</given-names></name>, <name><surname>Robinson</surname><given-names>James T</given-names></name>, <name><surname>Sanborn</surname><given-names>Adrian L</given-names></name>, <name><surname>Machol</surname><given-names>Ido</given-names></name>, <name><surname>Omer</surname><given-names>Arina D</given-names></name>, <name><surname>Lander</surname><given-names>Eric S</given-names></name>, <etal/><year>2014</year>. <article-title>A 3D map of the human genome at kilobase resolution reveals principles of chromatin looping</article-title>. <source>Cell</source><volume>159</volume>, <issue>7</issue> (<comment>2014</comment>), <fpage>1665</fpage>‚Äì<lpage>1680</lpage>.<pub-id pub-id-type="pmid">25497547</pub-id>
</mixed-citation>
    </ref>
    <ref id="R49">
      <label>[49]</label>
      <mixed-citation publication-type="journal"><name><surname>Ren</surname><given-names>Pengzhen</given-names></name>, <name><surname>Xiao</surname><given-names>Yun</given-names></name>, <name><surname>Chang</surname><given-names>Xiaojun</given-names></name>, <name><surname>Huang</surname><given-names>Po-Yao</given-names></name>, <name><surname>Li</surname><given-names>Zhihui</given-names></name>, <name><surname>Brij B Gupta</surname><given-names>Xiaojiang Chen</given-names></name>, and <name><surname>Wang</surname><given-names>Xin</given-names></name>. <year>2021</year>. <article-title>A survey of deep active learning</article-title>. <source>Comput. Surveys</source><volume>54</volume>, <issue>9</issue> (<comment>2021</comment>), <fpage>1</fpage>‚Äì<lpage>40</lpage>.</mixed-citation>
    </ref>
    <ref id="R50">
      <label>[50]</label>
      <mixed-citation publication-type="journal"><name><surname>Russakovsky</surname><given-names>Olga</given-names></name>, <name><surname>Deng</surname><given-names>Jia</given-names></name>, <name><surname>Su</surname><given-names>Hao</given-names></name>, <name><surname>Krause</surname><given-names>Jonathan</given-names></name>, <name><surname>Satheesh</surname><given-names>Sanjeev</given-names></name>, <name><surname>Ma</surname><given-names>Sean</given-names></name>, <name><surname>Huang</surname><given-names>Zhiheng</given-names></name>, <name><surname>Karpathy</surname><given-names>Andrej</given-names></name>, <name><surname>Khosla</surname><given-names>Aditya</given-names></name>, <name><surname>Bernstein</surname><given-names>Michael</given-names></name>, <name><surname>Berg</surname><given-names>Alexander C.</given-names></name>, and <name><surname>Fei-Fei</surname><given-names>Li</given-names></name>. <year>2015</year>. <article-title>ImageNet Large Scale Visual Recognition Challenge</article-title>. <source>International Journal of Computer Vision (IJCV)</source><volume>115</volume>, <issue>3</issue> (<comment>2015</comment>), <fpage>211</fpage>‚Äì<lpage>252</lpage>. <pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></mixed-citation>
    </ref>
    <ref id="R51">
      <label>[51]</label>
      <mixed-citation publication-type="book"><name><surname>Smilkov</surname><given-names>Daniel</given-names></name>, <name><surname>Thorat</surname><given-names>Nikhil</given-names></name>, <name><surname>Nicholson</surname><given-names>Charles</given-names></name>, <name><surname>Reif</surname><given-names>Emily</given-names></name>, <name><surname>Vi√©gas</surname><given-names>Fernanda B</given-names></name>, and <name><surname>Martin Wattenberg</surname></name>. <year>2016</year>. <part-title>Embedding projector: Interactive visualization and interpretation of embeddings</part-title>. In <source>arXiv preprint</source>, <comment>Vol. arXiv:1611.05469.</comment><publisher-name>Cornell University</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>4</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R52">
      <label>[52]</label>
      <mixed-citation publication-type="webpage"><name><surname>Sothivelr</surname><given-names>Karthick</given-names></name>. <year>2020</year>. <source>Breast Cancer Classification With PyTorch and Deep Learning</source>. <comment><ext-link xlink:href="https://medium.com/swlh/" ext-link-type="uri">https://medium.com/swlh/</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R53">
      <label>[53]</label>
      <mixed-citation publication-type="book"><name><surname>Jost Tobias Springenberg</surname><given-names>Alexey Dosovitskiy</given-names></name>, <name><surname>Brox</surname><given-names>Thomas</given-names></name>, and <name><surname>Ried-miller</surname><given-names>Martin</given-names></name>. <year>2014</year>. <part-title>Striving for simplicity: The all convolutional net</part-title>. In <source>ICLR (workshop track)</source>. <publisher-name>International Machine Learning Society</publisher-name>, <publisher-loc>USA</publisher-loc>, <fpage>14</fpage> pages.</mixed-citation>
    </ref>
    <ref id="R54">
      <label>[54]</label>
      <mixed-citation publication-type="journal"><name><surname>Strobelt</surname><given-names>Hendrik</given-names></name>, <name><surname>Gehrmann</surname><given-names>Sebastian</given-names></name>, <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>, and <name><surname>Rush</surname><given-names>Alexander M</given-names></name>. <year>2017</year>. <article-title>Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>24</volume>, <issue>1</issue> (<comment>2017</comment>), <fpage>667</fpage>‚Äì<lpage>676</lpage>.<pub-id pub-id-type="pmid">28866526</pub-id>
</mixed-citation>
    </ref>
    <ref id="R55">
      <label>[55]</label>
      <mixed-citation publication-type="journal"><name><surname>Sutcliffe</surname><given-names>Alistair</given-names></name>. <year>2000</year>. <article-title>On the effective use and reuse of HCI knowledge</article-title>. <source>ACM Transactions on Computer-Human Interaction (TOCHI)</source><volume>7</volume>, <issue>2</issue> (<comment>2000</comment>), <fpage>197</fpage>‚Äì<lpage>221</lpage>.</mixed-citation>
    </ref>
    <ref id="R56">
      <label>[56]</label>
      <mixed-citation publication-type="journal"><name><surname>Sutcliffe</surname><given-names>Alistair G</given-names></name> and <name><surname>Carroll</surname><given-names>John M</given-names></name>. <year>1999</year>. <article-title>Designing claims for reuse in interactive systems design</article-title>. <source>International Journal of Human-Computer Studies</source>
<volume>50</volume>, <issue>3</issue> (<comment>1999</comment>), <fpage>213</fpage>‚Äì<lpage>241</lpage>.</mixed-citation>
    </ref>
    <ref id="R57">
      <label>[57]</label>
      <mixed-citation publication-type="book"><name><surname>Tufte</surname><given-names>Edward R</given-names></name>, <name><surname>Goeler</surname><given-names>Nora Hillman</given-names></name>, and <name><surname>Benson</surname><given-names>Richard</given-names></name>. <year>1990</year>. <source>Envisioning information</source>. Vol. <volume>126</volume>. <publisher-name>Graphics press</publisher-name>, <publisher-loc>Cheshire, CT</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="R58">
      <label>[58]</label>
      <mixed-citation publication-type="journal"><collab>Laurens Van der Maaten and Geoffrey Hinton</collab>. <year>2008</year>. <article-title>Visualizing data using t-SNE</article-title>. <source>Journal of Machine Learning Research</source><volume>9</volume>, <issue>11</issue> (<comment>2008</comment>), <fpage>2579</fpage>‚Äì<lpage>2605</lpage>.</mixed-citation>
    </ref>
    <ref id="R59">
      <label>[59]</label>
      <mixed-citation publication-type="book"><name><surname>Wang</surname><given-names>Junpeng</given-names></name>, <name><surname>Zhang</surname><given-names>Wei</given-names></name>, and <name><surname>Yang</surname><given-names>Hao</given-names></name>. <year>2020</year>. <part-title>SCANViz: Interpreting the symbol-concept association captured by deep neural networks through visual analytics</part-title>. In <source>2020 IEEE Pacific Visualization Symposium (PacificVis)</source>. <publisher-name>IEEE</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>51</fpage>‚Äì<lpage>60</lpage>.</mixed-citation>
    </ref>
    <ref id="R60">
      <label>[60]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Alexander</surname><given-names>William</given-names></name>, <name><surname>Pegg</surname><given-names>Jack</given-names></name>, <name><surname>Qu</surname><given-names>Huamin</given-names></name>, and <name><surname>Chen</surname><given-names>Min</given-names></name>. <year>2020</year>. <article-title>HypoML: Visual analysis for hypothesis-based evaluation of machine learning models</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>27</volume>, <issue>2</issue> (<comment>2020</comment>), <fpage>1417</fpage>‚Äì<lpage>1426</lpage>.</mixed-citation>
    </ref>
    <ref id="R61">
      <label>[61]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Chen</surname><given-names>Zhutian</given-names></name>, <name><surname>Wang</surname><given-names>Yong</given-names></name>, and <name><surname>Qu</surname><given-names>Huamin</given-names></name>. <year>2021</year>. <article-title>A Survey on ML4VIS: Applying MachineLearning Advances to Data Visualization</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>12</issue> (<comment>2021</comment>), <fpage>5134</fpage>‚Äì<lpage>5153</lpage>.</mixed-citation>
    </ref>
    <ref id="R62">
      <label>[62]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Li</surname><given-names>Zhen</given-names></name>, <name><surname>Fu</surname><given-names>Siwei</given-names></name>, <name><surname>Cui</surname><given-names>Weiwei</given-names></name>, and <name><surname>Qu</surname><given-names>Huamin</given-names></name>. <year>2019</year>. <article-title>Narvis: Authoring Narrative Slideshows for Introducing Data Visualization Designs</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>25</volume>, <issue>1</issue> (<month>jan</month>
<comment>2019</comment>), <fpage>779</fpage>‚Äì<lpage>788</lpage>.</mixed-citation>
    </ref>
    <ref id="R63">
      <label>[63]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Mazor</surname><given-names>Tali</given-names></name>, <name><surname>Theresa A Harbig</surname><given-names>Ethan Cerami</given-names></name>, and <name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>. <year>2021</year>. <article-title>ThreadStates: State-based Visual Analysis of Disease Progression</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>1</issue> (<comment>2021</comment>), <fpage>238</fpage>‚Äì<lpage>247</lpage>.<pub-id pub-id-type="pmid">34587068</pub-id>
</mixed-citation>
    </ref>
    <ref id="R64">
      <label>[64]</label>
      <mixed-citation publication-type="book"><name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Ming</surname><given-names>Yao</given-names></name>, <name><surname>Jin</surname><given-names>Zhihua</given-names></name>, <name><surname>Shen</surname><given-names>Qiaomu</given-names></name>, <name><surname>Liu</surname><given-names>Dongyu</given-names></name>, <name><surname>Micah J Smith</surname><given-names>Kalyan Veeramachaneni</given-names></name>, and <name><surname>Qu</surname><given-names>Huamin</given-names></name>. <year>2019</year>. <part-title>Atmseer: Increasing transparency and controllability in automated machine learning</part-title>. In <source>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</source>. <publisher-name>ACM</publisher-name>, <publisher-loc>NY, USA</publisher-loc>, <fpage>1</fpage>‚Äì<lpage>12</lpage>.</mixed-citation>
    </ref>
    <ref id="R65">
      <label>[65]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Qianwen</given-names></name>, <name><surname>Xu</surname><given-names>Zhenhua</given-names></name>, <name><surname>Chen</surname><given-names>Zhutian</given-names></name>, <name><surname>Wang</surname><given-names>Yong</given-names></name>, <name><surname>Liu</surname><given-names>Shixia</given-names></name>, and <name><surname>Qu</surname><given-names>Huamin</given-names></name>. <year>2020</year>. <article-title>Visual analysis of discrimination in machine learning</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>27</volume>, <issue>2</issue> (<comment>2020</comment>), <fpage>1470</fpage>‚Äì<lpage>1480</lpage>.</mixed-citation>
    </ref>
    <ref id="R66">
      <label>[66]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Yinqiao</given-names></name>, <name><surname>Chen</surname><given-names>Lu</given-names></name>, <name><surname>Jo</surname><given-names>Jaemin</given-names></name>, and <name><surname>Wang</surname><given-names>Yunhai</given-names></name>. <year>2021</year>. <article-title>Joint t-SNE for Comparable Projections of Multiple High-Dimensional Datasets</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>1</issue> (<comment>2021</comment>), <fpage>623</fpage>‚Äì<lpage>632</lpage>.<pub-id pub-id-type="pmid">34587021</pub-id>
</mixed-citation>
    </ref>
    <ref id="R67">
      <label>[67]</label>
      <mixed-citation publication-type="journal"><name><surname>Yuan</surname><given-names>Jun</given-names></name>, <name><surname>Chen</surname><given-names>Changjian</given-names></name>, <name><surname>Yang</surname><given-names>Weikai</given-names></name>, <name><surname>Liu</surname><given-names>Mengchen</given-names></name>, <name><surname>Xia</surname><given-names>Jiazhi</given-names></name>, and <name><surname>Liu</surname><given-names>Shixia</given-names></name>. <year>2021</year>. <article-title>A survey of visual analytics techniques for machine learning</article-title>. <source>Computational Visual Media</source><volume>7</volume>, <issue>1</issue> (<comment>2021</comment>), <fpage>3</fpage>‚Äì<lpage>36</lpage>.</mixed-citation>
    </ref>
    <ref id="R68">
      <label>[68]</label>
      <mixed-citation publication-type="journal"><name><surname>Zhao</surname><given-names>Zhenge</given-names></name>, <name><surname>Xu</surname><given-names>Panpan</given-names></name>, <name><surname>Scheidegger</surname><given-names>Carlos</given-names></name>, and <name><surname>Ren</surname><given-names>Liu</given-names></name>. <year>2021</year>. <article-title>Human-in-the-loop Extraction of Interpretable Concepts in Deep Learning Models</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source><volume>28</volume>, <issue>1</issue> (<comment>2021</comment>), <fpage>780</fpage>‚Äì<lpage>790</lpage>.<pub-id pub-id-type="pmid">34587066</pub-id>
</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="F1">
    <label>Figure 1:</label>
    <caption>
      <p id="P82">Drava enables concept-driven exploration by aligning semantic latent dimensions with human concepts. (a) UMAP projection of image patches of breast cancer specimens. (b) All image patches are organized and piled up based on the density of tissues. (c) All image patches are grouped into a grid layout according to the tissue density and color. The two visual concepts reveal a strong association of the presentation of invasive ductal carcinomas (IDC), <italic toggle="yes">i.e</italic>., the orange label. (d‚Äìe) More examples.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0001" position="float"/>
  </fig>
  <fig position="float" id="F2">
    <label>Figure 2:</label>
    <caption>
      <p id="P83">Mismatches between semantic latent dimensions and human concepts (red dashed boxes). (a): Synthesized images through value traversal of a latent dimension. (b): Items with the same latent values as the left- and right-most synthesized images.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0002" position="float"/>
  </fig>
  <fig position="float" id="F3">
    <label>Figure 3:</label>
    <caption>
      <p id="P84">A three-step workflow that guides the application of DRL for the concept-driven exploration of small multiples.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0003" position="float"/>
  </fig>
  <fig position="float" id="F4">
    <label>Figure 4:</label>
    <caption>
      <p id="P85">The user interface of Drava. The <italic toggle="yes">Concept View</italic> presents latent dimensions and other metadata as rows. The <italic toggle="yes">Item Browser</italic> enables user exploration of items based on selected concepts from the <italic toggle="yes">Concept View</italic> and can be controlled through the Configuration panel on the right. The <italic toggle="yes">Spatial View</italic> provides context information of the items when applicable.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0004" position="float"/>
  </fig>
  <fig position="float" id="F5">
    <label>Figure 5:</label>
    <caption>
      <p id="P86">Drava provides various grouping and labeling methods to help users interpret semantic dimension. (a‚Äìb) Different approaches to pile up items based on their characteristics. (c) Labels can be added to both items and item groups.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0005" position="float"/>
  </fig>
  <fig position="float" id="F6">
    <label>Figure 6:</label>
    <caption>
      <p id="P87">Overview of the interactions and mechanisms that Drava provides for identifying concept mismatches (a), refining items and groups (b), and updating items and underlying models (c).</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0006" position="float"/>
  </fig>
  <fig position="float" id="F7">
    <label>Figure 7:</label>
    <caption>
      <p id="P88">Architecture of the encoder, the decoder, and the concept adaptor.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0007" position="float"/>
  </fig>
  <fig position="float" id="F8">
    <label>Figure 8:</label>
    <caption>
      <p id="P89">Dimensions (rows) with different salience scores (a‚Äìb). The visual changes are clearer when changing the values of the dimensions with the highest salience scores (a) compared to the dimensions with the lowest scores (b).</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0008" position="float"/>
  </fig>
  <fig position="float" id="F9">
    <label>Figure 9:</label>
    <caption>
      <p id="P90">Examples of reconstructed outputs on four different datasets (a‚Äìd). The first row shows the original inputs and the second row represents the reconstructed outputs.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0009" position="float"/>
  </fig>
  <fig position="float" id="F10">
    <label>Figure 10:</label>
    <caption>
      <p id="P91">We compared our concept adaptor with active learning (baseline) on three different concepts (<italic toggle="yes">i.e</italic>., scale, smiling, and bangs) under three conditions (<italic toggle="yes">i.e</italic>., <italic toggle="yes">N</italic> = 1%, 2%, 5%). Each line graph shows the accuracy over 15 iterations. The concept adaptor (yellow) overall showed higher accuracy.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0010" position="float"/>
  </fig>
  <fig position="float" id="F11">
    <label>Figure 11:</label>
    <caption>
      <p id="P92">The application scenario using the simple shape data. (a) A UMAP projection puts images together even though the positions of shapes are different. (b) Users can arrange images based on shape positions. (c) Images are arranged based on the scales of shape, but the left-most side is mostly squares (c1), indicating the need for user refinement.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0011" position="float"/>
  </fig>
  <fig position="float" id="F12">
    <label>Figure 12:</label>
    <caption>
      <p id="P93">The application scenario using celebrity images. (a) Items are grouped based on a visual concept that is related to skin color. (b) Items are then rearranged by adding another visual concept that is related to the background darkness as the <italic toggle="yes">y</italic> axis. The dataset contains both items that have fair skin and dark background (b1) and items that have dark skin and light background (b2).</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0012" position="float"/>
  </fig>
  <fig position="float" id="F13">
    <label>Figure 13:</label>
    <caption>
      <p id="P94">The application scenario using a genomic interaction matrix. (a1‚Äìa4) Four representative items vary on three concepts: the thickness of the diagonal (a1 and a4), the presence of nested squares (a1 and a2), and the asymmetric structure of the nested squares (a2 and a3). (b) A group has both the items with nested squares and the items with thick diagonals (orange marks). Each item is displayed upon mouse hovering. (c) Arranging items using <monospace>dim_nest</monospace> as <italic toggle="yes">x</italic> axis and <monospace>dim_thick</monospace> as <italic toggle="yes">y</italic> axis clearly separates these two concepts.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0013" position="float"/>
  </fig>
  <fig position="float" id="F14">
    <label>Figure 14:</label>
    <caption>
      <p id="P95">The application scenario using the breast histopathology images. (a) Arranging image patches from breast cancer specimens based on concepts learned by Drava shows a strong association between the presence of IDC (the color of item labels) and the two visual concepts, <italic toggle="yes">i.e</italic>., the tissue density (a2 vs. a3, the <italic toggle="yes">x</italic> axis) and the tissue color (a1 vs. a2, the <italic toggle="yes">y</italic> axis). We further (b) filter these items and (c) display them in a grid layout to identify confident false-positive predictions without visual clutter. (d) The spatial view enables us to locate the items in the original whole-mount slide image.</p>
    </caption>
    <graphic xlink:href="nihms-1947573-f0014" position="float"/>
  </fig>
  <table-wrap position="float" id="T1">
    <label>Table 1:</label>
    <caption>
      <p id="P96">Semantic meaning of individual latent dimensions. We compare Drava (<italic toggle="yes">i.e</italic>., using the value of one latent dimension to classify the corresponding concept) with random guesses on five concepts from two datasets. The results show that the latent dimension value could effectively indicate the corresponding concept.</p>
    </caption>
    <table frame="box" rules="all">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-left: hidden" rowspan="1" colspan="1">dataset</th>
          <th colspan="3" align="center" valign="middle" rowspan="1">dsprites</th>
          <th colspan="2" align="center" valign="middle" style="border-right: hidden" rowspan="1">CelebA</th>
        </tr>
        <tr>
          <th align="center" valign="middle" style="border-left: hidden" rowspan="1" colspan="1">concept</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">pos_x</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">pos_y</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">scale</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">smiling</th>
          <th align="center" valign="middle" style="border-right: hidden" rowspan="1" colspan="1">bangs</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" style="border-left: hidden" rowspan="1" colspan="1">random guess</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.333</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.333</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.333</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.5</td>
          <td align="center" valign="middle" style="border-right: hidden" rowspan="1" colspan="1">0.5</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-left: hidden" rowspan="1" colspan="1">Drava (no human refine)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.87</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.93</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.62</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.70</td>
          <td align="center" valign="middle" style="border-right: hidden" rowspan="1" colspan="1">0.77</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <boxed-text id="BX1" position="float">
    <caption>
      <title>CCS CONCEPTS</title>
    </caption>
    <list list-type="bullet" id="L2">
      <list-item>
        <p id="P97">Human-centered computing ‚Üí Interactive systems and tools</p>
      </list-item>
      <list-item>
        <p id="P98">Visual analytics</p>
      </list-item>
      <list-item>
        <p id="P99">Information visualization</p>
      </list-item>
    </list>
  </boxed-text>
</floats-group>
