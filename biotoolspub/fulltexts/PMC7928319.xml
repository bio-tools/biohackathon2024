<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Genet</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Genet</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Genet.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Genetics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1664-8021</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7928319</article-id>
    <article-id pub-id-type="doi">10.3389/fgene.2021.639930</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Genetics</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MSU-Net: Multi-Scale U-Net for 2D Medical Image Segmentation</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Su</surname>
          <given-names>Run</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Deyun</given-names>
        </name>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/946585/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Jinhuai</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Cheng</surname>
          <given-names>Chuandong</given-names>
        </name>
        <xref ref-type="aff" rid="aff4">
          <sup>4</sup>
        </xref>
        <xref ref-type="aff" rid="aff5">
          <sup>5</sup>
        </xref>
        <xref ref-type="aff" rid="aff6">
          <sup>6</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Institute of Intelligent Machines, Hefei Institutes of Physical Science, Chinese Academy of Sciences</institution>, <addr-line>Hefei</addr-line>, <country>China</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Science Island Branch of Graduate School, University of Science and Technology of China</institution>, <addr-line>Hefei</addr-line>, <country>China</country></aff>
    <aff id="aff3"><sup>3</sup><institution>School of Engineering, Anhui Agricultural University</institution>, <addr-line>Hefei</addr-line>, <country>China</country></aff>
    <aff id="aff4"><sup>4</sup><institution>Department of Neurosurgery, The First Affiliated Hospital of University of Science and Technology of China (USTC)</institution>, <addr-line>Hefei</addr-line>, <country>China</country></aff>
    <aff id="aff5"><sup>5</sup><institution>Division of Life Sciences and Medicine, University of Science and Technology of China</institution>, <addr-line>Hefei</addr-line>, <country>China</country></aff>
    <aff id="aff6"><sup>6</sup><institution>Anhui Province Key Laboratory of Brain Function and Brain Disease</institution>, <addr-line>Hefei</addr-line>, <country>China</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Jialiang Yang, Geneis Co. Ltd, China</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Khanh N. Q. Le, Taipei Medical University, Taiwan; Bing Wang, Anhui University of Technology, China</p>
      </fn>
      <corresp id="c001">*Correspondence: Jinhuai Liu <email>jhliu@iim.ac.cn</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Computational Genomics, a section of the journal Frontiers in Genetics</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>11</day>
      <month>2</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>12</volume>
    <elocation-id>639930</elocation-id>
    <history>
      <date date-type="received">
        <day>10</day>
        <month>12</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>20</day>
        <month>1</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2021 Su, Zhang, Liu and Cheng.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Su, Zhang, Liu and Cheng</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Aiming at the limitation of the convolution kernel with a fixed receptive field and unknown prior to optimal network width in U-Net, multi-scale U-Net (MSU-Net) is proposed by us for medical image segmentation. First, multiple convolution sequence is used to extract more semantic features from the images. Second, the convolution kernel with different receptive fields is used to make features more diverse. The problem of unknown network width is alleviated by efficient integration of convolution kernel with different receptive fields. In addition, the multi-scale block is extended to other variants of the original U-Net to verify its universality. Five different medical image segmentation datasets are used to evaluate MSU-Net. A variety of imaging modalities are included in these datasets, such as electron microscopy, dermoscope, ultrasound, etc. Intersection over Union (IoU) of MSU-Net on each dataset are 0.771, 0.867, 0.708, 0.900, and 0.702, respectively. Experimental results show that MSU-Net achieves the best performance on different datasets. Our implementation is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/CN-zdy/MSU_Net">https://github.com/CN-zdy/MSU_Net</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>multi-scale block</kwd>
      <kwd>U-net</kwd>
      <kwd>medical image segmentation</kwd>
      <kwd>convolution kernel</kwd>
      <kwd>receptive field</kwd>
    </kwd-group>
    <counts>
      <fig-count count="8"/>
      <table-count count="6"/>
      <equation-count count="11"/>
      <ref-count count="55"/>
      <page-count count="14"/>
      <word-count count="9125"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>Medical imaging analysis has made a significant breakthrough with the rapid progress of deep learning (Long et al., <xref rid="B28" ref-type="bibr">2015</xref>; Chen et al., <xref rid="B6" ref-type="bibr">2018a</xref>; Salehi et al., <xref rid="B41" ref-type="bibr">2018</xref>; Wang et al., <xref rid="B50" ref-type="bibr">2019b</xref>). Among these techniques, encoder-decoder architecture has been widely used in the medical image segmentation task (Salehi et al., <xref rid="B40" ref-type="bibr">2017</xref>; Xiao et al., <xref rid="B51" ref-type="bibr">2018</xref>; Guan et al., <xref rid="B13" ref-type="bibr">2019</xref>). U-Net (Ronneberger et al., <xref rid="B38" ref-type="bibr">2015</xref>) is the most classic encoder-decoder structure for medical image segmentation. In recent years, the original U-Net has been modified by many researchers. As a result, many variants of the original U-Net have been proposed (Poudel et al., <xref rid="B36" ref-type="bibr">2016</xref>; Oktay et al., <xref rid="B32" ref-type="bibr">2018</xref>; Roth et al., <xref rid="B39" ref-type="bibr">2018</xref>).</p>
    <p>However, the variants of the original U-Net come with two limitations. First, the diversity of features is lost due to the fixed receptive field of the convolution kernel. The same scale feature maps extracted from the convolution kernel with different receptive fields are semantically different. As a result, the performance of the network may vary with the size of the receptive field, and the performance depends on the size of the receptive field in the convolution kernel. Redundant features will be extracted when the receptive field of the convolution kernel is too small. Smaller targets are ignored when the receptive field of the convolution kernel is too large. For example, in the pulmonary lesion or multi-organ segmentation task, the edge detail of the smaller lesion/organ is not fine by the large receptor field and the structure of the lesion/organ is not obvious by the small receptor field. Therefore, it is very important to use the convolution kernel with different receptive fields to process the image (Luo et al., <xref rid="B29" ref-type="bibr">2016</xref>; Peng et al., <xref rid="B34" ref-type="bibr">2017</xref>; Shen et al., <xref rid="B44" ref-type="bibr">2019</xref>). In the natural image processing task, satisfactory results are obtained by combining the convolution of different receptive fields (Seif and Androutsos, <xref rid="B42" ref-type="bibr">2018</xref>). To the best of our knowledge, there are few reports based on different receptive fields in medical image segmentation tasks. Second, some information may be lost using a single convolutional sequence to extract features at each scale. More feature information can be obtained by multiple convolutional sequences. The loss of feature information can be reduced by the structure of multiple convolutional sequences in the process of down-sampling and up-sampling. Therefore, the learning capacity of the network is aided by multiple convolutional sequences (He et al., <xref rid="B15" ref-type="bibr">2015</xref>).</p>
    <p>In this paper, a new image segmentation architecture (multi-scale U-Net) is proposed by us to overcome the above limitations. This architecture is a generalization segmentation architecture. Multi-scale U-Net (MSU-Net) consists of blocks of multi-scale whose multi-scale blocks are composed of convolution sequences with different receptive fields. The multi-scale block introduced in MSU-Net achieves the following advantages. First, more feature information can be obtained because of the multiple convolutional sequences structure embedded in the network. The input of the convolution sequence is all the same, while their convolution kernel is not shared. This design not only improves the performance of segmentation but also facilitates the learning of network in the training process. Second, the features extracted from the multi-scale block are diversified. This is caused by the multiple convolution sequences with different receptive fields in multi-scale block. This is helpful for intensive forecasting tasks that require detailed spatial information. The semantics extracted from the convolution sequence with different receptive fields are different on the same scale feature map. This structure enables the encoder of the network to extract features better and the decoder to restore features better. We construct different types of multi-scale blocks with several commonly used convolution kernels. An extensive evaluation of different types of multi-scale blocks is performed on three segmentation datasets. Our results demonstrate that MSU-Net built by integrated multiple convolution sequences with different receptive fields enables significant improvement of semantic segmentation. Compared with the traditional U-Net architecture, the main improvement of MSU-Net is the integration of multiple convolution sequences with different sizes of receptive fields. This improvement enables the object features to become more conspicuous with forward propagation. In addition, the proposed multi-scale block can be easily integrated into other network structures.</p>
    <p>In summary, the main contributions of this paper are summarized as follows:</p>
    <p>(1) Multi-scale blocks are proposed by us based on several commonly used convolution kernel. More diverse feature information and better feature maps are captured from the images through multi-scale block.</p>
    <p>(2) MSU-Net, a new segmentation architecture for medical image, is proposed for medical image segmentation. This is an improvement on the basic structure of U-Net. Compared to the existing algorithms, the proposed method has a stronger ability to overcome the problems of class-imbalance and overwhelmed.</p>
    <p>(3) Different receptive fields are crucial for dense prediction tasks requiring detailed spatial information. It can stimulates learning capacity of network and make the network more robust. Experimental results demonstrate that the proposed method is outperforms the state-of-the-art methods in medical image segmentation task under different imaging modalities.</p>
  </sec>
  <sec id="s2">
    <title>2. Related Works</title>
    <p>With the development of convolutional neural network (CNN) in the field of natural image processing and medical image analysis, automatic feature learning algorithm using deep learning has become a feasible method for biomedical image segmentation (Le et al., <xref rid="B23" ref-type="bibr">2019</xref>, <xref rid="B22" ref-type="bibr">2020</xref>; Sua et al., <xref rid="B46" ref-type="bibr">2020</xref>). Segmentation method based on deep learning is a learning method with pixel-classification, which is different from the traditional pixel or superpixel classification method (Abramoff et al., <xref rid="B1" ref-type="bibr">2007</xref>; Kitrungrotsakul et al., <xref rid="B19" ref-type="bibr">2015</xref>; Tian et al., <xref rid="B47" ref-type="bibr">2015</xref>) using hand-made features. The limitations of hand-made features are overcome when deep learning approaches are used to learn features. The limitations of hand-made features are overcome when deep learning approaches are used to learn features. Early deep learning methods for medical image segmentation are mostly based on patch. The strategy based on plaque and sliding window was proposed by Ciresan et al. (<xref rid="B8" ref-type="bibr">2012</xref>) to segment neuronal membranes from microscopic images. Kamnitsas et al. (<xref rid="B18" ref-type="bibr">2017</xref>) adopted a multi-scale 3D CNN architecture with fully connected conditional random field (CRF) to enhance patch based brain leasion segmentation. Pereira et al. (<xref rid="B35" ref-type="bibr">2016</xref>) proposed an automatic segmentation method based on CNN to segment brain tumors. Obviously, two main drawbacks are introduced by this solution: the redundant computation caused by sliding window and the global feature cannot be learned.</p>
    <p>With the emerging of end-to-end FCN (Long et al., <xref rid="B28" ref-type="bibr">2015</xref>), Ronneberger et al. (<xref rid="B38" ref-type="bibr">2015</xref>) proposed U-Net for biomedical image segmentation. U-Net has shown good performance in fields of medical image segmentation. It has become a popular neural network architecture for biomedical image segmentation tasks (LaLonde and Bagci, <xref rid="B21" ref-type="bibr">2018</xref>; Fan et al., <xref rid="B11" ref-type="bibr">2019</xref>; Song et al., <xref rid="B45" ref-type="bibr">2019</xref>). Li et al. (<xref rid="B24" ref-type="bibr">2019</xref>) proposed a new dual-U-Net architecture to solve the problem of nuclei segmentation. Milletari et al. (<xref rid="B30" ref-type="bibr">2016</xref>) proposed a 3D image segmentation method based on U-Net to perform end-to-end training on prostate MRI. Guan et al. (<xref rid="B13" ref-type="bibr">2019</xref>) proposed an improved CNN structure for removing artifact from 2D PAT images reconstructed. Many variants of U-Net has been appeared for different medical image segmentation tasks. In order to improve the learning ability of feature, some new modules are proposed to replace the original modules. Seo et al. (<xref rid="B43" ref-type="bibr">2019</xref>) proposed an up-sampling method based on an object and redesigned the remaining paths and skip-connection. The limitation of the traditional U-Net algorithm was overcome in this way. Ge et al. (<xref rid="B12" ref-type="bibr">2019</xref>) proposed a k-shaped network of end-to-end deep neural network. The network was used for multi-view segmentation and multi-dimensional quantification of LV in PEAV sequences. Myronenko (<xref rid="B31" ref-type="bibr">2018</xref>) proposed a semantic segmentation method for 3D brain tumor segmentation from multimodal 3D MRIs. An asymmetric encoder was used to extract features, and then two decoders segment the brain tumor and reconstruct the input image, respectively. Oktay et al. (<xref rid="B32" ref-type="bibr">2018</xref>) proposed AttU-Net in combination with attention gate. Alom et al. (<xref rid="B2" ref-type="bibr">2018</xref>) integrated the structure of Recurrent Neural Network (RNN) and ResNet into the original U-Net. RNN could make the network extract better features. ResNet enables the training of deeper networks. Liu et al. (<xref rid="B27" ref-type="bibr">2020</xref>) proposed a ψ-shaped depth neural network (ψ-Net). In the deep stage, semantic information was featured by selective aggregation. In the shallow stage, the semantic information obtained in the deep stage was used to improve the detailed information. Therefore, discriminative features were obtained to provide the basis for accurate subcortical segmentation of brain structures. In addition to the above achievements in medical image segmentation based on U-Net, some researchers have also improved U-Net to apply in general image segmentation. Zhang et al. (<xref rid="B53" ref-type="bibr">2018</xref>) proposed a semantic segmentation neural network based on residual learning and U-Net for road area extraction. Kohl et al. (<xref rid="B20" ref-type="bibr">2018</xref>) proposed a generative segmentation model based on a combination of a U-Net with a conditional variational auto-encoder. A new Recurrent U-Net had been proposed by Wang et al. (<xref rid="B49" ref-type="bibr">2019a</xref>). This model not only retained the compactness of U-Net, but also achieved a good performance improvement in some benchmarks. TernausNet was proposed by Iglovikov and Shvets (<xref rid="B16" ref-type="bibr">2018</xref>). The network replaces the encoder in U-Net with VGG11 and conducts pre-training on ImageNet. TernausNet achieved the best results in the Kaggle Carvana Image Masking Challenge.</p>
    <p>Although the architecture of U-Net has been widely used, the most basic architecture has not changed. The convolution blocks of the original U-Net network are adjusted by us to improve the efficiency of the segmentation algorithm. The convolution blocks are arranged in parallel to form a multiple convolution sequence. Richer semantic information is provided by this design. In addition, the convolution kernel of the multiple convolution sequence is adjusted to have different receptive fields. The convolution kernel with different receptive fields enables the network to better extract and restore features.</p>
  </sec>
  <sec id="s3">
    <title>3. Method</title>
    <p>The proposed MSU-Net consists of major part: multi-scale block (37), as shown in <xref ref-type="fig" rid="F1">Figure 1</xref>. In the following, we first trace the types of multi-scale block and then explain the structure of MSU-Net and extended work of multi-scale block.</p>
    <fig id="F1" position="float">
      <label>Figure 1</label>
      <caption>
        <p>Detailed description of MSU-Net and multi-scale block (37). Panel <bold>(A)</bold> is the architecture of MSU-Net. The overall architecture is similar to the original U-Net. The dimensions of the network are represented by numbers on the block structure. Panel <bold>(B)</bold> is the architecture of multi-scale block (37). This module is embedded in the original U-Net to get MSU-Net.</p>
      </caption>
      <graphic xlink:href="fgene-12-639930-g0001"/>
    </fig>
    <sec>
      <title>3.1. Multi-Scale Block</title>
      <p>The multi-scale block is proposed by us, which is composed of multiple convolution sequences with different receptive fields. More diverse semantic information is extracted by this module and more detailed feature maps are generated. The widely used convolution kernel is shown in <xref ref-type="fig" rid="F2">Figure 2</xref>.</p>
      <fig id="F2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>The type of convolution kernel used in this article. Combining the above seven convolution kernels, different types of multi-scale blocks are proposed.</p>
        </caption>
        <graphic xlink:href="fgene-12-639930-g0002"/>
      </fig>
      <p>The convolution kernel with different receptive fields is matched to obtain a multi-scale block. We designed 31 kinds of multi-scale blocks according to the above several convolution kernels. The multi-scale block evolved from the different convolution kernels is shown in <xref ref-type="fig" rid="F3">Figure 3</xref>.</p>
      <fig id="F3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>An overview of 31 multi-scale blocks. m-s block represents the multi-scale block. Different multi-scale blocks are designed according to several commonly used convolution kernels. More richer and diverse features can be extracted through this design. Meanwhile, the problem of unknown network width can be alleviated by this design. Conducive to intensive prediction tasks that require detailed spatial information.</p>
        </caption>
        <graphic xlink:href="fgene-12-639930-g0003"/>
      </fig>
      <p>The 3 × 3 convolution kernel has been used in all experiments. The features of the input multi-scale block are processed by the convolution kernel with different receptive fields, and then the obtained features are output after 1 × 1 convolution. A comprehensive ablation experiment is used to verify the performance of different types of multi-scale blocks. In the experiment, three datasets are used by us. The datasets are EM, BUL, and CXR, respectively (detailed in section 4.1). The experiments are carried out after integrated each multi-scale block into the original U-Net. The experimental results are illustrated in <xref rid="T1" ref-type="table">Table 1</xref>. The performance of multi-scale block (37) is the best. The details of multi-scale block (37) are shown in <xref ref-type="fig" rid="F4">Figure 4</xref>.</p>
      <table-wrap id="T1" position="float">
        <label>Table 1</label>
        <caption>
          <p>Ablation study on MSU-Nets of the convolution kernel with different receptive fields.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Applications</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>BUL</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>EM</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>NS</bold>
              </th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>M ± SD</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>M ± SD</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>M ± SD</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (13)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.548 ± 0.076</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.871 ± 0.002</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.678 ± 0.017</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (23)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.610 ± 0.029</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.840 ± 0.035</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.661 ± 0.028</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (35)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.690 ± 0.047</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.884 ± 0.017</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.670 ± 0.036</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (37)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.708</bold>
                <bold>±</bold>
                <bold>0.011</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.900</bold>
                <bold>±</bold>
                <bold>0.001</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.702</bold>
                <bold>±</bold>
                <bold>0.010</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (39)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.699 ± 0.016</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.895 ± 0.009</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.660 ± 0.011</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (123)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.547 ± 0.067</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.862 ± 0.012</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.672 ± 0.015</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (135)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.679 ± 0.005</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.883 ± 0.010</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.676 ± 0.021</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (137)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.696 ± 0.018</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.890 ± 0.015</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.684 ± 0.025</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (139)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.682 ± 0.037</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.880 ± 0.015</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.674 ± 0.020</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (235)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.673 ± 0.036</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.873 ± 0.023</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.684 ± 0.025</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (237)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.703 ± 0.042</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.888 ± 0.017</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.687 ± 0.019</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (239)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.664 ± 0.029</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.893 ± 0.011</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.672 ± 0.023</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (357)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.679 ± 0.018</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.888 ± 0.016</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.682 ± 0.015</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (359)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.693 ± 0.007</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.894 ± 0.006</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.686 ± 0.020</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (379)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.705 ± 0.008</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.894 ± 0.011</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.671 ± 0.023</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (1,235)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.652 ± 0.015</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.877 ± 0.015</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.662 ± 0.038</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (1,237)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.655 ± 0.008</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.886 ± 0.009</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.693 ± 0.025</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (1,239)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.699 ± 0.017</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.885 ± 0.014</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.687 ± 0.031</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (1,357)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.689 ± 0.033</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.895 ± 0.005</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.673 ± 0.023</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (1,359)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.700 ± 0.028</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.898 ± 0.002</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.689 ± 0.015</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (1,379)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.702 ± 0.025</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.898 ± 0.003</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.692 ± 0.017</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (2,357)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.694 ± 0.040</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.894 ± 0.004</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.687 ± 0.023</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (2,359)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.681 ± 0.023</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.884 ± 0.014</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.702 ± 0.018</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (2,379)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.694 ± 0.036</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.882 ± 0.014</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.675 ± 0.013</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (3,579)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.696 ± 0.338</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.893 ± 0.010</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.695 ± 0.011</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (12,357)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.680 ± 0.017</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.893 ± 0.005</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.696 ± 0.027</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (12,359)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.705 ± 0.014</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.892 ± 0.006</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.687 ± 0.040</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (12,379)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.667 ± 0.023</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.893 ± 0.002</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.695 ± 0.021</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (13,579)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.697 ± 0.032</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.899 ± 0.001</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.685 ± 0.025</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (23,579)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.705 ± 0.020</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.889 ± 0.014</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.697 ± 0.008</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (123,579)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.693 ± 0.028</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.896 ± 0.002</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.696 ± 0.017</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>The numbers in brackets represent the size of receptive field in MSU-Net. This is corresponds to the different multi-scale blocks in <xref ref-type="fig" rid="F3">Figure 3</xref>. Intersection over Union (IoU) is used as the evaluation metric for comparative. Bold values represent the best results</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <fig id="F4" position="float">
        <label>Figure 4</label>
        <caption>
          <p>Detailed description of multi-scale block. First, two 3X3 and 7X7 convolution kernels are used to extract features. Second, the extracted features are merged by the feature by cat. Finally, the fused features are output after dimensionality reduction by 1X1 convolution.</p>
        </caption>
        <graphic xlink:href="fgene-12-639930-g0004"/>
      </fig>
      <p><italic>x</italic> represents the characteristics of the input. <italic>x</italic><sub>1</sub> and <italic>x</italic><sub>2</sub> represent the characteristics obtained by the convolution kernel of different sizes. <italic>F</italic> is the output result of multi-scale block. <italic>F</italic> is computed as follows:</p>
      <disp-formula id="E1">
        <label>(1)</label>
        <mml:math id="M1">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>x</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>w</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mn>32</mml:mn>
                  </mml:mrow>
                </mml:msub>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>w</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>31</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>b</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>31</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>+</mml:mo>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>b</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mn>32</mml:mn>
                  </mml:mrow>
                </mml:msub>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <disp-formula id="E2">
        <label>(2)</label>
        <mml:math id="M2">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>x</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mn>2</mml:mn>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>w</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mn>72</mml:mn>
                  </mml:mrow>
                </mml:msub>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>w</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>71</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>b</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>71</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>+</mml:mo>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>b</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mn>72</mml:mn>
                  </mml:mrow>
                </mml:msub>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <disp-formula id="E3">
        <label>(3)</label>
        <mml:math id="M3">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>X</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mi>C</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mrow>
                  <mml:mo>[</mml:mo>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>x</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>x</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mo>]</mml:mo>
                </mml:mrow>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <disp-formula id="E4">
        <label>(4)</label>
        <mml:math id="M4">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>F</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>w</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>f</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mi>X</mml:mi>
                <mml:mo>+</mml:mo>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>b</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>f</mml:mi>
                  </mml:mrow>
                </mml:msub>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>Feature fusion needs to be used in multi-scale block before 1X1 convolution. Therefore, different fusion methods are validated by us (results in <xref rid="T2" ref-type="table">Table 2</xref>). MSU-Net (37+sum) uses element summation for feature fusion. MSU-Net (37) uses concatenation for feature fusion.</p>
      <table-wrap id="T2" position="float">
        <label>Table 2</label>
        <caption>
          <p>Ablation study for MSU-Net and its variants.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Architecture</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>BUL</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>EM</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>NS</bold>
              </th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>M ± SD</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>M ± SD</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>M ± SD</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.708</bold>
                <bold>±</bold>
                <bold>0.011</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.900</bold>
                <bold>±</bold>
                <bold>0.001</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.702</bold>
                <bold>±</bold>
                <bold>0.010</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net(37+sum)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.694 ± 0.020</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.894 ± 0.013</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.683 ± 0.017</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net(encoder)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.646 ± 0.061</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.889 ± 0.013</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.679 ± 0.021</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net(decoder)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.656 ± 0.027</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.883 ± 0.018</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.661 ± 0.024</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net(37+concatenated)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.642 ± 0.036</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.899 ± 0.004</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.674 ± 0.024</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net(73+concatenated)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.707 ± 0.061</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.900</bold>
                <bold>±</bold>
                <bold>0.001</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.667 ± 0.022</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net(37+dilated)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.640 ± 0.033</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.877 ± 0.005</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.662 ± 0.013</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>MSU-Net is MSU-Net (37) in <xref rid="T1" ref-type="table">Table 1</xref>. MSU-Net (37+ sum) is an MSU-Net with feature fusion by adding. MSU-Net (encoder) and MSU-Net (decoder) are obtained by using multi-scale block to replace the convolution block between encoder and decoder in U-Net. MSU-Net (73+concatenated) and MSU-Net (37+concatenated) are obtained after concatenated the convolution kernel with different receptive fields. MSU-Net (37+dilated) is obtained by dilated convolution. Intersection over Union (IoU) is used as the evaluation metric for comparison. Bold values represent the best results</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>The dilated convolution is introduced into the multi-scale block after the optimal convolution kernel is obtained. The dilated convolution used in the experiment is described in <xref ref-type="fig" rid="F2">Figure 2</xref>. Convolution kernels with different receptive fields are concatenated to verify the effectiveness of the multiple convolution sequence. The details are shown in <xref ref-type="fig" rid="F5">Figure 5</xref>. The experimental results are shown in <xref rid="T2" ref-type="table">Table 2</xref>.</p>
      <fig id="F5" position="float">
        <label>Figure 5</label>
        <caption>
          <p>Arrangement of convolution kernel with different receptive fields and dilated convolution. Panels <bold>(A,B)</bold> lay out the convolution kernel in different order, respectively. In <bold>(C)</bold>, the large convolution kernel in the multi-scale block (37) is replaced by dilated convolution. The receptive field of the convolution kernel is enlarged without increasing the number of parameters.</p>
        </caption>
        <graphic xlink:href="fgene-12-639930-g0005"/>
      </fig>
    </sec>
    <sec>
      <title>3.2. Network Architecture</title>
      <p>The architecture of MSU-Net is illustrated in <xref ref-type="fig" rid="F1">Figure 1</xref>. MSU-Net has a contraction path and an expansion path. The network architecture follows encoder-decoder. In original U-Net, each block consists of two convolutional layers. However, there is still a drawback in this block. Due to the limitation of the receptive field, the network does not achieve better performance in feature extraction and feature restoration. The convolution blocks in encoder of the original U-Net are replaced with multi-scale blocks to obtain MSU-Net (encoder). The convolution blocks in decoder of the original U-Net are replaced with multi-scale blocks to obtain MSU-Net (decoder). The experimental results are illustrated in <xref rid="T2" ref-type="table">Table 2</xref>. In MSU-Net, the multi-scale block (37) is used to replace the all convolution block in the original U-Net. Multi-scale block enables encoder to extract more detailed information. Multi-scale block makes the features of decoder restoration more complete.</p>
    </sec>
    <sec>
      <title>3.3. Extension of Model</title>
      <p>Residual (He et al., <xref rid="B14" ref-type="bibr">2016</xref>) is expanded into our model. The residual multi-scale block is shown in <xref ref-type="fig" rid="F6">Figure 6</xref>. In addition, multi-scale blocks are also extended to variants of U-Net.</p>
      <fig id="F6" position="float">
        <label>Figure 6</label>
        <caption>
          <p>Residual multi-scale block. Panel <bold>(A)</bold> is the first structure designed to incorporate residual thinking. Panel <bold>(B)</bold> is the second. Experimental results show that the performance of <bold>(A)</bold> structure is better than <bold>(B)</bold>. Panel <bold>(C)</bold> describes <bold>(A)</bold> in detail. The different multi-scale blocks are described in <xref ref-type="fig" rid="F3">Figure 3</xref>. The information of the input features is directly transmitted to the deep layer of the network through the residual connection.</p>
        </caption>
        <graphic xlink:href="fgene-12-639930-g0006"/>
      </fig>
      <sec>
        <title>3.3.1. Residual Multi-Scale Block</title>
        <p>The idea of residual is introduced with multi-scale blocks to obtain residual multi-scale block (0) and residual multi-scale block (1). Residual multi-scale block (0) and residual multi-scale block (1) are shown in <xref ref-type="fig" rid="F6">Figures 6A,B</xref>, respectively. The original convolution block in U-Net was replaced by residual multi-scale block (0) and residual multi-scale block (1) to get Res MSU-Net (0) and Res MSU-Net (1). The experimental results are described in <bold>Table 4</bold>. In <bold>Table 4</bold>, the performance of residual multi-scale block (0) is better than residual multi-scale block (1).</p>
        <p>The structure of residual multi-scale block (1) is described below. <italic>x</italic><sub><italic>r</italic></sub> represents the characteristics of the input. <italic>x</italic><sub><italic>r</italic>1</sub> and <italic>x</italic><sub><italic>r</italic>2</sub> represent the characteristics obtained by the convolution kernel of different receptive fields. <italic>F</italic><sub><italic>R</italic></sub> is the output result of the multi-scale block. <italic>F</italic><sub><italic>R</italic></sub> is computed as follows:</p>
        <disp-formula id="E5">
          <label>(5)</label>
          <mml:math id="M5">
            <mml:mtable class="eqnarray" columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>x</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>r</mml:mi>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>=</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>w</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>r</mml:mi>
                      <mml:mn>32</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>w</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>r</mml:mi>
                          <mml:mn>31</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>r</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>+</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>b</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>r</mml:mi>
                          <mml:mn>31</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                  <mml:mo>+</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>b</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>r</mml:mi>
                      <mml:mn>32</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <disp-formula id="E6">
          <label>(6)</label>
          <mml:math id="M6">
            <mml:mtable class="eqnarray" columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>x</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>r</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>=</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>w</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>r</mml:mi>
                      <mml:mn>72</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>w</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>r</mml:mi>
                          <mml:mn>71</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>r</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>+</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>b</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>r</mml:mi>
                          <mml:mn>71</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                  <mml:mo>+</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>b</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>r</mml:mi>
                      <mml:mn>72</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <disp-formula id="E7">
          <label>(7)</label>
          <mml:math id="M7">
            <mml:mtable class="eqnarray" columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>X</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>R</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>=</mml:mo>
                  <mml:mi>C</mml:mi>
                  <mml:mi>a</mml:mi>
                  <mml:mi>t</mml:mi>
                  <mml:mrow>
                    <mml:mo>[</mml:mo>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>r</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>,</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>r</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>,</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>r</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo>]</mml:mo>
                  </mml:mrow>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <disp-formula id="E8">
          <label>(8)</label>
          <mml:math id="M8">
            <mml:mtable class="eqnarray" columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>F</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>R</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>=</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>w</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>r</mml:mi>
                      <mml:mi>f</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>X</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>R</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>+</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>b</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>r</mml:mi>
                      <mml:mi>f</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <p>Residual connection can make the forward and backward propagation of multi-scale block smoother. In forward propagation, the input signal can be propagated directly from the bottom to the top. The problem of network degradation can be alleviated. In back propagation, the error signal can be propagated directly to the lower layer without any intermediate weight matrix transformation. The problem of gradient dispersion can be alleviated. In addition, the generalization capacity of the network can be enhanced by the structure.</p>
      </sec>
      <sec>
        <title>3.3.2. Other Structures</title>
        <p>In addition to combining the structure with our proposed multi-scale block, we also extend our multi-scale block on the variants of original U-Net. The convolution blocks in AttU-Net (Oktay et al., <xref rid="B32" ref-type="bibr">2018</xref>) and U-Net++ (Zhou et al., <xref rid="B54" ref-type="bibr">2020</xref>) are replaced with multi-scale block, namely MSAttU-Net and MSU-Net++, respectively.</p>
      </sec>
    </sec>
  </sec>
  <sec id="s4">
    <title>4. Experiment</title>
    <sec>
      <title>4.1. Dataset</title>
      <p><xref rid="T3" ref-type="table">Table 3</xref> summarizes the five biomedical image segmentation datasets used in this study. These lesions/organs are derived from the most common medical imaging modalities, such as microscopy, X-ray, B-mode ultrasound, etc. The dataset was randomly divided into six subsets. Five of six are used as a training-validation dataset, and the remaining data as a test dataset. Five-fold cross validation is applied by randomly dividing training-validation into five subsets. The training process alternates with a fixed ratio of 4:1 between the training dataset and the validation dataset.</p>
      <table-wrap id="T3" position="float">
        <label>Table 3</label>
        <caption>
          <p>Summary of biomedical image segmentation datasets used in our experiments.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Applications</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Images</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Input size</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Modality</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Provider</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">EM</td>
              <td valign="top" align="left" rowspan="1" colspan="1">30</td>
              <td valign="top" align="center" rowspan="1" colspan="1">512 × 512</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Microscopy</td>
              <td valign="top" align="left" rowspan="1" colspan="1">ISBI 2012 (Cardona et al., <xref rid="B5" ref-type="bibr">2010</xref>)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">BUL</td>
              <td valign="top" align="left" rowspan="1" colspan="1">163</td>
              <td valign="top" align="center" rowspan="1" colspan="1">128 × 128</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Ultrasound</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Breast Ultrasound Lesions Dataset (Yap et al., <xref rid="B52" ref-type="bibr">2017</xref>)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">CXR</td>
              <td valign="top" align="left" rowspan="1" colspan="1">704</td>
              <td valign="top" align="center" rowspan="1" colspan="1">128 × 128</td>
              <td valign="top" align="left" rowspan="1" colspan="1">X-ray</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Chest X-ray Database (Candemir et al., <xref rid="B4" ref-type="bibr">2013</xref>; Jaeger et al., <xref rid="B17" ref-type="bibr">2013</xref>)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">SL</td>
              <td valign="top" align="left" rowspan="1" colspan="1">2594</td>
              <td valign="top" align="center" rowspan="1" colspan="1">256 × 256</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Demoscopy</td>
              <td valign="top" align="left" rowspan="1" colspan="1">ISIC 2018 (Tschandl et al., <xref rid="B48" ref-type="bibr">2018</xref>; Codella et al., <xref rid="B9" ref-type="bibr">2019</xref>)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">NS</td>
              <td valign="top" align="left" rowspan="1" colspan="1">30</td>
              <td valign="top" align="center" rowspan="1" colspan="1">512 × 512</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Digitize</td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/ipateam/segmentation-of-nuclei-in-cryosectioned-he-images">Kaggle</ext-link>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p><italic>(1) Electron Microscopy (EM):</italic> The dataset is provided by the EM segmentation challenge (Cardona et al., <xref rid="B5" ref-type="bibr">2010</xref>), which is a part of ISBI 2012. The dataset contains 30 images (512 × 512 pixels) from a serial section Transmission Electron Microscopy (ssTEM) dataset of the Drosophila first instar larva ventral nerve cord (VNC). The images has not been resized. The images size of the input network is 512 × 512. An example of dataset is shown in <xref ref-type="fig" rid="F7">Figure 7</xref>. Each image has a completely annotated ground truth segmentation map of the corresponding cell (white) and membranes (black).</p>
      <fig id="F7" position="float">
        <label>Figure 7</label>
        <caption>
          <p>Qualitative comparison among SegNet, DeepLabV3+, U-Net, U-Net++, and MSU-Net. It shows the segmentation application of the architectures on five different biomedical image datasets. The red arrows indicate areas of incorrect segmentation. SegNet can not be trained on EM datasets. Therefore, the result of SegNet on the EM dataset is vacant. The ground truth is illustrated in the second column (from <bold>left</bold> to <bold>right</bold>).</p>
        </caption>
        <graphic xlink:href="fgene-12-639930-g0007"/>
      </fig>
      <p><italic>(2) Breast Ultrasound Lesions (BUL):</italic> The Breast Ultrasound Dataset B (BUL) open-sourced in (Yap et al., <xref rid="B52" ref-type="bibr">2017</xref>) is used in this study. This dataset includes 163 ultrasound images of breast lesions from different women. The image size of average is 760 × 570 pixels where each of the images presented one or more lesions. For our experiments, the data is resampled to 128 × 128 pixels. The ground truths provided in the BUL are in the form of binary masks of the lesions, as illustrated in <xref ref-type="fig" rid="F7">Figure 7</xref>.</p>
      <p><italic>(3) Chest X-ray (CXR):</italic> The standard digital image database for Tuberculosis (Candemir et al., <xref rid="B4" ref-type="bibr">2013</xref>; Jaeger et al., <xref rid="B17" ref-type="bibr">2013</xref>) is created by the National Library of Medicine, Maryland, USA in collaboration with Shenzhen No.3 People's Hospital, Guangdong Medical College, Shenzhen, China. The Chest X-rays are from out-patient clinics. There are 800 images in the Chest X-rays dataset. However, the ground truth of 96 images is unknown. Seven hundred and four images of corresponding GT in the dataset were used by us. The image size of average is 4456 × 4456 pixels. The images are rescaled to 128 × 128 for this implementation. Referring to the example in <xref ref-type="fig" rid="F7">Figure 7</xref>.</p>
      <p><italic>(4) Skin Lesions (SL):</italic> The dataset is provided by the ISIC 2018: Skin Lesion Analysis Toward Melanoma Detection grand challenge dataset (Tschandl et al., <xref rid="B48" ref-type="bibr">2018</xref>; Codella et al., <xref rid="B9" ref-type="bibr">2019</xref>). This dataset consists of 2594 RGB images of skin lesions with an average image size of 2166 × 3188 pixels. For our experiments, the dataset is resampled to 256 × 256 pixels with cross validation. The training samples include the original image and the binary image containing the lesion. Pixels outside the target lesion are represented by 0.</p>
      <p><italic>(5) Nuclei Segmentation (NS):</italic> This dataset is provided by The Cancer Genome Atlas (TCGA). This dataset can be downloaded from <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/ipateam/segmentation-of-nuclei-in-cryosectioned-he-images">Kaggle</ext-link>. The dataset comprising 30 digitized Hematoxylin and Eosin (H&amp;E)-stained frozen sections (512 × 512 pixels) derived from 10 different human organs. The dataset were selected from different laboratories to maximize the staining variability in the data set. Image tiles (3 per tissue) were extracted from adrenal gland, larynx, lymph nodes, mediastinum, pancreas, pleura, skin, testes, thymus, and thyroid gland. Like the EM dataset, this dataset was not sampled prior to input. The image size of the input is 512 × 512.</p>
    </sec>
    <sec>
      <title>4.2. Baselines and Implementation</title>
      <p>For comparison, the original U-Net is used to implement the segmentation task. U-Net is a common performance baseline for medical image segmentation. In addition, a wide U-Net with a similar number of parameters to our proposed architecture was designed. This is to ensure that the performance gain yielded by our architecture is not simply due to the increased number of parameters.</p>
      <p>In this experiment, the program was based on the Pytorch (Paszke et al., <xref rid="B33" ref-type="bibr">2019</xref>) framework. SGD (Robbins and Monro, <xref rid="B37" ref-type="bibr">1951</xref>) was used as the optimizer with the learning rate of 1e-2. Both networks were constructed from the original U-Net. All the experiments are performed using an NVIDIA GeForce RTX 2080 Ti GPUs with 11 GB memory.</p>
    </sec>
    <sec>
      <title>4.3. Evaluation Measures</title>
      <p>In this paper, the Intersection over Union (IoU) is used as the main evaluation indicator to evaluate the results. Alternative measurement metrics could be found in <bold>Table 6</bold>, such as dice coefficient, precision, area Under Curve (AUC), and statistical analysis. These metrics were calculated as follows:</p>
      <disp-formula id="E9">
        <label>(9)</label>
        <mml:math id="M9">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>I</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>U</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>F</mml:mi>
                    <mml:mi>N</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <disp-formula id="E10">
        <label>(10)</label>
        <mml:math id="M10">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>D</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>c</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mn>2</mml:mn>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mn>2</mml:mn>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>F</mml:mi>
                    <mml:mi>N</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <disp-formula id="E11">
        <label>(11)</label>
        <mml:math id="M11">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>P</mml:mi>
                <mml:mi>r</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>c</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>s</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where TP, FP, and FN represent the number of true positive, false positive, and false negative, respectively. In addition, the area under receiver operation characteristic curve (AUC) is used to measure the segmentation performance. The closer the AUC is to 1.0, the higher authenticity of the segmentation method. When it is equal to 0.5, it has the lowest authenticity and no application value.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s5">
    <title>5. Results</title>
    <sec>
      <title>5.1. Selection of Multi-Scale Block</title>
      <p>31 kinds of multi-scale blocks were designed by combining the convolution kernel with different receptive fields. The different multi-scale blocks are shown in <xref ref-type="fig" rid="F3">Figure 3</xref>. All multi-scale blocks were embedded into the original U-Net respectively. Subsequently, an ablation analysis of multi-scale block is made on three datasets. The experimental results of different multi-scale blocks on the dataset are illustrated in <xref rid="T1" ref-type="table">Table 1</xref>. Two key findings are illustrated in our results: (1) The wider network structure is not always better, (2) The optimal width of the network depends on the difficulty and size of the dataset. Although these findings may facilitate the automatic search of neural structures, this approach is hampered by limited computational resources (Elsken et al., <xref rid="B10" ref-type="bibr">2018</xref>; Liu et al., <xref rid="B26" ref-type="bibr">2018</xref>, <xref rid="B25" ref-type="bibr">2019</xref>; Zoph et al., <xref rid="B55" ref-type="bibr">2018</xref>).</p>
      <p>The influence of the difference receptive field on the network performance is shown in <xref rid="T1" ref-type="table">Table 1</xref>. Among them, multi-scale block (37) achieves the best performance on datasets.</p>
      <p>Different arrangements of convolution blocks and different convolution kernels are verified in <xref rid="T2" ref-type="table">Table 2</xref>. The robustness of the multiple convolution sequence is demonstrated by experimental results.</p>
    </sec>
    <sec>
      <title>5.2. Results of the Extended Model</title>
      <p>The multi-scale block was extended by us. First, the idea of residuals was introduced into the proposed module. Two multi-scale blocks based on residuals were constructed. The structure is shown in <xref ref-type="fig" rid="F6">Figure 6</xref>. Second, the proposed multi-scale block was extended to the existing U-Net variants. Convolution kernel in AttU-Net and U-Net++ was replaced by multi-scale block. The experimental results are shown in <xref rid="T4" ref-type="table">Tables 4</xref>, <xref rid="T5" ref-type="table">5</xref>. Experimental results show that the proposed method has good scalability and compatibility.</p>
      <table-wrap id="T4" position="float">
        <label>Table 4</label>
        <caption>
          <p>Ablation study for U-Net, wide U-Net, MSU-Net, Res MSU-Net(0), and Res MSU-Net(1).</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Architecture</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>BUL</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>EM</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>NS</bold>
              </th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>M ± SD</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>M ± SD</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>M ± SD</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">U-Net (Ronneberger et al., <xref rid="B38" ref-type="bibr">2015</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.608 ± 0.037</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.884 ± 0.007</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.675 ± 0.018</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">wide U-Net (Ours)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.643 ± 0.025</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.889 ± 0.016</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.677 ± 0.012</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net (Ours)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.708 ± 0.011</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.900</bold>
                <bold>±</bold>
                <bold>0.001</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.702 ± 0.010</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Res MSU-Net (0) (Ours)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.713</bold>
                <bold>±</bold>
                <bold>0.032</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.900</bold>
                <bold>±</bold>
                <bold>0.001</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.704</bold>
                <bold>±</bold>
                <bold>0.010</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Res MSU-Net (1) (Ours)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.628 ± 0.025</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.848 ± 0.056</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.675 ± 0.022</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>Wide U-Net is obtained by extending the width of the U-Net network. The wide U-Net has the same number of parameters as the MSU-Net. Res MSU-Net (0)/Res MSU-Net (1) are proposed based on Residual multi-block. Intersection over Union (IoU) is used as the evaluation metric for comparison. Bold values represent the best results</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <table-wrap id="T5" position="float">
        <label>Table 5</label>
        <caption>
          <p>Ablation study for AttU-Net, MSAttU-Net, U-Net++, and MSU-Net++.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Architecture</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>BUL</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>EM</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>NS</bold>
              </th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>M ± SD</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>M ± SD</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>M ± SD</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">AttU-Net (Oktay et al., <xref rid="B32" ref-type="bibr">2018</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.607 ± 0.039</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.853 ± 0.043</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.655 ± 0.020</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSAttU-Net (Ours)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.674 ± 0.005</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.895 ± 0.004</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.677 ± 0.010</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">U-Net++ (Zhou et al., <xref rid="B54" ref-type="bibr">2020</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.670 ± 0.020</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.885 ± 0.013</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.665 ± 0.012</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net++ (Ours)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.687 ± 0.009</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.895 ± 0.002</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.691 ± 0.022</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>MSAttU-Net and MSU-Net ++ are extended versions of AttU-Net and U-Net ++. Intersection over Union (IoU) is used as the evaluation metric for comparison</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>It can be seen from the experimental results that the performance of wide U-Net is better than U-Net. The main reason is that there are more parameters in wide U-Net. When the residual idea is not introduced, MSU-Net achieves very robust performance on all three data sets. Compared with U-Net, MSU-Net is higher than 0.1, 0.016, and 0.027 on the three datasets. The performance of the network is improved by introducing residual ideas. In addition, the extended experiment on U-Net variants also confirmed the effectiveness and universality of multi-scale block. By comparing the performance of MSU-Net (37+encoder) and U-Net, we found that the ability of network to extract features was enhanced by combining multi-scale blocks.</p>
    </sec>
    <sec>
      <title>5.3. Semantic Segmentation Results</title>
      <p>In order to verify the performance of the network, MSU-Net was compared with the current more advanced segmentation network (Ronneberger et al., <xref rid="B38" ref-type="bibr">2015</xref>; Badrinarayanan et al., <xref rid="B3" ref-type="bibr">2017</xref>; Chen et al., <xref rid="B7" ref-type="bibr">2018b</xref>; Zhou et al., <xref rid="B54" ref-type="bibr">2020</xref>). In addition, chest X-ray and skin lesion segmentation datasets were added to the experiment. These two datasets are larger than the three previously mentioned datasets. <xref ref-type="fig" rid="F7">Figure 7</xref> depicts a qualitative comparison of the results between the different split schemas. Compared with other architectures, the segmentation results of MSU-Net are more detailed. SegNet cannot be trained on EM datasets. Therefore, SegNet has not experimented on the EM dataset.</p>
      <p><xref rid="T6" ref-type="table">Table 6</xref> shows the segmentation performance of the architectures on different datasets. A statistical analysis based on independent two-sample <italic>t</italic>-tests is performed by us for each pair of data between different structures. Our results show that MSU-Net is an effective network structure.</p>
      <table-wrap id="T6" position="float">
        <label>Table 6</label>
        <caption>
          <p>Semantic segmentation results measured by different metrics for different network architectures.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Metric</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Architecture</bold>
              </th>
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>SL</bold>
              </th>
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>CXR</bold>
              </th>
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>BUL</bold>
              </th>
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>EM</bold>
              </th>
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>NS</bold>
              </th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>M ± SD</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>p-value</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>M ± SD</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>p-value</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>M ± SD</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>p-value</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>M ± SD</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>p-value</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>M ± SD</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>p-value</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">IoU</td>
              <td valign="top" align="left" rowspan="1" colspan="1">SegNet (Badrinarayanan et al., <xref rid="B3" ref-type="bibr">2017</xref>)</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.752 ± 0.007</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">9.824e-4</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.832 ± 0.008</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">6.179e-5</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.630 ± 0.033</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.001</td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.586 ± 0.021</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">4.084e-6</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">DeepLabV3+ (Chen et al., <xref rid="B7" ref-type="bibr">2018b</xref>)</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.762 ± 0.002</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">2.202e-3</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.847 ± 0.005</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">3.261e-4</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.558 ± 0.034</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">1.761e-5</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.837 ± 0.015</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">1.582e-5</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.582 ± 0.019</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">1.717e-6</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">U-Net (Ronneberger et al., <xref rid="B38" ref-type="bibr">2015</xref>)</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.751 ± 0.005</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">1.872e-4</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.857 ± 0.005</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.020</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.608 ± 0.037</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">4.789e-4</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.884 ± 0.007</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">6.873e-4</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.675 ± 0.018</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.020</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">U-Ne++ (Zhou et al., <xref rid="B54" ref-type="bibr">2020</xref>)</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.746 ± 0.008</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">2.725e-4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.863 ± 0.004</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.232</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.670 ± 0.020</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.013</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.885 ± 0.013</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.031</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.665 ± 0.012</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">8.243e-4</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net(Ours)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.771</bold>
                <bold>±</bold>
                <bold>0.004</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.867</bold>
                <bold>±</bold>
                <bold>0.006</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.708</bold>
                <bold>±</bold>
                <bold>0.011</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.900</bold>
                <bold>±</bold>
                <bold>0.001</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.702</bold>
                <bold>±</bold>
                <bold>0.011</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Dice</td>
              <td valign="top" align="left" rowspan="1" colspan="1">SegNet (Badrinarayanan et al., <xref rid="B3" ref-type="bibr">2017</xref>)</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.852 ± 0.006</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.002</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.908 ± 0.005</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">6.393e-5</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.770 ± 0.026</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.002</td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.738 ± 0.017</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">5.941e-6</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">DeepLabV3+ (Chen et al., <xref rid="B7" ref-type="bibr">2018b</xref>)</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.857 ± 0.003</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.002</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.917 ± 0.003</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">3.123e-4</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.713 ± 0.029</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">3.215e-5</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.911 ± 0.009</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">2.104e-5</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.734 ± 0.016</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">2.830e-6</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">U-Net (Ronneberger et al., <xref rid="B38" ref-type="bibr">2015</xref>)</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.850 ± 0.004</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">1.696e-4</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.923 ± 0.003</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.020</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.753 ± 0.029</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">6.919e-4</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.938 ± 0.004</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">7.314e-4</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.805 ± 0.013</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.022</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">U-Ne++ (Zhou et al., <xref rid="B54" ref-type="bibr">2020</xref>)</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.847 ± 0.006</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">2.892e-4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.926 ± 0.002</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.230</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.800 ± 0.014</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.015</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.939 ± 0.007</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.032</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.797 ± 0.008</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">5.129e-4</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net(Ours)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.865</bold>
                <bold>±</bold>
                <bold>0.003</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.929</bold>
                <bold>±</bold>
                <bold>0.004</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.827</bold>
                <bold>±</bold>
                <bold>0.008</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.947</bold>
                <bold>±</bold>
                <bold>0.001</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.824</bold>
                <bold>±</bold>
                <bold>0.007</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Precision</td>
              <td valign="top" align="left" rowspan="1" colspan="1">SegNet (Badrinarayanan et al., <xref rid="B3" ref-type="bibr">2017</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.886 ± 0.010</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.161</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.856 ± 0.009</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">4.465e-4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.725 ± 0.040</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.115</td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.873 ± 0.008</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.203</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">DeepLabV3+ (Chen et al., <xref rid="B7" ref-type="bibr">2018b</xref>)</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.892 ± 0.008</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.037</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.875 ± 0.005</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.029</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.798 ± 0.054</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">2.227e-4</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.864 ± 0.029</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">6.076e-4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.860 ± 0.019</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.065</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">U-Net (Ronneberger et al., <xref rid="B38" ref-type="bibr">2015</xref>)</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">
                <bold>0.899</bold>
                <bold>±</bold>
                <bold>0.014</bold>
              </td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.024</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.878 ± 0.006</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.079</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.760 ± 0.061</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.018</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.913 ± 0.014</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.007</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.888</bold>
                <bold>±</bold>
                <bold>0.019</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.917</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">U-Net++ (Zhou et al., <xref rid="B54" ref-type="bibr">2020</xref>)</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.895 ± 0.010</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.030</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.882 ± 0.005</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.274</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.786 ± 0.043</td>
              <td valign="top" align="center" style="background-color:#facbcc" rowspan="1" colspan="1">0.011</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.919 ± 0.025</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.196</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.853 ± 0.059</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.267</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">MSU-Net(Ours)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.873 ± 0.015</td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.887</bold>
                <bold>±</bold>
                <bold>0.009</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.842</bold>
                <bold>±</bold>
                <bold>0.006</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.935</bold>
                <bold>±</bold>
                <bold>0.003</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.887 ± 0.021</td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>We have performed independent two sample t-test between and highlighted boxes in red when the differences are statistically significant (p &lt; 0.05). Bold values represent the best results</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>The results in <xref rid="T5" ref-type="table">Table 5</xref> suggest that our proposed MSU-Net is more robust in semantic segmentation. Compared with the U-Net, MSU-Net achieves a significant IoU gain over both architectures for all the five tasks of SL (↑0.01), CXR (↑0.01), BUL (↑0.1), EM (↑0.016), NS (↑0.027) segmentation. AUC of different architectures on the data set is illustrated in <xref ref-type="fig" rid="F8">Figure 8</xref>. <xref ref-type="fig" rid="F8">Figure 8</xref> shows the ROC curve of different architectures on the datasets. Our model achieves the best performance in all datasets. Fine Precision is not captured by our model on the SL dataset. However, the high sensitivity of our model is shown in <xref ref-type="fig" rid="F8">Figure 8</xref>. This allows false positives and false negatives in the data to be better balanced by our model. It is mainly due to the multiple convolution sequence with different receptive fields. This design makes the features in the network richer and more diverse.</p>
      <fig id="F8" position="float">
        <label>Figure 8</label>
        <caption>
          <p>ROC curves for different architectures. AUC is the area under the curve.</p>
        </caption>
        <graphic xlink:href="fgene-12-639930-g0008"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s6">
    <title>6. Discussion</title>
    <p>Medical image segmentation plays an important role in diagnosis, treatment and prognosis evaluation. In the process of diagnosis, the main applications include morphological analysis, volume calculation, anatomical structure analysis, etc. In surgical treatment planning, the commonly used methods include preoperative biopsy guidance, target area planning of radiotherapy, image registration fusion and path planning, and target tracking in medical robot, etc. In the prognostic assessment, the most important segmentation is the analysis of lesion volume change and the analysis of lesion histological characteristics. In addition, medical image segmentation can be applied to three-dimensional reconstruction visualization, which can provide clinicians with more intuitive pathological morphology and spatial anatomy. In recent years, the method based on deep learning has been widely used in medical image segmentation. However, the performance of segmentation is greatly affected by the network architecture and the ability to acquire features in learning process.</p>
    <p>U-Net is a very classical network architecture in the field of medical image segmentation. At present, U-Net is widely used in medical image segmentation. However, the basic architecture of U-Net has not been significantly modified by the researchers. Large receptive fields play an important role when we need to make dense per-pixel predictions. In order to improve the existing segmentation model, multi-scale blocks are constructed by convolution sequence and multiple convolution kernel with different receptive fields. The different types of multi-scale blocks are illustrated in <xref ref-type="fig" rid="F3">Figure 3</xref>. In addition, MSU-Net is proposed after all the convolution blocks in the original U-Net are replaced by multi-scale block. The details of the MSU-Net are illustrated in <xref ref-type="fig" rid="F1">Figure 1</xref>. Multiple convolution sequences are used to extract more semantic features from images. In addition, convolution kernels with different receptive fields are used to make features more diverse. The problem of unknown network width is alleviated by effective integration of multiple convolution sequences with different receptive fields.</p>
    <p>The most important innovation described in this paper is the combination of multiple convolution sequences and convolution kernel with different receptive fields to improve the segmentation performance. It can be seen from the <xref rid="T1" ref-type="table">Table 1</xref> that the performance of the network is affected by different receptive fields. Good performance was achieved by combining advanced ideas with multi-scale blocks. In addition, multi-scale blocks are extended to the variants of original U-Net. The results in <xref rid="T4" ref-type="table">Tables 4</xref>, <xref rid="T5" ref-type="table">5</xref> describes that the segmentation performance of the network is improved by combining the multiple convolution sequence and the convolution kernel with the different receptive fields. The strategies of our proposed strategy has the following advantages: (1) More diverse features are extracted through the convolution kernel of different receptive fields. This is useful for intensive forecasting tasks that require detailed spatial information. At the same time, the problem of unknown network width can be alleviated. (2) More feature information is extracted by multi-convolution sequence, which is helpful to the segmentation task. Our method has obtained the best performance compared with the advanced models through the demonstration of multiple medical image segmentation datasets (see in <xref rid="T6" ref-type="table">Table 6</xref>). The highest AUC is obtained by our architecture (see in <xref ref-type="fig" rid="F8">Figure 8</xref>). This suggests that our model has a stronger ability to balance false positives and false negatives in the data. In general, the proposed method is useful for intensive forecasting tasks requiring detailed spatial information. Different receptive fields can provide diverse semantic information for tasks, which is beneficial to the segmentation of lesions. More detailed segmentation results can provide doctors with more detailed lesion areas, which is helpful for the diagnosis of disease and the formulation of treatment plan.</p>
    <p>Although we have widely evaluated the performance of the network on different datasets, there are still some deficiencies in our network. First, the convolution kernels with a larger receptive field are not attempted due to objective factors. The performance of the network may be improved through greater receptive field. Second, the dilated convolution can increase the receptive field of the convolution kernel without increasing the number of parameters. Unfortunately, dilated convolution was not attempted in our experiment. Third, our network has not been validated against the 3D medical image segmentation dataset. The above work may be completed by us in the future.</p>
  </sec>
  <sec sec-type="conclusions" id="s7">
    <title>7. Conclusion</title>
    <p>In order to obtain more accurate segmentation image, a new structure called multi-scale block was proposed by us. The convolution blocks in the original U-Net are replaced by multi-scale blocks to obtain MSU-Net. The improvement of MSU-Net performance is attributed to multiple convolution sequence and convolution kernels with different receptive fields. Two key issues are addressed by this design: (1) The diversity of features is lost due to the fixed size of the convolution kernel. (2) Feature information may be lost at each scale using a single convolutional sequence to extract features. Five different public datasets were used to conduct an extensive evaluation of MSU-Net. The experimental results show that MSU-Net achieves the best performance.</p>
  </sec>
  <sec sec-type="data-availability" id="s8">
    <title>Data Availability Statement</title>
    <p>Publicly available datasets were analyzed in this study. This data can be found here: all datasets can be found in <xref rid="T3" ref-type="table">Table 3</xref>.</p>
  </sec>
  <sec id="s9">
    <title>Author Contributions</title>
    <p>RS, DZ, and JL: conceptualization and writing (review and editing). RS, DZ, and CC: data curation. RS and DZ: methodology, validation, and writing (original draft). RS: project administration and visualization. All authors have read and agreed to the published version of the manuscript.</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
</body>
<back>
  <ack>
    <p>The authors express their sincere gratitude to the creator of the public dataset for many valuable discussions and educational help in the growing field of medical image analysis.</p>
  </ack>
  <fn-group>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding.</bold> This research was supported by two research grants: (1) National Natural Science Foundation of China (62033002). (2) Science and Technology Project grant from Anhui Province (Grant Nos. 1508085QHl84, 201904a07020098). (3) Fundamental Research Fund for the Central Universities (Grant No. WK 9110000032).</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abramoff</surname><given-names>M. D.</given-names></name><name><surname>Alward</surname><given-names>W. L.</given-names></name><name><surname>Greenlee</surname><given-names>E. C.</given-names></name><name><surname>Shuba</surname><given-names>L.</given-names></name><name><surname>Kim</surname><given-names>C. Y.</given-names></name><name><surname>Fingert</surname><given-names>J. H.</given-names></name><etal/></person-group>. (<year>2007</year>). <article-title>Automated segmentation of the optic disc from stereo color photographs using physiologically plausible features</article-title>. <source>Investig. Ophthalmol. Vis. Sci</source>. <volume>48</volume>, <fpage>1665</fpage>–<lpage>1673</lpage>. <pub-id pub-id-type="doi">10.1167/iovs.06-1081</pub-id><?supplied-pmid 17389498?><pub-id pub-id-type="pmid">17389498</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alom</surname><given-names>M. Z.</given-names></name><name><surname>Hasan</surname><given-names>M.</given-names></name><name><surname>Yakopcic</surname><given-names>C.</given-names></name><name><surname>Taha</surname><given-names>T. M.</given-names></name><name><surname>Asari</surname><given-names>V. K.</given-names></name></person-group> (<year>2018</year>). <article-title>Recurrent residual convolutional neural network based on U-Net (R2U-Net) for medical image segmentation</article-title>. <source>arXiv [preprint]. arXiv:1802.06955</source>. <pub-id pub-id-type="doi">10.1109/NAECON.2018.8556686</pub-id><?supplied-pmid 30944843?><pub-id pub-id-type="pmid">30944843</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badrinarayanan</surname><given-names>V.</given-names></name><name><surname>Kendall</surname><given-names>A.</given-names></name><name><surname>Cipolla</surname><given-names>R.</given-names></name></person-group> (<year>2017</year>). <article-title>Segnet: A deep convolutional encoder-decoder architecture for image segmentation</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>. <volume>39</volume>, <fpage>2481</fpage>–<lpage>2495</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2016.2644615</pub-id><?supplied-pmid 28060704?><pub-id pub-id-type="pmid">28060704</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Candemir</surname><given-names>S.</given-names></name><name><surname>Jaeger</surname><given-names>S.</given-names></name><name><surname>Palaniappan</surname><given-names>K.</given-names></name><name><surname>Musco</surname><given-names>J. P.</given-names></name><name><surname>Singh</surname><given-names>R. K.</given-names></name><name><surname>Xue</surname><given-names>Z.</given-names></name><etal/></person-group>. (<year>2013</year>). <article-title>Lung segmentation in chest radiographs using anatomical atlases with nonrigid registration</article-title>. <source>IEEE Trans. Med. Imaging</source><volume>33</volume>, <fpage>577</fpage>–<lpage>590</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2013.2290491</pub-id><?supplied-pmid 24239990?><pub-id pub-id-type="pmid">24239990</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cardona</surname><given-names>A.</given-names></name><name><surname>Saalfeld</surname><given-names>S.</given-names></name><name><surname>Preibisch</surname><given-names>S.</given-names></name><name><surname>Schmid</surname><given-names>B.</given-names></name><name><surname>Cheng</surname><given-names>A.</given-names></name><name><surname>Pulokas</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2010</year>). <article-title>An integrated micro-and macroarchitectural analysis of the drosophila brain by computer-assisted serial section electron microscopy</article-title>. <source>PLoS Biol</source>. <volume>8</volume>:<fpage>e1000502</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.1000502</pub-id><?supplied-pmid 20957184?><pub-id pub-id-type="pmid">20957184</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L.</given-names></name><name><surname>Bentley</surname><given-names>P.</given-names></name><name><surname>Mori</surname><given-names>K.</given-names></name><name><surname>Misawa</surname><given-names>K.</given-names></name><name><surname>Fujiwara</surname><given-names>M.</given-names></name><name><surname>Rueckert</surname><given-names>D.</given-names></name></person-group> (<year>2018a</year>). <article-title>Drinet for medical image segmentation</article-title>. <source>IEEE Trans. Med. Imaging</source>
<volume>37</volume>, <fpage>2453</fpage>–<lpage>2462</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2018.2835303</pub-id><pub-id pub-id-type="pmid">29993738</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L.-C.</given-names></name><name><surname>Zhu</surname><given-names>Y.</given-names></name><name><surname>Papandreou</surname><given-names>G.</given-names></name><name><surname>Schroff</surname><given-names>F.</given-names></name><name><surname>Adam</surname><given-names>H.</given-names></name></person-group> (<year>2018b</year>). <article-title>“Encoder-decoder with atrous separable convolution for semantic image segmentation,”</article-title> in <source>Proceedings of the European Conference on Computer Vision (ECCV)</source>, <volume>11211</volume>, <fpage>833</fpage>–<lpage>851</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-030-01234-2_49</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Ciresan</surname><given-names>D.</given-names></name><name><surname>Giusti</surname><given-names>A.</given-names></name><name><surname>Gambardella</surname><given-names>L. M.</given-names></name><name><surname>Schmidhuber</surname><given-names>J.</given-names></name></person-group> (<year>2012</year>). <article-title>“Deep neural networks segment neuronal membranes in electron microscopy images,”</article-title> in <source>Advances in Neural Information Processing Systems</source>. <fpage>2843</fpage>–<lpage>2851</lpage>. Available online at: <ext-link ext-link-type="uri" xlink:href="https://dl.acm.org/doi/10.5555/2999325.2999452">https://dl.acm.org/doi/10.5555/2999325.2999452</ext-link></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Codella</surname><given-names>N.</given-names></name><name><surname>Rotemberg</surname><given-names>V.</given-names></name><name><surname>Tschandl</surname><given-names>P.</given-names></name><name><surname>Celebi</surname><given-names>M. E.</given-names></name><name><surname>Dusza</surname><given-names>S.</given-names></name><name><surname>Gutman</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2019</year>). <source>Skin lesion analysis toward melanoma detection 2018: a challenge hosted by the international skin imaging collaboration (ISIC). arXiv [preprint].</source>. Available online at: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1902.03368">https://arxiv.org/abs/1902.03368</ext-link></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elsken</surname><given-names>T.</given-names></name><name><surname>Metzen</surname><given-names>J. H.</given-names></name><name><surname>Hutter</surname><given-names>F.</given-names></name></person-group> (<year>2018</year>). <article-title>Neural architecture search: a survey</article-title>. <source>arXiv [preprint]. arXiv:1808.05377</source>. <pub-id pub-id-type="doi">10.1007/978-3-030-05318-5_11</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>F.</given-names></name><name><surname>Huang</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Xiong</surname><given-names>X.</given-names></name><name><surname>Jiang</surname><given-names>Z.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name><etal/></person-group>. (<year>2019</year>). <source>A semantic-based medical image fusion approach. arXiv [preprint]</source>. Available online at: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1906.00225">https://arxiv.org/abs/1906.00225</ext-link></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ge</surname><given-names>R.</given-names></name><name><surname>Yang</surname><given-names>G.</given-names></name><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Luo</surname><given-names>L.</given-names></name><name><surname>Feng</surname><given-names>C.</given-names></name><name><surname>Ma</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>K-Net: Integrate left ventricle segmentation and direct quantification of paired echo sequence</article-title>. <source>IEEE Trans. Med. Imaging</source><volume>39</volume>, <fpage>1690</fpage>–<lpage>1702</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2019.2955436</pub-id><?supplied-pmid 31765307?><pub-id pub-id-type="pmid">31765307</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guan</surname><given-names>S.</given-names></name><name><surname>Khan</surname><given-names>A. A.</given-names></name><name><surname>Sikdar</surname><given-names>S.</given-names></name><name><surname>Chitnis</surname><given-names>P. V.</given-names></name></person-group> (<year>2019</year>). <article-title>Fully dense unet for 2-D sparse photoacoustic tomography artifact removal</article-title>. <source>IEEE J. Biomed. Health Inform</source>. <volume>24</volume>, <fpage>568</fpage>–<lpage>576</lpage>. <pub-id pub-id-type="doi">10.1109/JBHI.2019.2912935</pub-id><?supplied-pmid 31021809?><pub-id pub-id-type="pmid">31021809</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). <article-title>“Deep residual learning for image recognition,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>, <fpage>770</fpage>–<lpage>778</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id><?supplied-pmid 32166560?><pub-id pub-id-type="pmid">32166560</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>He</surname><given-names>P.</given-names></name><name><surname>Huang</surname><given-names>W.</given-names></name><name><surname>Qiao</surname><given-names>Y.</given-names></name><name><surname>Loy</surname><given-names>C. C.</given-names></name><name><surname>Tang</surname><given-names>X.</given-names></name></person-group> (<year>2015</year>). <source>Reading scene text in deep convolutional sequences. arXiv [preprint]</source>. Available online at: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1506.04395">https://arxiv.org/abs/1506.04395</ext-link></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Iglovikov</surname><given-names>V.</given-names></name><name><surname>Shvets</surname><given-names>A.</given-names></name></person-group> (<year>2018</year>). <source>Ternausnet: U-Net with VGG11 encoder pre-trained on imagenet for image segmentation. arXiv [preprint]</source>. Available online at: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1801.05746">https://arxiv.org/abs/1801.05746</ext-link></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaeger</surname><given-names>S.</given-names></name><name><surname>Karargyris</surname><given-names>A.</given-names></name><name><surname>Candemir</surname><given-names>S.</given-names></name><name><surname>Folio</surname><given-names>L.</given-names></name><name><surname>Siegelman</surname><given-names>J.</given-names></name><name><surname>Callaghan</surname><given-names>F.</given-names></name><etal/></person-group>. (<year>2013</year>). <article-title>Automatic tuberculosis screening using chest radiographs</article-title>. <source>IEEE Trans. Med. Imaging</source><volume>33</volume>, <fpage>233</fpage>–<lpage>245</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2013.2284099</pub-id><?supplied-pmid 29959539?><pub-id pub-id-type="pmid">24108713</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamnitsas</surname><given-names>K.</given-names></name><name><surname>Ledig</surname><given-names>C.</given-names></name><name><surname>Newcombe</surname><given-names>V. F.</given-names></name><name><surname>Simpson</surname><given-names>J. P.</given-names></name><name><surname>Kane</surname><given-names>A. D.</given-names></name><name><surname>Menon</surname><given-names>D. K.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Efficient multi-scale 3d cnn with fully connected crf for accurate brain lesion segmentation</article-title>. <source>Med. Image Anal</source>. <volume>36</volume>, <fpage>61</fpage>–<lpage>78</lpage>. <pub-id pub-id-type="doi">10.1016/j.media.2016.10.004</pub-id><?supplied-pmid 27865153?><pub-id pub-id-type="pmid">27865153</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kitrungrotsakul</surname><given-names>T.</given-names></name><name><surname>Han</surname><given-names>X.-H.</given-names></name><name><surname>Chen</surname><given-names>Y.-W.</given-names></name></person-group> (<year>2015</year>). <article-title>“Liver segmentation using superpixel-based graph cuts and restricted regions of shape constrains,”</article-title> in <source>2015 IEEE International Conference on Image Processing (ICIP)</source> (<publisher-loc>IEEE</publisher-loc>), <fpage>3368</fpage>–<lpage>3371</lpage>. <pub-id pub-id-type="doi">10.1109/ICIP.2015.7351428</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Kohl</surname><given-names>S.</given-names></name><name><surname>Romera-Paredes</surname><given-names>B.</given-names></name><name><surname>Meyer</surname><given-names>C.</given-names></name><name><surname>De Fauw</surname><given-names>J.</given-names></name><name><surname>Ledsam</surname><given-names>J. R.</given-names></name><name><surname>Maier-Hein</surname><given-names>K.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>“A probabilistic U-Net for segmentation of ambiguous images,”</article-title> in <source>Advances in Neural Information Processing Systems</source>, 6965–6975. Available online at: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1806.05034v4">https://arxiv.org/abs/1806.05034v4</ext-link></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>LaLonde</surname><given-names>R.</given-names></name><name><surname>Bagci</surname><given-names>U.</given-names></name></person-group> (<year>2018</year>). <source>Capsules for object segmentation. arXiv [preprint]</source>. Available online at: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1804.04241">https://arxiv.org/abs/1804.04241</ext-link></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le</surname><given-names>N. Q. K.</given-names></name><name><surname>Ho</surname><given-names>Q.-T.</given-names></name><name><surname>Yapp</surname><given-names>E. K. Y.</given-names></name><name><surname>Ou</surname><given-names>Y.-Y.</given-names></name><name><surname>Yeh</surname><given-names>H.-Y.</given-names></name></person-group> (<year>2020</year>). <article-title>Deepetc: A deep convolutional neural network architecture for investigating and classifying electron transport chain's complexes</article-title>. <source>Neurocomputing</source>
<volume>375</volume>, <fpage>71</fpage>–<lpage>79</lpage>. <pub-id pub-id-type="doi">10.1016/j.neucom.2019.09.070</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le</surname><given-names>N. Q. K.</given-names></name><name><surname>Yapp</surname><given-names>E. K. Y.</given-names></name><name><surname>Nagasundaram</surname><given-names>N.</given-names></name><name><surname>Yeh</surname><given-names>H.-Y.</given-names></name></person-group> (<year>2019</year>). <article-title>Classifying promoters by interpreting the hidden information of dna sequences via deep learning and combination of continuous fasttext n-grams</article-title>. <source>Front. Bioeng. Biotechnol</source>. <volume>7</volume>:<fpage>305</fpage>. <pub-id pub-id-type="doi">10.3389/fbioe.2019.00305</pub-id><?supplied-pmid 31750297?><pub-id pub-id-type="pmid">31750297</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Tang</surname><given-names>Q.</given-names></name><name><surname>Fan</surname><given-names>Z.</given-names></name><name><surname>Yu</surname><given-names>J.</given-names></name></person-group> (<year>2019</year>). <article-title>Dual U-Net for the segmentation of overlapping glioma nuclei</article-title>. <source>IEEE Access</source>
<volume>7</volume>, <fpage>84040</fpage>–<lpage>84052</lpage>. <pub-id pub-id-type="doi">10.1109/ACCESS.2019.2924744</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>C.</given-names></name><name><surname>Chen</surname><given-names>L.-C.</given-names></name><name><surname>Schroff</surname><given-names>F.</given-names></name><name><surname>Adam</surname><given-names>H.</given-names></name><name><surname>Hua</surname><given-names>W.</given-names></name><name><surname>Yuille</surname><given-names>A. L.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>“Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>, <fpage>82</fpage>–<lpage>92</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2019.00017</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>H.</given-names></name><name><surname>Simonyan</surname><given-names>K.</given-names></name><name><surname>Yang</surname><given-names>Y.</given-names></name></person-group> (<year>2018</year>). <source>DARTS: differentiable architecture search. arXiv [preprint]</source>. Available online at: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1806.09055">https://arxiv.org/abs/1806.09055</ext-link></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>L.</given-names></name><name><surname>Hu</surname><given-names>X.</given-names></name><name><surname>Zhu</surname><given-names>L.</given-names></name><name><surname>Fu</surname><given-names>C.-W.</given-names></name><name><surname>Qin</surname><given-names>J.</given-names></name><name><surname>Heng</surname><given-names>P.-A.</given-names></name></person-group> (<year>2020</year>). <article-title>ψ-net: Stacking densely convolutional lstms for sub-cortical brain structure segmentation</article-title>. <source>IEEE Trans. Med. Imaging</source>. <volume>39</volume>:<fpage>2806</fpage>–<lpage>2817</lpage>
<pub-id pub-id-type="doi">10.1109/TMI.2020.2975642</pub-id><?supplied-pmid 32091996?><pub-id pub-id-type="pmid">32091996</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>J.</given-names></name><name><surname>Shelhamer</surname><given-names>E.</given-names></name><name><surname>Darrell</surname><given-names>T.</given-names></name></person-group> (<year>2015</year>). <article-title>“Fully convolutional networks for semantic segmentation,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>, <fpage>3431</fpage>–<lpage>3440</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2015.7298965</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>W.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Urtasun</surname><given-names>R.</given-names></name><name><surname>Zemel</surname><given-names>R.</given-names></name></person-group> (<year>2016</year>). <article-title>“Understanding the effective receptive field in deep convolutional neural networks,”</article-title> in <source>Advances in Neural Information Processing Systems</source>, <fpage>4898</fpage>–<lpage>4906</lpage>. Available online at: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1701.04128">https://arxiv.org/abs/1701.04128</ext-link></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Milletari</surname><given-names>F.</given-names></name><name><surname>Navab</surname><given-names>N.</given-names></name><name><surname>Ahmadi</surname><given-names>S.-A.</given-names></name></person-group> (<year>2016</year>). <article-title>“V-Net: fully convolutional neural networks for volumetric medical image segmentation,”</article-title> in <source>2016 Fourth International Conference on 3D Vision (3DV)</source> (<publisher-loc>IEEE</publisher-loc>), <fpage>565</fpage>–<lpage>571</lpage>. <pub-id pub-id-type="doi">10.1109/3DV.2016.79</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Myronenko</surname><given-names>A.</given-names></name></person-group> (<year>2018</year>). <article-title>“3D MRI brain tumor segmentation using autoencoder regularization,”</article-title> in <source>International MICCAI Brainlesion Workshop</source> (<publisher-loc>Springer</publisher-loc>), <volume>11384</volume>, <fpage>311</fpage>–<lpage>320</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-030-11726-9_28</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Oktay</surname><given-names>O.</given-names></name><name><surname>Schlemper</surname><given-names>J.</given-names></name><name><surname>Folgoc</surname><given-names>L. L.</given-names></name><name><surname>Lee</surname><given-names>M.</given-names></name><name><surname>Heinrich</surname><given-names>M.</given-names></name><name><surname>Misawa</surname><given-names>K.</given-names></name><etal/></person-group>. (<year>2018</year>). <source>Attention U-Net: Learning where to look for the pancreas. arXiv [preprint]</source>. Available online at: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1804.03999">https://arxiv.org/abs/1804.03999</ext-link></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A.</given-names></name><name><surname>Gross</surname><given-names>S.</given-names></name><name><surname>Massa</surname><given-names>F.</given-names></name><name><surname>Lerer</surname><given-names>A.</given-names></name><name><surname>Bradbury</surname><given-names>J.</given-names></name><name><surname>Chanan</surname><given-names>G.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>“Pytorch: an imperative style, high-performance deep learning library,”</article-title> in <source>Advances in Neural Information Processing Systems</source>, 8026–8037. Available online at: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1912.01703">https://arxiv.org/abs/1912.01703</ext-link></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>C.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Yu</surname><given-names>G.</given-names></name><name><surname>Luo</surname><given-names>G.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group> (<year>2017</year>). <article-title>“Large kernel matters-improve semantic segmentation by global convolutional network,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>, <fpage>4353</fpage>–<lpage>4361</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2017.189</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>S.</given-names></name><name><surname>Pinto</surname><given-names>A.</given-names></name><name><surname>Alves</surname><given-names>V.</given-names></name><name><surname>Silva</surname><given-names>C. A.</given-names></name></person-group> (<year>2016</year>). <article-title>Brain tumor segmentation using convolutional neural networks in mri images</article-title>. <source>IEEE Trans. Med. Imaging</source>
<volume>35</volume>, <fpage>1240</fpage>–<lpage>1251</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2016.2538465</pub-id><?supplied-pmid 31342192?><pub-id pub-id-type="pmid">26960222</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Poudel</surname><given-names>R. P.</given-names></name><name><surname>Lamata</surname><given-names>P.</given-names></name><name><surname>Montana</surname><given-names>G.</given-names></name></person-group> (<year>2016</year>). <article-title>“Recurrent fully convolutional neural networks for multi-slice mri cardiac segmentation,”</article-title> in <source>Reconstruction, Segmentation, and Analysis of Medical Images</source>, eds M. A. Zuluaga, K. Bhatia, B. Kainz, M. H. Moghari, and D. F. Pace (<publisher-loc>Athens</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>83</fpage>–<lpage>94</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-52280-7_8</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robbins</surname><given-names>H.</given-names></name><name><surname>Monro</surname><given-names>S.</given-names></name></person-group> (<year>1951</year>). <article-title>A stochastic approximation method</article-title>. <source>Ann. Math. Stat</source>. <volume>22</volume>, <fpage>400</fpage>–<lpage>407</lpage>. <pub-id pub-id-type="doi">10.1214/aoms/1177729586</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O.</given-names></name><name><surname>Fischer</surname><given-names>P.</given-names></name><name><surname>Brox</surname><given-names>T.</given-names></name></person-group> (<year>2015</year>). <article-title>“U-Net: convolutional networks for biomedical image segmentation,”</article-title> in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source> (<publisher-loc>Springer</publisher-loc>), <volume>9351</volume>, <fpage>234</fpage>–<lpage>241</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>H. R.</given-names></name><name><surname>Shen</surname><given-names>C.</given-names></name><name><surname>Oda</surname><given-names>H.</given-names></name><name><surname>Sugino</surname><given-names>T.</given-names></name><name><surname>Oda</surname><given-names>M.</given-names></name><name><surname>Hayashi</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>“A multi-scale pyramid of 3D fully convolutional networks for abdominal multi-organ segmentation,”</article-title> in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source> (<publisher-loc>Springer</publisher-loc>), <volume>11073</volume>, <fpage>417</fpage>–<lpage>425</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-030-00937-3_48</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salehi</surname><given-names>S. S. M.</given-names></name><name><surname>Erdogmus</surname><given-names>D.</given-names></name><name><surname>Gholipour</surname><given-names>A.</given-names></name></person-group> (<year>2017</year>). <article-title>Auto-context convolutional neural network (auto-net) for brain extraction in magnetic resonance imaging</article-title>. <source>IEEE Trans. Med. Imaging</source>
<volume>36</volume>, <fpage>2319</fpage>–<lpage>2330</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2017.2721362</pub-id><?supplied-pmid 28678704?><pub-id pub-id-type="pmid">28678704</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salehi</surname><given-names>S. S. M.</given-names></name><name><surname>Khan</surname><given-names>S.</given-names></name><name><surname>Erdogmus</surname><given-names>D.</given-names></name><name><surname>Gholipour</surname><given-names>A.</given-names></name></person-group> (<year>2018</year>). <article-title>Real-time deep pose estimation with geodesic loss for image-to-template rigid registration</article-title>. <source>IEEE Trans. Med. Imaging</source>
<volume>38</volume>, <fpage>470</fpage>–<lpage>481</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2018.2866442</pub-id><?supplied-pmid 30138909?><pub-id pub-id-type="pmid">30138909</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seif</surname><given-names>G.</given-names></name><name><surname>Androutsos</surname><given-names>D.</given-names></name></person-group> (<year>2018</year>). <article-title>“Large receptive field networks for high-scale image super-resolution,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</source>, <fpage>763</fpage>–<lpage>772</lpage>. <pub-id pub-id-type="doi">10.1109/CVPRW.2018.00120</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seo</surname><given-names>H.</given-names></name><name><surname>Huang</surname><given-names>C.</given-names></name><name><surname>Bassenne</surname><given-names>M.</given-names></name><name><surname>Xiao</surname><given-names>R.</given-names></name><name><surname>Xing</surname><given-names>L.</given-names></name></person-group> (<year>2019</year>). <article-title>Modified U-Net (MU-Net) with incorporation of object-dependent high level features for improved liver and liver-tumor segmentation in ct images</article-title>. <source>IEEE Trans. Med. Imaging</source>
<volume>39</volume>, <fpage>1316</fpage>–<lpage>1325</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2019.2948320</pub-id><?supplied-pmid 31634827?><pub-id pub-id-type="pmid">31634827</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>C.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Yu</surname><given-names>Z.</given-names></name><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Wen</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>“RF-Net: an end-to-end image matching network based on receptive field,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>, <fpage>8132</fpage>–<lpage>8140</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2019.00832</pub-id></mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>T.</given-names></name><name><surname>Meng</surname><given-names>F.</given-names></name><name><surname>Rodriguez-Paton</surname><given-names>A.</given-names></name><name><surname>Li</surname><given-names>P.</given-names></name><name><surname>Zheng</surname><given-names>P.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name></person-group> (<year>2019</year>). <article-title>U-next: a novel convolution neural network with an aggregation u-net architecture for gallstone segmentation in CT images</article-title>. <source>IEEE Access</source>
<volume>7</volume>, <fpage>166823</fpage>–<lpage>166832</lpage>. <pub-id pub-id-type="doi">10.1109/ACCESS.2019.2953934</pub-id></mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sua</surname><given-names>J. N.</given-names></name><name><surname>Lim</surname><given-names>S. Y.</given-names></name><name><surname>Yulius</surname><given-names>M. H.</given-names></name><name><surname>Su</surname><given-names>X.</given-names></name><name><surname>Yapp</surname><given-names>E. K. Y.</given-names></name><name><surname>Le</surname><given-names>N. Q. K.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Incorporating convolutional neural networks and sequence graph transform for identifying multilabel protein lysine ptm sites</article-title>. <source>Chemometr. Intell. Lab. Syst</source>. <volume>206</volume>:<fpage>104171</fpage>. <pub-id pub-id-type="doi">10.1016/j.chemolab.2020.104171</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>Z.</given-names></name><name><surname>Liu</surname><given-names>L.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Fei</surname><given-names>B.</given-names></name></person-group> (<year>2015</year>). <article-title>Superpixel-based segmentation for 3d prostate mr images</article-title>. <source>IEEE Trans. Med. Imaging</source>
<volume>35</volume>, <fpage>791</fpage>–<lpage>801</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2015.2496296</pub-id><?supplied-pmid 26540678?><pub-id pub-id-type="pmid">26540678</pub-id></mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tschandl</surname><given-names>P.</given-names></name><name><surname>Rosendahl</surname><given-names>C.</given-names></name><name><surname>Kittler</surname><given-names>H.</given-names></name></person-group> (<year>2018</year>). <article-title>The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</article-title>. <source>Sci. Data</source>
<volume>5</volume>:<fpage>180161</fpage>. <pub-id pub-id-type="doi">10.1038/sdata.2018.161</pub-id><?supplied-pmid 30106392?><pub-id pub-id-type="pmid">30106392</pub-id></mixed-citation>
    </ref>
    <ref id="B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>W.</given-names></name><name><surname>Yu</surname><given-names>K.</given-names></name><name><surname>Hugonot</surname><given-names>J.</given-names></name><name><surname>Fua</surname><given-names>P.</given-names></name><name><surname>Salzmann</surname><given-names>M.</given-names></name></person-group> (<year>2019a</year>). <article-title>“Recurrent U-Net for resource-constrained segmentation,”</article-title> in <source>Proceedings of the IEEE International Conference on Computer Vision</source>, <fpage>2142</fpage>–<lpage>2151</lpage>. <pub-id pub-id-type="doi">10.1109/ICCV.2019.00223</pub-id></mixed-citation>
    </ref>
    <ref id="B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Zhou</surname><given-names>Y.</given-names></name><name><surname>Shen</surname><given-names>W.</given-names></name><name><surname>Park</surname><given-names>S.</given-names></name><name><surname>Fishman</surname><given-names>E. K.</given-names></name><name><surname>Yuille</surname><given-names>A. L.</given-names></name></person-group> (<year>2019b</year>). <article-title>Abdominal multi-organ segmentation with organ-attention networks and statistical fusion</article-title>. <source>Med. Image Anal</source>. <volume>55</volume>, <fpage>88</fpage>–<lpage>102</lpage>. <pub-id pub-id-type="doi">10.1016/j.media.2019.04.005</pub-id><?supplied-pmid 31035060?><pub-id pub-id-type="pmid">31035060</pub-id></mixed-citation>
    </ref>
    <ref id="B51">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>X.</given-names></name><name><surname>Lian</surname><given-names>S.</given-names></name><name><surname>Luo</surname><given-names>Z.</given-names></name><name><surname>Li</surname><given-names>S.</given-names></name></person-group> (<year>2018</year>). <article-title>“Weighted RES-UNet for high-quality retina vessel segmentation,”</article-title> in <source>2018 9th International Conference on Information Technology in Medicine and Education (ITME)</source> (<publisher-loc>IEEE</publisher-loc>), <fpage>327</fpage>–<lpage>331</lpage>. <pub-id pub-id-type="doi">10.1109/ITME.2018.00080</pub-id></mixed-citation>
    </ref>
    <ref id="B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yap</surname><given-names>M. H.</given-names></name><name><surname>Pons</surname><given-names>G.</given-names></name><name><surname>Marti</surname><given-names>J.</given-names></name><name><surname>Ganau</surname><given-names>S.</given-names></name><name><surname>Sentis</surname><given-names>M.</given-names></name><name><surname>Zwiggelaar</surname><given-names>R.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Automated breast ultrasound lesions detection using convolutional neural networks</article-title>. <source>IEEE J. Biomed. Health Inform</source>. <volume>22</volume>, <fpage>1218</fpage>–<lpage>1226</lpage>. <pub-id pub-id-type="doi">10.1109/JBHI.2017.2731873</pub-id><?supplied-pmid 28796627?><pub-id pub-id-type="pmid">28796627</pub-id></mixed-citation>
    </ref>
    <ref id="B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Liu</surname><given-names>Q.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name></person-group> (<year>2018</year>). <article-title>Road extraction by deep residual U-Net</article-title>. <source>IEEE Geosci. Remote Sens. Lett</source>. <volume>15</volume>, <fpage>749</fpage>–<lpage>753</lpage>. <pub-id pub-id-type="doi">10.1109/LGRS.2018.2802944</pub-id></mixed-citation>
    </ref>
    <ref id="B54">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Z.</given-names></name><name><surname>Siddiquee</surname><given-names>M. M. R.</given-names></name><name><surname>Tajbakhsh</surname><given-names>N.</given-names></name><name><surname>Liang</surname><given-names>J.</given-names></name></person-group> (<year>2020</year>). <article-title>UNet++: redesigning skip connections to exploit multiscale features in image segmentation</article-title>. <source>IEEE Trans. Med. Imaging</source>
<volume>39</volume>, <fpage>1856</fpage>–<lpage>1867</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2019.2959609</pub-id><?supplied-pmid 31841402?><pub-id pub-id-type="pmid">31841402</pub-id></mixed-citation>
    </ref>
    <ref id="B55">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoph</surname><given-names>B.</given-names></name><name><surname>Vasudevan</surname><given-names>V.</given-names></name><name><surname>Shlens</surname><given-names>J.</given-names></name><name><surname>Le</surname><given-names>Q. V.</given-names></name></person-group> (<year>2018</year>). <article-title>“Learning transferable architectures for scalable image recognition,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>, <fpage>8697</fpage>–<lpage>8710</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2018.00907</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
