<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9925104</article-id>
    <article-id pub-id-type="pmid">36734596</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btad075</article-id>
    <article-id pub-id-type="publisher-id">btad075</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Gene Expression</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>scBGEDA: deep single-cell clustering analysis via a dual denoising autoencoder with bipartite graph ensemble clustering</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Yunhe</given-names>
        </name>
        <aff><institution>School of Artificial Intelligence, Hebei University of Technology</institution>, Tianjin, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yu</surname>
          <given-names>Zhuohan</given-names>
        </name>
        <aff><institution>School of Artificial Intelligence, Jilin University</institution>, Jilin, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Shaochuan</given-names>
        </name>
        <aff><institution>School of Artificial Intelligence, Jilin University</institution>, Jilin, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bian</surname>
          <given-names>Chuang</given-names>
        </name>
        <aff><institution>School of Artificial Intelligence, Jilin University</institution>, Jilin, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liang</surname>
          <given-names>Yanchun</given-names>
        </name>
        <aff><institution>Zhuhai Laboratory of Key Laboratory of Symbol Computation and Knowledge Engineering of Ministry of Education, Zhuhai College of Science and Technology</institution>, Zhuhai, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-6062-733X</contrib-id>
        <name>
          <surname>Wong</surname>
          <given-names>Ka-Chun</given-names>
        </name>
        <aff><institution>Department of Computer Science, City University of Hong Kong</institution>, Kowloon Tong, Hong Kong SAR</aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-8716-9823</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>Xiangtao</given-names>
        </name>
        <aff><institution>School of Artificial Intelligence, Jilin University</institution>, Jilin, <country country="CN">China</country></aff>
        <xref rid="btad075-cor1" ref-type="corresp"/>
        <!--lixt314@jlu.edu.cn-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Mathelier</surname>
          <given-names>Anthony</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btad075-cor1">To whom correspondence should be addressed. Email: <email>lixt314@jlu.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2023-02-03">
      <day>03</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>03</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <volume>39</volume>
    <issue>2</issue>
    <elocation-id>btad075</elocation-id>
    <history>
      <date date-type="received">
        <day>07</day>
        <month>6</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>08</day>
        <month>12</month>
        <year>2022</year>
      </date>
      <date date-type="editorial-decision">
        <day>31</day>
        <month>1</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>02</day>
        <month>2</month>
        <year>2023</year>
      </date>
      <date date-type="corrected-typeset">
        <day>13</day>
        <month>2</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btad075.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Single-cell RNA sequencing (scRNA-seq) is an increasingly popular technique for transcriptomic analysis of gene expression at the single-cell level. Cell-type clustering is the first crucial task in the analysis of scRNA-seq data that facilitates accurate identification of cell types and the study of the characteristics of their transcripts. Recently, several computational models based on a deep autoencoder and the ensemble clustering have been developed to analyze scRNA-seq data. However, current deep autoencoders are not sufficient to learn the latent representations of scRNA-seq data, and obtaining consensus partitions from these feature representations remains under-explored.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>To address this challenge, we propose a single-cell deep clustering model via a dual denoising autoencoder with bipartite graph ensemble clustering called scBGEDA, to identify specific cell populations in single-cell transcriptome profiles. First, a single-cell dual denoising autoencoder network is proposed to project the data into a compressed low-dimensional space and that can learn feature representation via explicit modeling of synergistic optimization of the zero-inflated negative binomial reconstruction loss and denoising reconstruction loss. Then, a bipartite graph ensemble clustering algorithm is designed to exploit the relationships between cells and the learned latent embedded space by means of a graph-based consensus function. Multiple comparison experiments were conducted on 20 scRNA-seq datasets from different sequencing platforms using a variety of clustering metrics. The experimental results indicated that scBGEDA outperforms other state-of-the-art methods on these datasets, and also demonstrated its scalability to large-scale scRNA-seq datasets. Moreover, scBGEDA was able to identify cell-type specific marker genes and provide functional genomic analysis by quantifying the influence of genes on cell clusters, bringing new insights into identifying cell types and characterizing the scRNA-seq data from different perspectives.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The source code of scBGEDA is available at <ext-link xlink:href="https://github.com/wangyh082/scBGEDA" ext-link-type="uri">https://github.com/wangyh082/scBGEDA</ext-link>. The software and the supporting data can be downloaded from <ext-link xlink:href="https://figshare.com/articles/software/scBGEDA/19657911" ext-link-type="uri">https://figshare.com/articles/software/scBGEDA/19657911</ext-link>.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>62206086</award-id>
        <award-id>62076109</award-id>
        <award-id>61972174</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Fundamental Research Funds for the Central Universities</institution>
            <institution-id institution-id-type="DOI">10.13039/501100012226</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="9"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>The cell is the basic unit of growth and development of an organism and has unique biological functions. The heterogeneity between cells in a cell population has isogenic properties, which can ascend from stochastic expression of genes, proteins and metabolites (<xref rid="btad075-B22" ref-type="bibr">Syed <italic toggle="yes">et al.</italic>, 2019</xref>). Conventional bulk RNA sequencing (RNA-seq) averages the transcriptional profiles of cells in a population, ignoring cell–cell heterogeneity in transcription (<xref rid="btad075-B4" ref-type="bibr">Ben-Dor <italic toggle="yes">et al.</italic>, 1999</xref>). The recent advances in single-cell RNA sequencing (scRNA-seq) technology allow measuring transcriptomes and understanding disease dysregulation at the single-cell resolution (<xref rid="btad075-B3" ref-type="bibr">Aviv <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btad075-B250" ref-type="bibr">Zhuohan <italic toggle="yes">et al.</italic>, 2023</xref>). However, scRNA-seq transcript expression profiles are particularly sparse due to low RNA capture rates, leading to spurious zero-count observations (<xref rid="btad075-B2" ref-type="bibr">Angerer <italic toggle="yes">et al.</italic>, 2017</xref>). Moreover, scRNA-seq data have high dimensionality and massive noise and often have very non-linear complex structures, which pose a major challenge for designing effective computational models.</p>
    <p>Annotation of cell types by unsupervised learning, called clustering, is one of the first and most important steps of scRNA-seq data analysis; however, these constraints of the original scRNA-seq data make the process tricky. Autoencoder is a deep neural network that learns data representation using an encoder and a decoder in an unsupervised way. It is worth noting that the autoencoder realizes non-linear dimensionality reduction by projecting high-dimensional data into a low dimension in the latent space and then reconstructing the denoised data at the same time. Recently, a succession of deep embedded clustering (DEC) algorithms inspired by autoencoder were developed; for instance, Li <italic toggle="yes">et al.</italic> proposed DESC, which optimizes the objective function iteratively to achieve the clustering result and combines a deep autoencoder network with the clustering loss (<xref rid="btad075-B14" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2020</xref>). Eraslan <italic toggle="yes">et al.</italic> developed a depth-counting autoencoder network named deep count autoencoder (DCA) to denoise the scRNA-seq data (<xref rid="btad075-B7" ref-type="bibr">Eraslan <italic toggle="yes">et al.</italic>, 2019</xref>). Further, Tian <italic toggle="yes">et al.</italic> designed single-cell model-based deep embedded clustering method (scDeepCluster) to cluster scRNA-seq data by combining DCA and DEC to conduct the dimension reduction and clustering process, respectively (<xref rid="btad075-B24" ref-type="bibr">Tian <italic toggle="yes">et al.</italic>, 2019</xref>). In particular, DCA and scDeepCluster apply a zero-inflated negative binomial (ZINB) model to capture the non-linear structure of scRNA-seq data. Chen <italic toggle="yes">et al.</italic> investigated a single-cell zero-inflated deep soft K-means (scziDesk) model to further exploit the clustering performance of ZINB using a soft <italic toggle="yes">K-means</italic> loss (<xref rid="btad075-B6" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2020</xref>). Most of those algorithms employ <italic toggle="yes">K-means</italic> clustering to generate the initial center points for KL loss to optimize the cluster results. However, it is hard to believe that such a single pattern can always perform well on all the scRNA-seq datasets.</p>
    <p>Recently, emerging ensemble clustering methods have been demonstrated to naturally capture multiple scenarios to produce a consensus clustering result based on the consensus function; for instance, Kiselev <italic toggle="yes">et al.</italic> developed single-cell consensus clustering (SC3) algorithm to integrate basic clusterings into the final clustering solution by a hierarchical clustering (<xref rid="btad075-B13" ref-type="bibr">Kiselev <italic toggle="yes">et al.</italic>, 2017</xref>). Gan <italic toggle="yes">et al.</italic> proposed a consensus clustering framework using an ensemble strategy to fuze multiple basic clustering results (<xref rid="btad075-B9" ref-type="bibr">Gan <italic toggle="yes">et al.</italic>, 2018</xref>). Yang <italic toggle="yes">et al.</italic> proposed a SAFE-clustering method that combines solutions from four different methods with three hypergraph-based partitioning algorithms (<xref rid="btad075-B29" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2019</xref>). Huh <italic toggle="yes">et al.</italic> presented a SAME-clustering which uses clustering results from different methods and chooses a subset of maximum diversity to generate an ensemble solution (<xref rid="btad075-B12" ref-type="bibr">Huh <italic toggle="yes">et al.</italic>, 2020</xref>). Motivated by the above observations, ensemble clustering of compressed features obtained from deep autoencoders could be a good alternative for analyzing single-cell sequencing data, and even though there may be some cell types that are not necessarily completely precise, ensemble clustering methods tend to have advantages over each individual method.</p>
    <p>In our study, we propose a deep single-cell clustering model via a dual denoising autoencoder with bipartite graph ensemble clustering, called scBGEDA, to perform clustering of scRNA-seq data. The scBGEDA pipeline consists of three core modules. The first module preprocesses the high-dimensional sparse scRNA-seq data into compressed low-dimensional data. The second module is a single-cell denoising autoencoder based on a dual reconstruction loss that characterizes the scRNA-seq data by learning the robust feature representations. In particular, by simultaneously optimizing the dual reconstruction loss and mean square error (MSE) loss, scBGEDA jointly improves the feature representation information of each cell preserved in an end-to-end manner. The third module comprises a bipartite graph ensemble clustering method used on the learned latent space to obtain the optimal clustering result. By developing a dual denosing autoencoder to capture the robust latent representations of scRNA-seq data, our scBGEDA algorithm encodes the scRNA-seq data in a discriminative representation, on which two decoders are trained to reconstruct the scRNA-seq data. Furthermore, bipartite graph ensemble clustering is proposed to address the clustering process, which is equivalent to solve the generalized eigen-problem to refine the clustering result. Multiple comparisons were conducted on 20 real scRNA-seq datasets from diverse sequencing platforms. The experimental results demonstrated the superior performance of the proposed algorithm, scBGEDA, compared with other clustering methods in several perspectives. We also carried out an extensive analysis on a large-scale scRNA-seq dataset to demonstrate that our algorithm is capable of dealing with large-scale data. Furthermore, functional gene analyses were carried out to further validate the effectiveness and interpretability of the scBGEDA model. The results indicated that scBGEDA may be adopted as a promising model for clustering scRNA-seq data.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Methodology overview of scBGEDA</title>
      <p>In our study, we propose scBGEDA for effective exploration of cell and gene representation in scRNA-seq data. The framework of scBGEDA has three components, including a data processing step to model the high-dimensional scRNA-seq data, a single-cell dual denoising autoencoder network and a bipartite graph ensemble clustering algorithm (<xref rid="btad075-F1" ref-type="fig">Fig. 1</xref>). We propose a single-cell dual denoising autoencoder that incorporates the ZINB model into the denoising autoencoder network, to better capture the structure of the scRNA-seq data. The encoder of the second module intakes the preprocessed gene expression matrix after data filtering and normalization. The latent representation of the scRNA-seq data is reconstructed through the master decoder and follower decoder. Then, we design a bipartite graph ensemble clustering method in scBGEDA based on the bipartite graph and transfer cut approach inspired from ensemble clustering (<xref rid="btad075-B11" ref-type="bibr">Huang <italic toggle="yes">et al.</italic>, 2019</xref>). It comprises two phases, in the first, a set of basic clusterings are generated by the <italic toggle="yes">K-means</italic> clustering method; in the second, we produce a bipartite graph adopting both samples and clusters as the graph nodes to perform the consensus function by incorporating the multiple basic clusterings. Finally, the consensus clustering result is provided by solving the generalized eigen-problem.</p>
      <fig position="float" id="btad075-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>The overall workflow of the scBGEDA pipeline, comprising three components: the data preprocessing mechanism, the single-cell dual denoising autoencoder network and the bipartite graph ensemble clustering method</p>
        </caption>
        <graphic xlink:href="btad075f1" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.2 Data collection and preprocessing</title>
      <p>We collected 20 real scRNA-seq datasets from different species and organs available from various sequencing platforms (Drop-seq, 10x, inDrop, CEL-seq2 and Smart-seq2). Their characteristics are detailed in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>, showing the source organ, the platform, the number of cell types, the number of cells, the zero percentage and the reference. Specifically, the Quake_10x_Trachea dataset is a single-cell transcriptome of the trachea, including 11 269 cells of 5 groups and a zero observation rate of 93.66%; the Tosches_turtle dataset contains 18 664 cells from the Drop-seq platform with 15 cell types and a zero observation rate of 90.83%; the Bach dataset from the 10x genomics platform has 23 184 cells, 8 cell types and a zero observation rate of 88.04%; and the Chen dataset is from Drop-seq with 12 089 cells, 46 cell types and a zero observation rate of 93.74%. In our study, these four scRNA-seq datasets were marked as large-scale datasets while the remaining sixteen scRNA-seq datasets have no more than 10 000 cells, and were marked as small-scale datasets.</p>
      <p>ScBGEDA adopts the scRNA-seq gene expression matrix <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">Y</mml:mi></mml:math></inline-formula> with <italic toggle="yes">n</italic> samples as the input. Since there is a large amount of technical and biological noise in the stochastic single-cell gene expression pattern, we first filter the genes that have almost no expression value. Then, we normalize the matrix by multiplying the division result between each row and each row’s sum by the total expression values’ medians of all cells, and transform them using the nature log scale in a continuous form. Afterwards, to further discard the genes having low identification and descriptive information, the top <italic toggle="yes">m</italic> highly variable genes are chosen by the filter_genes_dispersion function in single-cell analysis in python (Scanpy) package (<xref rid="btad075-B27" ref-type="bibr">Wolf <italic toggle="yes">et al.</italic>, 2018</xref>). Finally, the gene expression data are transferred by <italic toggle="yes">Z</italic>-score normalization with zero mean and unit variance. We record that normalized scRNA-seq matrix as an <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> scRNA-seq data <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>}</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> and its corresponding original count matrix as <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> for data modeling.</p>
    </sec>
    <sec>
      <title>2.3 Single-cell dual denoising autoencoder network</title>
      <p>The single-cell dual denoising autoencoder network is based on the ZINB model to learn the latent feature representation of the scRNA-seq data. It captures the representation embedding of the scRNA-seq expression matrix by stacked layers in the encoder and decoder. The encoder is used to map the scRNA-seq data matrix <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi></mml:math></inline-formula> into the low-dimensional latent feature representation <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi></mml:math></inline-formula>, extracting the unique information from the inputs. The dimension of <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi></mml:math></inline-formula> is much smaller than that of <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi></mml:math></inline-formula> to avoid the ‘curse of dimensionality’ (<xref rid="btad075-B28" ref-type="bibr">Xie <italic toggle="yes">et al.</italic>, 2016</xref>). To prevent the overfitting phenomenon in deep learning, the input scRNA-seq data are corrupted with the random Gaussian noise, then, the autoencoder is constructed with fully connected layers. Therefore, the mapping function of encoder can be defined as follows:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">X</mml:mi></mml:mrow><mml:mrow><mml:mtext>corrupt</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo>+</mml:mo><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>W</mml:mi></mml:msup><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">X</mml:mi></mml:mrow><mml:mrow><mml:mtext>corrupt</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi></mml:math></inline-formula> is the input scRNA-seq expression matrix, <italic toggle="yes">e</italic> is the random Gaussian noise that can be incorporated into each layer of the encoder, <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">X</mml:mi></mml:mrow><mml:mrow><mml:mtext>corrupt</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the corrupted data of the input, <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>W</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the encoder function, <italic toggle="yes">W</italic> is the learnable weights of the function and <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi></mml:math></inline-formula> is the output feature representation vector of the encoder.</p>
      <p>The decoder takes the latent feature representation <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi></mml:math></inline-formula> as the input, aiming to reconstruct the input from the low-dimensional feature representation <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi></mml:math></inline-formula>. Due to the inevitable trade-off between reconstruction and clustering tasks, reconstruction loss is commonly the secondary optimum for clustering. Generally, the reconstruction loss is mainly determined by the distribution of the latent space and the reconstruction capacity of the decoder. However, the reconstruction capacity of the decoder network is unnecessary in the clustering procedure. To generate more discriminative features for the cluster assignments of scRNA-seq data, we construct a follower decoder to approximate the master decoder based on ZINB. The decoder of the dual denoising autoencoder network can be defined as follows:
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>2</mml:mn><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>2</mml:mn><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> are the functions of the master decoder and follower decoder, <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>2</mml:mn><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> represent the weight parameter matrices and <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are the reconstruction of inputs for the two decoders, respectively. To capture the characteristics of scRNA-seq data, the master decoder adopts ZINB autoencoder model-based loss to characterize the raw count data. Specifically, ZINB is used for mathematical modeling of dropout events in scRNA-seq data based on a combination of zero component and NB distribution, which can be defined as follows:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mtext>ZINB</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>∣</mml:mo><mml:mo>π</mml:mo><mml:mo>,</mml:mo><mml:mo>μ</mml:mo><mml:mo>,</mml:mo><mml:mo>θ</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>π</mml:mo><mml:msub><mml:mrow><mml:mo>δ</mml:mo></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo>π</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:mtext>NB</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>∣</mml:mo><mml:mo>μ</mml:mo><mml:mo>,</mml:mo><mml:mo>θ</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:mtext>NB</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo>∣</mml:mo><mml:mo>μ</mml:mo><mml:mo>,</mml:mo><mml:mo>θ</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>Γ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mo>θ</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>Γ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>Γ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>θ</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mo>θ</mml:mo><mml:mrow><mml:mo>θ</mml:mo><mml:mo>+</mml:mo><mml:mo>μ</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>θ</mml:mo></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mo>μ</mml:mo><mml:mrow><mml:mo>θ</mml:mo><mml:mo>+</mml:mo><mml:mo>μ</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is the original raw count matrix and <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mo>π</mml:mo></mml:math></inline-formula> is the probability of dropout events and <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mo>μ</mml:mo></mml:math></inline-formula> and <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mo>θ</mml:mo></mml:math></inline-formula> are the mean and dispersion in the negative binomial distribution, respectively, and are the parameters to be estimated. To model the ZINB distribution, the decoder network has three output layers to compute the three sets of parameters. The estimated parameters can be defined as follows:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mo>Π</mml:mo><mml:mo>=</mml:mo><mml:mtext>sigmoid</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>π</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>μ</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mo>Θ</mml:mo><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mo>Π</mml:mo></mml:math></inline-formula>, <italic toggle="yes">M</italic> and <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mo>Θ</mml:mo></mml:math></inline-formula> denote the matrix form of the estimations of <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mo>π</mml:mo></mml:math></inline-formula>, <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mo>μ</mml:mo></mml:math></inline-formula> and <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mo>θ</mml:mo></mml:math></inline-formula>. Since the mean and dispersion parameters are non-negative values, we choose the exponential activation function for them. In terms of the additional coefficient <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mo>π</mml:mo></mml:math></inline-formula>, the suitable activation function for it is sigmoid because the interval of <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mo>π</mml:mo></mml:math></inline-formula> is between 0 and 1. The reconstruction loss function of the master decoder takes the negative log of ZINB likelihood, which can be expressed as follows:
<disp-formula id="E6"><label>(6)</label><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>π</mml:mo><mml:mo>,</mml:mo><mml:mo>μ</mml:mo><mml:mo>,</mml:mo><mml:mo>θ</mml:mo><mml:mo>∣</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>ZINB</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>∣</mml:mo><mml:mo>π</mml:mo><mml:mo>,</mml:mo><mml:mo>μ</mml:mo><mml:mo>,</mml:mo><mml:mo>θ</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>For the follower decoder, it is proposed to approximate the master decoder by transferring the latent representation <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi></mml:math></inline-formula> to reconstruct the mean <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mo>μ</mml:mo></mml:math></inline-formula> parameters in the ZINB model-based loss. In this manner, the follower decoder makes this dual denoising autoencoder model robust by exclusion rather than inclusion. Therefore, the loss of the follower decoder can be written as:
<disp-formula id="E7"><label>(7)</label><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>2</mml:mn><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>μ</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mo>μ</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:mrow><mml:mo>‖</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the Frobenius norm. It is the conventional MSE loss function and takes the ReLU function as the activation function.</p>
      <p>To guarantee the quality of the feature representations in the latent space, the MSE loss is added to the original reconstruction loss, producing a dual reconstruction loss to learn the decoder network. The learning process of the dual denoising autoencoder aims to train the model by minimizing the dual objective loss function, which can be defined as follows:
<disp-formula id="E8"><label>(8)</label><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mi>h</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>π</mml:mo><mml:mo>,</mml:mo><mml:mo>μ</mml:mo><mml:mo>,</mml:mo><mml:mo>θ</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi><mml:mo>∣</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>λ</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>γ</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>min</mml:mi><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mi>h</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>π</mml:mo><mml:mo>,</mml:mo><mml:mo>μ</mml:mo><mml:mo>,</mml:mo><mml:mo>θ</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi><mml:mo>∣</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>min</mml:mi><mml:mo>λ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>ZINB</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script" class="calligraphy">X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>∣</mml:mo><mml:mo>π</mml:mo><mml:mo>,</mml:mo><mml:mo>μ</mml:mo><mml:mo>,</mml:mo><mml:mo>θ</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mo>γ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mo>μ</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mn>2</mml:mn><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE35"><mml:math id="IM35" display="inline" overflow="scroll"><mml:mo>γ</mml:mo></mml:math></inline-formula> and <inline-formula id="IE36"><mml:math id="IM36" display="inline" overflow="scroll"><mml:mo>λ</mml:mo></mml:math></inline-formula> are the hyperparameters to control the relative impact of <inline-formula id="IE37"><mml:math id="IM37" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE38"><mml:math id="IM38" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
    </sec>
    <sec>
      <title>2.4 Basic clustering generation</title>
      <p>After obtaining the scRNA-seq data representation <inline-formula id="IE39"><mml:math id="IM39" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with <italic toggle="yes">n</italic> samples from the latent space, our proposed model scBGEDA intends to exploit the relationship between the samples in <inline-formula id="IE40"><mml:math id="IM40" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi></mml:math></inline-formula> and identify the cell types of scRNA-seq data by the bipartite graph ensemble clustering. At first, to ensure a fast running time for clustering the scRNA-seq datasets, we adopt the <italic toggle="yes">K-means</italic> clustering algorithm to produce a set of basic clusterings, which can be represented as follows:
<disp-formula id="E9"><label>(9)</label><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:mo>Ψ</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>ψ</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mo>ψ</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mo>ψ</mml:mo></mml:mrow><mml:mi>K</mml:mi></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE41"><mml:math id="IM41" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>ψ</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> represents the <italic toggle="yes">i</italic>th basic clustering. We note that the number of clusters <inline-formula id="IE42"><mml:math id="IM42" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> in <inline-formula id="IE43"><mml:math id="IM43" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>ψ</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is an integer randomly chosen from <inline-formula id="IE44"><mml:math id="IM44" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE45"><mml:math id="IM45" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">min</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE46"><mml:math id="IM46" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">min</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE47"><mml:math id="IM47" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represent the lower bound and upper bound of the cluster number, respectively.</p>
    </sec>
    <sec>
      <title>2.5 Bipartite graph generation</title>
      <p>To achieve a robust consensus clustering result, we adopt both samples and clusters as graph nodes <xref rid="btad075-B11" ref-type="bibr">Huang <italic toggle="yes">et al.</italic> (2019)</xref>, a bipartite graph <italic toggle="yes">G</italic> can be defined as follows:
<disp-formula id="E10"><label>(10)</label><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi><mml:mo>,</mml:mo><mml:mo>ϕ</mml:mo><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>}</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE48"><mml:math id="IM48" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi></mml:math></inline-formula> is the feature representation; <inline-formula id="IE49"><mml:math id="IM49" display="inline" overflow="scroll"><mml:mo>ϕ</mml:mo></mml:math></inline-formula> is the cluster set, and can expressed as follows:
<disp-formula id="E11"><label>(11)</label><mml:math id="M11" display="block" overflow="scroll"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE50"><mml:math id="IM50" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script" class="calligraphy">C</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the <italic toggle="yes">i</italic>th cluster, <inline-formula id="IE51"><mml:math id="IM51" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the number of clusters in the basic clustering <inline-formula id="IE52"><mml:math id="IM52" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>ψ</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE53"><mml:math id="IM53" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the total number of clusters in <inline-formula id="IE54"><mml:math id="IM54" display="inline" overflow="scroll"><mml:mo>Ψ</mml:mo></mml:math></inline-formula>. Moreover, <italic toggle="yes">B</italic> stands for the cross-affinity matrix that reflects the relationship between <inline-formula id="IE55"><mml:math id="IM55" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi></mml:math></inline-formula> and <inline-formula id="IE56"><mml:math id="IM56" display="inline" overflow="scroll"><mml:mo>ϕ</mml:mo></mml:math></inline-formula>, defined as follows:
<disp-formula id="E12"><label>(12)</label><mml:math id="M12" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo> </mml:mo><mml:mtext>if</mml:mtext><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo> </mml:mo><mml:mtext>otherwise</mml:mtext><mml:mo>.</mml:mo><mml:mo> </mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p>It conveys that there is an edge between two nodes if, and only if, one node is a sample and the other is the cluster that contains that sample.</p>
    </sec>
    <sec>
      <title>2.6 Bipartite graph ensemble clustering</title>
      <p>After bipartite graph generation, we observe that it is equivalent to solve the generalized eigen-problem (<xref rid="btad075-B21" ref-type="bibr">Shi and Malik, 2000</xref>) in the spectral clustering, which can be denoted as:
<disp-formula id="E13"><label>(13)</label><mml:math id="M13" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mo>γ</mml:mo><mml:mo>τ</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo>τ</mml:mo><mml:mo>−</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msup><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>B</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">L</italic> is the Laplacian matrix, <inline-formula id="IE57"><mml:math id="IM57" display="inline" overflow="scroll"><mml:mrow><mml:mo>τ</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the degree matrix and <italic toggle="yes">E</italic> is the full affinity matrix of <italic toggle="yes">G</italic>, <inline-formula id="IE58"><mml:math id="IM58" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the number of nodes in <italic toggle="yes">G</italic>, since <inline-formula id="IE59"><mml:math id="IM59" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi><mml:mo>∪</mml:mo><mml:mo>ϕ</mml:mo></mml:mrow></mml:math></inline-formula> are the nodes in <italic toggle="yes">G</italic>.</p>
      <p>However, taking <italic toggle="yes">G</italic> as a general graph is not computationally suitable for large-scale datasets. It has been demonstrated that solving the eigen-problem on graph <italic toggle="yes">G</italic> is equivalent to solve it on a much smaller graph (<xref rid="btad075-B15" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2012</xref>). Therefore, to reduce the complexity to exploit the bipartite structure, we employ the transfer cut (<xref rid="btad075-B15" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2012</xref>) to efficiently partition the graph <italic toggle="yes">G</italic> by transferring the eigen-problem with <italic toggle="yes">G</italic> to the eigen-problem with a smaller graph <inline-formula id="IE60"><mml:math id="IM60" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (with <inline-formula id="IE61"><mml:math id="IM61" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> nodes). In particular, <inline-formula id="IE62"><mml:math id="IM62" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is conducted as <inline-formula id="IE63"><mml:math id="IM63" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which consists of the node set <inline-formula id="IE64"><mml:math id="IM64" display="inline" overflow="scroll"><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:math></inline-formula> and the affinity matrix <inline-formula id="IE65"><mml:math id="IM65" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:msubsup><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula> (<inline-formula id="IE66"><mml:math id="IM66" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi mathvariant="script" class="calligraphy">Z</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is a diagonal matrix). Then, the eigen-problem on <inline-formula id="IE67"><mml:math id="IM67" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> can be formulated as follows:
<disp-formula id="E14"><label>(14)</label><mml:math id="M14" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:msub><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mo>λ</mml:mo><mml:msub><mml:mrow><mml:mo>τ</mml:mo></mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:msub><mml:mi>v</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE68"><mml:math id="IM68" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>τ</mml:mo></mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the Laplacian for <inline-formula id="IE69"><mml:math id="IM69" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE70"><mml:math id="IM70" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>τ</mml:mo></mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the degree matrix for <inline-formula id="IE71"><mml:math id="IM71" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. According to the first <italic toggle="yes">k</italic> eigenvectors <inline-formula id="IE72"><mml:math id="IM72" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> for <inline-formula id="IE73"><mml:math id="IM73" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mi mathvariant="script" class="calligraphy">R</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the first <italic toggle="yes">k</italic> eigenvectors <inline-formula id="IE74"><mml:math id="IM74" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> for <italic toggle="yes">G</italic> can be calculated (<xref rid="btad075-B11" ref-type="bibr">Huang <italic toggle="yes">et al.</italic>, 2019</xref>). Finally, the consensus clustering result is provided using <italic toggle="yes">K-means</italic> clustering on the new matrix through stacking <inline-formula id="IE75"><mml:math id="IM75" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>. We calculate the time complexity of scBGEDA in <xref rid="sup1" ref-type="supplementary-material">Supplementary Section S1</xref>.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Model parameter settings</title>
      <p>In our study, we trained each model, obtained the clustering results of the scRNA-seq data to evaluate the competitive methods. In scBGEDA, 2000 highly variable genes (<inline-formula id="IE76"><mml:math id="IM76" display="inline" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>2000</mml:mn></mml:mrow></mml:math></inline-formula>) were picked as input of the single-cell dual denoising autoencoder network. The size of the hidden layers of the encoder network was 256 and 32. The setting of the decoder network was the opposite of that of the encoder. Hence, the size of the bottleneck layer was 32, indicating that the dimension of the latent representation was 32. During the training process, we adopted the Adam optimizer with a learning rate of 0.0001 to update the autoencoder and set the mini batch size to 256. Further, the default values of <inline-formula id="IE77"><mml:math id="IM77" display="inline" overflow="scroll"><mml:mo>λ</mml:mo></mml:math></inline-formula> and <inline-formula id="IE78"><mml:math id="IM78" display="inline" overflow="scroll"><mml:mo>γ</mml:mo></mml:math></inline-formula> were 1 and 0.00001 in the loss function of the model. Finally, the number of basic clusterings was fixed to 100 (<inline-formula id="IE79"><mml:math id="IM79" display="inline" overflow="scroll"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>), and the upper and lower bounds of the number of clusters were set to 2 and 60 (<inline-formula id="IE80"><mml:math id="IM80" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">min</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE81"><mml:math id="IM81" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula>), respectively. The hyperparameter selection is discussed in <xref rid="sup1" ref-type="supplementary-material">Supplementary Sections S2–S4</xref>.</p>
    </sec>
    <sec>
      <title>3.2 Related methods from the literature</title>
      <p>Multiple existing computational methods were chosen for a comparative analysis of scRNA-seq data. First, we compared seven scRNA-seq data clustering algorithms to our proposed algorithm scBGEDA including hyper-fast with accurate processing via ensemble random projection (SHARP) (<xref rid="btad075-B25" ref-type="bibr">Wan <italic toggle="yes">et al.</italic>, 2020</xref>), clustering through imputation and dimensionality reduction (CIDR) (<xref rid="btad075-B16" ref-type="bibr">Lin <italic toggle="yes">et al.</italic>, 2017</xref>), semi-soft clustering with pure cells (SOUP) (<xref rid="btad075-B32" ref-type="bibr">Zhu <italic toggle="yes">et al.</italic>, 2019</xref>), spatial reconstruction model (Seurat) (<xref rid="btad075-B20" ref-type="bibr">Satija <italic toggle="yes">et al.</italic>, 2015</xref>), SC3 (<xref rid="btad075-B13" ref-type="bibr">Kiselev <italic toggle="yes">et al.</italic>, 2017</xref>), Scanpy (<xref rid="btad075-B27" ref-type="bibr">Wolf <italic toggle="yes">et al.</italic>, 2018</xref>) and principal components analysis (PCA) (<xref rid="btad075-B24" ref-type="bibr">Tian <italic toggle="yes">et al.</italic>, 2019</xref>). Then, we compared scBGEDA with four deep learning-based models including DCA (<xref rid="btad075-B7" ref-type="bibr">Eraslan <italic toggle="yes">et al.</italic>, 2019</xref>), scDeepCluster (<xref rid="btad075-B24" ref-type="bibr">Tian <italic toggle="yes">et al.</italic>, 2019</xref>), scziDesk (<xref rid="btad075-B6" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2020</xref>) and an unsupervised deep embedding algorithm (DESC) (<xref rid="btad075-B14" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2020</xref>). The clustering evaluation metrics including NMI, ARI and two biological metrics (ASW and cLISI) are detailed in <xref rid="sup1" ref-type="supplementary-material">Supplementary Section S5</xref>. In addition, we have added the experiment to optimize the hyperparameters for those four deep learning-based competitors in a similar way to our study in <xref rid="sup1" ref-type="supplementary-material">Supplementary Section S6</xref> and their hyperparameter optimizations are summarized in <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S3–S11</xref>.</p>
    </sec>
    <sec>
      <title>3.3 Evaluations on real data</title>
      <p>To demonstrate the effectiveness of scBGEDA, we used the 11 state-of-the-art clustering algorithms described above to compare to scBGEDA clustering on 20 real scRNA-seq datasets. To ensure the reliability of the clustering results for each method, we ran all methods 10 times under 10 random seeds, including 1111, 2222, … , 9999 and 10 000. After obtaining 10 ARI and NMI values, we computed average values to estimate the performance of each method. The experimental results are summarized in <xref rid="btad075-F2" ref-type="fig">Figure 2A and B</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S14–S17</xref> measured by NMI, ARI, ASW and cLISI. As observed, scBGEDA provides the highest average NMI and ARI values of all the clustering methods. We also show a dot plot in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S1A</xref>, where the scatter size represents the score rank of the methods and the color represents the ARI level score value. The results of the NMI comparison (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S1B</xref>) are almost identical to those of the ARI comparison. It can be observed that scBGEDA is orange or red with the biggest scatter in most datasets, always ranking in the top 3 of the 12 methods, elaborating the effectiveness of our proposed algorithm.</p>
      <fig position="float" id="btad075-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>Real scRNA-seq data analysis results. (<bold>A and B</bold>) Box plots of ARI and NMI values on the 20 real scRNA-seq datasets with different clustering methods, respectively. The <italic toggle="yes">X</italic>-axis denotes the method and the <italic toggle="yes">Y</italic>-axis the ARI or NMI value. (<bold>C</bold>) Clustering performance comparison of the different clustering algorithms on the 20 real scRNA-seq datasets measured by ARI, the <italic toggle="yes">X</italic>-axis denotes the method and the <italic toggle="yes">Y</italic>-axis the ARI value. (<bold>D</bold>) 2D-visualization of the feature representations for five scRNA-seq datasets, Adam, Klein, Muraro, QS_Trachea and Qx_Limb_Muscle, learned by scBGEDA and four other deep learning-based algorithms. Each color in the cell-type panel on the outermost right side denotes a specific cell type</p>
        </caption>
        <graphic xlink:href="btad075f2" position="float"/>
      </fig>
      <p>In <xref rid="btad075-F2" ref-type="fig">Figure 2C</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S15</xref>, the ARI performance of each method on the 20 real datasets indicates that scBGEDA and scziDesk produce ARI values &gt;0.6 on most scRNA-seq datasets. On Bach, Chen, Plasschaert, Qx_Spleen, Qx_Trachea and Wang_Lung datasets, scziDesk achieves better ARI results than scBGEDA. However, for the all other datasets, our proposed model scBGEDA is superior to scziDesk, with a 19% better ARI value on the large-scale scRNA-seq dataset QS_Trachea. DESC, scDeepCluster and DCA are surpassed by the other methods on only one scRNA-seq dataset, while SHARP performs best among all the methods on two scRNA-seq datasets. In addition, PCA has the lowest ARI value, even lower than 0.1, on 5 out of 20 scRNA-seq datasets. In terms of NMI values (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S1B</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S3</xref>), our proposed scBGEDA outperformed the other methods on 11 out of the 20 scRNA-seq datasets. Of note, compared to the other deep-learning models (scziDesk, DCA, DESC and scDeepCluster), scBGEDA obtains the best clustering performance on 11 out of the 20 datasets, demonstrating that scBGEDA obtains a more discriminative latent space. Moreover, to assess the variability of ARI and NMI values for significant differences, we calculate the Wilcoxon test to test the significant differences for those datasets. The Wilcoxon analysis results on those 20 scRNA-seq datasets are summarized in <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S18 and S19</xref>. From <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S18</xref>, we find that for Adam, Klein, Muraro, QS_Diaphragm, QS_Heart, QS_Limb_Muscle, QS_Trachea, Qx_Bladder, Qx_Limb_Muscle and Romanov, scBGEDA performs better than other algorithms in terms of NMI, with significant differences between scBGEDA and the other different algorithms (<inline-formula id="IE82"><mml:math id="IM82" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo></mml:mrow></mml:math></inline-formula>0.05). For Bach and Qx_Spleen, scziDesk outperforms other algorithms with significant differences, while for Tosches_turtle, there is significant difference between DESC and the other compared algorithms. We find that scBGEDA was able to significantly improve upon other methods with a rate of 0.65; while other methods were significantly better with a frequency of 0.35 (using the parameters optimized for the 20 datasets). From <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S19</xref>, we observe that there are significant differences for ARI values between scBGEDA and the other clustering algorithms on nine scRNA-seq datasets (<inline-formula id="IE83"><mml:math id="IM83" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo></mml:mrow></mml:math></inline-formula>0.05), including Adam, Muraro, QS_Diaphragm, QS_Heart, QS_Limb_Muscle, QS_Trachea, Qx_Bladder, Qx_Limb_Muscle and Romanov. It demonstrates the superiority of the proposed algorithm scBGEDA.</p>
      <p>To compare intuitively the discrimination ability of scBGEDA to the four other deep learning-based algorithms, two-dimensional visualizations were plotted on five scRNA-seq datasets, including Adam, Klein, Muraro, QS_Trachea and Qx_Limb_Muscle. First, we obtained the feature representation of the scRNA-seq data from the latent space with 32 dimensions. Then, we applied the Uniform Manifold Approximation and Projection (UMAP) dimension reduction method to visualize the embedded data in a 2D plane using the default parameters. <xref rid="btad075-F2" ref-type="fig">Figure 2D</xref> summarizes the visual results using the five scRNA-seq datasets. From the figure, we see that scBGEDA clustering results in almost no overlap between cell types, indicating that scBGEDA clearly distinguishes the cell groups in a 2D plane for both simple and complex scRNA-seq datasets. The other clustering methods, especially DCA and DESC, fail to partition cells into correct cell clusters, indicating that scBGEDA is superior to other scRNA-seq clustering methods in separating similar cells. Moreover, we also reveal that our proposed method achieves a more discriminative latent representation to separate cells in a visual perspective. In summary, from different angles, we observed that the proposed scBGEDA presents competitive clustering performance compared with other single-cell clustering algorithms on simpler and more complex scRNA-seq datasets of various cell types.</p>
    </sec>
    <sec>
      <title>3.4 Effects of different numbers of highly variable genes on scBGEDA</title>
      <p>For this experiment, to test the effect of input number of highly variable genes, several highly variable genes were set as input features for the dual denoising autoencoder network in scBGEDA. Indeed, different numbers of highly variable genes could have dissimilar effects on the clustering performance of scBGEDA. Taking too many genes as input features in the model could lead to a slow running speed and a high memory requirement. Using only a few genes may lead to multiple informative genes being dropped and the remaining genes not covering all the dataset, resulting in low-quality clustering. To investigate the effect of different numbers of highly variable genes (<italic toggle="yes">m</italic>), we varied them as within <inline-formula id="IE84"><mml:math id="IM84" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>500</mml:mn><mml:mo>,</mml:mo><mml:mn>1000</mml:mn><mml:mo>,</mml:mo><mml:mn>2000</mml:mn><mml:mo>,</mml:mo><mml:mn>3000</mml:mn><mml:mo>,</mml:mo><mml:mn>4000</mml:mn><mml:mo>,</mml:mo><mml:mn>5000</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and tested this on the 20 real scRNA-seq datasets. The experimental results expressed as ARI values are summarized in <xref rid="btad075-F3" ref-type="fig">Figure 3A</xref>. We observe that the model with <inline-formula id="IE85"><mml:math id="IM85" display="inline" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>2000</mml:mn></mml:mrow></mml:math></inline-formula> is superior to all other models <inline-formula id="IE86"><mml:math id="IM86" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>500</mml:mn><mml:mo>,</mml:mo><mml:mn>1000</mml:mn><mml:mo>,</mml:mo><mml:mn>3000</mml:mn><mml:mo>,</mml:mo><mml:mn>4000</mml:mn><mml:mo>,</mml:mo><mml:mn>5000</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> on 16, 14, 9, 9 and 10 scRNA-seq datasets, respectively. Specifically, the model with <inline-formula id="IE87"><mml:math id="IM87" display="inline" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>2000</mml:mn></mml:mrow></mml:math></inline-formula> achieves the best clustering results on six scRNA-seq datasets, Klein, Bach, Qx_Bladder, Qx_Limb_Muscle, QS_Trachea and Romanov and while the other models provide best clustering performances on at most three scRNA-seq datasets. Therefore, <inline-formula id="IE88"><mml:math id="IM88" display="inline" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>2000</mml:mn></mml:mrow></mml:math></inline-formula> was chosen for the scBGEDA model, which is also consistent with the analysis of the number of highly variable genes in scBGEDA.</p>
      <fig position="float" id="btad075-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>(<bold>A</bold>) Bar plot of ARI values measuring scBGEDA with different numbers of highly variable genes. (<bold>B</bold>) Clustering result comparison of scBGEDA with or without MSE loss measured by ARI</p>
        </caption>
        <graphic xlink:href="btad075f3" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.5 Effects of MSE loss</title>
      <p>In our proposed scBGEDA, the dual denoising autoencoder network adopts a double loss function to learn the latent representation, which synergistically optimizes ZINB reconstruction loss and denoising MSE loss. To explore the importance of MSE loss on clustering performance, we compared scBGEDA with and without MSE loss on the 20 real scRNA-seq datasets by ARI metric. We summarize the clustering results in <xref rid="btad075-F3" ref-type="fig">Figure 3B</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S20</xref>. From the figure, we see that complete scBGEDA obtains a better clustering performance than scBGEDA without MSE loss on most datasets, showing that a dual denoising autoencoder structure with dual loss function often (but not always) improves performance. In summary, we conclude that the MSE loss in the autoencoder brings a positive effect on clustering performance in scBGEDA.</p>
    </sec>
    <sec>
      <title>3.6 Running time comparison of scBGEDA with other deep-learning methods</title>
      <p>The running time of the proposed scBGEDA was investigated compared with four deep learning-based algorithms, including DCA, DESC, scDeepCluster and scziDesk, on the 20 real scRNA-seq datasets. We summarize the running times of the different computational algorithms on the 20 datasets in <xref rid="btad075-F4" ref-type="fig">Figure 4A</xref>. From the figure, we observe that the time complexity of our proposed algorithm, scBGEDA and the scziDesk algorithm, is nearly linear with the increasing number of cells, however the slope of the scBGEDA plot is lower, meaning that scBGEDA has higher computational power than scziDesk on very large scRNA-seq datasets. DESC, a soft clustering algorithm with useful cluster assignment probabilities, has a slightly lower time cost for larger scRNA-seq datasets containing more than 10 000 cells. DCA and scDeepCluster are more time consuming than scBGEDA on scRNA-seq datasets of different cell sizes. Due to the early stopping mechanism, the time trend curves of DCA and scDeepCluster show substantial fluctuation. Moreover, from the total times of all datasets summarized in <xref rid="btad075-F4" ref-type="fig">Figure 4A</xref>, scBGEDA surpasses scziDesk and scDeepCluster and although the total time cost of DESC is slightly lower than scBGEDA, DESC does not obtain the desired clustering result. In summary, we can conclude that scBGEDA has an appropriate running time and is an efficient tool in variable size scRNA-seq data analysis.</p>
      <fig position="float" id="btad075-F4">
        <label>Fig. 4.</label>
        <caption>
          <p>(<bold>A</bold>) The running time and total time cost comparisons of different deep learning-based models on specific datasets of varying cell size. (<bold>B</bold>) Results of comparative analysis of different clustering methods on the large-scale scRNA-seq dataset PBMC_68k measured by ARI. (<bold>C</bold>) The change of ARI values from whole data to 20%, 40%, 60% and 80% downsampling data in PBMC_68k dataset. (<bold>D</bold>) Comparison results for scBGEDA and different clustering methods on ‘Tabula Muris’ measured by NMI and ARI</p>
        </caption>
        <graphic xlink:href="btad075f4" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.7 Evaluations on two large-scale scRNA-seq datasets</title>
      <p>To demonstrate the scalability of scBGEDA to large-scale scRNA-seq data, several experiments were conducted on two large-scale scRNA-seq datasets, PBMC_68k (<xref rid="btad075-B31" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic>, 2017</xref>) and ‘Tabula Muris’ (<xref rid="btad075-B23" ref-type="bibr">Tabula Muris Consortium <italic toggle="yes">et al.</italic>, 2018</xref>). PBMC_68k has 68 000 peripheral blood mononuclear cells with 10 cell types including Activated CD8+; Naive CD8+; Memory and Reg T cells; Naive CD4+; NK; Naive CD8+; B; Megakaryocytes; Monocytes and dendritic cells; and B, dendritic, T cells. ‘Tabula Muris’ has nearly 100 000 cells from 20 organs and tissues and 19 179 genes with 55 cell types. We compared the proposed scBGEDA with six scRNA-seq clustering algorithms and four deep learning-based models, SHARP, SOUP, Seurat, SC3, Scanpy, PCA, DCA, scDeepCluster, scziDesk and DESC. CIDR was not chosen as it takes more than 141G memory to run on PBMC_68k and ‘Tabula Muris’. The comparison results on PBMC_68k measured by ARI, NMI and Wilcoxon test analysis are summarized in <xref rid="btad075-F4" ref-type="fig">Figure 4B</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S2</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S21</xref>, respectively. We see that our proposed method outperforms the other clustering methods. Moreover, from <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S9</xref>, we find significant differences between scBGEDA and the other algorithms on the large-scale scRNA-seq dataset PBMC_68k (<inline-formula id="IE89"><mml:math id="IM89" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo></mml:mrow></mml:math></inline-formula>0.05), further confirming that scBGEDA performs well for clustering tasks on larger scRNA-seq datasets. Moreover, <xref rid="btad075-F4" ref-type="fig">Figure 4D</xref> illustrates the clustering performance of scBGEDA measured by NMI and ARI compared to the other clustering algorithms on ‘Tabula Muris’. As shown in <xref rid="btad075-F4" ref-type="fig">Figure 4D</xref>, it further demonstrates that scBGEDA can provide excellent clustering performance on the large-scale scRNA-seq dataset.</p>
      <p>To further validate the robustness of scBGEDA, we downsampled the PBMC_68k dataset to yield partial datasets containing 20%, 40%, 60% and 80% of the cells from the whole data. The same 10 random seeds were used to select the cells randomly to ensure fairness. The median ARI values out of 10 runs are summarized in <xref rid="btad075-F4" ref-type="fig">Figure 4C</xref>. We observe that scBGEDA performs excellently on each dataset size. Besides, the Wilconxon test result on the ARI results is summarized in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S22</xref>, showing there is no significance difference between the ARI values of scBGEDA on different data sizes of PMBC_68k (<inline-formula id="IE90"><mml:math id="IM90" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&gt;</mml:mo></mml:mrow></mml:math></inline-formula>0.05), indicating the robustness of scBGEDA.</p>
    </sec>
    <sec>
      <title>3.8 Distribution analysis</title>
      <p>To explore the suitable distribution in modeling the scRNA-seq data, we apply the NB and ZINB models in our proposed algorithm scBGEDA on those 20 scRNA-seq datasets. To conduct a fair comparison, we replaced the ZINB distribution in the scBGEDA algorithm with the NB distribution. The performance comparison is summarized in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S3</xref>, measured by NMI and ARI. From <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S3</xref>, we can observe that scBGEDA with NB or ZINB models shows comparable clustering results, with the average NMI and ARI of scBGEDA with NB model just slightly higher than those of scBGEDA with ZINB model. In particular, for the Adam, Klein, Muraro, Plasschaert, Pollen, QS_Heart, QS_Limb_Muscle, Qx_Limb_Muscle, Tosches_turtle, Wang_Lung and Young datasets, scBGEDA with the NB model performs better than scBGEDA with ZINB.</p>
    </sec>
    <sec>
      <title>3.9 Trajectory inference</title>
      <p>To demonstrate the performance of the different computational methods for trajectory inference, we applied Monocle3 (<xref rid="btad075-B5" ref-type="bibr">Cao <italic toggle="yes">et al.</italic>, 2019</xref>) for the gene expression data and prediction labels obtained by the different computational methods. The experimental results for the trajectory inference are summarized in <xref rid="btad075-F5" ref-type="fig">Figure 5</xref>, where we can observe that our proposed scBGEDA and scziDesk both produce a more accurate order of pseudotime of kidney development from early proximal tubule progenitor cell to the two major subgroups. One of the subgroups is the loop of Henle (left branch), which leads from the proximal convoluted tubule to the distal convoluted tubule and represents a more mature developmental stage. The other subgroup (right branch) has endothelial cell populations, which have extensive diversity in the kidney. We notice that ureteric buds can be seen in both branches. This is consistent with the fact that ureteric bud appears during the embryological development of kidney (<xref rid="btad075-B1" ref-type="bibr">Adam <italic toggle="yes">et al.</italic>, 2017</xref>).</p>
      <fig position="float" id="btad075-F5">
        <label>Fig. 5.</label>
        <caption>
          <p>UMAP visualization of the trajectory inference on ‘Adam’ by the different computation methods. Black lines represent branched cell trajectories, and cell types are different colors</p>
        </caption>
        <graphic xlink:href="btad075f5" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.10 Ablation analysis</title>
      <p>In this experiment, we analyzed the impact of each component of scBGEDA. The single-cell dual denoising autoencoder and the bipartite graph ensemble clustering were ablated and the model tested on the 20 scRNA-seq datasets. (i) We removed the bipartite graph ensemble clustering in scBGEDA, and investigated the latent feature representation provided by the single-cell dual denoising autoencoder with the regular clustering including the Leiden and Louvain clustering (<xref rid="btad075-B27" ref-type="bibr">Wolf <italic toggle="yes">et al.</italic>, 2018</xref>), called scBGEDA<inline-formula id="IE91"><mml:math id="IM91" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi mathvariant="italic">Leiden</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and scBGDDA<inline-formula id="IE92"><mml:math id="IM92" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi mathvariant="italic">Louvain</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, respectively; (ii) we removed the single-cell dual denoising autoencoder in scBGEDA, and applied the proposed bipartite graph ensemble clustering on a reduced-dimensional space with 25 dimensions obtained by PCA (<xref rid="btad075-B8" ref-type="bibr">Fodor, 2002</xref>), factor analysis (FA) (<xref rid="btad075-B8" ref-type="bibr">Fodor, 2002</xref>) and UMAP (<xref rid="btad075-B17" ref-type="bibr">McInnes <italic toggle="yes">et al.</italic>, 2018</xref>), called scBGEDA<inline-formula id="IE93"><mml:math id="IM93" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi mathvariant="italic">PCA</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, scBGEDA<inline-formula id="IE94"><mml:math id="IM94" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi>F</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and scBGEDA<inline-formula id="IE95"><mml:math id="IM95" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi mathvariant="italic">UMAP</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, respectively. The experimental results are summarized in <xref rid="btad075-F6" ref-type="fig">Figure 6</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S23 and S24</xref> measured by NMI, ARI, ASW and cLISI. We observe that the synergistic use of a single-cell dual denoising autoencoder and the bipartite graph ensemble clustering often (but not always) enhance the clustering performance. As observed from <xref rid="btad075-F6" ref-type="fig">Figure 6C and D</xref>, we find that for most scRNA-seq datasets, the single-cell dual denoising autoencoder improves performance with respect to other competitor methods (by typically a small amount). While for some scRNA-seq datasets including Chen, Klein, Plasschaert, Tosches_turtle, Wang_Lung and Young, the single-cell dual denoising autoencoder can significantly reduce performance as well. Each component of scBGEDA plays an important role in characterizing scRNA-seq data. Moreover, to assess different dimension reduction models, including the AE in our proposed scBGEDA, UMAP, PCA and FA. The nearest neighbor error (NNE) (<xref rid="btad075-B18" ref-type="bibr">Pouyan and Kostka, 2018</xref>) was employed to measure these dimension reduction methods. NNE is calculated using a nearest neighbor classifier based on the reduced-dimensional space to be evaluated, which can reflect the goodness of the distance measure from the latent features directly. Predictions for each cell were obtained using 10-fold cross-validation (9 for training and 1 for validation), and the proportion of misclassified cells was reported by NNE (<xref rid="btad075-B26" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2017</xref>). Accordingly, we report the average over 20 runs of the average validation error from the 10 folders as the final NNE error. The NNE obtained from the methods is summarized in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S4</xref>. As can be seen from this figure, the latent potential features acquired by the proposed AE are comparable to PCA and superior to UMAP and FA in the comparison of the reduced-dimensional space of the various approaches.</p>
      <fig position="float" id="btad075-F6">
        <label>Fig. 6.</label>
        <caption>
          <p>Ablation analysis results. (<bold>A and B</bold>) Comparative performance of scBGEDA with different clustering algorithms measured by NMI and ARI; (<bold>C and D</bold>) Comparative performance of scBGEDA with different dimension reduction algorithms measured by NMI and ARI</p>
        </caption>
        <graphic xlink:href="btad075f6" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.11 scBGEDA assists marker gene identification</title>
      <p>We employed the gene expression matrix of the QS_Heart dataset and took the predicted clustering labels of scBGEDA to identify differentially expressed genes (DEGs) and thereby, the marker genes for each cluster. First, we conducted differential expression analysis to determine the DEGs in each cluster using the Wilcoxon Ranksum test, to ascertain whether two independent cell types are from the same distribution in a non-parametric form. To identify the predominantly expressed genes in each cluster, the top 20 DEGs with <italic toggle="yes">P</italic>-values &lt;0.05 were reported as shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S5</xref> that drive the expression–distribution separation of the different cell clusters. Then, we visualized the expression levels of the top five genes in each cluster to observe the expression levels of the highly expressed genes in each cluster. <xref rid="btad075-F7" ref-type="fig">Figure 7A</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S6</xref> indicate the average expression of each of the top five DEGs of each cluster. To verify the obtained marker genes, we matched them manually to the published marker genes in the cell marker database CellMark (<xref rid="btad075-B30" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic>, 2019</xref>). It can be seen that most of the DEGs identified by scBGEDA can be matched to published marker genes within the clusters; for instance, Gsn, Col3a1, Col1a2 and Mmp2, are marker genes for fibroblast; and Fabp4, Egfl7, Flt1 and Pecam1 marker genes for endothelial cells.</p>
      <fig position="float" id="btad075-F7">
        <label>Fig. 7.</label>
        <caption>
          <p>Marker gene analysis in the scRNA-seq dataset QS_Heart. (<bold>A</bold>) Dot plot of average expression of the top five DEGs within each cell type, implying marker genes of QS_Heart. (<bold>B</bold>) The violin plot of identified marker gene expression in the different cell types</p>
        </caption>
        <graphic xlink:href="btad075f7" position="float"/>
      </fig>
      <p>It is noteworthy that although some DEGs cannot be matched to any of those in the cell marker database, they are clearly more highly expressed in some specific cell groups than in others and may perhaps indicate novel markers. For instance, in <xref rid="btad075-F7" ref-type="fig">Figure 7B</xref>, the expression levels of cd36 and Fabp4 were higher in endothelial cells than in other cell types, and they are therefore potential marker genes for endothelial cells. Cryab and Fxyd1 are highly expressed in cardiac muscle and cardiac neuron cell types, respectively, and could be candidate markers for myocardium and cardiac neurons. Dcn and Dpt are highly expressed in fibroblasts, which indicates possible new markers for fibroblasts. Moreover, extended analysis including functional genomic analysis, batch effect analysis and extend experiments are summarized in <xref rid="sup1" ref-type="supplementary-material">Supplementary Sections S7–S9</xref>.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p>In this study, a deep single-cell clustering model via a dual denoising autoencoder with bipartite graph ensemble clustering, scBGEDA, was developed to identify cell populations in scRNA-seq datasets. Stepwise, the high-dimensional scRNA-seq data are first preprocessed and the top highly variable genes are selected to eliminate redundant genes with low expression that may disturb the clustering result. This leads to a significant improvement in clustering performance. Next, we designed a dual denoising autoencoder by optimizing the dual reconstruction loss to learn the discriminative feature representation of the scRNA-seq data. Then, we developed a bipartite graph ensemble clustering with a graph-based consensus function to identify the cell types from the learned latent representation. To validate our model, we carried out a comprehensive study comparing it to other benchmark methods in terms of cell-type identification and characterization mechanisms from different perspectives and demonstrated the superiority of scBGEDA over current methods. As the development of the advanced high-throughput technologies for scRNA-seq and the emerging cell atlas (<xref rid="btad075-B10" ref-type="bibr">Han <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btad075-B19" ref-type="bibr">Rozenblatt-Rosen <italic toggle="yes">et al.</italic>, 2017</xref>), we will explore the performance of the proposed scBGEDA on larger scale scRNA-seq datasets in the future.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>The work described in this article was substantially supported by the National Natural Science Foundation of China. [62206086, 62076109 and 61972174] and also funded by ‘the Fundamental Research Funds for the Central Universities’.</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btad075_Supplementary_Data</label>
      <media xlink:href="btad075_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <sec sec-type="data-availability">
    <title>Data availability</title>
    <p>The data underlying this article are available in the article and in its online <xref rid="sup1" ref-type="supplementary-material">supplementary material</xref>.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btad075-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Adam</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>Psychrophilic proteases dramatically reduce single-cell RNA-seq artifacts: a molecular atlas of kidney development</article-title>. <source>Development</source>, <volume>144</volume>, <fpage>3625</fpage>–<lpage>3632</lpage>.<pub-id pub-id-type="pmid">28851704</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Angerer</surname><given-names>P.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>Single cells make big data: new challenges and opportunities in transcriptomics</article-title>. <source>Curr. Opin. Syst. Biol</source>., <volume>4</volume>, <fpage>85</fpage>–<lpage>91</lpage>.</mixed-citation>
    </ref>
    <ref id="btad075-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aviv</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>The human cell atlas</article-title>. <source>Elife</source>, <volume>6</volume>, <fpage>e27041</fpage>.<pub-id pub-id-type="pmid">29206104</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ben-Dor</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>1999</year>) <article-title>Clustering gene expression patterns</article-title>. <source>J. Comput. Biol</source>., <volume>6</volume>, <fpage>281</fpage>–<lpage>297</lpage>.<pub-id pub-id-type="pmid">10582567</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cao</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>The single-cell transcriptional landscape of mammalian organogenesis</article-title>. <source>Nature</source>, <volume>566</volume>, <fpage>496</fpage>–<lpage>502</lpage>.<pub-id pub-id-type="pmid">30787437</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>L.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Deep soft K-means clustering with self-training for single-cell RNA sequence data</article-title>. <source>NAR Genom. Bioinform</source>., <volume>2</volume>, <fpage>lqaa039</fpage>.<pub-id pub-id-type="pmid">33575592</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eraslan</surname><given-names>G.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Single-cell RNA-seq denoising using a deep count autoencoder</article-title>. <source>Nat. Commun</source>., <volume>10</volume>, <fpage>1</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">30602773</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B8">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Fodor</surname><given-names>I.K.</given-names></string-name></person-group> (<year>2002</year>) A survey of dimension reduction techniques. <italic toggle="yes">Technical report</italic>. Lawrence Livermore National Lab., CA, USA.</mixed-citation>
    </ref>
    <ref id="btad075-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gan</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Identification of cancer subtypes from single-cell RNA-seq data using a consensus clustering method</article-title>. <source>BMC Med. Genomics</source>, <volume>11</volume>, <fpage>65</fpage>–<lpage>72</lpage>.<pub-id pub-id-type="pmid">30092803</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Han</surname><given-names>X.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Mapping the mouse cell atlas by Microwell-seq</article-title>. <source>Cell</source>, <volume>172</volume>, <fpage>1091</fpage>–<lpage>1107.e17</lpage>.<pub-id pub-id-type="pmid">29474909</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Ultra-scalable spectral clustering and ensemble clustering</article-title>. <source>IEEE Trans. Knowl. Data Eng</source>., <volume>32</volume>, <fpage>1212</fpage>–<lpage>1226</lpage>.</mixed-citation>
    </ref>
    <ref id="btad075-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huh</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>SAME-clustering: single-cell aggregated clustering via mixture model ensemble</article-title>. <source>Nucleic Acids Res</source>., <volume>48</volume>, <fpage>86</fpage>–<lpage>95</lpage>.<pub-id pub-id-type="pmid">31777938</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kiselev</surname><given-names>V.Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>SC3: consensus clustering of single-cell RNA-seq data</article-title>. <source>Nat. Methods</source>, <volume>14</volume>, <fpage>483</fpage>–<lpage>486</lpage>.<pub-id pub-id-type="pmid">28346451</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>X.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Deep learning enables accurate clustering with batch effect removal in single-cell RNA-seq analysis</article-title>. <source>Nat. Commun</source>., <volume>11</volume>, <fpage>1</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">31911652</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B15">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>Z.</given-names></string-name></person-group><etal>et al</etal> (<year>2012</year>) Segmentation using superpixels: a bipartite graph partitioning approach. In: <italic toggle="yes">2012 IEEE Conference on Computer Vision and Pattern Recognition, Rhode Island, USA</italic>, pp. <fpage>789</fpage>–<lpage>796</lpage>. IEEE.</mixed-citation>
    </ref>
    <ref id="btad075-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>P.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>CIDR: ultrafast and accurate clustering through imputation for single-cell RNA-seq data</article-title>. <source>Genome Biol</source>., <volume>18</volume>, <fpage>59</fpage>.<pub-id pub-id-type="pmid">28351406</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B17">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>McInnes</surname><given-names>L.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) UMAP: uniform manifold approximation and projection for dimension reduction. <italic toggle="yes">The Journal of Open Source Software,</italic><bold>3</bold>(29), 861. <ext-link xlink:href="https://arxiv.org/abs/1802.03426" ext-link-type="uri">https://arxiv.org/abs/1802.03426</ext-link>.</mixed-citation>
    </ref>
    <ref id="btad075-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pouyan</surname><given-names>M.B.</given-names></string-name>, <string-name><surname>Kostka</surname><given-names>D.</given-names></string-name></person-group> (<year>2018</year>) <article-title>Random forest based similarity learning for single cell RNA sequencing data</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>i79</fpage>–<lpage>i88</lpage>.<pub-id pub-id-type="pmid">29950006</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rozenblatt-Rosen</surname><given-names>O.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>The human cell atlas: from vision to reality</article-title>. <source>Nature</source>, <volume>550</volume>, <fpage>451</fpage>–<lpage>453</lpage>.<pub-id pub-id-type="pmid">29072289</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Satija</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2015</year>) <article-title>Spatial reconstruction of single-cell gene expression data</article-title>. <source>Nat. Biotechnol</source>., <volume>33</volume>, <fpage>495</fpage>–<lpage>502</lpage>.<pub-id pub-id-type="pmid">25867923</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shi</surname><given-names>J.</given-names></string-name>, <string-name><surname>Malik</surname><given-names>J.</given-names></string-name></person-group> (<year>2000</year>) <article-title>Normalized cuts and image segmentation</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>., <volume>22</volume>, <fpage>888</fpage>–<lpage>905</lpage>.</mixed-citation>
    </ref>
    <ref id="btad075-B22">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Syed</surname><given-names>N.A.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <part-title>Chapter 8 - single-cell omics in metabolic disorders</part-title>. In: <person-group person-group-type="editor"><string-name><surname>Barh</surname><given-names>D.</given-names></string-name>, <string-name><surname>Azevedo</surname><given-names>V.</given-names></string-name></person-group> (eds) <source>Single-Cell Omics</source>. <publisher-name>Academic Press</publisher-name>, Elsevier. pp. <fpage>153</fpage>–<lpage>164</lpage>. <ext-link xlink:href="https://www.sciencedirect.com/science/article/pii/B9780128175323000086" ext-link-type="uri">https://www.sciencedirect.com/science/article/pii/B9780128175323000086</ext-link>.</mixed-citation>
    </ref>
    <ref id="btad075-B23">
      <mixed-citation publication-type="journal"><collab>Tabula Muris Consortium</collab>. <etal>et al</etal> (<year>2018</year>) <article-title>Single-cell transcriptomics of 20 mouse organs creates a Tabula Muris</article-title>. <source>Nature</source>, <volume>562</volume>, <fpage>367</fpage>–<lpage>372</lpage>.<pub-id pub-id-type="pmid">30283141</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tian</surname><given-names>T.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Clustering single-cell RNA-seq data with a model-based deep learning approach</article-title>. <source>Nat. Mach. Intell</source>., <volume>1</volume>, <fpage>191</fpage>–<lpage>198</lpage>.</mixed-citation>
    </ref>
    <ref id="btad075-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wan</surname><given-names>S.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>SHARP: hyperfast and accurate processing of single-cell RNA-seq data via ensemble random projection</article-title>. <source>Genome Res</source>., <volume>30</volume>, <fpage>205</fpage>–<lpage>213</lpage>.<pub-id pub-id-type="pmid">31992615</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>B.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>Visualization and analysis of single-cell RNA-seq data by kernel-based similarity learning</article-title>. <source>Nat. Methods</source>, <volume>14</volume>, <fpage>414</fpage>–<lpage>416</lpage>.<pub-id pub-id-type="pmid">28263960</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolf</surname><given-names>F.A.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>SCANPY: large-scale single-cell gene expression data analysis</article-title>. <source>Genome Biol</source>., <volume>19</volume>, <fpage>1</fpage>–<lpage>5</lpage>.<pub-id pub-id-type="pmid">29301551</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B28">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Xie</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) Unsupervised deep embedding for clustering analysis. In: <italic toggle="yes">International Conference on Machine Learning, New York, NY, USA</italic>, pp. <fpage>478</fpage>–<lpage>487</lpage>. PMLR.</mixed-citation>
    </ref>
    <ref id="btad075-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>SAFE-clustering: single-cell aggregated (from ensemble) clustering for single-cell RNA-seq data</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>1269</fpage>–<lpage>1277</lpage>.<pub-id pub-id-type="pmid">30202935</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>X.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>CellMarker: a manually curated resource of cell markers in human and mouse</article-title>. <source>Nucleic Acids Res</source>., <volume>47</volume>, <fpage>D721</fpage>–<lpage>D728</lpage>.<pub-id pub-id-type="pmid">30289549</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zheng</surname><given-names>G.X.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>Massively parallel digital transcriptional profiling of single cells</article-title>. <source>Nat. Commun</source>., <volume>8</volume>, <fpage>14049</fpage>–<lpage>14012</lpage>.<pub-id pub-id-type="pmid">28091601</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhu</surname><given-names>L.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Semisoft clustering of single-cell data</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>116</volume>, <fpage>466</fpage>–<lpage>471</lpage>.<pub-id pub-id-type="pmid">30587579</pub-id></mixed-citation>
    </ref>
    <ref id="btad075-B250">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhuohan</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2023</year>) <article-title>Topological identification and interpretation for single-cell gene regulation elucidation across multiple platforms using scMGCA</article-title>. <source>Nature Communications</source>, <volume>14</volume>(1), <fpage>400</fpage>. <ext-link xlink:href="https://www.nature.com/articles/s41467-023-36134-7" ext-link-type="uri">https://www.nature.com/articles/s41467-023-36134-7</ext-link></mixed-citation>
    </ref>
  </ref-list>
</back>
