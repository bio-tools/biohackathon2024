<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName nihms2pmcx2.xsl?>
<?ConverterInfo.Version 1?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr SoftwareX?>
<?submitter-system nihms?>
<?submitter-userid 7614779?>
<?submitter-authority eRA?>
<?submitter-login markpagel?>
<?submitter-name Mark Pagel?>
<?domain nihpa?>
<?ppubseason Jul-Dec?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">101660267</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">43949</journal-id>
    <journal-id journal-id-type="nlm-ta">SoftwareX</journal-id>
    <journal-id journal-id-type="iso-abbrev">SoftwareX</journal-id>
    <journal-title-group>
      <journal-title>SoftwareX</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2352-7110</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8188855</article-id>
    <article-id pub-id-type="doi">10.1016/j.softx.2019.100347</article-id>
    <article-id pub-id-type="manuscript">nihpa1582136</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Deep learning application engine (DLAE): Development and integration of deep learning algorithms in medical imaging</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Sanders</surname>
          <given-names>Jeremiah W.</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
        <xref ref-type="aff" rid="A2">b</xref>
        <xref rid="CR1" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fletcher</surname>
          <given-names>Justin R.</given-names>
        </name>
        <xref ref-type="aff" rid="A3">c</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Frank</surname>
          <given-names>Steven J.</given-names>
        </name>
        <xref ref-type="aff" rid="A4">d</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Ho-Ling</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
        <xref ref-type="aff" rid="A2">b</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Johnson</surname>
          <given-names>Jason M.</given-names>
        </name>
        <xref ref-type="aff" rid="A5">e</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhou</surname>
          <given-names>Zijian</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chen</surname>
          <given-names>Henry Szu-Meng</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Venkatesan</surname>
          <given-names>Aradhana M.</given-names>
        </name>
        <xref ref-type="aff" rid="A2">b</xref>
        <xref ref-type="aff" rid="A5">e</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kudchadker</surname>
          <given-names>Rajat J.</given-names>
        </name>
        <xref ref-type="aff" rid="A2">b</xref>
        <xref ref-type="aff" rid="A6">f</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pagel</surname>
          <given-names>Mark D.</given-names>
        </name>
        <xref ref-type="aff" rid="A2">b</xref>
        <xref ref-type="aff" rid="A7">g</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ma</surname>
          <given-names>Jingfei</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
        <xref ref-type="aff" rid="A2">b</xref>
      </contrib>
    </contrib-group>
    <aff id="A1"><label>a</label>Department of Imaging Physics, The University of Texas MD Anderson Cancer Center, 1515 Holcombe Blvd., Unit 1472, Houston, TX 77030, United States of America</aff>
    <aff id="A2"><label>b</label>Medical Physics Graduate Program, MD Anderson Cancer Center UTHealth Graduate School of Biomedical Sciences, Houston, 1515 Holcombe Blvd., Unit 1472, TX 77030, United States of America</aff>
    <aff id="A3"><label>c</label>Odyssey Systems Consulting, LLC, 550 Lipoa Parkway, Kihei, Maui, HI, United States of America</aff>
    <aff id="A4"><label>d</label>Department of Radiation Oncology, The University of Texas MD Anderson Cancer Center, 1515 Holcombe Blvd., Unit 1422, Houston, TX 77030, United States of America</aff>
    <aff id="A5"><label>e</label>Department of Diagnostic Radiology, The University of Texas MD Anderson Cancer Center, 1515 Holcombe Blvd., Unit 1473, Houston, TX 77030, United States of America</aff>
    <aff id="A6"><label>f</label>Department of Radiation Physics, The University of Texas MD Anderson Cancer Center, 1515 Holcombe Blvd., Unit 1420, Houston, TX 77030, United States of America</aff>
    <aff id="A7"><label>g</label>Department of Cancer Systems Imaging, The University of Texas MD Anderson Cancer Center, 1515 Holcombe Blvd., Unit 1907, Houston, TX 77030, United States of America</aff>
    <author-notes>
      <corresp id="CR1"><label>*</label>Corresponding author at: Department of Imaging Physics, The University of Texas MD Anderson Cancer Center, 1515 Holcombe Blvd., Unit 1472, Houston, TX 77030, United States of America. <email>jsanders1@mdanderson.org</email> (J.W. Sanders).</corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>9</day>
      <month>4</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>29</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <season>Jul-Dec</season>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>09</day>
      <month>6</month>
      <year>2021</year>
    </pub-date>
    <volume>10</volume>
    <elocation-id>100347</elocation-id>
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="https://www.sciencedirect.com/science/article/pii/S2352711019302535"/>
    <abstract id="ABS1">
      <p id="P1">Herein we introduce a deep learning (DL) application engine (DLAE) system concept, present potential uses of it, and describe pathways for its integration in clinical workflows. An open-source software application was developed to provide a code-free approach to DL for medical imaging applications. DLAE supports several DL techniques used in medical imaging, including convolutional neural networks, fully convolutional networks, generative adversarial networks, and bounding box detectors. Several example applications using clinical images were developed and tested to demonstrate the capabilities of DLAE. Additionally, a model deployment example was demonstrated in which DLAE was used to integrate two trained models into a commercial clinical software package.</p>
    </abstract>
    <kwd-group>
      <kwd>Medical imaging</kwd>
      <kwd>Software</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Algorithm development</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>1.</label>
    <title>Introduction and background</title>
    <p id="P2">Deep learning (DL) [<xref rid="R1" ref-type="bibr">1</xref>] techniques automatically develop parameterized algorithms, known as models, to infer useful information given sufficient data. Enabled by the confluence of large data sets, general-purpose graphics processing units (GPUs), and open-source software frameworks, modern DL techniques routinely achieve state-of-the-art human-quality performance in information-processing tasks and have demonstrated success in the domain of computer vision for medical imaging.</p>
    <p id="P3">The general approach to developing supervised DL models for medical imaging applications is to identify a computational task, curate a data set containing images (inputs, X) and annotations (outputs, y), construct a model f(·) conforming to the input and output data domains, and train the model to accurately map the inputs to the outputs (y = f[X]) with the goal of making the trained model able to make accurate predictions on new images that it has not observed. In 2012, researchers trained a convolutional neural network (CNN) to accurately classify images, thereby establishing a new state of the art for that application domain [<xref rid="R2" ref-type="bibr">2</xref>]. This demonstration of capability inspired the application of DL to numerous new application domains, including medical imaging, where DL techniques have become the stateof-the-art for many tasks [<xref rid="R3" ref-type="bibr">3</xref>]. In the following years, public visual recognition competitions yielded refinement and extension of CNNs [<xref rid="R4" ref-type="bibr">4</xref>–<xref rid="R14" ref-type="bibr">14</xref>]. During this period, development of new model architectures resulted in substantial performance gains. The design and evaluation of model architectures to fit novel application domains remain core areas of activity for DL practitioners. Therefore, exploring a variety of architectures for a given DL application is critical to ensuring superior performance for production-level model deployment. This is especially true in the field of medical imaging, in which physicians use model predictions on patient medical images to inform their medical decisions and treatments on humans.</p>
    <p id="P4">Researchers have proposed the use of several DL architectures, largely based on CNNs and fully convolutional networks (FCNs) [<xref rid="R15" ref-type="bibr">15</xref>], for medical imaging applications. Some have proposed architectures based on CNNs for medical image classification and regression tasks [<xref rid="R16" ref-type="bibr">16</xref>–<xref rid="R21" ref-type="bibr">21</xref>]. Others have developed architectures based on FCNs for medical image segmentation [<xref rid="R22" ref-type="bibr">22</xref>–<xref rid="R30" ref-type="bibr">30</xref>] and image synthesis via regression [<xref rid="R31" ref-type="bibr">31</xref>–<xref rid="R38" ref-type="bibr">38</xref>]. Additionally, some have reposed the task of object detection as a segmentation task using FCNs [<xref rid="R39" ref-type="bibr">39</xref>]. Arguably the most popular of these architectures—the DL workhorse in medical imaging—is UNet (and its numerous minivariants), which includes a convolutional encoder–decoder constructed with (or, to generalize, without) skip connections between the encoder and decoder at equivalent resolution scales [<xref rid="R40" ref-type="bibr">40</xref>,<xref rid="R41" ref-type="bibr">41</xref>]. Although UNet and its numerous minivariants have been successful in several medical imaging applications, it is limited to pixel-wise classification and regression tasks, which represent a small subset of the potential applications in medical imaging. Furthermore, the replication and minute variations among many UNet- and FCN-based architectures have led to repeated efforts within the DL community.</p>
    <p id="P5">Generative adversarial networks (GANs) represent another class of DL techniques that perform information-processing tasks in medical imaging. GANs consist of two neural networks—a generator and a discriminator—that are trained to compete against one another in a minimax game [<xref rid="R42" ref-type="bibr">42</xref>]. GANs have demonstrated the ability to perform several tasks in medical imaging, including image synthesis [<xref rid="R43" ref-type="bibr">43</xref>–<xref rid="R45" ref-type="bibr">45</xref>], segmentation [<xref rid="R46" ref-type="bibr">46</xref>–<xref rid="R48" ref-type="bibr">48</xref>], reconstruction [<xref rid="R49" ref-type="bibr">49</xref>–<xref rid="R51" ref-type="bibr">51</xref>], and superresolution [<xref rid="R52" ref-type="bibr">52</xref>,<xref rid="R53" ref-type="bibr">53</xref>]. GANs’ ability to learn their own loss function makes them unique among all DL techniques and enables them to produce realistic samples from the data domain without explicitly programming a loss function. Because the annotations in medical images are most often paired pixel-wise with the input images, certain GANs are well suited for medical imaging applications [<xref rid="R54" ref-type="bibr">54</xref>]. Instances also arise in which the input images are not paired with their annotations. For example, in multiacquisition MRI protocols in which the patient may move between scans with different pulse sequences, the information content in the different images is similar, but this information is not paired pixel-wise due to patient motion. Certain GANs have demonstrated the ability to translate information from the input to the output under similar scenarios [<xref rid="R55" ref-type="bibr">55</xref>] and could be useful in medical imaging applications. Because FCNs and some GANs can make pixel-wise predictions on images, it is often unclear a priori which would produce superior performance for a given application. Therefore, the ability to easily and rapidly explore and compare these two approaches for a given application would be beneficial.</p>
    <p id="P6">Object-detection tasks are common in medical image processing. Computer vision researchers have created several approaches to object detection with DL whereby FCNs are trained to predict bounding boxes (bounding box detectors [BBDs]) around multiple classes of objects within an image [<xref rid="R56" ref-type="bibr">56</xref>–<xref rid="R59" ref-type="bibr">59</xref>]. Building on these developments, researchers have proposed using BBDs in some medical imaging applications [<xref rid="R60" ref-type="bibr">60</xref>,<xref rid="R61" ref-type="bibr">61</xref>]. The general approach with BBDs is to map input images to a set of bounding box coordinates, the object class in the bounding box, and the associated predictive confidence. The training and inference procedures for BBDs require unique programmatic routines that are distinct from those used for CNNs, FCNs, and GANs. Because other DL techniques can be reposed as detection tasks, BBDs add an additional layer of complexity to selecting the appropriate DL technique for a given medical imaging application.</p>
    <p id="P7">Given the variety of DL approaches available for novel medical imaging applications, the need for rapid prototyping of DL models in medical imaging tasks has become apparent. Moreover, many clinical workflows involving medical images inherently produce the annotations required to train DL models (e.g., segmentation masks, registered multimodal images, object/disease presence and/or location). These data sets enable new DL applications to support research programs, streamline clinical workflows, help manage treatments, and support clinical quality assurance programs. However, the design and implementation of DL models require extensive and specialized programming and computational knowledge. This creates a barrier to the application of DL techniques to clinically relevant problems by physicians and scientists. Additionally, fruitful integration of trained models into clinical workflows requires that models be encapsulated and deployed as software systems. This requirement poses even greater technical hurdles for DL adoption.</p>
    <p id="P8">In this work, we ease DL model design, implementation, training, and deployment using a graphical user interface (GUI). We support a comprehensive set of DL architectures and loss functions for several tasks in medical imaging, and enable full automation of DL model development and inference. As such, with an appropriately curated data set, users would be able to explore a variety of DL techniques for numerous applications and integrate them into workflows using a single coherent software framework. The deep learning application engine (DLAE) is a software framework and application that enables users to design, train, validate, and coherently encapsulate and deploy a DL model while hiding programmatic implementation details. We provide a high-level overview of the design of DLAE, demonstrate separability between graphical model construction and graph processes through the use of configuration files, demonstrate the automation of training and inference processes, and present some example DL applications in medical imaging produced by DLAE.</p>
  </sec>
  <sec id="S2">
    <label>2.</label>
    <title>Software framework</title>
    <sec id="S3">
      <label>2.1.</label>
      <title>Software concept</title>
      <p id="P9">DLAE is an application that abstracts the task of DL model development to a series of button clicks, making use of lower level libraries. Primarily, it is an application built around the Keras library [<xref rid="R62" ref-type="bibr">62</xref>], which is above the TensorFlow [<xref rid="R63" ref-type="bibr">63</xref>] and CUDA libraries.</p>
      <p id="P10">Several prebuilt applications are included in DLAE that are compatible with many tasks encountered in medical imaging, and they can be loaded to be modified and/or trained. As such, no explicit programming is required to utilize these applucations. This is facilitated by graphical user interface input (GUI) controls which construct engine configurations (or configuration files) that store all information about a model developed for a given application. Sharing of configuration files facilitates collaborations across research groups and serves as an easy means for reproducing research results.</p>
      <p id="P11">The engine in DLAE runs solely with configuration structures as inputs, which can be loaded from configuration files. Configuration files decouple the GUI from the processing engine. Therefore, the engine can be run without user input. Once a DL model with a specific layer configuration is developed, the configuration file defining the model can be passed to the engine to perform an automated training session. This enables streamlined ablation studies and hyperparameter searches for a given DL application development. Configuration files also facilitate the incorporation of trained DL models into workflows by enabling automated inference.</p>
    </sec>
    <sec id="S4">
      <label>2.2.</label>
      <title>Software GUI</title>
      <p id="P12">A GUI was developed to facilitate DL model development (<xref rid="F1" ref-type="fig">Fig. 1</xref>). This enables the adoption of deep neural networks without the programming experience required by modern DL frameworks. This constitutes a tradeoff, reducing the expressiveness available to DLAE users while increasing the number of individuals capable of applying modern DL models to problems in medical imaging. Knowledge of the statistical foundations underlying modern DL is still required to effectively develop applications. However, this knowledge is more common among domain experts than is the corresponding level of software engineering expertise required to field DL applications.</p>
      <p id="P13">The GUI home screen is partitioned into menus, each corresponding to a step in a DL workflow. DLAE configuration files (JSON format) can be loaded or saved from the File menu. Training, validation, and testing images and annotations can be loaded using an interface in the Data menu. The format of the datasets for the various DL techniques are described in <xref rid="S5" ref-type="sec">Section 2.3</xref>. The View menu allows users to view all of the active parameter states of the GUI. The Layers and Options menu provide a majority of the core functions for building and training DL models, respectively. They contain the GUI objects for building the layer configurations, specifying the loss function for training, applying transfer learning from networks pre-trained on ImageNet, choosing the optimizer, defining the training configurations, specifying metrics to monitor during the training process, and specifying what type of information to save from a training session. The Tools menu contains the functions for deleting a model (or generator and discriminator) to start over, or open the most recently saved TensorBoard log in the user’s default web browser. The Run menu contains the command to run the engine based on the current GUI state. The Help menu takes the user to the repository where DLAE is hosted, as well displays any errors that may be encountered during engine execution. Finally, in addition to hosting the submenu, the GUI home screen provides a simple means to load in a number of prebuilt applications, or start to build a custom model from scratch.</p>
    </sec>
    <sec id="S5">
      <label>2.3.</label>
      <title>Input data formats</title>
      <p id="P14">Input images and annotation pairs are loaded into DLAE with a consistent format. For planar (two-dimensional) images, the input format is (<italic>n</italic><sub>imgs</sub>, <italic>n</italic><sub>rows</sub>, <italic>n</italic><sub>cols</sub>, <italic>n</italic><sub>chans</sub>), in which <italic>n</italic><sub>imgs</sub> is the number of images in the data set, <italic>n</italic><sub>rows</sub> is the number of rows in the images, <italic>n</italic><sub>cols</sub> is the number of columns in the images, and <italic>n</italic><sub>chans</sub> is the number of image channels. For volumetric (three-dimensional) images, the input format is (<italic>n</italic><sub>imgs</sub>, <italic>n</italic><sub>rows</sub>, <italic>n</italic><sub>cols</sub>, <italic>n</italic><sub>slices</sub>, <italic>n</italic><sub>chans</sub>), in which <italic>n</italic><sub>slices</sub> is the number of slices in the image volumes and the other parameters are the same as those for planar images.</p>
      <p id="P15">The annotation format is specific to the type of DL model developed. For classification tasks with CNNs, the annotations are a vector of size (<italic>n</italic><sub>imgs</sub>,), with integers in sequentially increasing order defining the object class in each corresponding image. These classes can be converted to categorical data in DLAE if desired. For regression tasks with CNNs, the annotations have the format (<italic>n</italic><sub>imgs</sub>, <italic>n</italic>coords), in which <italic>n</italic>coords is the number of coordinates to fit in the regression. The annotations for FCNs and GANs have the same shape as the input images, allowing for pixel-wise mapping from input images to output predictions. Finally, the annotations for BBDs have a stricter requirement; they are a vector of size (<italic>n</italic><sub>imgs</sub>,), with each entry containing an array of size (<italic>n</italic><sub>box,i</sub>, 5) box coordinates and object classes contained within the image, where <italic>n</italic><sub>box,i</sub> is the number of ground truth boxes in image i <italic>ϵ</italic> [1, <italic>n</italic><sub>imgs</sub>].</p>
      <p id="P16">The images and annotations should be saved as a pair of HDF5 files. Specifically, the images should be saved as a single data set in one file, and the corresponding annotations should be saved as a single data set in the other. The exception to this format is the pair of images and annotations for BBDs. Both the images should be saved as separate data sets with the same data set name in each file. Example datasets can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/jeremiahws/dlae/tree/master/datasets">https://github.com/jeremiahws/dlae/tree/master/datasets</ext-link> to help users understand the data shapes and structures.</p>
    </sec>
    <sec id="S6">
      <label>2.4.</label>
      <title>Prebuilt DL models</title>
      <p id="P17">A variety of DL architectures have been developed that are compatible with medical imaging applications. Users may either create a custom application with a user-defined layer configuration via GUI controls or load a prebuilt application with a layer configuration reported in the literature. Prebuilt CNN applications currently incorporated are: LeNet5 [<xref rid="R64" ref-type="bibr">64</xref>], AlexNet [<xref rid="R2" ref-type="bibr">2</xref>] and other CNNs that were developed on the ImageNet dataset [<xref rid="R4" ref-type="bibr">4</xref>–<xref rid="R14" ref-type="bibr">14</xref>]. FCN-based models conforming to the UNet formalism (an encoder and decoder that both use convolutions with or without skip connections between the encoder and decoder) [<xref rid="R40" ref-type="bibr">40</xref>,<xref rid="R41" ref-type="bibr">41</xref>] are also prebuilt for the user. These include FCNs that use one of the CNNs developed for image classification on ImageNet as the convolutional encoder, with a series of transpose or resize convolutions [<xref rid="R65" ref-type="bibr">65</xref>] as the decoders to upsample the encoding back to the input image size. GANs in the literature that are currently incorporated in DLAE include paired image-to-image translation conditional GANs (pix2pix) [<xref rid="R54" ref-type="bibr">54</xref>] and unpaired image-to-image cycle-consistent GANs (cycleGAN) [<xref rid="R55" ref-type="bibr">55</xref>]. Users can either load the original architectures reported in the literature or build their own custom generators and discriminators using GUI controls. Finally, single-shot detectors (SSDs) are currently incorporated as the BBDs in DLAE [<xref rid="R59" ref-type="bibr">59</xref>]. Users can load a prebuilt SSD architecture reported in the literature or build new encoders with custom predictor layers at different resolution scales via hook layers.</p>
    </sec>
    <sec id="S7">
      <label>2.5.</label>
      <title>Building custom DL models</title>
      <p id="P18">Custom DL models can be built using DLAE. All DL models, both custom and prebuilt, start with definition of the input image shape. Serial layer configurations can then be built upon the input using GUI controls. Both skip connections (for FCNs) and hook layer connections (for BBDs) can be injected at the desired resolution scales. GANs have two neural networks competing against each other, and users can specify separate custom layer configurations for the generator and discriminator. In addition, users can define a custom set of training options, learning rate schedules, and the desired loss function and optimizer to train the layer configuration for a given application.</p>
    </sec>
    <sec id="S8">
      <label>2.6.</label>
      <title>Transfer learning</title>
      <p id="P19">DLAE primarily makes use of convolutional encoders pretrained on the ImageNet data set for transfer learning. Users can employ these pretrained convolutional encoders to train a new image classifier, an FCN for semantic segmentation or image regression, or a new SSD. We added skip connections to the pretrained convolutional encoders, which give users the option of concatenating features from the encoder with the decoder at equivalent resolution scales when creating a custom FCN (i.e., in a UNet fashion). We also added hook connections to the pretrained convolutional encoders, which give users the ability to apply the pretrained convolutional encoders to new SSD applications.</p>
    </sec>
    <sec id="S9">
      <label>2.7.</label>
      <title>Objective functions for training DL models</title>
      <p id="P20">The parameters in DL algorithms are trained through optimization of some loss function. Thus, appropriate selection of the loss function is one of the most important aspects of successfully training a DL model. The loss function used to train a deep neural network depends on the type of inference task and may be application-specific. DLAE currently has seven loss functions that can be supplemented in the future:
<list list-type="order" id="L2"><list-item><p id="P21">Cross-entropy: for image classification with CNNs and pixel-wise classification with FCNs.</p></list-item><list-item><p id="P22">Mean squared error: for image regression with CNNs and pixel-wise regression with FCNs.</p></list-item><list-item><p id="P23">Mean absolute error: for image regression with CNNs and pixel-wise regression with FCNs.</p></list-item><list-item><p id="P24">Tversky index: for image segmentation with FCNs [<xref rid="R66" ref-type="bibr">66</xref>].</p></list-item><list-item><p id="P25">Conditional GAN + L1 loss: for paired image-to-image translation with conditional adversarial networks [<xref rid="R54" ref-type="bibr">54</xref>].</p></list-item><list-item><p id="P26">Adversarial + cycle consistency loss: for unpaired image-to-image translation with cycle-consistent adversarial networks [<xref rid="R55" ref-type="bibr">55</xref>].</p></list-item><list-item><p id="P27">SSD loss: for object detection with SSDs [<xref rid="R59" ref-type="bibr">59</xref>].</p></list-item><list-item><p id="P28">Jaccard distance: for image segmentation with FCNs.</p></list-item><list-item><p id="P29">Focal loss [<xref rid="R67" ref-type="bibr">67</xref>]: for image segmentation with FCNs.</p></list-item><list-item><p id="P30">Soft dice: for image segmentation with FCNs.</p></list-item></list></p>
    </sec>
    <sec id="S10">
      <label>2.8.</label>
      <title>Additional features of DLAE</title>
      <p id="P31">Other useful features are incorporated in DLAE. First, users can save TensorBoard summaries during training. Second, users can save model checkpoints at a specified frequency, data from monitors of the training history, and the final trained model. Third, options are available to perform data preprocessing and augmentation. Users can either perform these preprocessing steps before or have DLAE perform them. Finally, multiple GPU training is possible if they are available on the computer.</p>
    </sec>
  </sec>
  <sec id="S11">
    <label>3.</label>
    <title>Configuration files</title>
    <p id="P32">Once a DL model has been created, it can be saved to a configuration file and shared by project collaborators. Given a configuration file, DLAE allows a user to view, modify, retrain, and/or make predictions using a DL model created by a collaborator. Configuration files contain all of the specifications that the engine analyzes. They include the layer configurations for a model, training specifications (batch size, epochs, etc.), which optimizer to use, learning rate schedules, the desired loss function, and all other engine configurations. The structure of a configuration file is shown in <xref rid="F2" ref-type="fig">Fig. 2</xref>.</p>
    <p id="P33">Configuration files decouple the GUI from the engine, which enables three primary functionalities (<xref rid="F3" ref-type="fig">Fig. 3</xref>). First, after a custom model is built using the GUI and a training session is initialized, a configuration structure is passed to the engine to be executed (<xref rid="S12" ref-type="sec">Section 4</xref>).</p>
    <p id="P34">Second, configuration files enable the instantiation of several automated training sessions using an experiment generator. This is particularly useful for exploring several different DL model constructs, whereby an experiment generator can spawn several model experiments to identify the superior model for a given application. For example, users can modify layers in the configuration file of a template model to determine how certain layers impact the performance of the model for a given application, such as in ablation studies. This functionality is also useful for hyperparameter searches. For example, users can create a series of configuration files that iterate over a set of hyperparameters for a given layer configuration and pass them to the engine for automatic processing.</p>
    <p id="P35">Third, configuration files enable automated inference, which facilitates the deployment of a trained DL model. When a configuration file is passed to the engine in inference mode, the engine performs the specified image-preprocessing steps, loads a trained model from a specified file path, and makes predictions on the images with the specified model. The inferences are stored in a file location defined in the configuration file.</p>
  </sec>
  <sec id="S12">
    <label>4.</label>
    <title>Processing engine</title>
    <p id="P36">The engine in DLAE is run solely from engine configurations that can either be passed as structures from the GUI to the engine or as configuration files to the engine as inputs. Several elements are constructed to prepare for the training or inference session. All of these elements are structured into individual class objects, together making up the EngineConfigurations class that is processed by the engine. The following elements make up the EngineConfigurations class and are described in <xref rid="F4" ref-type="fig">Fig. 4</xref>:
<list list-type="order" id="L4"><list-item><p id="P37">Dispatcher class: defines the DL action to be performed and the DL technique to be applied.</p></list-item><list-item><p id="P38">Preprocessing class: defines the routines to apply to the data before they flow into the DL model.</p></list-item><list-item><p id="P39">TrainData, ValidationData, and TestData classes: contain path locations where the images and annotations will be loaded from and prepared based on the specifications in the Preprocessing class.</p></list-item><list-item><p id="P40">LearningRate class: defines the learning rate and learning rate schedule to be used by the optimizer during model training.</p></list-item><list-item><p id="P41">Optimizer class: defines the optimizer to be used during model training.</p></list-item><list-item><p id="P42">LossFunction class: defines the loss function to be used during model training.</p></list-item><list-item><p id="P43">Monitors class: defines metrics to be monitored during a model training session (e.g., accuracy).</p></list-item><list-item><p id="P44">TrainingOptions class: defines a number of parameters related to a training session (e.g., batch size, number of epochs).</p></list-item><list-item><p id="P45">Saver class: defines data to be saved during training and their associated path locations (e.g., model checkpoints and the locations where they should be stored).</p></list-item><list-item><p id="P46">Loader class: defines the path to a trained model to be loaded for inference or a model checkpoint to be loaded to continue training (or retrain a model on a new data set).</p></list-item><list-item><p id="P47">Layers class: contains lists of layers that define the DL model to be constructed. They include separate lists for serial models, for a GAN generator, and for a GAN discriminator.</p></list-item><list-item><p id="P48">Callbacks class: defines a set of Keras callbacks used to train a model (e.g., save TensorBoard logs). This class is constructed from the Saver, LearningRate, and TrainingOptions classes.</p></list-item><list-item><p id="P49">Augmentation class: defines the augmentation operation to apply to the training data.</p></list-item></list></p>
  </sec>
  <sec id="S13">
    <label>5.</label>
    <title>Illustrative examples</title>
    <p id="P50">Some of the primary DL functionalities in DLAE are image classification, image regression, image segmentation, image synthesis, object detection, ablation studies, hyperparameter searches, and DL model deployment. We created one of each of these applications using DLAE to demonstrate some of its core functionalities. Unless otherwise stated, the example applications below were developed using an NVIDIA DGX-1 workstation. Approval from The University of Texas MD Anderson Cancer Center Institutional Review Board was received for all of these applications. Applications 5.1–5.4 are shown in <xref rid="F5" ref-type="fig">Fig. 5</xref>.</p>
    <sec id="S14">
      <label>5.1.</label>
      <title>Radioactive seed identification for MRI-assisted radiosurgery</title>
      <p id="P51">Radioactive seed identification after low-dose-rate brachytherapy of prostate cancer is essential for postimplant quality assurance. Currently, a CT and/or MRI is acquired after seed implantation to verify the radioactive seed locations and compute the radiation dose delivered to the prostate and surrounding normal tissues; a medical dosimetrist manually identifies the radioactive seeds. To demonstrate the image classification and regression functionalities of DLAE, it was used to create a sliding-window CNN to automatically perform radioactive seed identification and localization using postimplant prostate MRI [<xref rid="R20" ref-type="bibr">20</xref>,<xref rid="R21" ref-type="bibr">21</xref>]. The MRI acquisitions were performed with a 3D fully balanced steady-state free precession pulse sequence. Typical scan parameters were: TR/TE/FA = 5.29 ms/2.31 ms/52<sup>◦</sup>, readout bandwidth = 560 Hz/pixel, field of view 15 cm,voxel dimensions = 0.59 × 0.59 × 1.20 mm (interpolated to 0.29 × 0.29 × 1.20 mm), and total scan time of 4–5 min. A sliding-windowalgorithm was written to scan the prostate in three-dimensional subwindows. A chain of CNNs was trained to perform radioactive seed identification and localization tasks. One CNN was trained to classify seed subwindows from background subwindows through cross-entropy minimization. Another CNN was trained to localize the centroids of the radioactive seeds in the seed subwindows through mean-squared-error minimization. The seed centroid locations from the seed subwindows were mapped back to the global image. The CNNs were trained on subwindows extracted from 18 patients. For 20 test patients, the mean ± one standard deviation precision was 96.9% ± 1.9%, and the recall was 98.0% ± 2.1% (<xref rid="F5" ref-type="fig">Fig. 5</xref>). The inference time was about 2 min per patient. The root mean square error of the seed locations was 0.18 mm ± 0.04 mm.</p>
    </sec>
    <sec id="S15">
      <label>5.2.</label>
      <title>Pelvic anatomy contouring in prostate brachytherapy MRI</title>
      <p id="P52">The second component of postimplant quality assurance after low-dose-rate brachytherapy is to contour the prostate and its surrounding anatomical structures, including the rectum, seminal vesicles (SV), external urinary sphincter (EUS), and bladder. With knowledge of the organ contours and radioactive seed locations, objectively quantifying the radiation doses delivered to the prostate and these structures using a commercial treatment planning system is possible. Currently, these structures are manually contoured by radiation oncologists. To demonstrate the segmentation and ablation study/hyperparameter searching functionalities of DLAE, we trained six FCNs with different pretrained convolutional encoders and with two objective functions (cross-entropy and Tversky) to perform multiorgan segmentation in postimplant prostate MRI [<xref rid="R68" ref-type="bibr">68</xref>,<xref rid="R69" ref-type="bibr">69</xref>]. The MRI acquisitions were performed with the same technique as in example application 5.1, and detailed information about the exact acquisition can be found in previous work [<xref rid="R70" ref-type="bibr">70</xref>]. The classification accuracy of the pretrained convolutional encoders on ImageNet correlated positively with pixel-wise classification on postimplant prostate brachytherapy MR image segmentation (Tversky loss: y = 0.0949x + 0.9067, R<sup>2</sup> = 0.8449; crossentropy loss: y = 0.1134x + 0.8908, R<sup>2</sup> = 0.8586) (<xref rid="F5" ref-type="fig">Fig. 5</xref>). Overall, use of Tversky loss (with <italic>α</italic> = <italic>β</italic> = 0.5 → Dice loss) with an Xception encoder produced greater segmentation accuracy across all the networks than did that of cross-entropy. All model training sessions were performed automatically and in succession using an experiment generator and DLAE. The experiment generator used to perform this study can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/jeremiahws/dlae/blob/master/fcn_experiment_generator.py">https://github.com/jeremiahws/dlae/blob/master/fcn_experiment_generator.py</ext-link>.</p>
    </sec>
    <sec id="S16">
      <label>5.3.</label>
      <title>Brain perfusion synthesis in brain tumor imaging</title>
      <p id="P53">Dynamic contrast enhanced (DCE)and dynamic susceptibility contrast (DSC)-MRI are perfusion imaging techniques that provide valuable information for diagnosis and evaluation of treatment response in patients with brain cancer [<xref rid="R71" ref-type="bibr">71</xref>]. DCE-MRI is best suited for evaluating vessel permeability, whereas DSC-MRI provides robust relative cerebral blood volume (rCBV) imaging. Blood plasma volume can be measured using pharmacokinetic modeling of DCE-MRI. However, the quality is generally inferior to the rCBV obtained from DSC-MRI due to limitations in the sensitivity and temporal resolution of DCE-MRI. Previous studies demonstrated that these two perfusion techniques are complementary and that using both of them provided better diagnostic performance than did using either one alone [<xref rid="R72" ref-type="bibr">72</xref>]. However, acquiring both DCE- and DSC-MRI in clinical practice requires two gadolinium-based contrast agent injections, which increase the total dose of the agent. To demonstrate the image-synthesis functionality of DLAE, we trained a conditional adversarial network to map multi-time-point DCE-MRI to rCBV maps computed from DSC-MRI [<xref rid="R73" ref-type="bibr">73</xref>] (<xref rid="F5" ref-type="fig">Fig. 5</xref>). The MRI acquisitions were performed on a GE 3T MR750 scanner using an 8-channel brain coil. DCE-MRI was performed by using a 3D spoiled gradient echo sequence with TR/TE/FA = 3.6 ms/1.3 ms/30<sup>◦</sup>, matrix = 256 × 160, 20 slices, slice thickness = 5 mm, 60 phases, and temporal = resolution 5.4 s. DSC-MRI was performed using a gradient-echo EPI sequence with TR/TE/FA = 1500 ms/25 ms/90<sup>◦</sup>, matrix 128 × 128, slice thickness and locations matched with DCE, 60=phases, and temporal resolution = 1.5 s. The tumor:white matter ratio obtained for the synthetic rCBV was significantly correlated with that for the real rCBV obtained via DSC-MRI in the test patient group.</p>
    </sec>
    <sec id="S17">
      <label>5.4.</label>
      <title>Brain metastases detection in MRI</title>
      <p id="P54">Presently, brain cancer metastases are manually identified on MR images for stereotactic radiation treatment by radiologists/radiation oncologists, which can be time-consuming. To demonstrate the object-detection functionality of DLAE, a BBD was trained to detect brain cancer metastases. Three-dimensional postcontrast T1-weighted gradient echo images of patients undergoing Gamma Knife radiosurgery were acquired. The MRI acquisitions were performed with a 3D spoiled gradient echo sequence. Typical scan parameters were: TR/TE/FA 6.9 ms/2.6 ms/12<sup>◦</sup>, NEX = 2, matrix size = 256 × 256, FOV 24 × 24 voxel size 0.94 × 0.94 × 1.00 mm. The metastases were manually identified by a neuroradiologist, and bounding boxes were constructed around each metastasis. We constructed a multibox SSD with 16 convolutional layers and predictor layers at six resolution scales. Inputs to the network were axial slices of the MR images, and the outputs were the bounding box coordinates and associated confidences of the classifier. An average sensitivity rate of 73.5% and FP/TP ratio of 15.8 were achieved across all sizes of brain metastases and all confidence thresholds. For metastases at least 6 mm in size, the average sensitivity was 94.5%, and the FP/TP ratio was 7.9 across all confidence thresholds. Overall, the SSD detected almost all brain metastases 6 mm or larger with greater than 90% confidence and nearly half of the metastases smaller than 6 mm with low specificity (<xref rid="F5" ref-type="fig">Fig. 5</xref>).</p>
    </sec>
    <sec id="S18">
      <label>5.5.</label>
      <title>DL model integration into clinical workflows</title>
      <p id="P55">Although developing new DL algorithms in medical imaging for research purposes is important, one ultimate end goal is to make use of the algorithms in a clinical setting. To demonstrate the DL model deployment functionality of DLAE, the sliding-window CNN for radioactive seed identification and the FCN for anatomy segmentation were integrated in a commercial software package used at our institution (MIM Software) (<xref rid="F6" ref-type="fig">Fig. 6</xref>) [<xref rid="R74" ref-type="bibr">74</xref>].</p>
    </sec>
  </sec>
  <sec id="S19">
    <label>6.</label>
    <title>Impact and discussion</title>
    <p id="P56">DL has become increasingly popular in medical imaging research, and a variety of new medical imaging applications using DL techniques have become possible due to advances in the computer vision field. We present herein a comprehensive software package that encapsulates several of the most popular DL architectures reported in the literature, making explicit use of wellmaintained open-source software libraries. With sufficiently curated data sets, DLAE users can explore a variety of DL techniques to develop new applications in medical imaging. We sought to tailor DLAE to applications involving medical images and allow users to train new models, refine models through ablation studies and hyperparameter searches, retrain models across institutions with different data sets, and incorporate trained DL models into clinical workflows within a single software framework.</p>
    <p id="P57">We created DLAE as a graphical approach to DL model development as opposed to being explicitly programmatic. This has three benefits. First, we abstracted the tasks of building a new DL model to a series of button clicks rather than explicit computer programming. This makes possible building a DL model quickly across a range of DL techniques. Second, because all of its programmatic implementation details are hidden from the user DLAE may help reduce the learning curve for new DL users and lend itself to expanded collaborations with clinical colleagues who may not have much programming experience. Third, prebuilt DL applications can be loaded and modified with a few button clicks. This enables rapid prototyping of DL models with visual feedback to the user.</p>
    <p id="P58">Configuration files streamline and automate several aspects of a DL workflow, which facilitate rapid prototyping, development, and implementation of DL algorithms in a clinical environment. Collaborations among research groups on a given application are facilitated by sharing configuration files. Collaborators on a project can share configuration files to load, train, or modify and retrain their DL model. Sharing configuration files, which have a consistent format, may easier than maintaining code from multiple collaborators who have their own styles of coding. Moreover, automation of DL model training and inference with configuration files may enable DLAE to serve as a DL engine in larger software frameworks.</p>
    <p id="P59">New, more powerful DL architectures continue to be developed in the computer vision field, and the performance of DL models continues to increase. A widely known example is the ImageNet challenge, in which a number of new DL architectures were developed in the literature. As such, these developments demonstrated that not all neural networks perform equally, and developers of DL applications should explore the space of DL solutions and how different network architectures and hyperparameters impact the performance of their models so that they can be confident they have created the highest performing models for a given application. Our example with 12 DL models for segmentation in prostate MRI demonstrated that classification accuracy was positively correlated with pixel-wise classification accuracy in MRI when using a transfer learning approach with convolutional encoders pretrained on ImageNet. DLAE makes it possible to rapidly and automatically perform these types of experiments.</p>
    <p id="P60">Although we did not present any imaging informatics applications, DLAE lends itself to supporting quality control (QC) programs in medical imaging. Routine QC testing of medical imaging systems has produced large archives of annotated QC data across medical institutions. Imaging physicists can develop DL models with DLAE to perform daily clinical tasks, find the best model for a given QC application, and potentially deploy the application in a QC software program. Moreover, a move toward clinical image-based measurements as opposed to phantom-based measurements for medical imaging informatics applications is underway [<xref rid="R75" ref-type="bibr">75</xref>–<xref rid="R79" ref-type="bibr">79</xref>]. DLAE can serve as a tool to incorporate DL into these efforts to make use of the many benefits of modern DL techniques.</p>
    <p id="P61">Because many tasks in a DL workflow can be automated with DLAE using configuration files, DLAE can serve as a support application in a much larger clinical software framework. Some imaging physics groups have constructed in-house informatics frameworks that store, organize, and process large quantities of patient images coupled with their medical data [<xref rid="R80" ref-type="bibr">80</xref>]. With some additional software engineering, DLAE could be incorporated into these software packages to serve as the engine for DL applications.</p>
    <p id="P62">Although the example applications presented were all related to MRI, this is not a requirement. Images acquired from the numerous imaging modalities available in medical imaging can be used to develop DL algorithms with DLAE. Additionally, DL techniques have become popular in several imaging domains. Some examples are optical space imaging, microscopy, art, astronomy, and satellite ground imaging. We originally designed DLAE for imaging applications in medical imaging, but other types of imaging domains could benefit from DLAE. With an approprately curated data set containing images and annotations, DLAE can be used to train algorithms to map input images to a desired output annotation.</p>
    <p id="P63">The present work has some limitations. First, recurrent neural networks, which can incorporate temporal information into CNNs, are not yet implemented in DLAE. These are a planned update for the next rendition of DLAE. A second limitation is that data curation and model evaluation must be done outside of DLAE. Institutions have unique ways of storing data, and directory structures and file formats are inconsistent across users. Requiring a consistent standard for loading data into DLAE (HDF5 format) was more appropriate than having users conform to a specific data directory structure, which could be confusing and problematic.</p>
  </sec>
  <sec id="S20">
    <label>7.</label>
    <title>Conclusions</title>
    <p id="P64">We developed DLAE for end-to-end design, development, and deployment of DL models in medical imaging. A variety of new DL applications in medical physics can be developed and deployed using DLAE and existing institutional data sets. DLAE supports custom model building and contains several prebuilt applications that can be easily loaded by the user. Configuration files decouple the GUI from the DL engine, which enables automation of both training and inference sessions without any user input. DLAE can be integrated into clinical workflows to make patientspecific predictions using patients’ medical images to inform medical decisions. Although familiarity with statistical learning theory is still required for its use, DLAE has a coherent design-todeployment architecture for DL applications in medical imaging, requiring substantially less software development expertise than do existing DL approaches.</p>
  </sec>
</body>
<back>
  <ack id="S21">
    <title>Acknowledgments</title>
    <p id="P65">Jeremiah Sanders would like to acknowledge the generous donors of the Pauline Altman-Goldstein Foundation Discovery Fellowship. We thank MD Anderson Research Medical Library Scientific Publication Services for their help in editing this paper. We also acknowledge the Github repository at <ext-link ext-link-type="uri" xlink:href="https://github.com/pierluigiferrari/ssd_keras">https://github.com/pierluigiferrari/ssd_keras</ext-link>, from which we adapted some of our code for implementing SSDs in DLAE.</p>
  </ack>
  <fn-group>
    <fn fn-type="COI-statement" id="FN1">
      <p id="P66">Declaration of competing interest</p>
      <p id="P67">We wish to draw the attention of the Editor to the following facts, which may be considered as potential conflicts of interest, and to significant financial contributions to this work. The nature of potential conflict of interest is described below: Steven J. Frank is a co-founder of C4 Imaging and has ownership interests in this company, and he holds U.S. and international patents of the C4 seed marker technology. Jingfei Ma holds U.S. patents licensed to Siemens Healthineers and GE Healthcare and is a consultant for C4 Imaging.</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <label>[1]</label>
      <mixed-citation publication-type="journal"><name><surname>LeCun</surname><given-names>Y</given-names></name>, <name><surname>Bengio</surname><given-names>Y</given-names></name>, <name><surname>Hinton</surname><given-names>G</given-names></name>. <article-title>Deep learning</article-title>. <source>Nature</source><year>2015</year>;<volume>521</volume>:<fpage>436</fpage>–<lpage>44</lpage>. <pub-id pub-id-type="doi">10.1038/nature14539</pub-id><comment>.</comment><pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="R2">
      <label>[2]</label>
      <mixed-citation publication-type="journal"><name><surname>Krizhevsky</surname><given-names>A</given-names></name>, <name><surname>Sutskever</surname><given-names>I</given-names></name>, <name><surname>Hinton</surname><given-names>GE</given-names></name>. <article-title>Imagenet classification with deep convolutional neural networks</article-title>. <source>Adv Neural Inf Process Syst</source><year>2012</year>;<fpage>25</fpage>. <pub-id pub-id-type="doi">10.1145/3065386</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R3">
      <label>[3]</label>
      <mixed-citation publication-type="web"><name><surname>van Ginneken</surname><given-names>B</given-names></name>, <name><surname>Kerkstra</surname><given-names>S</given-names></name>, <name><surname>Meakin</surname><given-names>J</given-names></name>. <source>Grand challenges in biomedical image analysis</source>. <year>2019</year>, <comment><ext-link ext-link-type="uri" xlink:href="https://grand-challenge.org">https://grand-challenge.org</ext-link> Accessed</comment><date-in-citation>15.09.19</date-in-citation>.</mixed-citation>
    </ref>
    <ref id="R4">
      <label>[4]</label>
      <mixed-citation publication-type="web"><name><surname>Simonyan</surname><given-names>K</given-names></name>, <name><surname>Zisserman</surname><given-names>A</given-names></name>. <source>Very deep convolutional networks for large-scale image recognition</source>. <year>2014</year>, <comment>arXiv:1409.1556.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R5">
      <label>[5]</label>
      <mixed-citation publication-type="web"><name><surname>Szegedy</surname><given-names>C</given-names></name>, <name><surname>Liu</surname><given-names>W</given-names></name>, <name><surname>Jia</surname><given-names>Y</given-names></name>, <name><surname>Sermanet</surname><given-names>P</given-names></name>, <name><surname>Reed</surname><given-names>S</given-names></name>, <name><surname>Anguelov</surname><given-names>D</given-names></name>, <name><surname>Erhan</surname><given-names>D</given-names></name>, <name><surname>Vanhoucke</surname><given-names>V</given-names></name>, <name><surname>Rabinovich</surname><given-names>R</given-names></name>. <source>Going deeper with convolutions</source>. <year>2014</year>, <comment>arXiv: 1409.4842.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1409.4842">https://arxiv.org/abs/1409.4842</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R6">
      <label>[6]</label>
      <mixed-citation publication-type="web"><name><surname>Chollet</surname><given-names>F</given-names></name><source>Xception: Deep learning with depthwise separable convolutions</source>. <year>2016</year>, <comment>arXiv:1610.02357.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1610.02357">https://arxiv.org/abs/1610.02357</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R7">
      <label>[7]</label>
      <mixed-citation publication-type="web"><name><surname>He</surname><given-names>K</given-names></name>, <name><surname>Zhang</surname><given-names>X</given-names></name>, <name><surname>Ren</surname><given-names>S</given-names></name>, <name><surname>Sun</surname><given-names>J</given-names></name>. <source>Deep residual learning for image recognition</source>. <year>2015</year>, <comment>arXiv:1512.03385.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R8">
      <label>[8]</label>
      <mixed-citation publication-type="web"><name><surname>He</surname><given-names>K</given-names></name>, <name><surname>Zhang</surname><given-names>X</given-names></name>, <name><surname>Ren</surname><given-names>S</given-names></name>, <name><surname>Sun</surname><given-names>J</given-names></name>. <source>Identity mappings in deep residual networks</source>. <year>2016</year>, <comment>arXiv:1603.05027.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1603.05027">https://arxiv.org/abs/1603.05027</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R9">
      <label>[9]</label>
      <mixed-citation publication-type="web"><name><surname>Xie</surname><given-names>S</given-names></name>, <name><surname>Girshick</surname><given-names>R</given-names></name>, <name><surname>Dollár</surname><given-names>P</given-names></name>, <name><surname>Tu</surname><given-names>Z</given-names></name>, <name><surname>He</surname><given-names>K</given-names></name>. <source>Aggregated residual transformations for deep neural networks</source>. <year>2016</year>, <comment>arXiv:1611.05431.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1611.05431">https://arxiv.org/abs/1611.05431</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R10">
      <label>[10]</label>
      <mixed-citation publication-type="web"><name><surname>Szegedy</surname><given-names>C</given-names></name>, <name><surname>Vanhoucke</surname><given-names>V</given-names></name>, <name><surname>Ioffe</surname><given-names>S</given-names></name>, <name><surname>Shlens</surname><given-names>J</given-names></name>, <name><surname>Wojna</surname><given-names>Z</given-names></name>. <source>Rethinking the inception architecture for computer vision</source>. <year>2015</year>, <comment>arXiv:1512.0056.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1512.0056">https://arxiv.org/abs/1512.0056</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R11">
      <label>[11]</label>
      <mixed-citation publication-type="web"><name><surname>Szegedy</surname><given-names>C</given-names></name>, <name><surname>Ioffe</surname><given-names>S</given-names></name>, <name><surname>Vanhoucke</surname><given-names>V</given-names></name>, <name><surname>Alemi</surname><given-names>A</given-names></name>. <source>Inception-v4, inception-resnet and the impact of residual connections on learning</source>. <year>2016</year>, <comment>arXiv:1602. 07261.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1602.07261">https://arxiv.org/abs/1602.07261</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R12">
      <label>[12]</label>
      <mixed-citation publication-type="web"><name><surname>Howard</surname><given-names>AG</given-names></name>, <name><surname>Zhu</surname><given-names>M</given-names></name>, <name><surname>Chen</surname><given-names>B</given-names></name>, <name><surname>Kalenichenko</surname><given-names>D</given-names></name>, <name><surname>Wang</surname><given-names>W</given-names></name>, <name><surname>Weyand</surname><given-names>T</given-names></name>, <name><surname>Andreetto</surname><given-names>M</given-names></name>, <name><surname>Adam</surname><given-names>H</given-names></name>. <source>MobileNets: Efficient convolutional neural networks for mobile vision applications</source>. <year>2017</year>, <comment>arXiv:1704.04861.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1704.04861">https://arxiv.org/abs/1704.04861</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R13">
      <label>[13]</label>
      <mixed-citation publication-type="web"><name><surname>Huang</surname><given-names>G</given-names></name>, <name><surname>Liu</surname><given-names>Z</given-names></name>, <name><surname>van der Maaten</surname><given-names>L</given-names></name>, <name><surname>Weinberger</surname><given-names>KQ</given-names></name>. <source>Densely connected convolutional networks</source>. <year>2016</year>, <comment>arXiv:1608.06993.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1608.06993">https://arxiv.org/abs/1608.06993</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R14">
      <label>[14]</label>
      <mixed-citation publication-type="web"><name><surname>Sandler</surname><given-names>M</given-names></name>, <name><surname>Howard</surname><given-names>A</given-names></name>, <name><surname>Zhu</surname><given-names>M</given-names></name>, <name><surname>Zhmoginov</surname><given-names>A</given-names></name>, <name><surname>Chen</surname><given-names>LC</given-names></name>. <source>Mobilenetv2: Inverted residuals and linear bottlenecks</source>. <year>2018</year>, <comment>arXiv:1801.04381.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1801.04381">https://arxiv.org/abs/1801.04381</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R15">
      <label>[15]</label>
      <mixed-citation publication-type="web"><name><surname>Long</surname><given-names>J</given-names></name>, <name><surname>Shelhamer</surname><given-names>E</given-names></name>, <name><surname>Darrell</surname><given-names>T</given-names></name>. <source>Fully convolutional networks for semantic segmentation</source>. <year>2014</year>, <comment>arXiv:1411.4038.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1411.4038">https://arxiv.org/abs/1411.4038</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R16">
      <label>[16]</label>
      <mixed-citation publication-type="journal"><name><surname>Tajbakhsh</surname><given-names>N</given-names></name>, <name><surname>Shin</surname><given-names>JY</given-names></name>, <name><surname>Gurudu</surname><given-names>SR</given-names></name>, <name><surname>Hurst</surname><given-names>RT</given-names></name>, <name><surname>Kendall</surname><given-names>CB</given-names></name>, <name><surname>Gotway</surname><given-names>MB</given-names></name>, <name><surname>Liang</surname><given-names>J</given-names></name>. <article-title>Convolutional neural networks for medical image analysis: Full training or fine tuning?</article-title>. <source>IEEE Trans Med Imaging</source><year>2016</year>;<volume>35</volume>(<issue>5</issue>):<fpage>1299</fpage>–<lpage>312</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2016.2535302</pub-id><comment>.</comment><pub-id pub-id-type="pmid">26978662</pub-id></mixed-citation>
    </ref>
    <ref id="R17">
      <label>[17]</label>
      <mixed-citation publication-type="journal"><name><surname>Esteva</surname><given-names>A</given-names></name>, <name><surname>Kuprel</surname><given-names>B</given-names></name>, <name><surname>Novoa</surname><given-names>RA</given-names></name>, <name><surname>Ko</surname><given-names>J</given-names></name>, <name><surname>Swetter</surname><given-names>SM</given-names></name>, <name><surname>Blau</surname><given-names>HM</given-names></name>, <name><surname>Thrun</surname><given-names>S</given-names></name>. <article-title>Dermatologist-level classification of skin cancer with deep neural networks</article-title>. <source>Nature</source><year>2017</year>;<volume>542</volume>:<fpage>115</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1038/nature21056</pub-id><comment>.</comment><pub-id pub-id-type="pmid">28117445</pub-id></mixed-citation>
    </ref>
    <ref id="R18">
      <label>[18]</label>
      <mixed-citation publication-type="confproc"><name><surname>Li</surname><given-names>Q</given-names></name>, <name><surname>Cai</surname><given-names>W</given-names></name>, <name><surname>Wang</surname><given-names>X</given-names></name>, <name><surname>Zhou</surname><given-names>Y</given-names></name>, <name><surname>Feng</surname><given-names>DD</given-names></name>, <name><surname>Chen</surname><given-names>M</given-names></name>. <article-title>Medical image classification with convolutional neural network</article-title>. In: <conf-name>Proc. 13th int. conf. cont. auto. rob. vis. (ICARCV)</conf-name>. <year>2014</year>, p. <fpage>844</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1109/ICARCV.2014.7064414</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R19">
      <label>[19]</label>
      <mixed-citation publication-type="web"><name><surname>Lévy</surname><given-names>D</given-names></name>, <name><surname>Jain</surname><given-names>A</given-names></name>. <source>Breast mass classification from mammograms using deep convolutional neural networks</source>. <year>2016</year>, <comment>arXiv:1612.00542.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1612.00542">https://arxiv.org/abs/1612.00542</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R20">
      <label>[20]</label>
      <mixed-citation publication-type="journal"><name><surname>Sanders</surname><given-names>JW</given-names></name>, <name><surname>Frank</surname><given-names>SJ</given-names></name>, <name><surname>Kudchadker</surname><given-names>RJ</given-names></name>, <name><surname>Bruno</surname><given-names>TL</given-names></name>, <name><surname>Ma</surname><given-names>J</given-names></name>. <article-title>Development and clinical implementation of seednet: A sliding-window convolutional neural network for radioactive seed identification in MRI-assisted radiosurgery (MARS)</article-title>. <source>Magn Reson Med</source><year>2019</year>;<volume>81</volume>(<issue>6</issue>):<fpage>3888</fpage>–<lpage>900</lpage>. <pub-id pub-id-type="doi">10.1002/mrm.27677</pub-id><comment>.</comment><pub-id pub-id-type="pmid">30737827</pub-id></mixed-citation>
    </ref>
    <ref id="R21">
      <label>[21]</label>
      <mixed-citation publication-type="journal"><name><surname>Sanders</surname><given-names>J</given-names></name>, <name><surname>Frank</surname><given-names>S</given-names></name>, <name><surname>Bruno</surname><given-names>T</given-names></name>, <name><surname>Ma</surname><given-names>J</given-names></name>. <article-title>Seednet for automated detection and localization of radioactive seeds in post-implant MRI: A comparison with and without the use of an endorectal coil for imaging</article-title>. <source>Brachytherapy</source><year>2018</year>;<volume>17</volume>(<issue>4</issue>):<fpage>S80</fpage>–<lpage>1</lpage>. <pub-id pub-id-type="doi">10.1016/j.brachy.2018.04.136</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R22">
      <label>[22]</label>
      <mixed-citation publication-type="journal"><name><surname>Bai</surname><given-names>W</given-names></name>, <name><surname>Sinclair</surname><given-names>M</given-names></name>, <name><surname>Tarroni</surname><given-names>G</given-names></name>, <name><surname>Oktay</surname><given-names>O</given-names></name>, <name><surname>Rajchl</surname><given-names>M</given-names></name>, <name><surname>Vaillant</surname><given-names>G</given-names></name>, <name><surname>Lee</surname><given-names>AM</given-names></name>, <name><surname>Aung</surname><given-names>N</given-names></name>, <name><surname>Lukaschuk</surname><given-names>E</given-names></name>, <name><surname>Sanghvi</surname><given-names>MM</given-names></name>, <etal/><article-title>Automated cardiovascular magnetic resonance image analysis with fully convolutional networks</article-title>. <source>J Cardiovasc Magn Reson</source><year>2018</year>;<volume>20</volume>(<issue>1</issue>):<fpage>65</fpage>. <pub-id pub-id-type="doi">10.1186/s12968-018-0471-x</pub-id><comment>.</comment><pub-id pub-id-type="pmid">30217194</pub-id></mixed-citation>
    </ref>
    <ref id="R23">
      <label>[23]</label>
      <mixed-citation publication-type="journal"><name><surname>Zhu</surname><given-names>Y</given-names></name>, <name><surname>Wei</surname><given-names>R</given-names></name>, <name><surname>Gao</surname><given-names>G</given-names></name>, <name><surname>Ding</surname><given-names>L</given-names></name>, <name><surname>Zhang</surname><given-names>X</given-names></name>, <name><surname>Wang</surname><given-names>X</given-names></name>, <name><surname>Zhang</surname><given-names>J</given-names></name>. <article-title>Fully automatic segmentation on prostate MR images based on cascaded fully convolution network</article-title>. <source>J Magn Reson Imaging</source><year>2018</year>;<volume>49</volume>:<fpage>1149</fpage>–<lpage>56</lpage>. <pub-id pub-id-type="doi">10.1002/jmri.26337</pub-id><comment>.</comment><pub-id pub-id-type="pmid">30350434</pub-id></mixed-citation>
    </ref>
    <ref id="R24">
      <label>[24]</label>
      <mixed-citation publication-type="journal"><name><surname>Jackson</surname><given-names>P</given-names></name>, <name><surname>Hardcastle</surname><given-names>N</given-names></name>, <name><surname>Dawe</surname><given-names>N</given-names></name>, <name><surname>Kron</surname><given-names>T</given-names></name>, <name><surname>Hofman</surname><given-names>MS</given-names></name>, <name><surname>Hicks</surname><given-names>RJ</given-names></name>. <article-title>Deep learning renal segmentation for fully automated radiation dose estimation in unsealed source therapy</article-title>. <source>Front Oncol</source><year>2018</year>;<volume>8</volume>(<issue>215</issue>). <pub-id pub-id-type="doi">10.3389/fonc.2018.00215</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R25">
      <label>[25]</label>
      <mixed-citation publication-type="journal"><name><surname>Bobo</surname><given-names>MF</given-names></name>, <name><surname>Bao</surname><given-names>S</given-names></name>, <name><surname>Huo</surname><given-names>Y</given-names></name>, <name><surname>Yao</surname><given-names>Y</given-names></name>, <name><surname>Virostko</surname><given-names>J</given-names></name>, <name><surname>Plassard</surname><given-names>AJ</given-names></name>, <name><surname>Lyu</surname><given-names>I</given-names></name>, <name><surname>Assad</surname><given-names>A</given-names></name>, <name><surname>Abramson</surname><given-names>RG</given-names></name>, <name><surname>Hilmes</surname><given-names>MA</given-names></name>, <etal/><article-title>Fully convolutional neural networks improve abdominal organ segmentation</article-title>. In: <source>Proc. SPIE int. soc. opt. eng</source>, Vol. <volume>10574</volume>. <year>2018</year>, <pub-id pub-id-type="doi">10.1117/12.2293751</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R26">
      <label>[26]</label>
      <mixed-citation publication-type="journal"><name><surname>Gibson</surname><given-names>E</given-names></name>, <name><surname>Giganti</surname><given-names>F</given-names></name>, <name><surname>Hu</surname><given-names>Y</given-names></name>, <name><surname>Bonmati</surname><given-names>E</given-names></name>, <name><surname>Bandula</surname><given-names>S</given-names></name>, <name><surname>Gurusamy</surname><given-names>K</given-names></name>, <name><surname>Davidson</surname><given-names>B</given-names></name>, <name><surname>Pereira</surname><given-names>SP</given-names></name>, <name><surname>Clarkson</surname><given-names>MJ</given-names></name>, <name><surname>Barratt</surname><given-names>DC</given-names></name>. <article-title>Automatic multi-organ segmentation on abdominal CT with dense V-networks</article-title>. <source>IEEE Trans Med Imaging</source><year>2018</year>;<volume>37</volume>(<issue>8</issue>):<fpage>1822</fpage>–<lpage>34</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2018.2806309</pub-id><comment>.</comment><pub-id pub-id-type="pmid">29994628</pub-id></mixed-citation>
    </ref>
    <ref id="R27">
      <label>[27]</label>
      <mixed-citation publication-type="journal"><name><surname>Sharma</surname><given-names>K</given-names></name>, <name><surname>Rupprecht</surname><given-names>C</given-names></name>, <name><surname>Caroli</surname><given-names>A</given-names></name>, <name><surname>Aparicio</surname><given-names>MC</given-names></name>, <name><surname>Remuzzi</surname><given-names>A</given-names></name>, <name><surname>Baust</surname><given-names>M</given-names></name>, <name><surname>Navab</surname><given-names>N</given-names></name>. <article-title>Automatic segmentation of kidneys using deep learning for total kidney volume quantification in autosomal dominant polycystic kidney disease</article-title>. <source>Sci Rep</source><year>2017</year>;<volume>7</volume>(<issue>2049</issue>). <pub-id pub-id-type="doi">10.1038/s41598-017-01779-0</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R28">
      <label>[28]</label>
      <mixed-citation publication-type="web"><name><surname>Nandamuri</surname><given-names>S</given-names></name>, <name><surname>China</surname><given-names>D</given-names></name>, <name><surname>Mitra</surname><given-names>P</given-names></name>, <name><surname>Sheet</surname><given-names>D</given-names></name>. <source>SUMNet: Fully convolutional model for fast segmentation of anatomical structures in ultrasound volumes</source>. <year>2019</year>, <comment>arXiv:1901.06920.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1901.06920">https://arxiv.org/abs/1901.06920</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R29">
      <label>[29]</label>
      <mixed-citation publication-type="journal"><name><surname>Fan</surname><given-names>J</given-names></name>, <name><surname>Yang</surname><given-names>J</given-names></name>, <name><surname>Wang</surname><given-names>Y</given-names></name>, <name><surname>Yang</surname><given-names>S</given-names></name>, <name><surname>Ai</surname><given-names>D</given-names></name>, <name><surname>Huang</surname><given-names>Y</given-names></name>, <name><surname>Song</surname><given-names>H</given-names></name>, <name><surname>Hao</surname><given-names>A</given-names></name>, <name><surname>Wang</surname><given-names>Y</given-names></name>. <article-title>Multichannel fully convolutional network for coronary artery segmentation in X-ray angiograms</article-title>. <source>IEEE Access</source><year>2018</year>;<volume>6</volume>:<fpage>44635</fpage>–<lpage>43</lpage>. <pub-id pub-id-type="doi">10.1109/ACCESS.2018.2864592</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R30">
      <label>[30]</label>
      <mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>J</given-names></name>, <name><surname>Nishikawa</surname><given-names>RM</given-names></name>. <article-title>Automated mammographic breast density estimation using a fully convolutional network</article-title>. <source>Med Phys</source><year>2018</year>;<volume>45</volume>(<issue>3</issue>):<fpage>1178</fpage>–<lpage>90</lpage>. <pub-id pub-id-type="doi">10.1002/mp.12763</pub-id><comment>.</comment><pub-id pub-id-type="pmid">29363774</pub-id></mixed-citation>
    </ref>
    <ref id="R31">
      <label>[31]</label>
      <mixed-citation publication-type="journal"><name><surname>Leynes</surname><given-names>AP</given-names></name>, <name><surname>Yang</surname><given-names>J</given-names></name>, <name><surname>Wiesinger</surname><given-names>F</given-names></name>, <name><surname>Kaushik</surname><given-names>SS</given-names></name>, <name><surname>Shanbhag</surname><given-names>DD</given-names></name>, <name><surname>Seo</surname><given-names>Y</given-names></name>, <name><surname>Hope</surname><given-names>TA</given-names></name>, <name><surname>Larson</surname><given-names>PEZ</given-names></name>. <article-title>Zero-echo-time and dixon deep -pseudo-CT (ZeDD CT): Direct generation of pseudo-CT images for pelvic PET/MRI attenuation correction using deep convolutional neural networks with multiparametric MRI</article-title>. <source>J Nucl Med</source><year>2018</year>;<volume>59</volume>(<issue>5</issue>):<fpage>852</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.2967/jnumed.117.198051</pub-id><comment>.</comment><pub-id pub-id-type="pmid">29084824</pub-id></mixed-citation>
    </ref>
    <ref id="R32">
      <label>[32]</label>
      <mixed-citation publication-type="journal"><name><surname>Han</surname><given-names>X</given-names></name><article-title>MR-based synthetic CT generation using a deep convolutional neural network method</article-title>. <source>Med Phys</source><year>2017</year>;<volume>44</volume>(<issue>4</issue>):<fpage>1408</fpage>–<lpage>19</lpage>. <pub-id pub-id-type="doi">10.1002/mp.12155</pub-id><comment>.</comment><pub-id pub-id-type="pmid">28192624</pub-id></mixed-citation>
    </ref>
    <ref id="R33">
      <label>[33]</label>
      <mixed-citation publication-type="web"><name><surname>Fu</surname><given-names>J</given-names></name>, <name><surname>Yang</surname><given-names>Y</given-names></name>, <name><surname>Singhrao</surname><given-names>K</given-names></name>, <name><surname>Ruan</surname><given-names>D</given-names></name>, <name><surname>Low</surname><given-names>DA</given-names></name>, <name><surname>Lewis</surname><given-names>JH</given-names></name>. <source>Male pelvic synthetic CT generation from T1-weighted MRI using 2D and 3D convolutional neural networks</source>. <year>2018</year>, <comment>arXiv:1803.00131.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1803.00131">https://arxiv.org/abs/1803.00131</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R34">
      <label>[34]</label>
      <mixed-citation publication-type="journal"><name><surname>Stadelmann</surname><given-names>JV</given-names></name>, <name><surname>Schulz</surname><given-names>H</given-names></name>, <name><surname>van der Heide</surname><given-names>UA</given-names></name>, <name><surname>Renisch</surname><given-names>S</given-names></name>. <article-title>Pseudo-CT image generation from mdixon MRI images using fully convolutional neural networks</article-title>. In: <source>Proc. SPIE med. imag.: Biomedical applications in molecular, structural, and functional imaging</source>, Vol. <volume>10953</volume>. <year>2019</year>, <pub-id pub-id-type="doi">10.1117/12.2512741</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R35">
      <label>[35]</label>
      <mixed-citation publication-type="journal"><name><surname>Kamnitsas</surname><given-names>K</given-names></name>, <name><surname>Ledig</surname><given-names>C</given-names></name>, <name><surname>Newcombe</surname><given-names>VFJ</given-names></name>, <name><surname>Simpson</surname><given-names>JP</given-names></name>, <name><surname>Kane</surname><given-names>AD</given-names></name>, <name><surname>Menon</surname><given-names>DK</given-names></name>, <name><surname>Rueckert</surname><given-names>D</given-names></name>, <name><surname>Glocker</surname><given-names>B</given-names></name>. <article-title>Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation</article-title>. <source>Med Image Anal</source><year>2017</year>;<volume>36</volume>:<fpage>61</fpage>–<lpage>78</lpage>. <pub-id pub-id-type="doi">10.1016/j.media.2016.10.004</pub-id><comment>.</comment><pub-id pub-id-type="pmid">27865153</pub-id></mixed-citation>
    </ref>
    <ref id="R36">
      <label>[36]</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>F</given-names></name>, <name><surname>Jang</surname><given-names>H</given-names></name>, <name><surname>Kijowski</surname><given-names>R</given-names></name>, <name><surname>Zhao</surname><given-names>G</given-names></name>, <name><surname>Bradshaw</surname><given-names>T</given-names></name>, <name><surname>McMillan</surname><given-names>AB</given-names></name>. <article-title>A deep learning approach for 18F-FDG PET attenuation correction</article-title>. <source>EJNMMI Phys</source><year>2018</year>;<volume>5</volume>:<fpage>24</fpage>. <pub-id pub-id-type="doi">10.1186/s40658-018-0225-8</pub-id><comment>.</comment><pub-id pub-id-type="pmid">30417316</pub-id></mixed-citation>
    </ref>
    <ref id="R37">
      <label>[37]</label>
      <mixed-citation publication-type="journal"><name><surname>Gong</surname><given-names>K</given-names></name>, <name><surname>Yang</surname><given-names>J</given-names></name>, <name><surname>Kim</surname><given-names>K</given-names></name>, <name><surname>Fakhri</surname><given-names>GE</given-names></name>, <name><surname>Seo</surname><given-names>Y</given-names></name>, <name><surname>Li</surname><given-names>Q</given-names></name>. <article-title>Attenuation correction for brain PET imaging using deep neural network based on Dixon and ZTE MR images</article-title>. <source>Phys Med Biol</source><year>2018</year>;<volume>63</volume>(<issue>12</issue>). <fpage>125011</fpage>. <pub-id pub-id-type="doi">10.1088/1361-6560/aac763</pub-id><comment>.</comment><pub-id pub-id-type="pmid">29790857</pub-id></mixed-citation>
    </ref>
    <ref id="R38">
      <label>[38]</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>F</given-names></name>, <name><surname>Jang</surname><given-names>H</given-names></name>, <name><surname>Kijowski</surname><given-names>R</given-names></name>, <name><surname>Bradshaw</surname><given-names>R</given-names></name>, <name><surname>McMillan</surname><given-names>AB</given-names></name>. <article-title>Deep learning MR imaging–based attenuation correction for PET/MR imaging</article-title>. <source>Radiology</source><year>2017</year>;<volume>286</volume>(<issue>2</issue>):<fpage>676</fpage>–<lpage>84</lpage>. <pub-id pub-id-type="doi">10.1148/radiol.2017170700</pub-id><comment>.</comment><pub-id pub-id-type="pmid">28925823</pub-id></mixed-citation>
    </ref>
    <ref id="R39">
      <label>[39]</label>
      <mixed-citation publication-type="web"><name><surname>Grøvik</surname><given-names>E</given-names></name>, <name><surname>Yi</surname><given-names>D</given-names></name>, <name><surname>Iv</surname><given-names>M</given-names></name>, <name><surname>Tong</surname><given-names>E</given-names></name>, <name><surname>Rubin</surname><given-names>DL</given-names></name>, <name><surname>Zaharchuk</surname><given-names>G</given-names></name>. <source>Deep learning enables automatic detection and segmentation of brain metastases on multi-sequence MRI</source>. <year>2019</year>, <comment>arXiv:1903.07988.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1903.07988">https://arxiv.org/abs/1903.07988</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R40">
      <label>[40]</label>
      <mixed-citation publication-type="web"><name><surname>Ronneberger</surname><given-names>O</given-names></name>, <name><surname>Fischer</surname><given-names>P</given-names></name>, <name><surname>Brox</surname><given-names>T</given-names></name>. <source>U-Net: Convolutional networks for biomedical image segmentation</source>. <year>2015</year>, <comment>arXiv:1505.04597.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R41">
      <label>[41]</label>
      <mixed-citation publication-type="web"><name><surname>Çiçek</surname><given-names>Ö</given-names></name>, <name><surname>Abdulkadir</surname><given-names>A</given-names></name>, <name><surname>Lienkamp</surname><given-names>SS</given-names></name>, <name><surname>Brox</surname><given-names>T</given-names></name>, <name><surname>Ronneberger</surname><given-names>O</given-names></name>. <source>3D U-Net: Learning dense volumetric segmentation from sparse annotation</source>. <year>2016</year>, <comment>arXiv:1606.06650.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1606.06650">https://arxiv.org/abs/1606.06650</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R42">
      <label>[42]</label>
      <mixed-citation publication-type="journal"><name><surname>Goodfellow</surname><given-names>IJ</given-names></name>, <name><surname>Pouget-Abadie</surname><given-names>J</given-names></name>, <name><surname>Mirza</surname><given-names>M</given-names></name>, <name><surname>Xu</surname><given-names>B</given-names></name>, <name><surname>Warde-Farley</surname><given-names>D</given-names></name>, <name><surname>Ozair</surname><given-names>S</given-names></name>, <name><surname>Courville</surname><given-names>A</given-names></name>, <name><surname>Bengio</surname><given-names>Y</given-names></name>. <article-title>Generative adversarial nets</article-title>. <source>Adv Neural Inf Process Syst</source><year>2014</year>;<fpage>27</fpage>.</mixed-citation>
    </ref>
    <ref id="R43">
      <label>[43]</label>
      <mixed-citation publication-type="web"><name><surname>Armanious</surname><given-names>K</given-names></name>, <name><surname>Yang</surname><given-names>C</given-names></name>, <name><surname>Fischer</surname><given-names>M</given-names></name>, <name><surname>Küstner</surname><given-names>T</given-names></name>, <name><surname>Nikolaou</surname><given-names>K</given-names></name>, <name><surname>Gatidis</surname><given-names>S</given-names></name>, <name><surname>Yang</surname><given-names>B</given-names></name>. <source>Medgan: Medical image translation using GANs</source>. <year>2018</year>, <comment>arXiv:1806.06397.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1806.06397">https://arxiv.org/abs/1806.06397</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R44">
      <label>[44]</label>
      <mixed-citation publication-type="journal"><name><surname>Jung</surname><given-names>MM</given-names></name>, <name><surname>van den Berg</surname><given-names>B</given-names></name>, <name><surname>Postma</surname><given-names>E</given-names></name>, <name><surname>Huijbers</surname><given-names>W</given-names></name>. <article-title>Inferring PET from MRI with pix2pix</article-title>. <source>Benelux Conf. Art. Intell</source><year>2018</year>;<fpage>30</fpage>.</mixed-citation>
    </ref>
    <ref id="R45">
      <label>[45]</label>
      <mixed-citation publication-type="confproc"><name><surname>Teixeira</surname><given-names>B</given-names></name>, <name><surname>Singh</surname><given-names>V</given-names></name>, <name><surname>Chen</surname><given-names>T</given-names></name>, <name><surname>Ma</surname><given-names>K</given-names></name>, <name><surname>Tamersoy</surname><given-names>B</given-names></name>, <name><surname>Wu</surname><given-names>Y</given-names></name>, <name><surname>Balashova</surname><given-names>E</given-names></name>, <name><surname>Comaniciu</surname><given-names>D</given-names></name>. <article-title>Generating synthetic x-ray images of a person from the surface geometry</article-title>. In: <conf-name>Proc. IEEE comput. soc. conf. comput. vis. pattern. recognit.</conf-name><year>2018</year>, p. <fpage>9059</fpage>–<lpage>67</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2018.00944</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R46">
      <label>[46]</label>
      <mixed-citation publication-type="web"><name><surname>Mondal</surname><given-names>AK</given-names></name>, <name><surname>Dolz</surname><given-names>J</given-names></name>, <name><surname>Desrosiers</surname><given-names>C</given-names></name>. <source>Few-shot 3D multi-modal medical image segmentation using generative adversarial learning</source>. <year>2018</year>, <comment>arXiv: 1806.06397.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1806.06397">https://arxiv.org/abs/1806.06397</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R47">
      <label>[47]</label>
      <mixed-citation publication-type="web"><name><surname>Rezaei</surname><given-names>M</given-names></name>, <name><surname>Harmuth</surname><given-names>K</given-names></name>, <name><surname>Gierke</surname><given-names>W</given-names></name>, <name><surname>Kellermeier</surname><given-names>T</given-names></name>, <name><surname>Fischer</surname><given-names>M</given-names></name>, <name><surname>Yang</surname><given-names>H</given-names></name>, <name><surname>Meinel</surname><given-names>C</given-names></name>. <source>Conditional adversarial network for semantic segmentation of brain tumor</source>. <year>2017</year>, <comment>arXiv:1708.05227.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1708.05227">https://arxiv.org/abs/1708.05227</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R48">
      <label>[48]</label>
      <mixed-citation publication-type="web"><name><surname>Rezaei</surname><given-names>M</given-names></name>, <name><surname>Yang</surname><given-names>H</given-names></name>, <name><surname>Meinel</surname><given-names>C</given-names></name>. <source>Conditional generative refinement adversarial networks for unbalanced medical image semantic segmentation</source>. <year>2018</year>, <comment>arXiv:1810.03871.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1810.03871">https://arxiv.org/abs/1810.03871</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R49">
      <label>[49]</label>
      <mixed-citation publication-type="journal"><name><surname>Mardani</surname><given-names>M</given-names></name>, <name><surname>Gong</surname><given-names>E</given-names></name>, <name><surname>Cheng</surname><given-names>JY</given-names></name>, <name><surname>Vasanawala</surname><given-names>SS</given-names></name>, <name><surname>Zaharchuk</surname><given-names>G</given-names></name>, <name><surname>Xing</surname><given-names>L</given-names></name>, <name><surname>Pauly</surname><given-names>JM</given-names></name>. <article-title>Deep generative adversarial neural networks for compressive sensing MRI</article-title>. <source>IEEE Trans Med Imag</source><year>2019</year>;<volume>38</volume>(<issue>1</issue>):<fpage>167</fpage>–<lpage>79</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2018.2858752</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R50">
      <label>[50]</label>
      <mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>G</given-names></name>, <name><surname>Yu</surname><given-names>S</given-names></name>, <name><surname>Dong</surname><given-names>H</given-names></name>, <name><surname>Slabaugh</surname><given-names>G</given-names></name>, <name><surname>Dragotti</surname><given-names>PL</given-names></name>, <name><surname>Ye</surname><given-names>X</given-names></name>, <name><surname>Liu</surname><given-names>F</given-names></name>, <name><surname>Arridge</surname><given-names>S</given-names></name>, <name><surname>Keegan</surname><given-names>J</given-names></name>, <name><surname>Guo</surname><given-names>Y</given-names></name>, <etal/><article-title>DAGAN: Deep de-aliasing generative adversarial networks for fast compressed sensing MRI reconstruction</article-title>. <source>IEEE Trans Med Imag</source><year>2018</year>;<volume>37</volume>(<issue>6</issue>):<fpage>1310</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2017.2785879</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R51">
      <label>[51]</label>
      <mixed-citation publication-type="web"><name><surname>Quan</surname><given-names>TM</given-names></name>, <name><surname>Nguyen-Duc</surname><given-names>T</given-names></name>, <name><surname>Jeong</surname><given-names>WK</given-names></name>. <source>Compressed sensing MRI reconstruction using a generative adversarial network with a cyclic loss</source>. <year>2017</year>, <comment>arXiv:1709.00753.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1709.00753">https://arxiv.org/abs/1709.00753</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R52">
      <label>[52]</label>
      <mixed-citation publication-type="journal"><name><surname>Mahapatra</surname><given-names>D</given-names></name>, <name><surname>Bozorgtabar</surname><given-names>B</given-names></name>, <name><surname>Garnavi</surname><given-names>R</given-names></name>. <article-title>Image super-resolution using progressive generative adversarial networks for medical image analysis</article-title>. <source>Comput Med Imaging Graph</source><year>2019</year>;<volume>71</volume>:<fpage>30</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1016/j.compmedimag.2018.10.005</pub-id><comment>.</comment><pub-id pub-id-type="pmid">30472408</pub-id></mixed-citation>
    </ref>
    <ref id="R53">
      <label>[53]</label>
      <mixed-citation publication-type="web"><name><surname>Sanchez</surname><given-names>I</given-names></name>, <name><surname>Vilaplana</surname><given-names>V</given-names></name>. <source>Brain MRI super-resolution using 3D generative adversarial networks</source>. <year>2018</year>, <comment>arXiv:1812.11440.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1812.11440">https://arxiv.org/abs/1812.11440</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R54">
      <label>[54]</label>
      <mixed-citation publication-type="web"><name><surname>Isola</surname><given-names>P</given-names></name>, <name><surname>Zhu</surname><given-names>JY</given-names></name>, <name><surname>Zhou</surname><given-names>T</given-names></name>, <name><surname>Efros</surname><given-names>AA</given-names></name>. <source>Image-to-image translation with conditional adversarial networks</source>. <year>2016</year>, <comment>arXiv:1611.07004.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1611.07004">https://arxiv.org/abs/1611.07004</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R55">
      <label>[55]</label>
      <mixed-citation publication-type="web"><name><surname>Zhu</surname><given-names>JY</given-names></name>, <name><surname>Park</surname><given-names>T</given-names></name>, <name><surname>Isola</surname><given-names>P</given-names></name>, <name><surname>Efros</surname><given-names>AA</given-names></name>. <source>Unpaired image-to-image translation using cycle-consistent adversarial networks</source>. <year>2017</year>, <comment>arXiv:1703.10593.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1703.10593">https://arxiv.org/abs/1703.10593</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R56">
      <label>[56]</label>
      <mixed-citation publication-type="journal"><name><surname>Girshick</surname><given-names>R</given-names></name>, <name><surname>Donahue</surname><given-names>J</given-names></name>, <name><surname>Darrell</surname><given-names>T</given-names></name>, <name><surname>Malik</surname><given-names>J</given-names></name>. <article-title>Region-based convolutional networks for accurate object detection and segmentation</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source><year>2016</year>;<volume>38</volume>(<issue>1</issue>):<fpage>142</fpage>–<lpage>58</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2015.2437384</pub-id><comment>.</comment><pub-id pub-id-type="pmid">26656583</pub-id></mixed-citation>
    </ref>
    <ref id="R57">
      <label>[57]</label>
      <mixed-citation publication-type="web"><name><surname>Redmon</surname><given-names>J</given-names></name>, <name><surname>Divvala</surname><given-names>S</given-names></name>, <name><surname>Girshick</surname><given-names>R</given-names></name>, <name><surname>Farhadi</surname><given-names>A</given-names></name>. <source>You only look once: Unified, real-time object detection</source>. <year>2015</year>, <comment>arXiv:1506.02640.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1506.02640">https://arxiv.org/abs/1506.02640</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R58">
      <label>[58]</label>
      <mixed-citation publication-type="web"><name><surname>Ren</surname><given-names>S</given-names></name>, <name><surname>He</surname><given-names>K</given-names></name>, <name><surname>Girshick</surname><given-names>R</given-names></name>, <name><surname>Sun</surname><given-names>J</given-names></name>. <source>Faster R-CNN: Towards real-time object detection with region proposal networks</source>. <year>2015</year>, <comment>arXiv:1506.01497.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1506.01497">https://arxiv.org/abs/1506.01497</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R59">
      <label>[59]</label>
      <mixed-citation publication-type="web"><name><surname>Liu</surname><given-names>W</given-names></name>, <name><surname>Anguelov</surname><given-names>D</given-names></name>, <name><surname>Erhan</surname><given-names>D</given-names></name>, <name><surname>Szegedy</surname><given-names>C</given-names></name>, <name><surname>Reed</surname><given-names>S</given-names></name>, <name><surname>Fu</surname><given-names>CY</given-names></name>, <etal/><source>SSD: Single shot multibox detector</source>. <year>2015</year>, <comment>arXiv preprint. arXiv:1512.02325</comment>, <comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1512.02325">https://arxiv.org/abs/1512.02325</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R60">
      <label>[60]</label>
      <mixed-citation publication-type="journal"><name><surname>Thian</surname><given-names>YL</given-names></name>, <name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Jagmohan</surname><given-names>P</given-names></name>, <name><surname>Sia</surname><given-names>D</given-names></name>, <name><surname>Chan</surname><given-names>VEY</given-names></name>, <name><surname>Tan</surname><given-names>RT</given-names></name>. <article-title>Convolutional neural networks for automated fracture detection and localization on wrist radiographs</article-title>. <source>Radiol: Artif Intell</source><year>2019</year>;<volume>1</volume>(<issue>1</issue>):<fpage>00</fpage>. <pub-id pub-id-type="doi">10.1148/ryai.2019180001</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R61">
      <label>[61]</label>
      <mixed-citation publication-type="journal"><name><surname>Lu</surname><given-names>Y</given-names></name>, <name><surname>Yu</surname><given-names>Q</given-names></name>, <name><surname>Gao</surname><given-names>Y</given-names></name>, <name><surname>Zhou</surname><given-names>Y</given-names></name>, <name><surname>Liu</surname><given-names>G</given-names></name>, <name><surname>Dong</surname><given-names>Q</given-names></name>, <etal/><article-title>Identification of metastatic lymph nodes in MR imaging with faster region-based convolutional neural networks</article-title>. <source>Cancer Res</source><year>2018</year>;<volume>78</volume>(<issue>17</issue>):<fpage>5135</fpage>–<lpage>43</lpage>. <pub-id pub-id-type="doi">10.1158/0008-5472.CAN-18-0494</pub-id><comment>.</comment><pub-id pub-id-type="pmid">30026330</pub-id></mixed-citation>
    </ref>
    <ref id="R62">
      <label>[62]</label>
      <mixed-citation publication-type="web"><name><surname>Chollet</surname><given-names>F</given-names></name><source>Keras: the python deep learning library</source>. <year>2015</year>, <comment><ext-link ext-link-type="uri" xlink:href="https://keras.io/">https://keras.io/</ext-link> Published March 27, 2015. Updated January 25, 2019. Accessed</comment><date-in-citation>15.05.19</date-in-citation>.</mixed-citation>
    </ref>
    <ref id="R63">
      <label>[63]</label>
      <mixed-citation publication-type="journal"><name><surname>Abadi</surname><given-names>M</given-names></name>, <name><surname>Agarwal</surname><given-names>A</given-names></name>, <name><surname>Barham</surname><given-names>P</given-names></name>, <name><surname>Brevdo</surname><given-names>E</given-names></name>, <name><surname>Chen</surname><given-names>Z</given-names></name>, <name><surname>Citro</surname><given-names>C</given-names></name>, <etal/><source>TensorFlow: Large-scale machine learning on heterogeneous distributed systems</source>. <year>1998</year>, <comment>arXiv preprint. arXiv:1603.04467</comment>, <volume>2016</volume>. <comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1603.04467">https://arxiv.org/abs/1603.04467</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R64">
      <label>[64]</label>
      <mixed-citation publication-type="journal"><name><surname>Lecun</surname><given-names>Y</given-names></name>, <name><surname>Bottou</surname><given-names>L</given-names></name>, <name><surname>Bengio</surname><given-names>Y</given-names></name>, <name><surname>Haffner</surname><given-names>P</given-names></name>. <article-title>Gradient-based learning applied to document recognition</article-title>. <source>Proc IEEE</source><year>1998</year>;<volume>86</volume>(<issue>11</issue>):<fpage>2278</fpage>–<lpage>324</lpage>. <pub-id pub-id-type="doi">10.1109/5.726791</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R65">
      <label>[65]</label>
      <mixed-citation publication-type="web"><name><surname>Odena</surname><given-names>A</given-names></name>, <name><surname>Dumoulin</surname><given-names>V</given-names></name>, <name><surname>Olah</surname><given-names>C</given-names></name>. <source>Deconvolution and checkerboard artifacts</source>. <year>2016</year>, <pub-id pub-id-type="doi">10.23915/distill.00003,Distill</pub-id><comment>. Online, accessed</comment><date-in-citation>28.03.19</date-in-citation>.</mixed-citation>
    </ref>
    <ref id="R66">
      <label>[66]</label>
      <mixed-citation publication-type="web"><name><surname>Salehi</surname><given-names>SSM</given-names></name>, <name><surname>Erdogmus</surname><given-names>D</given-names></name>, <name><surname>Gholipour</surname><given-names>A</given-names></name>. <source>Tversky loss function for image segmentation using 3D fully convolutional deep networks</source>. <year>2017</year>, <comment>arXiv: 1706.05721.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1706.05721">https://arxiv.org/abs/1706.05721</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R67">
      <label>[67]</label>
      <mixed-citation publication-type="web"><name><surname>Lin</surname><given-names>TY</given-names></name>, <name><surname>Goyal</surname><given-names>P</given-names></name>, <name><surname>Girshick</surname><given-names>R</given-names></name>, <name><surname>He</surname><given-names>K</given-names></name>, <name><surname>Dollár</surname><given-names>P</given-names></name>. <source>Focal loss for dense object detection</source>. <year>2017</year>, <comment>arXiv preprint.</comment><comment>arXiv:1708.02002.</comment><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1708.02002">https://arxiv.org/abs/1708.02002</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R68">
      <label>[68]</label>
      <mixed-citation publication-type="confproc"><name><surname>Sanders</surname><given-names>JW</given-names></name>, <name><surname>Lewis</surname><given-names>G</given-names></name>, <name><surname>Frank</surname><given-names>S</given-names></name>, <name><surname>Ma</surname><given-names>J</given-names></name>. <article-title>A fully convolutional network utilizing depth-wise separable convolutions for semantic segmentation of anatomy in MRI of the prostate after permanent implant brachytherapy</article-title>. In: <conf-name>Proc ISMRM workshop mach. learn.. Pacific Grove</conf-name>, <conf-loc>CA</conf-loc>; <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="R69">
      <label>[69]</label>
      <mixed-citation publication-type="journal"><name><surname>Sanders</surname><given-names>J</given-names></name>, <name><surname>Frank</surname><given-names>S</given-names></name>, <name><surname>Lewis</surname><given-names>G</given-names></name>, <name><surname>Kudchadker</surname><given-names>R</given-names></name>, <name><surname>Bruno</surname><given-names>T</given-names></name>, <name><surname>Ma</surname><given-names>J</given-names></name>. <article-title>Multi-tasking neural networks for anatomy segmentation in prostate brachytherapy MRI</article-title>. <source>Brachytherapy</source><year>2019</year>;<volume>18</volume>(<issue>3</issue>):<fpage>S16</fpage>. <pub-id pub-id-type="doi">10.1016/j.brachy.2019.04.039</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R70">
      <label>[70]</label>
      <mixed-citation publication-type="journal"><name><surname>Sanders</surname><given-names>JW</given-names></name>, <name><surname>Song</surname><given-names>H</given-names></name>, <name><surname>Frank</surname><given-names>SJ</given-names></name>, <name><surname>Bathala</surname><given-names>T</given-names></name>, <name><surname>Venkatesan</surname><given-names>AM</given-names></name>, <name><surname>Anscher</surname><given-names>M</given-names></name>, <etal/><article-title>Parallel imaging compressed sensing for accelerated imaging and improved signal-to-noise ratio in MRI-based post-implant dosimetry of prostate brachytherapy</article-title>. <source>Brachytherapy</source><year>2018</year>;<volume>17</volume>:<fpage>816</fpage>–<lpage>24</lpage>.<pub-id pub-id-type="pmid">29880449</pub-id></mixed-citation>
    </ref>
    <ref id="R71">
      <label>[71]</label>
      <mixed-citation publication-type="journal"><name><surname>Shiroishi</surname><given-names>M</given-names></name>, <name><surname>Boxerman</surname><given-names>J</given-names></name>, <name><surname>Pope</surname><given-names>W</given-names></name>. <article-title>Physiologic MRI for assessment of response to therapy and prognosis in glioblastoma</article-title>. <source>Neuro-Oncology</source><year>2015</year>;<volume>18</volume>(<issue>4</issue>):<fpage>467</fpage>–<lpage>78</lpage>.<pub-id pub-id-type="pmid">26364321</pub-id></mixed-citation>
    </ref>
    <ref id="R72">
      <label>[72]</label>
      <mixed-citation publication-type="journal"><name><surname>Seeger</surname><given-names>A</given-names></name>, <name><surname>Braun</surname><given-names>C</given-names></name>, <name><surname>Skardelly</surname><given-names>M</given-names></name>, <name><surname>Paulsen</surname><given-names>F</given-names></name>, <name><surname>Schittenhelm</surname><given-names>J</given-names></name>, <name><surname>Ernemann</surname><given-names>U</given-names></name>, <etal/><article-title>Comparison of three different MR perfusion techniques and MR spectroscopy for multiparametric assessment in distinguishing recurrent high-grade gliomas from stable disease</article-title>. <source>Acad Radiol</source><year>2013</year>;<volume>20</volume>(<issue>12</issue>):<fpage>1557</fpage>–<lpage>65</lpage>. <pub-id pub-id-type="doi">10.1016/j.acra.2013.09.003</pub-id><comment>.</comment><pub-id pub-id-type="pmid">24200483</pub-id></mixed-citation>
    </ref>
    <ref id="R73">
      <label>[73]</label>
      <mixed-citation publication-type="confproc"><name><surname>Sanders</surname><given-names>J</given-names></name>, <name><surname>Chen</surname><given-names>HSM</given-names></name>, <name><surname>Johnson</surname><given-names>J</given-names></name>, <name><surname>Schomer</surname><given-names>D</given-names></name>, <name><surname>Ma</surname><given-names>J</given-names></name>, <name><surname>Liu</surname><given-names>HL</given-names></name>. <article-title>Synthesizing rCBV maps from DCE-MRI of brain tumors using conditional adversarial networks</article-title>. In: <conf-name>Proc. ISMRM annual meeting</conf-name>. <conf-loc>Montreal, Canada</conf-loc>; <year>2019</year>, <comment><ext-link ext-link-type="uri" xlink:href="https://www.ismrm.org/19/program_files/O04.htm">https://www.ismrm.org/19/program_files/O04.htm</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R74">
      <label>[74]</label>
      <mixed-citation publication-type="confproc"><name><surname>Sanders</surname><given-names>J</given-names></name>, <name><surname>Fletcher</surname><given-names>J</given-names></name>, <name><surname>Frank</surname><given-names>S</given-names></name>, <name><surname>Liu</surname><given-names>HL</given-names></name>, <name><surname>Johnson</surname><given-names>J</given-names></name>, <name><surname>Stafford</surname><given-names>J</given-names></name>, <etal/><article-title>Deep learning application engine (DLAE): end-to-end development and deploy- ment of medical deep learning algorithms</article-title>. In: <conf-name>Proceedings of the ISMRM workshop on machine learning part II</conf-name>. <conf-loc>Washington, DC</conf-loc>; <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="R75">
      <label>[75]</label>
      <mixed-citation publication-type="journal"><name><surname>Sanders</surname><given-names>J</given-names></name>, <name><surname>Hurwitz</surname><given-names>L</given-names></name>, <name><surname>Samei</surname><given-names>E</given-names></name>. <article-title>Patient-specific quantification of image quality: An automated method for measuring spatial resolution in clinical CT images</article-title>. <source>Med Phys</source><year>2016</year>;<volume>43</volume>(<issue>10</issue>):<fpage>5330</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1118/1.4961984</pub-id><comment>.</comment><pub-id pub-id-type="pmid">27782718</pub-id></mixed-citation>
    </ref>
    <ref id="R76">
      <label>[76]</label>
      <mixed-citation publication-type="journal"><name><surname>Abadi</surname><given-names>E</given-names></name>, <name><surname>Sanders</surname><given-names>J</given-names></name>, <name><surname>Samei</surname><given-names>E</given-names></name>. <article-title>Patient-specific quantification of image quality: An automated technique for measuring the distribution of organ hounsfield units in clinical chest CT images</article-title>. <source>Med Phys</source><year>2017</year>;<volume>44</volume>(<issue>9</issue>):<fpage>4736</fpage>–<lpage>46</lpage>. <pub-id pub-id-type="doi">10.1002/mp.12438</pub-id><comment>.</comment><pub-id pub-id-type="pmid">28658516</pub-id></mixed-citation>
    </ref>
    <ref id="R77">
      <label>[77]</label>
      <mixed-citation publication-type="journal"><name><surname>Sanders</surname><given-names>J</given-names></name>, <name><surname>Tian</surname><given-names>X</given-names></name>, <name><surname>Segars</surname><given-names>WP</given-names></name>, <name><surname>Boone</surname><given-names>J</given-names></name>, <name><surname>Samei</surname><given-names>E</given-names></name>. <article-title>Automated, patientspecific estimation of regional imparted energy and dose from tube current modulated computed tomography exams across 13 protocols</article-title>. <source>J Med Imaging (Bellingham)</source><year>2017</year>;<volume>4</volume>(<issue>1</issue>). <fpage>013503</fpage>. <pub-id pub-id-type="doi">10.1117/1.JMI.4.1.013503</pub-id><comment>.</comment><pub-id pub-id-type="pmid">28149922</pub-id></mixed-citation>
    </ref>
    <ref id="R78">
      <label>[78]</label>
      <mixed-citation publication-type="journal"><name><surname>Smith</surname><given-names>TB</given-names></name>, <name><surname>Solomon</surname><given-names>JB</given-names></name>, <name><surname>Samei</surname><given-names>E</given-names></name>. <article-title>Estimating detectability index in vivo: development and validation of an automated methodology</article-title>. <source>J Med Imaging (Bellingham)</source><year>2017</year>;<volume>5</volume>(<issue>3</issue>). <fpage>031403</fpage>. <pub-id pub-id-type="doi">10.1117/1.JMI.5.3.031403</pub-id><comment>.</comment><pub-id pub-id-type="pmid">29250570</pub-id></mixed-citation>
    </ref>
    <ref id="R79">
      <label>[79]</label>
      <mixed-citation publication-type="journal"><name><surname>Ria</surname><given-names>F</given-names></name>, <name><surname>Wilson</surname><given-names>JM</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Samei</surname><given-names>E</given-names></name>. <article-title>Image noise and dose performance across a clinical population: Patient size adaptation as a metric of CT performance</article-title>. <source>Med Phys</source><year>2017</year>;<volume>44</volume>(<issue>6</issue>):<fpage>2141</fpage>–<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1002/mp.12172</pub-id><comment>.</comment><pub-id pub-id-type="pmid">28235130</pub-id></mixed-citation>
    </ref>
    <ref id="R80">
      <label>[80]</label>
      <mixed-citation publication-type="confproc"><name><surname>Ding</surname><given-names>A</given-names></name>, <name><surname>Wilson</surname><given-names>J</given-names></name>, <name><surname>Solomon</surname><given-names>JB</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Mann</surname><given-names>SD</given-names></name>, <name><surname>Nelson</surname><given-names>J</given-names></name>, <etal/><article-title>EMTIS: Next-generation performance informatics platform for value-based clinical imaging practice</article-title>. In: <conf-name>Proc RSNA annual meeting</conf-name>. <year>2018</year>, p. <fpage>SSJ21</fpage>–<lpage>01</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="F1" orientation="portrait" position="float">
    <label>Fig. 1.</label>
    <caption>
      <p id="P68">The DLAE GUI home screen was partitioned into menus, each corresponding to a step in a DL workflow. DLAE configuration files (JSON format) can be loaded or saved from the File menu. Training, validation, and testing images and annotations can be loaded using an interface in the Data menu. Options for data augmentation, and preprocessing of the images, annotations, and predictions were also included in the Data menu. The View menu allows users to view the states of the GUI parameters. The Layers menu provides the majority of the core functions for building DL models and applying transfer learning from networks pretrained on ImageNet. The Layers menu and Convolutional Layers submenu are shown as example in turquoise and red, respectively. The Options menu contains the GUI objects for specifying the loss function for training, choosing the optimizer, defining the training configurations, specifying metrics to monitor during the training process, and specifying the types of information to save from a training session. The Tools menu enables deleting a model to start over or open the most recently saved TensorBoard log in the user’s default web browser. The Run menu contains the command to run the engine based on the current GUI state. The Help menu displays error messages and can take the user to the repository where DLAE is hosted (<ext-link ext-link-type="uri" xlink:href="https://github.com/jeremiahws/dlae">https://github.com/jeremiahws/dlae</ext-link>). In addition to hosting the submenu, the DLAE GUI home screen provides a simple means of loading a number of architectures from the literature or building a custom model de novo. The prebuilt FCN submenu is shown in blue as an example. Because the GUI and engine are decoupled, the GUI serves as an interface for configuration file creation combined with an engine launch automation capability. If the user passes a configuration file to DLAE from the command line instead of running the dlae.py script, the GUI is suppressed, and either a training or inference session will be launched depending on the specifications in the configuration file.</p>
    </caption>
    <graphic xlink:href="nihms-1582136-f0001"/>
  </fig>
  <fig id="F2" orientation="portrait" position="float">
    <label>Fig. 2.</label>
    <caption>
      <p id="P69">Structure of a DLAE configuration file and the data types of the variables.</p>
    </caption>
    <graphic xlink:href="nihms-1582136-f0002"/>
  </fig>
  <fig id="F3" orientation="portrait" position="float">
    <label>Fig. 3.</label>
    <caption>
      <p id="P70">Current DL functionalities of DLAE. From left to right: (1) custom DL models can be built and trained from scratch or with warm starts via a transfer learning approach through inputs to the GUI. (2) Automated DL model training sessions can be conducted with the use of an experiment generator, which spawns DL models to be trained for a given application. This could be useful in determining the optimal DL architecture, performing ablation studies, or determining the appropriate set of hyperparameters for a given DL application. (3) Automated inference sessions can be conducted, enabling the integration of trained DL models into clinical workflows.</p>
    </caption>
    <graphic xlink:href="nihms-1582136-f0003"/>
  </fig>
  <fig id="F4" orientation="portrait" position="float">
    <label>Fig. 4.</label>
    <caption>
      <p id="P71">Brief overview of DLAE configuration structures and engine execution. The Dispatcher class is constructed to send signals related to the DL action to be applied. The dispatcher sends a train signal or an inference signal to the engine. A train signal instructs the engine to construct and train a DL model based on the layer configurations, objective function, and training options specified. An inference signal instructs the engine to load a trained DL model and images with which to make predictions and perform the predictions on the images. The images and annotations are loaded and prepared based on the attributes of the TrainData, ValidationData, and/or TestData classes. The data are then preprocessed based on the specifications in the Preprocessing class. If augmentation is applied for training, the training data is prepared based on the specifications in the Augmentation class. The dispatcher also sends a signal to the engine specifying the DL technique to be applied. The DL techniques currently incorporated in DLAE are CNNs, FCNs, GANs, and BBDs. For training CNNs and FCNs, the computational graph is constructed from the serial list of layers defined in the Layers class. Any inner skip connections (e.g., residual connections) or outer skip connections (e.g., concatenations between the encoder and decoder) are connected in the graph construction, and they must be specified when building the model. For training GANs, two separate networks are defined in the Layers class for the generator and discriminator. The graph construction for both the generator and discriminator follows the same construction algorithm as the serial model for CNNs and FCNs. For training BBDs, an FCN is constructed from the serial list of layers defined in the Layers class. The user must specify hook layers in the serial layers list at desired prediction resolution scales. After the graph is constructed, it is compiled with the configurations defined in the TrainingOptions, Optimizer, LearningRate, LossFunction, and Monitors classes. Multiple models are compiled for GANs: one each for the generator, discriminator, and combined generator and discriminator model. Finally, after the graph is compiled, it is trained based on the parameters defined in the TrainingOptions, Callbacks, TrainData, and ValidationData classes. If the engine receives an inference signal from the dispatcher, the engine loads a model via the Loader class and makes predictions on the data from the TestData class.</p>
    </caption>
    <graphic xlink:href="nihms-1582136-f0004"/>
  </fig>
  <fig id="F5" orientation="portrait" position="float">
    <label>Fig. 5.</label>
    <caption>
      <p id="P72">(First band, BBD) (Left) Example detection inferences for four patients with brain metastases. (Right) Detection sensitivity with respect to brain metastasis size at a 90% confidence threshold. (Second band, GAN) A comparison of true and synthetic rCBV maps generated from DCE-MRI. (Third band, FCN) (Left) Inferred radioactive seed locations for two representative patients. (Right) Precision versus recall for 20 test patients. (Fourth band, CNN) (Left) An example set of manual and inferred contours for a prostate cancer patient after brachytherapy seed implantation. (Right).</p>
    </caption>
    <graphic xlink:href="nihms-1582136-f0005"/>
  </fig>
  <fig id="F6" orientation="portrait" position="float">
    <label>Fig. 6.</label>
    <caption>
      <p id="P73">Example integration of trained DL models into a commercial software package with DLAE. An organ contour template and patient images were sent from this program to a MATLAB-based software development kit (SDK). In the SDK, DLAE received the patient images and imported a model trained to segment the prostate, rectum, bladder, SV, and EUS. DLAE then inferred semantic segmentation masks of these five organs. The segmentation masks were stored in five replicas of the organ contour template. The prostate segmentation mask was used to define a cuboid region of interest (ROI) around the prostate for the sliding-window CNN seed identification model. Subwindows were extracted from the cuboid ROI. DLAE then loaded the seed localization model and made inferences of the seed locations within the prostate. The organ contours and seed locations were sent back to MIM using the SDK.</p>
    </caption>
    <graphic xlink:href="nihms-1582136-f0006"/>
  </fig>
</floats-group>
