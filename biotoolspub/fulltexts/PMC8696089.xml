<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8696089</article-id>
    <article-id pub-id-type="pmid">34498030</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btab645</article-id>
    <article-id pub-id-type="publisher-id">btab645</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Gene Expression</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Learning sparse log-ratios for high-throughput sequencing data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2720-1756</contrib-id>
        <name>
          <surname>Gordon-Rodriguez</surname>
          <given-names>Elliott</given-names>
        </name>
        <xref rid="btab645-cor1" ref-type="corresp"/>
        <aff><institution>Department of Statistics, Columbia University</institution>, New York, NY 10025, <country country="US">USA</country></aff>
        <!--eg2912@columbia.edu-->
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-0286-6329</contrib-id>
        <name>
          <surname>Quinn</surname>
          <given-names>Thomas P</given-names>
        </name>
        <aff><institution>Applied Artificial Intelligence Institute, Deakin University</institution>, Geelong, VIC 3126, <country country="AU">Australia</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Cunningham</surname>
          <given-names>John P</given-names>
        </name>
        <aff><institution>Department of Statistics, Columbia University</institution>, New York, NY 10025, <country country="US">USA</country></aff>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Luigi Martelli</surname>
          <given-names>Pier</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btab645-cor1">To whom correspondence should be addressed. <email>eg2912@columbia.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>01</day>
      <month>1</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2021-09-08">
      <day>08</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>08</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <volume>38</volume>
    <issue>1</issue>
    <fpage>157</fpage>
    <lpage>163</lpage>
    <history>
      <date date-type="received">
        <day>06</day>
        <month>6</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>09</day>
        <month>8</month>
        <year>2021</year>
      </date>
      <date date-type="editorial-decision">
        <day>01</day>
        <month>9</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>03</day>
        <month>9</month>
        <year>2021</year>
      </date>
      <date date-type="corrected-typeset">
        <day>20</day>
        <month>9</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btab645.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>The automatic discovery of sparse biomarkers that are associated with an outcome of interest is a central goal of bioinformatics. In the context of high-throughput sequencing (HTS) data, and <italic toggle="yes">compositional data</italic> (CoDa) more generally, an important class of biomarkers are the log-ratios between the input variables. However, identifying predictive log-ratio biomarkers from HTS data is a combinatorial optimization problem, which is computationally challenging. Existing methods are slow to run and scale poorly with the dimension of the input, which has limited their application to low- and moderate-dimensional metagenomic datasets.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>Building on recent advances from the field of deep learning, we present <italic toggle="yes">CoDaCoRe</italic>, a novel learning algorithm that identifies sparse, interpretable and predictive log-ratio biomarkers. Our algorithm exploits a <italic toggle="yes">continuous relaxation</italic> to approximate the underlying combinatorial optimization problem. This relaxation can then be optimized efficiently using the modern ML toolbox, in particular, gradient descent. As a result, CoDaCoRe runs several orders of magnitude faster than competing methods, all while achieving state-of-the-art performance in terms of predictive accuracy and sparsity. We verify the outperformance of CoDaCoRe across a wide range of microbiome, metabolite and microRNA benchmark datasets, as well as a particularly high-dimensional dataset that is outright computationally intractable for existing sparse log-ratio selection methods.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The CoDaCoRe package is available at <ext-link xlink:href="https://github.com/egr95/R-codacore" ext-link-type="uri">https://github.com/egr95/R-codacore</ext-link>. Code and instructions for reproducing our results are available at <ext-link xlink:href="https://github.com/cunningham-lab/codacore" ext-link-type="uri">https://github.com/cunningham-lab/codacore</ext-link>.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Simons Foundation</institution>
            <institution-id institution-id-type="DOI">10.13039/100000893</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>542963</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Sloan Foundation</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>McKnight Endowment Fund</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>NSF</institution>
            <institution-id institution-id-type="DOI">10.13039/100000001</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>DBI-1707398</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Gatsby Charitable Foundation</institution>
            <institution-id institution-id-type="DOI">10.13039/501100000324</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="7"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>High-throughput sequencing (HTS) technologies have enabled the relative quantification of the different bacteria, metabolites or genes, that are present in a biological sample. However, the nature of these recording technologies results in <italic toggle="yes">sequencing biases</italic> that complicate the analysis of HTS data. In particular, HTS data come as counts, whose totals are constrained to the capacity of the measuring device. These totals are an artifact of the measurement process, and do not depend on the subject being measured. Hence, HTS counts arguably should be interpreted in terms of <italic toggle="yes">relative abundance</italic>; in statistical terminology, it follows that HTS data are an instance of <italic toggle="yes">compositional data</italic> (CoDa) (<xref rid="btab645-B5" ref-type="bibr">Calle, 2019</xref>; <xref rid="btab645-B17" ref-type="bibr">Gloor <italic toggle="yes">et al.</italic>, 2016</xref>, <xref rid="btab645-B18" ref-type="bibr">2017</xref>; <xref rid="btab645-B44" ref-type="bibr">Quinn <italic toggle="yes">et al.</italic>, 2018</xref>, <xref rid="btab645-B45" ref-type="bibr">2019</xref>).</p>
    <p>Mathematically, CoDa can be defined as a set of non-negative vectors whose totals are uninformative. Since the seminal work of <xref rid="btab645-B1" ref-type="bibr">Aitchison (1982)</xref>, the statistical analysis of CoDa has become a discipline in its own right (<xref rid="btab645-B35" ref-type="bibr">Pawlowsky-Glahn and Buccianti, 2011</xref>; <xref rid="btab645-B36" ref-type="bibr">Pawlowsky-Glahn and Egozcue, 2006</xref>). But why does CoDa deserve special treatment? Unlike unconstrained real-valued data, the compositional nature of CoDa results in each variable becoming negatively correlated to all others (increasing one component of a composition implies a relative decrease of the other components). It is well known that, as a result, the usual measures of association and feature attribution are problematic when applied to CoDa (<xref rid="btab645-B14" ref-type="bibr">Filzmoser <italic toggle="yes">et al.</italic>, 2009</xref>; <xref rid="btab645-B28" ref-type="bibr">Lovell <italic toggle="yes">et al.</italic>, 2015</xref>; <xref rid="btab645-B37" ref-type="bibr">Pearson, 1896</xref>). Consequently, bespoke methods are necessary for a valid statistical analysis (<xref rid="btab645-B18" ref-type="bibr">Gloor <italic toggle="yes">et al.</italic>, 2017</xref>). Indeed, the application of CoDa methodology to HTS data, especially microbiome data, has become increasingly popular in recent years (<xref rid="btab645-B5" ref-type="bibr">Calle, 2019</xref>; <xref rid="btab645-B11" ref-type="bibr">Fernandes <italic toggle="yes">et al.</italic>, 2013</xref>, <xref rid="btab645-B12" ref-type="bibr">2014</xref>; <xref rid="btab645-B46" ref-type="bibr">Quinn <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btab645-B48" ref-type="bibr">Rivera-Pinto <italic toggle="yes">et al.</italic>, 2018</xref>).</p>
    <p>The standard approach for analyzing CoDa is based on applying <italic toggle="yes">log-ratio</italic> transformations to map our data onto unconstrained Euclidean space, where the usual tools of statistical learning apply (<xref rid="btab645-B36" ref-type="bibr">Pawlowsky-Glahn and Egozcue, 2006</xref>). The choice of the log-ratio transform offers the necessary property of scale invariance, but in the CoDa literature, it holds primacy for a variety of other technical reasons, including <italic toggle="yes">subcompositional coherence</italic> (<xref rid="btab645-B1" ref-type="bibr">Aitchison, 1982</xref>; <xref rid="btab645-B35" ref-type="bibr">Pawlowsky-Glahn and Buccianti, 2011</xref>). Log-ratios can be taken over pairs of input variables (<xref rid="btab645-B1" ref-type="bibr">Aitchison, 1982</xref>; <xref rid="btab645-B3" ref-type="bibr">Bates and Tibshirani, 2019</xref>; <xref rid="btab645-B21" ref-type="bibr">Greenacre, 2019b</xref>) or aggregations thereof, typically geometric means (<xref rid="btab645-B1" ref-type="bibr">Aitchison, 1982</xref>; <xref rid="btab645-B10" ref-type="bibr">Egozcue, 2003</xref>; <xref rid="btab645-B9" ref-type="bibr">Egozcue and Pawlowsky-Glahn, 2005</xref>; <xref rid="btab645-B48" ref-type="bibr">Rivera-Pinto <italic toggle="yes">et al.</italic>, 2018</xref>) or summations (<xref rid="btab645-B20" ref-type="bibr">Greenacre, 2019a</xref>, <xref rid="btab645-B22" ref-type="bibr">2020</xref>; <xref rid="btab645-B42" ref-type="bibr">Quinn and Erb, 2020</xref>). The resulting features work well empirically, but also imply a clear interpretation: a log-ratio is a single composite score that expresses the overall quantity of one sub-population as compared with another. For example, in microbiome HTS data, the relative weights between sub-populations of related microorganisms are commonly used as clinical biomarkers (<xref rid="btab645-B7" ref-type="bibr">Crovesy <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab645-B31" ref-type="bibr">Magne <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab645-B47" ref-type="bibr">Rahat-Rozenbloom <italic toggle="yes">et al.</italic>, 2014</xref>). When the log-ratios are sparse, meaning they are taken over a small number of input variables, they define biomarkers that are particularly intuitive to understand, a key desiderata for predictive models that are of clinical relevance (<xref rid="btab645-B19" ref-type="bibr">Goodman and Flaxman, 2017</xref>).</p>
    <p><italic toggle="yes">Thus, learning sparse log-ratios is a central problem in CoDa.</italic> This problem is especially challenging in the context of HTS data, due to its high dimensionality (ranging from 100 to over 10 000 variables). Existing methods rely on stepwise search (<xref rid="btab645-B21" ref-type="bibr">Greenacre, 2019b</xref>; <xref rid="btab645-B48" ref-type="bibr">Rivera-Pinto <italic toggle="yes">et al.</italic>, 2018</xref>) or evolutionary algorithms (<xref rid="btab645-B39" ref-type="bibr">Prifti <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab645-B42" ref-type="bibr">Quinn and Erb, 2020</xref>), which scale poorly with the dimension of the input. These algorithms are prohibitively slow for most HTS datasets, and thus there is a new demand for sparse and interpretable models that scale to high dimensions (<xref rid="btab645-B6" ref-type="bibr">Cammarota <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab645-B26" ref-type="bibr">Li, 2015</xref>; <xref rid="btab645-B51" ref-type="bibr">Susin <italic toggle="yes">et al.</italic>, 2020</xref>).</p>
    <p>This demand motivates the present work, in which we present CoDaCoRe, a novel learning algorithm for <bold>Co</bold>mpositional <bold>Da</bold>ta via <bold>Co</bold>ntinuous <bold>Re</bold>laxations. CoDaCoRe builds on recent advances from the deep learning literature on <italic toggle="yes">continuous relaxations</italic> of discrete latent variables (<xref rid="btab645-B25" ref-type="bibr">Jang <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btab645-B27" ref-type="bibr">Linderman <italic toggle="yes">et al.</italic>, 2018</xref>); we design a novel relaxation that approximates a combinatorial optimization problem over the set of log-ratios. In turn, this approximation can be optimized efficiently using gradient descent, and subsequently discretized to produce a sparse log-ratio biomarker, thus dramatically reducing runtime without sacrificing interpretability nor predictive accuracy. The main contributions of our method can be summarized as follows:
</p>
    <list list-type="bullet">
      <list-item>
        <p><bold>Computational efficiency.</bold> CoDaCoRe scales linearly with the dimension of the input. It runs several orders of magnitude faster than its competitors.</p>
      </list-item>
      <list-item>
        <p><bold>Interpretability.</bold> CoDaCoRe identifies a set of log-ratios that are sparse, biologically meaningful and ranked in order of importance. Our model is highly interpretable, and much sparser, relative to competing methods of similar accuracy and computational complexity.</p>
      </list-item>
      <list-item>
        <p><bold>Predictive accuracy.</bold> CoDaCoRe achieves better out-of-sample accuracy than existing CoDa methods, and performs similarly to state-of-the-art black-box classifiers (which are neither sparse nor interpretable).</p>
      </list-item>
      <list-item>
        <p><bold>Ease of use.</bold> We devise an adaptive learning rate scheme that enables CoDaCoRe to converge reliably, requiring no additional hyperparameter tuning.</p>
      </list-item>
    </list>
  </sec>
  <sec>
    <title>2 Background</title>
    <p>Our work focuses on the supervised learning problem <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>↦</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, where the inputs <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are HTS data (or any CoDa), and the outputs <italic toggle="yes">y<sub>i</sub></italic> are the outcome of interest. For many microbiome applications, <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents a vector of frequencies of the different species of bacteria that compose the microbiome of the <italic toggle="yes">i</italic>th subject. In other words, <italic toggle="yes">x<sub>ij</sub></italic> denotes the abundance of the <italic toggle="yes">j</italic>th species (of which there are <italic toggle="yes">p</italic> total) in the <italic toggle="yes">i</italic>th subject. The response <italic toggle="yes">y<sub>i</sub></italic> is often a binary variable indicating whether the <italic toggle="yes">i</italic>th subject belongs to the case or the control groups (e.g. sick versus healthy). For HTS data, the input frequencies <italic toggle="yes">x<sub>ij</sub></italic> arise from an inexhaustive sampling procedure, so that the totals <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>p</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are arbitrary and the components should only be interpreted in relative terms (i.e. as CoDa) (<xref rid="btab645-B5" ref-type="bibr">Calle, 2019</xref>; <xref rid="btab645-B18" ref-type="bibr">Gloor <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btab645-B16" ref-type="bibr">Gloor and Reid, 2016</xref>; <xref rid="btab645-B44" ref-type="bibr">Quinn <italic toggle="yes">et al.</italic>, 2018</xref>). While many of our applications pertain to microbiome data, our method applies to any high-dimensional HTS data, including those produced by <italic toggle="yes">Liquid Chromatography Mass Spectrometry</italic> (<xref rid="btab645-B13" ref-type="bibr">Filzmoser and Walczak, 2014</xref>).</p>
    <sec>
      <title>2.1 Log-ratio analysis</title>
      <p>Our goal is to obtain sparse log-ratio transformed features that can be passed to a downstream classifier or regression function. As discussed, these log-ratios will result in interpretable features and scale-invariant models (that are also subcompositionally coherent), thus satisfying the key requirements for valid statistical inference in the context of CoDa. The simplest such choice is the <italic toggle="yes">pairwise log-ratio</italic>, defined as <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:mtext>log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msup><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msup><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">j</italic><sup>+</sup> and <italic toggle="yes">j</italic><sup>–</sup> denote the indexes of a pair of input variables (<xref rid="btab645-B1" ref-type="bibr">Aitchison, 1982</xref>). Note that the ratio cancels out any scaling factor applied to <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, preserving only the relative information, while the log transformation ensures the output is (unconstrained) real-valued. There are many such <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> pairs (to be precise, <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> of them). In order to select good pairwise log-ratios from a set of input variables, <xref rid="btab645-B21" ref-type="bibr">Greenacre (2019b</xref>) proposed a greedy step-wise search algorithm. This method produces a sparse and interpretable set of features, but it is prohibitively slow on high-dimensional datasets, as a result of the step-wise algorithm scaling quadratically in the dimension of the input. A heuristic search algorithm that is less accurate but computationally faster has been developed as part of <xref rid="btab645-B43" ref-type="bibr">Quinn <italic toggle="yes">et al.</italic> (2017)</xref>, though its computational cost is still troublesome (as we shall see in Section 4). The <italic toggle="yes">log-ratio lasso</italic> is a computationally efficient alternative for selecting pairwise log-ratios (<xref rid="btab645-B3" ref-type="bibr">Bates and Tibshirani, 2019</xref>).</p>
      <sec>
        <title>2.1.1 Balances</title>
        <p>Recently, a class of log-ratios known as <italic toggle="yes">balances</italic> (<xref rid="btab645-B9" ref-type="bibr">Egozcue and Pawlowsky-Glahn, 2005</xref>) have become of interest in microbiome applications, due to their interpretability as the relative weight between two sub-populations of bacteria (<xref rid="btab645-B34" ref-type="bibr">Morton <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btab645-B41" ref-type="bibr">Quinn and Erb, 2019</xref>). Balances are defined as the log-ratios between geometric means of two subsets of the input variables (Note that the original definition of balances includes a “normalization” constant, which we omit for clarity. This constant is in fact unnecessary, as it will get absorbed into a regression coefficient downstream.):
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
 <disp-formula id="E2"><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msup></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">J</italic><sup>+</sup> and <italic toggle="yes">J</italic><sup>–</sup> denote a pair of disjoint subsets of the indices <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, and <italic toggle="yes">p</italic><sup>+</sup> and <italic toggle="yes">p</italic><sup>–</sup> denote their respective sizes. For example, in microbiome data, <italic toggle="yes">J</italic><sup>+</sup> and <italic toggle="yes">J</italic><sup>–</sup> are groups of bacteria species that may be related by their environmental niche (<xref rid="btab645-B33" ref-type="bibr">Morton <italic toggle="yes">et al.</italic>, 2017</xref>) or genetic similarity (<xref rid="btab645-B50" ref-type="bibr">Silverman <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btab645-B54" ref-type="bibr">Washburne <italic toggle="yes">et al.</italic>, 2017</xref>). Note that when <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> (i.e. <italic toggle="yes">J</italic><sup>+</sup> and <italic toggle="yes">J</italic><sup>–</sup> each contain a single element), <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> reduces to a pairwise log-ratio. By allowing for the aggregation of more than one variable in the numerator and denominator of the log-ratio, balances provide a far richer set of features that allows for more flexible models than pairwise log-ratios. Insofar as the balances are taken over a small number of variables (i.e. <italic toggle="yes">J</italic><sup>+</sup> and <italic toggle="yes">J</italic><sup>–</sup> are sparse), they also provide highly interpretable biomarkers.</p>
        <p>The <italic toggle="yes">selbal</italic> algorithm (<xref rid="btab645-B48" ref-type="bibr">Rivera-Pinto <italic toggle="yes">et al.</italic>, 2018</xref>) has gained popularity as a method for automatically identifying balances that predict a response variable. However, this algorithm is also based on a greedy step-wise search through the combinatorial space of subset pairs <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, which scales poorly in the dimension of the input and becomes prohibitively slow for many HTS datasets (<xref rid="btab645-B51" ref-type="bibr">Susin <italic toggle="yes">et al.</italic>, 2020</xref>).</p>
      </sec>
      <sec>
        <title>2.1.2 Amalgamations</title>
        <p>An alternative to balances, known as <italic toggle="yes">amalgamation</italic>, is defined by aggregating components through summation:
<disp-formula id="E3"><label>(2)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where again <italic toggle="yes">J</italic><sup>+</sup> and <italic toggle="yes">J</italic><sup>–</sup> denote disjoint subsets of the input components. Amalgamations have the advantage of reducing the dimensionality of the data through an operation, the sum, that some authors argue is more interpretable than a geometric mean (<xref rid="btab645-B20" ref-type="bibr">Greenacre, 2019a</xref>; <xref rid="btab645-B23" ref-type="bibr">Greenacre <italic toggle="yes">et al.</italic>, 2020</xref>). On the other hand, amalgamations can be less effective than balances for identifying components that are statistically important, but small in magnitude, e.g. rare bacteria species (since small terms will have less impact on a summation than on a product).</p>
        <p>Recently, <xref rid="btab645-B22" ref-type="bibr">Greenacre (2020)</xref> has advocated for the use of expert-driven amalgamations, using domain knowledge to construct the relevant features. On the other hand, <xref rid="btab645-B42" ref-type="bibr">Quinn and Erb (2020)</xref> proposed <italic toggle="yes">amalgam</italic>, an evolutionary algorithm to automatically identify amalgamated log-ratios (<xref rid="E3" ref-type="disp-formula">Equation 2</xref>) that are predictive of a response variable. However, this algorithm does not scale to high-dimensional data (albeit, comparing favorably to selbal), nor does it produce sparse models (hindering interpretability of the results). A similar evolutionary algorithm can be found in <xref rid="btab645-B39" ref-type="bibr">Prifti <italic toggle="yes">et al.</italic> (2020)</xref>, however, their model is not scale invariant, as is required by most authors in the field (<xref rid="btab645-B36" ref-type="bibr">Pawlowsky-Glahn and Egozcue, 2006</xref>).</p>
        <p>Other relevant log-ratio methodology is briefly reviewed in <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref> (<xref rid="sup1" ref-type="supplementary-material">Supplementary Section SB</xref>).</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>3 Materials and methods</title>
    <p>We now present CoDaCoRe, a novel learning algorithm for HTS data, and more generally, high-dimensional CoDa. Unlike existing methods, CoDaCoRe is simultaneously scalable, interpretable, sparse and accurate. In <xref rid="btab645-T1" ref-type="table">Table 1</xref> from <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref> (<xref rid="sup1" ref-type="supplementary-material">Supplementary Section SC</xref>), we summarize the relative merits of CoDaCoRe and its competitors.</p>
    <table-wrap position="float" id="btab645-T1">
      <label>Table 1.</label>
      <caption>
        <p>Evaluation metrics shown for each method, averaged over 25 datasets × 20 random train/test splits</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col valign="top" align="left" span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th rowspan="1" colspan="1"/>
            <th rowspan="1" colspan="1">Runtime (s)</th>
            <th rowspan="1" colspan="1">Active inputs (%)</th>
            <th rowspan="1" colspan="1">Accuracy (%)</th>
            <th rowspan="1" colspan="1">AUC (%)</th>
            <th rowspan="1" colspan="1">F1 (%)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="1" colspan="1">CoDaCoRe—Balances (ours)</td>
            <td rowspan="1" colspan="1"><bold>4.5 </bold>±<bold>0.4</bold></td>
            <td rowspan="1" colspan="1"><bold>1.9 </bold>±<bold>0.3</bold></td>
            <td rowspan="1" colspan="1">75.2 ± 2.4</td>
            <td rowspan="1" colspan="1">79.5 ± 2.6</td>
            <td rowspan="1" colspan="1">73.7 ± 2.6</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">CoDaCoRe—Amalgamations (ours)</td>
            <td rowspan="1" colspan="1"><bold>4.4</bold> ± <bold>0.4</bold></td>
            <td rowspan="1" colspan="1">1.9 ± 0.3</td>
            <td rowspan="1" colspan="1">71.8 ± 2.4</td>
            <td rowspan="1" colspan="1">74.5 ± 2.8</td>
            <td rowspan="1" colspan="1">69.8 ± 2.9</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">selbal (<xref rid="btab645-B48" ref-type="bibr">Rivera-Pinto <italic toggle="yes">et al.</italic>, 2018</xref>)</td>
            <td rowspan="1" colspan="1">79 033.7 ± 2094.1</td>
            <td rowspan="1" colspan="1">2.4 ± 0.2</td>
            <td rowspan="1" colspan="1">61.2 ± 1.9</td>
            <td rowspan="1" colspan="1">80.0 ± 2.4</td>
            <td rowspan="1" colspan="1">70.9 ± 1.1</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Pairwise log-ratios (<xref rid="btab645-B21" ref-type="bibr">Greenacre, 2019b</xref>)</td>
            <td rowspan="1" colspan="1">14 207.0 ± 1038.4</td>
            <td rowspan="1" colspan="1">2.5 ± 0.4</td>
            <td rowspan="1" colspan="1">73.3 ± 1.7</td>
            <td rowspan="1" colspan="1">75.2 ± 2.4</td>
            <td rowspan="1" colspan="1">67.8 ± 3.0</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Lasso</td>
            <td rowspan="1" colspan="1"><bold>1.6</bold> ± <bold>0.1</bold></td>
            <td rowspan="1" colspan="1">4.4 ± 0.6</td>
            <td rowspan="1" colspan="1">72.4 ± 1.7</td>
            <td rowspan="1" colspan="1">75.2 ± 2.3</td>
            <td rowspan="1" colspan="1">65.2 ± 3.7</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">CoDaCoRe—balances with <italic toggle="yes">λ </italic>= 0 (ours)</td>
            <td rowspan="1" colspan="1"><bold>9.8</bold> ± <bold>2.2</bold></td>
            <td rowspan="1" colspan="1">6.1 ± 0.7</td>
            <td rowspan="1" colspan="1"><bold>77.6</bold> ± <bold>2.2</bold></td>
            <td rowspan="1" colspan="1"><bold>82.0</bold> ± <bold>2.3</bold></td>
            <td rowspan="1" colspan="1"><bold>76.0</bold> ± <bold>2.5</bold></td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Coda-lasso (<xref rid="btab645-B29" ref-type="bibr">Lu <italic toggle="yes">et al</italic>., 2019</xref>)</td>
            <td rowspan="1" colspan="1">1043.0 ± 55.4</td>
            <td rowspan="1" colspan="1">19.7 ± 2.7</td>
            <td rowspan="1" colspan="1">72.5 ± 2.3</td>
            <td rowspan="1" colspan="1">78.0 ± 2.4</td>
            <td rowspan="1" colspan="1">64.2 ± 4.4</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">amalgam (<xref rid="btab645-B42" ref-type="bibr">Quinn and Erb, 2020</xref>)</td>
            <td rowspan="1" colspan="1">7360.5 ± 209.8</td>
            <td rowspan="1" colspan="1">87.6 ± 2.1</td>
            <td rowspan="1" colspan="1">74.4 ± 2.5</td>
            <td rowspan="1" colspan="1">78.2 ± 2.7</td>
            <td rowspan="1" colspan="1">73.9 ± 2.8</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">DeepCoDA (<xref rid="btab645-B40" ref-type="bibr">Quinn <italic toggle="yes">et al</italic>., 2020</xref>)</td>
            <td rowspan="1" colspan="1">296.5 ± 21.4</td>
            <td rowspan="1" colspan="1">89.3 ± 0.6</td>
            <td rowspan="1" colspan="1">70.6 ± 2.9</td>
            <td rowspan="1" colspan="1">77.6 ± 2.9</td>
            <td rowspan="1" colspan="1">64.7 ± 7.4</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">CLR-lasso (<xref rid="btab645-B51" ref-type="bibr">Susin <italic toggle="yes">et al.</italic>, 2020</xref>)</td>
            <td rowspan="1" colspan="1"><bold>2.0</bold> ± <bold>0.2</bold></td>
            <td rowspan="1" colspan="1">100.0 ± 0.0</td>
            <td rowspan="1" colspan="1">77.5 ± 1.8</td>
            <td rowspan="1" colspan="1">81.6 ± 2.2</td>
            <td rowspan="1" colspan="1">75.8 ± 2.7</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Random Forest</td>
            <td rowspan="1" colspan="1">10.6 ± 0.4</td>
            <td rowspan="1" colspan="1">–</td>
            <td rowspan="1" colspan="1">78.0 ± 2.2</td>
            <td rowspan="1" colspan="1">82.2 ± 2.2</td>
            <td rowspan="1" colspan="1">77.3 ± 2.5</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Log-ratio lasso (<xref rid="btab645-B3" ref-type="bibr">Bates and Tibshirani, 2019</xref>)*</td>
            <td rowspan="1" colspan="1">135.0 ± 11.1</td>
            <td rowspan="1" colspan="1">0.7 ± 0.0</td>
            <td rowspan="1" colspan="1">72.0 ± 2.4</td>
            <td rowspan="1" colspan="1">76.4 ± 2.3</td>
            <td rowspan="1" colspan="1">69.2 ± 2.7</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="tblfn1">
          <p><italic toggle="yes">Note</italic>: Standard errors are computed independently on each dataset, and then averaged over the 25 datasets. The models are ordered by sparsity, i.e. percentage of active input variables. CoDaCoRe (with balances) is the only learning algorithm that is simultaneously fast, sparse and accurate. The penultimate row shows the performance of Random Forest, a powerful black-box classifier which can be thought of as providing an approximate upper bound on the predictive accuracy of any interpretable model. The bottom row is shown separately and marked with an asterisk because the corresponding algorithm failed to converge on 432 out our 500 runs (averages were taken after imputing these missing values with the corresponding values obtained with pairwise log-ratios, which is the most similar method). We highlight in bold the CoDa models that are fast to run, as well as the CoDa models that are most sparse and accurate.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <sec>
      <title>3.1 Optimization problem</title>
      <p>In its basic formulation, CoDaCoRe learns a regression function of the form:
<disp-formula id="E4"><label>(3)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>α</mml:mo><mml:mo>+</mml:mo><mml:mo>β</mml:mo><mml:mo>·</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">B</italic> denotes a balance (<xref rid="E1" ref-type="disp-formula">Equation 1</xref>), and <italic toggle="yes">α and β</italic> are scalar parameters. This regression function can be thought of in two stages: (i) we take the input and use it to compute a balance score and (ii) we feed the balance score to a logistic regression classifier. For clarity, we will restrict our exposition to this formulation, but note that our algorithm can be applied equally to learn amalgamations instead of balances (see Section 3.6), as well as generalizing straightforwardly to non-linear functions (provided they are suitably parameterized and differentiable).</p>
      <p>Let <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denote the cross-entropy loss, with <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:math></inline-formula> given in logit space. The goal of CoDaCoRe is to find the balance that is maximally associated with the response. Mathematically, this can be written as:
<disp-formula id="E5"><label>(4)</label><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mo>α</mml:mo><mml:mo>,</mml:mo><mml:mo>β</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>α</mml:mo><mml:mo>+</mml:mo><mml:mo>β</mml:mo><mml:mo>·</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>This objective function may look similar to a univariate logistic regression, however, our problem is complicated by the joint optimization over the subsets <italic toggle="yes">J</italic><sup>+</sup> and <italic toggle="yes">J</italic><sup>–,</sup> which determine the input variables that compose the balance. Note that the number of possible subsets of <italic toggle="yes">p</italic> variables is <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, so the set of possible balances is greater than <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and grows <italic toggle="yes">exponentially</italic> in <italic toggle="yes">p</italic>. Exact optimization is therefore computationally intractable for any but the smallest of datasets, and an approximate solution is required. Selbal corresponds to one such approximation, offering <italic toggle="yes">quadratic</italic> complexity in <italic toggle="yes">p</italic>, which is practical for low- to moderate-dimensional datasets (<inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo></mml:mrow></mml:math></inline-formula> 100), but does not scale to high dimensions (<inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&gt;</mml:mo></mml:mrow></mml:math></inline-formula> 1000). As we shall now see, CoDaCoRe represents a critical improvement, achieving <italic toggle="yes">linear</italic> complexity in <italic toggle="yes">p</italic> which dramatically reduces runtime and enables, for the first time, the use of balances and amalgamations for the analysis of high-dimensional HTS data.</p>
    </sec>
    <sec>
      <title>3.2 Continuous relaxation</title>
      <p><italic toggle="yes">The key insight of CoDaCoRe is to approximate our combinatorial optimization problem (<xref rid="E5" ref-type="disp-formula">Equation 4) with a continuous relaxation that can be trained efficiently by gradient descent.</xref></italic> Our relaxation is inspired by recent advances in deep learning models with discrete latent variables (<xref rid="btab645-B25" ref-type="bibr">Jang <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btab645-B27" ref-type="bibr">Linderman <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btab645-B30" ref-type="bibr">Maddison <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btab645-B32" ref-type="bibr">Mena <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btab645-B38" ref-type="bibr">Potapczynski <italic toggle="yes">et al.</italic>, 2020</xref>). However, we are not aware of any similar proposals for optimizing over disjoint subsets, nor for learning balances or amalgamations in the context of CoDa.</p>
      <p>Our relaxation is parameterized by an unconstrained vector of ‘assignment weights’, <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, with one scalar parameter per input dimension (e.g. one weight per bacteria species). The weights are mapped to a vector of ‘soft assignments’ via:
<disp-formula id="E6"><label>(5)</label><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>·</mml:mo><mml:mtext>sigmoid</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where the sigmoid is applied component-wise. Intuitively, large positive weights will max out the sigmoid, leading to soft assignments close to + 1, whereas large negative weights will zero out the sigmoid, resulting in soft assignments close to –1. This mapping is akin to softly assigning input variables to the groups <italic toggle="yes">J</italic><sup>+</sup> and <italic toggle="yes">J</italic><sup>–,</sup> respectively.</p>
      <p>Let us write <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mtext>ReLU</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mtext>ReLU</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for the (component-wise) positive and negative parts of <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, respectively. We approximate balances (<xref rid="E1" ref-type="disp-formula">Equation 1</xref>) with the following relaxation:
<disp-formula id="E7"><label>(6)</label><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
 <disp-formula id="E8"><label>(7)</label><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>·</mml:mo><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo></mml:msup><mml:mo>·</mml:mo><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>In other words, we approximate the geometric averages over subsets of the inputs, by <italic toggle="yes">weighted</italic> geometric averages over all components (compare <xref rid="E1" ref-type="disp-formula">Equations 1</xref> and <xref rid="E7" ref-type="disp-formula">6</xref>).</p>
      <p>Crucially, this relaxation is differentiable in <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mi mathvariant="bold">w</mml:mi></mml:math></inline-formula>, allowing us to construct a surrogate objective function that can be optimized jointly in <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo>,</mml:mo><mml:mo>α</mml:mo><mml:mo>,</mml:mo><mml:mo>β</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> by gradient descent:
<disp-formula id="E9"><label>(8)</label><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo>,</mml:mo><mml:mo>α</mml:mo><mml:mo>,</mml:mo><mml:mo>β</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>α</mml:mo><mml:mo>+</mml:mo><mml:mo>β</mml:mo><mml:mo>·</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>Moreover, the computational cost of differentiating this objective function scales linearly in the dimension of <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mi mathvariant="bold">w</mml:mi></mml:math></inline-formula>, which overall results in linear scaling for our algorithm. We also note that the functional form of our relaxation (<xref rid="E7" ref-type="disp-formula">Equation 6</xref>) can be exploited in order to select the learning rate adaptively (i.e. without tuning), resulting in robust convergence across all real and simulated datasets that we considered. We defer the details of our implementation of gradient descent to <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref> (<xref rid="sup1" ref-type="supplementary-material">Supplementary Section SC.1</xref>).</p>
    </sec>
    <sec>
      <title>3.3 Discretization</title>
      <p>While a set of features in the form of <xref rid="E7" ref-type="disp-formula">Equation 6</xref> may perform accurate classification, a weighted geometric average over all input variables is much harder for a biologist to interpret (and less intuitively appealing) than a bona fide balance over a small number of variables. For this reason, CoDaCoRe implements a ‘discretization’ procedure that exploits the information learned by the soft assignment vector <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, in order to efficiently identify a pair of sparse subsets, <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, which will define a balance.</p>
      <p>The most straightforward way to convert the (soft) assignment <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> into a (hard) pair of subsets is by fixing a threshold <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>:
<disp-formula id="E10"><label>(9)</label><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>j</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mi>t</mml:mi><mml:mo>}</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
 <disp-formula id="E11"><label>(10)</label><mml:math id="M11" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>j</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo>}</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>Note that given a trained <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and a fixed threshold <italic toggle="yes">t</italic>, we can evaluate the quality of the corresponding balance <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> (resp. amalgamation) by optimizing <xref rid="E5" ref-type="disp-formula">Equation 4</xref> over <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>α</mml:mo><mml:mo>,</mml:mo><mml:mo>β</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> alone, i.e. fitting a linear model. Computationally, fitting a linear model is much faster than optimizing <xref rid="E9" ref-type="disp-formula">Equation 8</xref>, and can be done repeatedly for a range of values of <italic toggle="yes">t</italic> with little overhead. In CoDaCoRe, we combine this strategy with cross-validation in order to select the threshold, <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, that optimizes predictive performance (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Section SC.2</xref> of <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref> for full detail). Finally, the trained regression function is:
<disp-formula id="E12"><label>(11)</label><mml:math id="M12" display="block" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>·</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE35"><mml:math id="IM35" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE36"><mml:math id="IM36" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> are the subsets corresponding to the optimal threshold <inline-formula id="IE37"><mml:math id="IM37" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, and <inline-formula id="IE38"><mml:math id="IM38" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are the coefficients obtained by regressing <italic toggle="yes">y<sub>i</sub></italic> against <inline-formula id="IE39"><mml:math id="IM39" display="inline" overflow="scroll"><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> on the entire training set.</p>
    </sec>
    <sec>
      <title>3.4 Regularization</title>
      <p>Note from <xref rid="E10" ref-type="disp-formula">Equations 9</xref> and <xref rid="E11" ref-type="disp-formula">10</xref> that larger values of <italic toggle="yes">t</italic> result in fewer input variables assigned to the balance <inline-formula id="IE40"><mml:math id="IM40" display="inline" overflow="scroll"><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, i.e. a sparser model. Thus, CoDaCoRe can be regularized simply by making <inline-formula id="IE41"><mml:math id="IM41" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> larger. Similar to lasso regression, CoDaCoRe uses the <italic toggle="yes">1-standard-error</italic> rule: namely, to pick the sparsest model (i.e. the highest <italic toggle="yes">t</italic>) with mean cross-validated score within 1 standard error of the optimum (<xref rid="btab645-B15" ref-type="bibr">Friedman <italic toggle="yes">et al.</italic>, 2001</xref>). Trivially, this rule can be generalized to a <italic toggle="yes">λ</italic>-standard-error rule (to pick the sparsest model within <italic toggle="yes">λ</italic> standard errors of the optimum), where <italic toggle="yes">λ</italic> becomes a regularization hyperparameter that can be tuned by the practitioner if so desired (with lower values trading off some sparsity in exchange for predictive accuracy). In our public implementation, <italic toggle="yes">λ</italic>  =  1 is our default value, and this is used throughout our experiments (except where we indicate otherwise). In practice, lower values (e.g. <italic toggle="yes">λ</italic>  =  0) can be useful when the emphasis is on predictive accuracy rather than interpretability or sparsity, though our benchmarks showed competitive performance for any <inline-formula id="IE42"><mml:math id="IM42" display="inline" overflow="scroll"><mml:mrow><mml:mo>λ</mml:mo><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
    </sec>
    <sec>
      <title>3.5 CoDaCoRe algorithm</title>
      <p>The computational efficiency of our continuous relaxation allows us to train multiple regressors of the form of <xref rid="E12" ref-type="disp-formula">Equation 11</xref> within a single model. In the full CoDaCoRe algorithm, we ensemble multiple such regressors in a stage-wise additive fashion, where each successive balance is fitted on the residual from the current model. Thus, CoDaCoRe identifies a <italic toggle="yes">sequence</italic> of balances, in decreasing order of importance, each of which is sparse and interpretable. Training terminates when an additional relaxation (<xref rid="E7" ref-type="disp-formula">Equation 6</xref>) cannot improve the cross-validation score relative to the existing ensemble (equivalently, when we obtain <inline-formula id="IE43"><mml:math id="IM43" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>). Typically, only a small number of balances is required to capture the signal in the data, and as a result CoDaCoRe produces very sparse models overall, further enhancing interpretability. In <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>, we summarize our procedure in <xref rid="sup1" ref-type="supplementary-material">Supplementary Algorithm S1</xref> (<xref rid="sup1" ref-type="supplementary-material">Supplementary Section SC</xref>) and we describe a number of extensions to the CoDaCoRe framework (<xref rid="sup1" ref-type="supplementary-material">Supplementary Section SD</xref>), including unsupervised learning.</p>
    </sec>
    <sec>
      <title>3.6 Amalgamations</title>
      <p>CoDaCoRe can be used to learn amalgamations (<xref rid="E3" ref-type="disp-formula">Equation 2</xref>) much in the same way as for balances (the choice of which to use depending on the goals of the biologist). In this case, our relaxation is defined as:
<disp-formula id="E13"><label>(12)</label><mml:math id="M13" display="block" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo></mml:msup><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>i.e. we approximate summations over subsets of the inputs, with <italic toggle="yes">weighted</italic> summations over all components (compare <xref rid="E3" ref-type="disp-formula">Equation 2</xref> and <xref rid="E13" ref-type="disp-formula">Equation 12</xref>). The rest of the argument follows verbatim, replacing <inline-formula id="IE44"><mml:math id="IM44" display="inline" overflow="scroll"><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with <inline-formula id="IE45"><mml:math id="IM45" display="inline" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE46"><mml:math id="IM46" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with <inline-formula id="IE47"><mml:math id="IM47" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> in <xref rid="E4" ref-type="disp-formula">Equations 3</xref>, <xref rid="E5" ref-type="disp-formula">4</xref>, <xref rid="E9" ref-type="disp-formula">8</xref> and <xref rid="E12" ref-type="disp-formula">11</xref>.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Experiments</title>
    <p>We evaluate CoDaCoRe on a collection of 25 benchmark datasets including 13 datasets from the <italic toggle="yes">Microbiome Learning Repo</italic> (<xref rid="btab645-B52" ref-type="bibr">Vangay <italic toggle="yes">et al.</italic>, 2019</xref>), and 12 microbiome, metabolite and microRNA datasets curated by <xref rid="btab645-B41" ref-type="bibr">Quinn and Erb (2019)</xref>. These data vary in dimension from 48 to 3090 input variables (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Section SE</xref> of <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref> for a full description). For each dataset, we fit CoDaCoRe and competing methods on 20 random 80/20 train/test splits, sampled with stratification by case–control (<xref rid="btab645-B24" ref-type="bibr">He and Ma, 2013</xref>). Competing methods and their implementation are described in <xref rid="sup1" ref-type="supplementary-material">Supplementary Section SF.2</xref> of <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>.</p>
    <sec>
      <title>4.1 Results</title>
      <p>We evaluate the quality of our models across the following criteria: computational efficiency (as measured by runtime), sparsity (as measured by the percentage of input variables that are active in the model) and predictive accuracy (as measured by out-of-sample accuracy, ROC AUC and F1 score). <xref rid="btab645-T1" ref-type="table">Table 1</xref> provides an aggregated summary of the results; CoDaCoRe (with balances) is performant on all metrics. Indeed, our method provides the only interpretable model that is simultaneously scalable, sparse and accurate. Detailed performance metrics on each of the 25 datasets are provided in <xref rid="sup1" ref-type="supplementary-material">Supplementary Section SF</xref> of <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>, together with critical difference diagrams for each of our success metrics.</p>
      <p><xref rid="btab645-F1" ref-type="fig">Figure 1</xref> shows the average runtime of our classifiers on each dataset, with larger points denoting larger datasets. On these common benchmark datasets, CoDaCoRe trains up to 5 orders of magnitude faster than existing interpretable CoDa methods. On our larger datasets (3090 inputs), selbal runs in <inline-formula id="IE48"><mml:math id="IM48" display="inline" overflow="scroll"><mml:mrow><mml:mo>∼</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> hours, pairwise log-ratios and amalgam both run in <inline-formula id="IE49"><mml:math id="IM49" display="inline" overflow="scroll"><mml:mrow><mml:mo>∼</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> hours, and CoDaCoRe runs in under 10 seconds (full runtimes are provided in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S5</xref> in <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>). All runs, including those involving gradient descent, were performed on identical CPU cores; CoDaCoRe can be accelerated further using GPUs, but we did not find it necessary to do so. It is also worth noting that the outperformance of CoDaCoRe is not merely as a result of the other methods failing on high-dimensional datasets. <xref rid="sup1" ref-type="supplementary-material">Supplementary Section SF.1.1</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S4</xref> in <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref> show that CoDaCoRe performs consistently across low- and high-dimensional datasets, and enjoys better sample efficiency than competing methods. Better sample efficiency could represent a particular advantage in biomedical studies, where most datasets have low <italic toggle="yes">n</italic> and high <italic toggle="yes">p</italic>.</p>
      <fig position="float" id="btab645-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>Gain in classification accuracy (relative to the “majority vote” baseline classifier) plotted against runtime. Each point represents one of 25 datasets, with size proportional to the input dimension. Note the <italic toggle="yes">x</italic>-axis is drawn on the log-scale. CoDaCoRe (with balances) is the only method that scales effectively to our larger datasets, while consistently achieving high predictive accuracy. Moreover, its performance is broadly consistent across smaller and larger datasets</p>
        </caption>
        <graphic xlink:href="btab645f1" position="float"/>
      </fig>
      <p>Not only is CoDaCoRe sparser and more accurate than other interpretable models, it also performs on par with state-of-the-art black-box classifiers. By simply reducing the regularization parameter, from <italic toggle="yes">λ</italic>  =  1 to <italic toggle="yes">λ</italic>  =  0, CoDaCoRe (with balances) achieved an average 77.6% out-of-sample accuracy of and 82.0% AUC, on par with Random Forest (penultimate row of <xref rid="btab645-T2" ref-type="table">Table 1</xref>), while only using 5.9% of the input variables, on average. This result indicates, first, that CoDaCoRe provides a highly effective algorithm for variable selection in high-dimensional HTS data. Second, the fact that CoDaCoRe achieves similar predictive accuracy as state-of-the-art black-box classifiers, suggests that our model may have captured a near-complete representation of the signal in the data. At any rate, we take this as evidence that log-ratio transformed features are indeed of biological importance in the context of HTS data, corroborating previous microbiome research (<xref rid="btab645-B7" ref-type="bibr">Crovesy <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab645-B31" ref-type="bibr">Magne <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab645-B47" ref-type="bibr">Rahat-Rozenbloom <italic toggle="yes">et al.</italic>, 2014</xref>).</p>
      <table-wrap position="float" id="btab645-T2">
        <label>Table 2.</label>
        <caption>
          <p>Evaluation metrics for the liquid biopsy data (<xref rid="btab645-B4" ref-type="bibr">Best <italic toggle="yes">et al.</italic>, 2015</xref>), averaged over 20 independent 80/20 train/test splits </p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">Runtime (s)</th>
              <th rowspan="1" colspan="1">Vars (#)</th>
              <th rowspan="1" colspan="1">Acc. (%)</th>
              <th rowspan="1" colspan="1">AUC (%)</th>
              <th rowspan="1" colspan="1">F1 (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">CoDaCoRe</td>
              <td rowspan="1" colspan="1">31±2.2</td>
              <td rowspan="1" colspan="1"><bold>3</bold> ± <bold>1</bold></td>
              <td rowspan="1" colspan="1"><bold>91.0</bold> ± <bold>1.9</bold></td>
              <td rowspan="1" colspan="1">93.6 ± 2.6</td>
              <td rowspan="1" colspan="1"><bold>94.4</bold> ± <bold>1.2</bold></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Lasso</td>
              <td rowspan="1" colspan="1">23±0.2</td>
              <td rowspan="1" colspan="1">22 ± 4</td>
              <td rowspan="1" colspan="1">87.8 ± 1.3</td>
              <td rowspan="1" colspan="1">94.7 ± 1.5</td>
              <td rowspan="1" colspan="1">92.7 ± 0.7</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RF</td>
              <td rowspan="1" colspan="1">383±8.6</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">89.0 ± 1.6</td>
              <td rowspan="1" colspan="1">94.1 ± 1.8</td>
              <td rowspan="1" colspan="1">93.1 ± 1.0</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">XGBoost</td>
              <td rowspan="1" colspan="1">108±1.6</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">90.6 ± 1.9</td>
              <td rowspan="1" colspan="1"><bold>95.9</bold> ± <bold>1.5</bold></td>
              <td rowspan="1" colspan="1">94.1 ± 1.1</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <p><italic toggle="yes">Note</italic>: CoDaCoRe (with balances) achieves equal predictive accuracy as competing methods, but with much sparser solutions. Note that sparsity is expressed as an (integer) number of active variables in the model (not as a percentage of the total, as was done in <xref rid="btab645-T1" ref-type="table">Table 1</xref>). We highlight in bold the sparsest and most accurate models.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>4.2 Interpretability</title>
      <p>The CoDaCoRe algorithm offers two kinds of interpretability. First, it provides the analyst with sets of input variables whose aggregated ratio predicts the outcome of interest. These sets are easy to understand because they are discrete, with each component making an equivalent (unweighted) contribution. They are also sparse, usually containing fewer than 10 features per ratio, and can be made sparser by adjusting the regularization parameter <italic toggle="yes">λ</italic>. Such ratios have a precedent in microbiome research, for example the Firmicutes-to-Bacteroidetes ratio is used as a biomarker of gut health (<xref rid="btab645-B7" ref-type="bibr">Crovesy <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab645-B31" ref-type="bibr">Magne <italic toggle="yes">et al.</italic>, 2020</xref>). Second, CoDaCoRe ranks predictive ratios hierarchically. Due to the ensembling procedure, the first ratio learned is the most predictive, the second ratio predicts the residual from the first, and so forth. Like principal components, the balances (or amalgamations) learned by CoDaCoRe are naturally ordered in terms of their explanatory power. This ordering aids interpretability by decomposing a multivariable model into comprehensible ‘chunks’ of information.</p>
      <p>Notably, we find a high degree of stability in the log-ratios selected by the model. We repeated CoDaCoRe on 10 independent training set splits of the Crohn disease data provided by <xref rid="btab645-B48" ref-type="bibr">Rivera-Pinto <italic toggle="yes">et al.</italic> (2018)</xref>, and found consensus among the learned models. <xref rid="btab645-F2" ref-type="fig">Figure 2</xref> shows which bacteria were included for each split. Importantly, the bacteria that were selected consistently by CoDaCoRe—notably Dialister, Roseburia and Clostridiales—were also identified by <xref rid="btab645-B48" ref-type="bibr">Rivera-Pinto <italic toggle="yes">et al.</italic> (2018)</xref>. In <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref> (<xref rid="sup1" ref-type="supplementary-material">Supplementary Section SF</xref>), we also present a comparison of <xref rid="btab645-F2" ref-type="fig">Figure 2</xref> when using CoDaCoRe to learn amalgamations instead of balances. The amalgamations tend to select more abundant bacteria species like Faecalibacterium rather than rarer species like Roseburia (due to the geometric mean being more sensitive to small numbers than the summation operator).</p>
      <fig position="float" id="btab645-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>CoDaCoRe variable selection for the first (most explanatory) log-ratio on the Crohn disease data (<xref rid="btab645-B48" ref-type="bibr">Rivera-Pinto <italic toggle="yes">et al.</italic>, 2018</xref>). For each of 10 independent bootstraps of the training set (80% of the data randomly sampled with stratification by case–control), we show which variables are selected in the numerator (blue) and denominator (orange) of the balance. CoDaCoRe learns remarkably consistent log-ratios across independent training sets</p>
        </caption>
        <graphic xlink:href="btab645f2" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>4.3 Scaling to liquid biopsy data</title>
      <p>HTS data generated from from clinical blood samples can be described as a ‘liquid biopsy’ that can be used for cancer diagnosis and surveillance (<xref rid="btab645-B2" ref-type="bibr">Alix-Panabières and Pantel, 2016</xref>; <xref rid="btab645-B4" ref-type="bibr">Best <italic toggle="yes">et al.</italic>, 2015</xref>). These data can be very high-dimensional, especially when they include all gene transcripts as input variables. In a clinical context, the use of log-ratio predictors is an attractive option because they automatically correct for inter-sample sequencing biases that might otherwise limit the generalizability of the models (<xref rid="btab645-B8" ref-type="bibr">Dillies <italic toggle="yes">et al.</italic>, 2013</xref>). Unfortunately, existing log-ratio methods like selbal and amalgam simply cannot scale to liquid biopsy datasets that contain as many as 50 000 or more input variables.</p>
      <p>The large dimensionality of such data has restricted its analysis to overly simplistic linear models, black-box models that are scalable but not interpretable, or suboptimal hybrid approaches where input variables must be pre-selected based on univariate measures (<xref rid="btab645-B4" ref-type="bibr">Best <italic toggle="yes">et al.</italic>, 2015</xref>; <xref rid="btab645-B49" ref-type="bibr">Sheng <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btab645-B55" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic>, 2017</xref>). Owing to its linear scaling, CoDaCoRe can be fitted to these data at a similar computational cost to a single lasso regression, i.e. under a minute on a single CPU core. Thus, CoDaCoRe can be used to discover interpretable and predictive log-ratios that are suitable for liquid biopsy cancer diagnostics, among other similar applications.</p>
      <p>We showcase the capabilities of CoDaCoRe in this high-dimensional setting, by applying our algorithm to the liquid biopsy data of (<xref rid="btab645-B4" ref-type="bibr">Best <italic toggle="yes">et al.</italic>, 2015</xref>). These data contain <inline-formula id="IE50"><mml:math id="IM50" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:math></inline-formula> 58 037 genes sequenced in <inline-formula id="IE51"><mml:math id="IM51" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:math></inline-formula> 288 human subjects, 60 of whom were healthy controls, the others having been previously diagnosed with cancer. Averaging over 20 random 80/20 train/test splits of this dataset, we found that CoDaCoRe achieved the same predictive accuracy as competing methods (within error), but obtained a much sparser model. Remarkably, CoDaCoRe identified log-ratios involving just 3 genes, that were equally predictive to both black-box classifiers and linear models with over 20 active variables. This case study again illustrates the potential of CoDaCoRe to derive novel biological insights, and also to develop learning algorithms for cancer diagnosis, a domain in which model interpretability—including sparsity—is of paramount importance (<xref rid="btab645-B53" ref-type="bibr">Wan <italic toggle="yes">et al.</italic>, 2017</xref>).</p>
    </sec>
    <sec>
      <title>4.4 Simulation study</title>
      <p>In addition to the above previous experiments, we provide a simulation study in Section G of <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>. For simulated HTS datasets of dimensionality ranging from 100 to 10 000 input variables, we find that CoDaCoRe is able to recover the true biomarkers used in the data-generating process, and does so with similar or higher accuracy (and orders of magnitude faster) than its competitors.</p>
    </sec>
  </sec>
  <sec>
    <title>5 Conclusion</title>
    <p>Our results corroborate the summary in <xref rid="btab645-T1" ref-type="table">Table 1</xref>: CoDaCoRe is the first sparse and interpretable CoDa model that can scale to high-dimensional HTS data. It does so convincingly, with linear scaling that results in runtimes similar to linear models. Our method is also competitive in terms of predictive accuracy, performing comparably to powerful black-box classifiers, but with interpretability. Our findings suggest that CoDaCoRe could play a significant role in the future analysis of high-throughput sequencing data, with broad implications in microbiology, statistical genetics and the field of CoDa.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>The authors thank the Simons Foundation 542963, Sloan Foundation, McKnight Endowment Fund, NSF DBI-1707398, and the Gatsby Charitable Foundation for support.</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="data-availability">
    <title>Data availability</title>
    <p>The 25 benchmark datasets of Section 4.1 can be found at <ext-link xlink:href="https://zenodo.org/record/3893986" ext-link-type="uri">https://zenodo.org/record/3893986</ext-link>, and the simulated datasets together with further instructions can be found at our repo <ext-link xlink:href="https://github.com/cunningham-lab/codacore" ext-link-type="uri">https://github.com/cunningham-lab/codacore</ext-link>. As for the liquid biopsy data of Section 4.3, we refer to the original publication (<xref rid="btab645-B4" ref-type="bibr">Best <italic toggle="yes">et al</italic>., 2015</xref>). </p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btab645_Supplementary_Data</label>
      <media xlink:href="btab645_supplementary_data.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btab645-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aitchison</surname><given-names>J.</given-names></string-name></person-group> (<year>1982</year>) <article-title>The statistical analysis of compositional data</article-title>. <source>J. R. Stat. Soc. Ser. B (Methodological)</source>, <volume>44</volume>, <fpage>139</fpage>–<lpage>160</lpage>.</mixed-citation>
    </ref>
    <ref id="btab645-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alix-Panabières</surname><given-names>C.</given-names></string-name>, <string-name><surname>Pantel</surname><given-names>K.</given-names></string-name></person-group> (<year>2016</year>) <article-title>Clinical applications of circulating tumor cells and circulating tumor DNA as liquid biopsy</article-title>. <source>Cancer Discov</source>., <volume>6</volume>, <fpage>479</fpage>–<lpage>491</lpage>.<pub-id pub-id-type="pmid">26969689</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bates</surname><given-names>S.</given-names></string-name>, <string-name><surname>Tibshirani</surname><given-names>R.</given-names></string-name></person-group> (<year>2019</year>) <article-title>Log-ratio lasso: scalable, sparse estimation for log-ratio models</article-title>. <source>Biometrics</source>, <volume>75</volume>, <fpage>613</fpage>–<lpage>624</lpage>.<pub-id pub-id-type="pmid">30387139</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Best</surname><given-names>M.G.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) <article-title>RNA-seq of tumor-educated platelets enables blood-based pan-cancer, multiclass, and molecular pathway cancer diagnostics</article-title>. <source>Cancer Cell</source>, <volume>28</volume>, <fpage>666</fpage>–<lpage>676</lpage>.<pub-id pub-id-type="pmid">26525104</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calle</surname><given-names>M.L.</given-names></string-name></person-group> (<year>2019</year>) <article-title>Statistical analysis of metagenomics data</article-title>. <source>Genomics Inf</source>., <volume>17</volume>, <fpage>e6</fpage>.</mixed-citation>
    </ref>
    <ref id="btab645-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cammarota</surname><given-names>G.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Gut microbiome, big data and machine learning to promote precision medicine for cancer</article-title>. <source>Nat. Rev. Gastroenterol. Hepatol</source>., <volume>17</volume>, <fpage>635</fpage>–<lpage>648</lpage>.<pub-id pub-id-type="pmid">32647386</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crovesy</surname><given-names>L.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Profile of the gut microbiota of adults with obesity: a systematic review</article-title>. <source>Eur. J. Clin. Nutr</source>., <volume>74</volume>, <fpage>1251</fpage>–<lpage>1262</lpage>.<pub-id pub-id-type="pmid">32231226</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dillies</surname><given-names>M.-A.</given-names></string-name></person-group>  <etal>et al</etal>; French StatOmique Consortium. (<year>2013</year>) <article-title>A comprehensive evaluation of normalization methods for illumina high-throughput RNA sequencing data analysis</article-title>. <source>Brief. Bioinf</source>., <volume>14</volume>, <fpage>671</fpage>–<lpage>683</lpage>.</mixed-citation>
    </ref>
    <ref id="btab645-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Egozcue</surname><given-names>J.J.</given-names></string-name>, <string-name><surname>Pawlowsky-Glahn</surname><given-names>V.</given-names></string-name></person-group> (<year>2005</year>) <article-title>Groups of parts and their balances in compositional data analysis</article-title>. <source>Math. Geol</source>., <volume>37</volume>, <fpage>795</fpage>–<lpage>828</lpage>.</mixed-citation>
    </ref>
    <ref id="btab645-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Egozcue</surname><given-names>J.J.</given-names></string-name></person-group> (<year>2003</year>) <article-title>Isometric logratio transformations for compositional data analysis</article-title>. <source>Math. Geol</source>., <volume>35</volume>, <fpage>279</fpage>–<lpage>300</lpage>.</mixed-citation>
    </ref>
    <ref id="btab645-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fernandes</surname><given-names>A.D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2013</year>) <article-title>Anova-like differential expression (ALDEX) analysis for mixed population RNA-seq</article-title>. <source>PLoS One</source>, <volume>8</volume>, <fpage>e67019</fpage>.<pub-id pub-id-type="pmid">23843979</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fernandes</surname><given-names>A.D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) <article-title>Unifying the analysis of high-throughput sequencing datasets: characterizing RNA-seq, 16s RRNA gene sequencing and selective growth experiments by compositional data analysis</article-title>. <source>Microbiome</source>, <volume>2</volume>, <fpage>15</fpage>.<pub-id pub-id-type="pmid">24910773</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Filzmoser</surname><given-names>P.</given-names></string-name>, <string-name><surname>Walczak</surname><given-names>B.</given-names></string-name></person-group> (<year>2014</year>) <article-title>What can go wrong at the data normalization step for identification of biomarkers?</article-title>  <source>J. Chromatography A</source>, <volume>1362</volume>, <fpage>194</fpage>–<lpage>205</lpage>.</mixed-citation>
    </ref>
    <ref id="btab645-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Filzmoser</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2009</year>) <article-title>Univariate statistical analysis of environmental (compositional) data: problems and possibilities</article-title>. <source>Sci. Total Environ</source>., <volume>407</volume>, <fpage>6100</fpage>–<lpage>6108</lpage>.<pub-id pub-id-type="pmid">19740525</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B15">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Friedman</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2001</year>) <italic toggle="yes">The Elements of Statistical Learning</italic>, Vol. 1. Springer Series in Statistics, New York.</mixed-citation>
    </ref>
    <ref id="btab645-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gloor</surname><given-names>G.B.</given-names></string-name>, <string-name><surname>Reid</surname><given-names>G.</given-names></string-name></person-group> (<year>2016</year>) <article-title>Compositional analysis: a valid approach to analyze microbiome high-throughput sequencing data</article-title>. <source>Can. J. Microbiol</source>., <volume>62</volume>, <fpage>692</fpage>–<lpage>703</lpage>.<pub-id pub-id-type="pmid">27314511</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gloor</surname><given-names>G.B.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) <article-title>It’s all relative: analyzing microbiome data as compositions</article-title>. <source>Ann. Epidemiol</source>., <volume>26</volume>, <fpage>322</fpage>–<lpage>329</lpage>.<pub-id pub-id-type="pmid">27143475</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gloor</surname><given-names>G.B.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Microbiome datasets are compositional: and this is not optional</article-title>. <source>Front. Microbiol</source>., <volume>8</volume>, <fpage>2224</fpage>.<pub-id pub-id-type="pmid">29187837</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goodman</surname><given-names>B.</given-names></string-name>, <string-name><surname>Flaxman</surname><given-names>S.</given-names></string-name></person-group> (<year>2017</year>) <article-title>European union regulations on algorithmic decision-making and a “right to explanation”</article-title>. <source>AI Mag</source>., <volume>38</volume>, <fpage>50</fpage>–<lpage>57</lpage>.</mixed-citation>
    </ref>
    <ref id="btab645-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greenacre</surname><given-names>M.</given-names></string-name></person-group> (<year>2019a</year>) <article-title>Comments on: compositional data: the sample space and its structure</article-title>. <source>TEST</source>, <volume>28</volume>, <fpage>644</fpage>–<lpage>652</lpage>.</mixed-citation>
    </ref>
    <ref id="btab645-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greenacre</surname><given-names>M.</given-names></string-name></person-group> (<year>2019b</year>) <article-title>Variable selection in compositional data analysis using pairwise logratios</article-title>. <source>Math. Geosci</source>., <volume>51</volume>, <fpage>649</fpage>–<lpage>682</lpage>.</mixed-citation>
    </ref>
    <ref id="btab645-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greenacre</surname><given-names>M.</given-names></string-name></person-group> (<year>2020</year>) <article-title>Amalgamations are valid in compositional data analysis, can be used in agglomerative clustering, and their logratios have an inverse transformation</article-title>. <source>Appl. Comput. Geosci</source>., <volume>5</volume>, <fpage>100017</fpage>.</mixed-citation>
    </ref>
    <ref id="btab645-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greenacre</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>A comparison of isometric and amalgamation logratio balances in compositional data analysis</article-title>. <source>Computers &amp; Geosciences, 104</source>, <fpage>104621</fpage>.</mixed-citation>
    </ref>
    <ref id="btab645-B24">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>He</surname><given-names>H.</given-names></string-name>, <string-name><surname>Ma</surname><given-names>Y.</given-names></string-name></person-group> (<year>2013</year>) <italic toggle="yes"><italic toggle="yes">Imbalanced Learning: Foundations, Algorithms, and Applications</italic>, Wiley-IEEE Press, New York</italic>.</mixed-citation>
    </ref>
    <ref id="btab645-B25">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Jang</surname><given-names>E.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) Categorical reparameterization with gumbel-softmax. <italic toggle="yes">In: 5th International Conference on Learning Representations, (ICLR) 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings</italic>.</mixed-citation>
    </ref>
    <ref id="btab645-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>H.</given-names></string-name></person-group> (<year>2015</year>) <article-title>Microbiome, metagenomics, and high-dimensional compositional data analysis</article-title>. <source>Annu. Rev. Stat. Appl</source>., <volume>2</volume>, <fpage>73</fpage>–<lpage>94</lpage>.</mixed-citation>
    </ref>
    <ref id="btab645-B27">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Linderman</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <part-title>Reparameterizing the Birkhoff polytope for variational permutation inference</part-title>. In: International Conference on Artificial Intelligence and Statistics, (AISTATS) 2018, 9-11 April 2018, Playa Blanca, Lanzarote, Canary Islands, Spain.</mixed-citation>
    </ref>
    <ref id="btab645-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lovell</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) <article-title>Proportionality: a valid alternative to correlation for relative data</article-title>. <source>PLoS Comput. Biol</source>., <volume>11</volume>, <fpage>e1004075</fpage>.<pub-id pub-id-type="pmid">25775355</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lu</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Generalized linear models with linear constraints for microbiome compositional data</article-title>. <source>Biometrics</source>, <volume>75</volume>, <fpage>235</fpage>–<lpage>244</lpage>.<pub-id pub-id-type="pmid">30039859</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B30">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Maddison</surname><given-names>C.J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <part-title>The concrete distribution: a continuous relaxation of discrete random variables</part-title>. In: 5th International Conference on Learning Representations, (ICLR) 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.</mixed-citation>
    </ref>
    <ref id="btab645-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Magne</surname><given-names>F.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>The firmicutes/bacteroidetes ratio: a relevant marker of gut dysbiosis in obese patients?</article-title>  <source>Nutrients</source>, <volume>12</volume>, <fpage>1474</fpage>.<pub-id pub-id-type="pmid">32438689</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B32">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Mena</surname><given-names>G.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <part-title>Learning latent permutations with Gumbel-Sinkhorn networks</part-title>. In: 6th International Conference on Learning Representations, (ICLR) 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.</mixed-citation>
    </ref>
    <ref id="btab645-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morton</surname><given-names>J.T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Balance trees reveal microbial niche differentiation</article-title>. <source>MSystems</source>, <volume>2</volume>, <fpage>e00162-16</fpage>.<pub-id pub-id-type="pmid">28144630</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morton</surname><given-names>J.T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Establishing microbial composition measurement standards with reference frames</article-title>. <source>Nat. Commun</source>., <volume>10</volume>, <fpage>2719</fpage>.<pub-id pub-id-type="pmid">31222023</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B35">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Pawlowsky-Glahn</surname><given-names>V.</given-names></string-name>, <string-name><surname>Buccianti</surname><given-names>A.</given-names></string-name></person-group> (<year>2011</year>) <source>Compositional Data Analysis: Theory and Applications</source>. <publisher-name>Wiley-Blackwell, Chichester, UK</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btab645-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pawlowsky-Glahn</surname><given-names>V.</given-names></string-name>, <string-name><surname>Egozcue</surname><given-names>J.J.</given-names></string-name></person-group> (<year>2006</year>) <article-title>Compositional data and their analysis: an introduction</article-title>. <source>Geol. Soc. Lond. Special Public</source>., <volume>264</volume>, <fpage>1</fpage>–<lpage>10</lpage>.</mixed-citation>
    </ref>
    <ref id="btab645-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pearson</surname><given-names>K.</given-names></string-name></person-group> (<year>1896</year>) <article-title>VII. Mathematical contributions to the theory of evolution. III. Regression, heredity, and panmixia</article-title>. <source>Philos. Trans. R. Soc. Lond. Ser. A</source>, <volume>187</volume>, <fpage>253</fpage>–<lpage>318</lpage>.</mixed-citation>
    </ref>
    <ref id="btab645-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Potapczynski</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Invertible gaussian reparameterization: revisiting the gumbel-softmax</article-title>. <source>Advances in Neural Information Processing Systems</source>, <fpage>33</fpage>.</mixed-citation>
    </ref>
    <ref id="btab645-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Prifti</surname><given-names>E.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Interpretable and accurate prediction models for metagenomics data</article-title>. <source>GigaScience</source>, <volume>9</volume>, <fpage>giaa010</fpage>.<pub-id pub-id-type="pmid">32150601</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B40">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Quinn</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <part-title>Deepcoda: personalized interpretability for compositional health data</part-title>. In: Proceedings of the 37th International Conference on Machine Learning, (ICML) 2020, 13-18 July 2020, Virtual Event.</mixed-citation>
    </ref>
    <ref id="btab645-B41">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Quinn</surname><given-names>T.P.</given-names></string-name>, <string-name><surname>Erb</surname><given-names>I.</given-names></string-name></person-group> (<year>2019</year>) Using balances to engineer features for the classification of health biomarkers: a new approach to balance selection. <italic toggle="yes">bioRxiv</italic>, 600122.</mixed-citation>
    </ref>
    <ref id="btab645-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Quinn</surname><given-names>T.P.</given-names></string-name>, <string-name><surname>Erb</surname><given-names>I.</given-names></string-name></person-group> (<year>2020</year>) <article-title>Amalgams: data-driven amalgamation for the dimensionality reduction of compositional data</article-title>. <source>NAR Genomics Bioinf</source>., <volume>2</volume>, <fpage>lqaa076</fpage>.</mixed-citation>
    </ref>
    <ref id="btab645-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Quinn</surname><given-names>T.P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>propr: an r-package for identifying proportionally abundant features using compositional data analysis</article-title>. <source>Sci. Rep</source>., <volume>7</volume>, <fpage>16252</fpage>–<lpage>16259</lpage>.<pub-id pub-id-type="pmid">29176663</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Quinn</surname><given-names>T.P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>Understanding sequencing data as compositions: an outlook and review</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>2870</fpage>–<lpage>2878</lpage>.<pub-id pub-id-type="pmid">29608657</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Quinn</surname><given-names>T.P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>A field guide for the compositional analysis of any-omics data</article-title>. <source>GigaScience</source>, <volume>8</volume>, <fpage>giz107</fpage>.<pub-id pub-id-type="pmid">31544212</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B46">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Quinn</surname><given-names>T.P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) A critique of differential abundance analysis, and advocacy for an alternative. <italic toggle="yes">arXiv, preprint arXiv:2104.07266</italic>.</mixed-citation>
    </ref>
    <ref id="btab645-B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rahat-Rozenbloom</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) <article-title>Evidence for greater production of colonic short-chain fatty acids in overweight than lean humans</article-title>. <source>Int. J. Obesity</source>, <volume>38</volume>, <fpage>1525</fpage>–<lpage>1531</lpage>.</mixed-citation>
    </ref>
    <ref id="btab645-B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rivera-Pinto</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>Balances: a new perspective for microbiome analysis</article-title>. <source>MSystems</source>, <volume>3</volume>, <fpage>e00053-18</fpage>.<pub-id pub-id-type="pmid">30035234</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sheng</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>Identification of tumor-educated platelet biomarkers of non-small-cell lung cancer</article-title>. <source>OncoTargets Ther</source>., <volume>11</volume>, <fpage>8143</fpage>–<lpage>8151</lpage>.</mixed-citation>
    </ref>
    <ref id="btab645-B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Silverman</surname><given-names>J.D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>A phylogenetic transform enhances analysis of compositional microbiota data</article-title>. <source>Elife</source>, <volume>6</volume>, <fpage>e21887</fpage>.<pub-id pub-id-type="pmid">28198697</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Susin</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Variable selection in microbiome compositional data analysis</article-title>. <source>NAR Genomics and Bioinformatics</source>, <volume>2</volume>, <fpage>lqaa029</fpage>.<pub-id pub-id-type="pmid">33575585</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vangay</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Microbiome Learning Repo (ML Repo): a public repository of microbiome regression and classification tasks</article-title>. <source>GigaScience</source>, <volume>8</volume>.</mixed-citation>
    </ref>
    <ref id="btab645-B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wan</surname><given-names>J.C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Liquid biopsies come of age: towards implementation of circulating tumour DNA</article-title>. <source>Nat. Rev. Cancer</source>, <volume>17</volume>, <fpage>223</fpage>–<lpage>238</lpage>.<pub-id pub-id-type="pmid">28233803</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B54">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Washburne</surname><given-names>A.D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Phylogenetic factorization of compositional data yields lineage-level associations in microbiome datasets</article-title>. <source>PeerJ</source>, <volume>5</volume>, <fpage>e2969</fpage>.<pub-id pub-id-type="pmid">28289558</pub-id></mixed-citation>
    </ref>
    <ref id="btab645-B55">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>Y.-H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Identifying and analyzing different cancer subtypes using RNA-seq data of blood platelets</article-title>. <source>Oncotarget</source>, <volume>8</volume>, <fpage>87494</fpage>–<lpage>87511</lpage>.<pub-id pub-id-type="pmid">29152097</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
