<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName archivearticle.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Genet</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Genet</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Genet.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Genetics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1664-8021</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7862712</article-id>
    <article-id pub-id-type="doi">10.3389/fgene.2020.632861</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Genetics</subject>
        <subj-group>
          <subject>Methods</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeCban: Prediction of circRNA-RBP Interaction Sites by Using Double Embeddings and Cross-Branch Attention Networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Yuan</surname>
          <given-names>Liangliang</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/1159384/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yang</surname>
          <given-names>Yang</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/789658/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Department of Computer Science and Engineering, Center for Brain-Like Computing and Machine Intelligence, Shanghai Jiao Tong University</institution>, <addr-line>Shanghai</addr-line>, <country>China</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</institution>, <addr-line>Shanghai</addr-line>, <country>China</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Wei Lan, Guangxi University, China</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Xiangxiang Zeng, Hunan University, China; Qi Zhao, University of Science and Technology Liaoning, China</p>
      </fn>
      <corresp id="c001">*Correspondence: Yang Yang <email>yangyang@cs.sjtu.edu.cn</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Computational Genomics, a section of the journal Frontiers in Genetics</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>22</day>
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>11</volume>
    <elocation-id>632861</elocation-id>
    <history>
      <date date-type="received">
        <day>24</day>
        <month>11</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>23</day>
        <month>12</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2021 Yuan and Yang.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Yuan and Yang</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Circular RNAs (circRNAs), as a rising star in the RNA world, play important roles in various biological processes. Understanding the interactions between circRNAs and RNA binding proteins (RBPs) can help reveal the functions of circRNAs. For the past decade, the emergence of high-throughput experimental data, like CLIP-Seq, has made the computational identification of RNA-protein interactions (RPIs) possible based on machine learning methods. However, as the underlying mechanisms of RPIs have not been fully understood yet and the information sources of circRNAs are limited, the computational tools for predicting circRNA-RBP interactions have been very few. In this study, we propose a deep learning method to identify circRNA-RBP interactions, called DeCban, which is featured by hybrid double embeddings for representing RNA sequences and a cross-branch attention neural network for classification. To capture more information from RNA sequences, the double embeddings include pre-trained embedding vectors for both RNA segments and their converted amino acids. Meanwhile, the cross-branch attention network aims to address the learning of very long sequences by integrating features of different scales and focusing on important information. The experimental results on 37 benchmark datasets show that both double embeddings and the cross-branch attention model contribute to the improvement of performance. DeCban outperforms the mainstream deep learning-based methods on not only prediction accuracy but also computational efficiency. The data sets and source code of this study are freely available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/AaronYll/DECban">https://github.com/AaronYll/DECban</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>circular RNAs</kwd>
      <kwd>RNA binding proteins</kwd>
      <kwd>deep learning</kwd>
      <kwd>double embeddings</kwd>
      <kwd>attention network</kwd>
    </kwd-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="2"/>
      <equation-count count="7"/>
      <ref-count count="33"/>
      <page-count count="9"/>
      <word-count count="6181"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>Circular RNAs (circRNAs) are a special kind of non-coding RNA molecules. Different from linear RNAs, circRNA molecules have closed-ring structures, which are not affected by RNA exonuclease, and their expression is more stable (Pamudurti et al., <xref rid="B19" ref-type="bibr">2017</xref>; Li et al., <xref rid="B15" ref-type="bibr">2018</xref>). Although natural circRNAs were discovered more than two decades ago, their important roles in gene regulation and disease development have just been revealed in recent years (Hansen et al., <xref rid="B8" ref-type="bibr">2013</xref>; Li et al., <xref rid="B16" ref-type="bibr">2015</xref>).</p>
    <p>Emerging studies have shown that circRNAs can bind to various types of proteins to affect protein localization, regulate protein expression, or influence protein-protein-interactions. The circRNA-binding-proteins (circRBPs) include transcription factors, RNA processing proteins, proteases, and common RNA-binding-proteins (RBPs) that can be bound with linear RNAs. Understanding the interactions between circRNAs and proteins is helpful for revealing the biological functions of circRNAs (Du et al., <xref rid="B5" ref-type="bibr">2017</xref>; Zang et al., <xref rid="B30" ref-type="bibr">2020</xref>). For the past decade, high-throughput experimental technologies have been widely used to detect the interactions between RNAs and proteins, like cross-linking and immunoprecipitation followed by RNA sequencing (CLIP-Seq) (Yang et al., <xref rid="B29" ref-type="bibr">2015</xref>). The large-scale experimental data makes it possible to predict RNA-protein interactions (RPIs) based on machine learning methods (Li et al., <xref rid="B14" ref-type="bibr">2013</xref>). Compared with expensive and time-consuming wet experiments, the computational methods have considerably sped up the identification of interactions, thus the automatic prediction of RPI has been a hot topic in the bioinformatics field (Pan et al., <xref rid="B23" ref-type="bibr">2019</xref>).</p>
    <p>The existing prediction tools include both RNA-oriented or protein-oriented, i.e., identifying the binding sites in the RNA chain and protein chain, respectively (Yan et al., <xref rid="B28" ref-type="bibr">2016</xref>). Benefitting from the abundant domain knowledge from protein databases, many studies perform prediction based on protein information. By contrast, much fewer studies focus on the binding sites on circRNAs (Ju et al., <xref rid="B12" ref-type="bibr">2019</xref>; Zhang et al., <xref rid="B31" ref-type="bibr">2019</xref>; Jia et al., <xref rid="B11" ref-type="bibr">2020</xref>; Wang and Lei, <xref rid="B27" ref-type="bibr">2020</xref>). The reasons are two-folds. For one thing, compared with other non-coding RNAs, like microRNAs and long non-coding RNAs, research on circRNAs has been largely lagged and their data is scarce. For another thing, the prediction for circRNAs is a very difficult task, due to the long sequences, sparsely distributed binding sites and limited information that could be extracted.</p>
    <p>As circRNAs have attracted more and more attention, experimental data of circRNAs has increased rapidly. Till now, a lot of circRNA-protein interactions have been revealed and released in public databases, e.g., CircInterome that houses the RBP/miRNA-binding sites on human circRNAs (Dudekula et al., <xref rid="B6" ref-type="bibr">2016</xref>). Thanks to the fast-growing circRNA data and the rise of deep learning, methods for predicting circRNA-RBP binding sites are emerging. For instance, Zhang et al. (<xref rid="B31" ref-type="bibr">2019</xref>) proposed a method called CRIP to predict circRNA-RBP binding sites, which is a hybrid architecture of convolutional neural networks (CNNs) and recurrent neural networks (RNNs); Jia et al. (<xref rid="B11" ref-type="bibr">2020</xref>) proposed an ensemble classifier, PASSION, which combines various statistical sequence features and performs feature selection to enhance the prediction accuracy.</p>
    <p>Note that learning long sequences has still been an open problem for neural networks. Biological sequences are much longer than natural language sentences, conventional learning models, including long short-term memory networks (LSTMs) which were designed to handle long-term dependencies (Hochreiter and Schmidhuber, <xref rid="B10" ref-type="bibr">1997</xref>), do not work well for extremely long sequences. Therefore, most of the existing predictors take short segments instead of full-length non-coding RNAs as input to identify the binding sites (Pan and Shen, <xref rid="B21" ref-type="bibr">2017</xref>, <xref rid="B22" ref-type="bibr">2018</xref>; Pan et al., <xref rid="B20" ref-type="bibr">2018</xref>; Zhang et al., <xref rid="B31" ref-type="bibr">2019</xref>), i.e., they divid the RNA sequences into short fragments and predict whether a fragment is a binding site or not. Obviously, such simplification does not accord with the real scenario. For one thing, RPIs are usually determined by the full-length RNA information rather than short fragments; and for the other thing, the binding regions only make up a tiny proportion in the whole RNA sequences, while the fragment-based prediction often constructs relatively balanced datasets, leading to a high false-positive-rate. Therefore, to address the sparse distribution of binding sites and reduce false positive predictions, this study aims to develop a model which allows full-length circRNA sequences as input and provides reliable predictions.</p>
    <p>Generally, the performance of machine learning methods depends on two factors, namely feature extraction and learning model. In traditional learning methods, RNA sequences are represented by statistical features, like the frequency of <italic>k</italic>-mers and secondary structure elements (Zhang et al., <xref rid="B32" ref-type="bibr">2011</xref>; Chen et al., <xref rid="B2" ref-type="bibr">2014</xref>). With the rise of deep learning, hand-crafted feature extraction has been largely replaced by automatic feature learning and pre-training via large-scale unlabeled datasets (Clauwaert and Waegeman, <xref rid="B3" ref-type="bibr">2019</xref>; Meher et al., <xref rid="B17" ref-type="bibr">2019</xref>). Word embedding is an emerging technique for representing biological sequence features. Unlike traditional features or one-hot encoding, word embedding is a kind of continuous distributed features. Commonly used word embedding methods include Word2vec (Mikolov et al., <xref rid="B18" ref-type="bibr">2013</xref>), Glove (Pennington et al., <xref rid="B24" ref-type="bibr">2014</xref>), ELMo (Peters et al., <xref rid="B25" ref-type="bibr">2018</xref>), GPT (Radford et al., <xref rid="B26" ref-type="bibr">2018</xref>), and Bert (Devlin et al., <xref rid="B4" ref-type="bibr">2018</xref>). The first two models yield static embedding, i.e., the embedding vector for each word is context-independent and fixed after training (Peters et al., <xref rid="B25" ref-type="bibr">2018</xref>), while the latter three methods yield context-dependent embedding vectors.</p>
    <p>At present, static embeddings learned by shallow models have been widely used in biological sequence analysis, while only a few studies applied dynamic embedding, like Elmo and Bert. One reason is that the models based on deep learning models such as Elmo and Bert are very computation-intensive. Especially, non-coding RNA sequences are much longer than protein sequences, thus learning dynamic embedding for RNA sequences may require more complex model. In this study, we also adopt static word embedding method to represent circRNA sequences. To better mine the sequence information, we propose a double-embedding method to expand the feature space, which is further learned by deep neural networks to extract abstract features for classification.</p>
    <p>As circRNAs are usually thousands of nucleotides, to handle the extremely long sequences, specialized model design is also required. Previous studies mainly used CNN (Alipanahi et al., <xref rid="B1" ref-type="bibr">2015</xref>), RNN, or CNN-RNN hybrid models (Pan and Shen, <xref rid="B21" ref-type="bibr">2017</xref>; Zhang et al., <xref rid="B31" ref-type="bibr">2019</xref>). As aforementioned, these models take short fragments as input and construct balanced datasets, while true binding sites are very rare. In this study, we design a new model called DeCban (<underline>D</underline>ouble <underline>e</underline>mbedding and <underline>C</underline>ross-<underline>b</underline>ranch <underline>a</underline>ttention <underline>n</underline>etwork) to predict the presence of RBP-binding sites on full-length circRNAs. This predictor is featured by not only a new sequence encoding scheme, i.e., double embedding, but also a cross-branch attention neural network. The network extracts sequence features of different abstract levels and different granularities, and the attention module allows the network to focus on important features for discrimination. Compared with the existing RPI prediction tools and mainstream deep learning models, DeCban has great advantages on both prediction accuracy and computational efficiency.</p>
  </sec>
  <sec sec-type="methods" id="s2">
    <title>2. Methodology</title>
    <sec>
      <title>2.1. Datasets</title>
      <p>To evaluate the prediction performance of DeCban, we collect circRNAs and their interacting proteins from Circular RNA Interactome (<ext-link ext-link-type="uri" xlink:href="https://circinteractome.nia.nih.gov/">https://circinteractome.nia.nih.gov/</ext-link>) (Dudekula et al., <xref rid="B6" ref-type="bibr">2016</xref>). The sequence redundancy is removed by CD-Hit (Fu et al., <xref rid="B7" ref-type="bibr">2012</xref>) with threshold 0.8, resulting into 32,216 circRNA sequences, which are bound to a total of 37 RBPs. We train a binary prediction model for each RBP and construct 37 datasets. The positive-to-negative ratio of each data set is 1:1, where the positive samples are the circRNAs binding to the RBP and negative samples are the remaining ones. The circRNAs in this set range from 100 to 30,000 nt in length, 90% of which are 500~7,000 nt. Therefore, to avoid the potential bias brought by too short and too long sequences, we only include the sequences falling in the range of 500~7,000 nt in the final data set. The data statistics are shown in <xref rid="T1" ref-type="table">Table 1</xref>.</p>
      <table-wrap id="T1" position="float">
        <label>Table 1</label>
        <caption>
          <p>Experimental datasets.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>RBP</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Train#</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Test#</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>RBP</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Train#</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Test#</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">AGO1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">33547</td>
              <td valign="top" align="center" rowspan="1" colspan="1">14377</td>
              <td valign="top" align="left" rowspan="1" colspan="1">IGF2BP2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">59467</td>
              <td valign="top" align="center" rowspan="1" colspan="1">25485</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">AGO2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">57697</td>
              <td valign="top" align="center" rowspan="1" colspan="1">24724</td>
              <td valign="top" align="left" rowspan="1" colspan="1">IGF2BP3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">83120</td>
              <td valign="top" align="center" rowspan="1" colspan="1">35622</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">AGO3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">8570</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3672</td>
              <td valign="top" align="left" rowspan="1" colspan="1">LIN28A</td>
              <td valign="top" align="center" rowspan="1" colspan="1">50769</td>
              <td valign="top" align="center" rowspan="1" colspan="1">21757</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">ALKBH5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4497</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1927</td>
              <td valign="top" align="left" rowspan="1" colspan="1">LIN28B</td>
              <td valign="top" align="center" rowspan="1" colspan="1">21601</td>
              <td valign="top" align="center" rowspan="1" colspan="1">9257</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">AUF1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3045</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1305</td>
              <td valign="top" align="left" rowspan="1" colspan="1">METTL3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">9033</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3871</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">C17ORF85</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6225</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2667</td>
              <td valign="top" align="left" rowspan="1" colspan="1">MOV10</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6309</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2703</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">C22ORF28</td>
              <td valign="top" align="center" rowspan="1" colspan="1">15680</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6720</td>
              <td valign="top" align="left" rowspan="1" colspan="1">PTB</td>
              <td valign="top" align="center" rowspan="1" colspan="1">67963</td>
              <td valign="top" align="center" rowspan="1" colspan="1">29127</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">CAPRIN1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">15503</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6643</td>
              <td valign="top" align="left" rowspan="1" colspan="1">PUM2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4903</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2101</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DGCR8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">57651</td>
              <td valign="top" align="center" rowspan="1" colspan="1">24707</td>
              <td valign="top" align="left" rowspan="1" colspan="1">QKI</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3036</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1300</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">EIF4A3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">25017</td>
              <td valign="top" align="center" rowspan="1" colspan="1">10721</td>
              <td valign="top" align="left" rowspan="1" colspan="1">SFRS1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">36563</td>
              <td valign="top" align="center" rowspan="1" colspan="1">15669</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">EWSR1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">13253</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5679</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TAF15</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3580</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1534</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">FMRP</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79392</td>
              <td valign="top" align="center" rowspan="1" colspan="1">34024</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TDP43</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2610</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1118</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">FOX2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2756</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1180</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TIA1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5127</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2197</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">FUS</td>
              <td valign="top" align="center" rowspan="1" colspan="1">60699</td>
              <td valign="top" align="center" rowspan="1" colspan="1">26013</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TIAL1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">9613</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4119</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">FXR1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2908</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1246</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TNRC6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3876</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1660</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">FXR2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">15400</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6600</td>
              <td valign="top" align="left" rowspan="1" colspan="1">U2AF65</td>
              <td valign="top" align="center" rowspan="1" colspan="1">16236</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6958</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">HNRNPC</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2588</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1108</td>
              <td valign="top" align="left" rowspan="1" colspan="1">WTAP</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1517</td>
              <td valign="top" align="center" rowspan="1" colspan="1">649</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">HUR</td>
              <td valign="top" align="center" rowspan="1" colspan="1">73352</td>
              <td valign="top" align="center" rowspan="1" colspan="1">31436</td>
              <td valign="top" align="left" rowspan="1" colspan="1">ZC3H7B</td>
              <td valign="top" align="center" rowspan="1" colspan="1">30175</td>
              <td valign="top" align="center" rowspan="1" colspan="1">12931</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">IGF2BP1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66355</td>
              <td valign="top" align="center" rowspan="1" colspan="1">28437</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>2.2. Model Architecture</title>
      <p><xref ref-type="fig" rid="F1">Figure 1</xref> shows the model architecture. The feature vectors generated by double embeddings are fed into a CNN-based neural network with multiple branches of different granularities. We introduce the self-attention mechanism to automatically integrate the semantic information extracted from different branches at each abstract level (an abstract level corresponds to a convolutional layer), and combine multiple levels of semantic information to determine whether binding sites exist in the RNA equences.</p>
      <fig id="F1" position="float">
        <label>Figure 1</label>
        <caption>
          <p>Model architecture of DeCban. The network consists of three convolutional layers and three branches (shown in green, orange, and red, respectively). An attention layer (shown in light blue) is used to integrate the outputs of the three branches. Then, the feature embeddings learned by the three layers are concatenated and fed to a fully connected layer to yield the final output.</p>
        </caption>
        <graphic xlink:href="fgene-11-632861-g0001"/>
      </fig>
      <sec>
        <title>2.2.1. Double Embeddings</title>
        <p>To work with deep neural networks (DNNs), input sequences are usually converted into numerical vectors by encoding schemes, such as one-hot, which encodes each nucleotide by a four-dimensional binary vector with only one element equal to 1. One-hot is unable to express the association between different nucleotides or context information, and the low dimensionality of its feature space limits the performance of further learning by DNN. By contrast, word embeddings, that are continuous dense vectors capturing semantic association of words, have been a mainstream method to represent words and sentences in natural language processing. The training of word embeddings is based on the language modeling task, like next-word prediction, which does not require sequence labels. Thus, the training of embeddings can be performed on large-scale unlabeled corpus.</p>
        <p>In recent years, word embeddings for <italic>k</italic>-mers have emerged in various bioinformatics applications. Here we also adopt word embeddings to represent circRNA sequence features. Besides, we notice that the word embedding technology has been applied more and achieved better performance in protein classification tasks, perhaps due to the bigger alphabet size and much shorter length of amino acid sequences compared with DNA/RNA sequences. To expand the alphabet, Zhang et al. (<xref rid="B31" ref-type="bibr">2019</xref>) developed a codon-based encoding scheme for circRNA sequences. A major advantage of this scheme lies in the enlarged feature space, as the classic one-hot has only 4 symbols while the codon-based encoding has 21 symbols, which are a combination of 3 nucleotides. The genetic codes define not only the alphabet of the new symbol system, but also the rules of correspondence between combinations of nucleotides and new symbols. Zhang et al. (<xref rid="B31" ref-type="bibr">2019</xref>) also showed that the three-nucleotide combinations defined by codons, are superior to random combinations defined in other encoding systems. Inspired by this idea, we convert RNA segments into pseudo-peptides and obtain word embeddings for them (we call them “pseudo-” because them are not real peptides). Then, we combine the two kinds of embeddings to generate the input features of our model. We call the new feature extraction method as double embeddings.</p>
        <p>For a circRNA fragment of length <italic>k</italic>, there are (<italic>k</italic> − 2) consecutive codons, where the codons are translated in an overlapping manner to retain more local context information. Then we perform pre-training of the word embeddings for <italic>k</italic>-mer RNA segments and (<italic>k</italic> − 2)-mer peptides, respectively. Since circRNA sequences are very long, to reduce the length of sentences, we need to set a large <italic>k</italic>, and long fragments also contain more local sequence information. However, training long words will require intensive computation resource. As a tradeoff, we set <italic>k</italic> to 7. We treat the segmented <italic>k</italic>-mers as words and adopt the GloVe algorithm to train their embeddings. Like NLP applications, to produce good embedding vector for words, a large corpus of text is required. Here we adopt the whole human genome as the corpus for RNA sequences (we replace “T” with “U” to convert DNAs to RNAs) and UniRef50 (<ext-link ext-link-type="uri" xlink:href="https://www.uniprot.org/help/uniref">https://www.uniprot.org/help/uniref</ext-link>) as the corpus for amino acid sequences. Finally, we construct the input matrix by using pre-trained word embeddings. Specifically, for each 7-mer fragment of a circRNA sequence, we concatenate the RNA embedding and the corresponding pseudo-peptide embedding. For example, as shown in <xref ref-type="fig" rid="F2">Figure 2</xref>, the first 7-mer “CACUAUA” contains the codons CAC, ACU, CUA, UAU, and AUA, which encode the amino acids H, T, L, Y, and I, respectively. Then, the embedding vectors of “CACUAUA” and “HTLYI” are concatenated to represent the feature vector of “CACUAUA.”</p>
        <fig id="F2" position="float">
          <label>Figure 2</label>
          <caption>
            <p>An example of double embeddings. An RNA sequence is segmented into 7-mers, and each 7-mer is converted into an embedding vector; meanwhile, the 7-mer is mapped to a pseudo-peptide, which is also converted into an embedding vector. The two embedding vectors are concatenated as a whole input.</p>
          </caption>
          <graphic xlink:href="fgene-11-632861-g0002"/>
        </fig>
        <p>Formally, for a given circRNA, let its length be <italic>L</italic>, which is divided into <italic>m</italic> segments (<italic>m</italic> = <italic>L</italic>/<italic>k</italic>}). Let the RNA and peptide embedding vectors for <italic>w</italic><sub><italic>i</italic></sub> are <italic>R</italic><sub><italic>i</italic></sub> and <italic>P</italic><sub><italic>i</italic></sub>, whose dimensions are <italic>p</italic> and <italic>q</italic>, respectively. Then the double embedding for <italic>w</italic><sub><italic>i</italic></sub> is defined as,</p>
        <disp-formula id="E1">
          <label>(1)</label>
          <mml:math id="M1">
            <mml:mtable class="eqnarray" columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>D</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>=</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>R</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>㊉</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>P</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>,</mml:mo>
                  <mml:mi>i</mml:mi>
                  <mml:mo>∈</mml:mo>
                  <mml:mrow>
                    <mml:mo>{</mml:mo>
                    <mml:mrow>
                      <mml:mn>1</mml:mn>
                      <mml:mo>,</mml:mo>
                      <mml:mn>2</mml:mn>
                      <mml:mo>,</mml:mo>
                      <mml:mo>⋯</mml:mo>
                      <mml:mspace width="0.3em" class="thinspace"/>
                      <mml:mo>,</mml:mo>
                      <mml:mi>m</mml:mi>
                    </mml:mrow>
                    <mml:mo>}</mml:mo>
                  </mml:mrow>
                  <mml:mo>,</mml:mo>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <p>where ㊉ denotes the concatenation operation. Then the circRNA is represented by a matrix of size (<italic>p</italic> + <italic>q</italic>) × <italic>m</italic>, i.e., [<italic>D</italic><sub>1</sub>, <italic>D</italic><sub>2</sub>, ⋯ , <italic>D</italic><sub><italic>m</italic></sub>].</p>
      </sec>
      <sec>
        <title>2.2.2. Cross-Branch Attention Network</title>
        <p>As shown in <xref ref-type="fig" rid="F1">Figure 1</xref>, the network has multiple branches, which have the same number of convolutional layers but vary in convolution kernel size. Thus, the branches can extract features at different granularities.</p>
        <p>Besides, at the same layer of all branches, we introduce the self-attention mechanism. As the length of the input sequences varies greatly, the best features extracted from different sequences may come from different branches. The self-attention module enables the network to assign weights to the branches and obtain weighted average features. We introduce such modules in each layer to extract features of different abstract levels. Therefore, we name the model cross-branch attention network.</p>
        <p>Formally, let the input of the network be <italic>X</italic>, and the first layer outputs of the three branches be <inline-formula><mml:math id="M2"><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>, and <inline-formula><mml:math id="M3"><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>, respectively, which can be expressed as,</p>
        <disp-formula id="E2">
          <label>(2)</label>
          <mml:math id="M4">
            <mml:mtable class="eqnarray" columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:msubsup>
                    <mml:mrow>
                      <mml:mi>X</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>j</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:msubsup>
                  <mml:mo>=</mml:mo>
                  <mml:mi>f</mml:mi>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>W</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>j</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msubsup>
                      <mml:mo>*</mml:mo>
                      <mml:mi>X</mml:mi>
                      <mml:mo>+</mml:mo>
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>b</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>j</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                  <mml:mo>,</mml:mo>
                  <mml:mi>j</mml:mi>
                  <mml:mo>∈</mml:mo>
                  <mml:mrow>
                    <mml:mo>{</mml:mo>
                    <mml:mrow>
                      <mml:mn>1</mml:mn>
                      <mml:mo>,</mml:mo>
                      <mml:mn>2</mml:mn>
                      <mml:mo>,</mml:mo>
                      <mml:mn>3</mml:mn>
                    </mml:mrow>
                    <mml:mo>}</mml:mo>
                  </mml:mrow>
                  <mml:mo>.</mml:mo>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <p>Similarly, for each subsequent layer <italic>i</italic>, the outputs <inline-formula><mml:math id="M5"><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> are computed as,</p>
        <disp-formula id="E3">
          <label>(3)</label>
          <mml:math id="M6">
            <mml:mtable class="eqnarray" columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:msubsup>
                    <mml:mrow>
                      <mml:mi>X</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>j</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:mrow>
                  </mml:msubsup>
                  <mml:mo>=</mml:mo>
                  <mml:mi>f</mml:mi>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>W</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>j</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mo>-</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msubsup>
                      <mml:mo>*</mml:mo>
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>X</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>j</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mo>-</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msubsup>
                      <mml:mo>+</mml:mo>
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>b</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>j</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                  <mml:mo>,</mml:mo>
                  <mml:mi>i</mml:mi>
                  <mml:mo>∈</mml:mo>
                  <mml:mrow>
                    <mml:mo>{</mml:mo>
                    <mml:mrow>
                      <mml:mn>2</mml:mn>
                      <mml:mo>,</mml:mo>
                      <mml:mn>3</mml:mn>
                    </mml:mrow>
                    <mml:mo>}</mml:mo>
                  </mml:mrow>
                  <mml:mo>,</mml:mo>
                  <mml:mi>j</mml:mi>
                  <mml:mo>∈</mml:mo>
                  <mml:mrow>
                    <mml:mo>{</mml:mo>
                    <mml:mrow>
                      <mml:mn>1</mml:mn>
                      <mml:mo>,</mml:mo>
                      <mml:mn>2</mml:mn>
                      <mml:mo>,</mml:mo>
                      <mml:mn>3</mml:mn>
                    </mml:mrow>
                    <mml:mo>}</mml:mo>
                  </mml:mrow>
                  <mml:mo>.</mml:mo>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <p>The <inline-formula><mml:math id="M7"><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>s are further processed via a maximum pooling operation, i.e.,</p>
        <disp-formula id="E4">
          <label>(4)</label>
          <mml:math id="M8">
            <mml:mtable class="eqnarray" columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:msubsup>
                    <mml:mrow>
                      <mml:mi>Y</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>j</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:mrow>
                  </mml:msubsup>
                  <mml:mo>=</mml:mo>
                  <mml:mi>h</mml:mi>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>X</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>j</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                  <mml:mo>,</mml:mo>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <p>where <italic>h</italic>(·) is the max-pooling function. Then, the self-attention module works on each layer to integrate the outputs of three branches,</p>
        <disp-formula id="E5">
          <label>(5)</label>
          <mml:math id="M9">
            <mml:mtable class="eqnarray" columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:msubsup>
                    <mml:mrow>
                      <mml:mi>Y</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>a</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>n</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:mrow>
                  </mml:msubsup>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>W</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>a</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>,</mml:mo>
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>Y</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>1</mml:mn>
                          <mml:mo>:</mml:mo>
                          <mml:mn>3</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                  <mml:mo>=</mml:mo>
                  <mml:mi>S</mml:mi>
                  <mml:mi>o</mml:mi>
                  <mml:mi>f</mml:mi>
                  <mml:mi>t</mml:mi>
                  <mml:mi>M</mml:mi>
                  <mml:mi>a</mml:mi>
                  <mml:mi>x</mml:mi>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:mi>g</mml:mi>
                      <mml:mrow>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mrow>
                          <mml:msub>
                            <mml:mrow>
                              <mml:mi>W</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>a</mml:mi>
                            </mml:mrow>
                          </mml:msub>
                          <mml:mo>*</mml:mo>
                          <mml:msup>
                            <mml:mrow>
                              <mml:mrow>
                                <mml:mo stretchy="false">(</mml:mo>
                                <mml:mrow>
                                  <mml:msubsup>
                                    <mml:mrow>
                                      <mml:mi>Y</mml:mi>
                                    </mml:mrow>
                                    <mml:mrow>
                                      <mml:mn>1</mml:mn>
                                      <mml:mo>:</mml:mo>
                                      <mml:mn>3</mml:mn>
                                    </mml:mrow>
                                    <mml:mrow>
                                      <mml:mi>i</mml:mi>
                                    </mml:mrow>
                                  </mml:msubsup>
                                </mml:mrow>
                                <mml:mo stretchy="false">)</mml:mo>
                              </mml:mrow>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>T</mml:mi>
                            </mml:mrow>
                          </mml:msup>
                        </mml:mrow>
                        <mml:mo stretchy="false">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                  <mml:mo>*</mml:mo>
                  <mml:msubsup>
                    <mml:mrow>
                      <mml:mi>Y</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mn>1</mml:mn>
                      <mml:mo>:</mml:mo>
                      <mml:mn>3</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:mrow>
                  </mml:msubsup>
                  <mml:mo>,</mml:mo>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <p>where <italic>g</italic>(·) denotes the activation function, and <inline-formula><mml:math id="M10"><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is the output yielded by the attention module for the <italic>i</italic>-th layer. The outputs of the three layers are combined as <italic>y</italic><sub><italic>out</italic></sub>,</p>
        <disp-formula id="E6">
          <label>(6)</label>
          <mml:math id="M11">
            <mml:mtable class="eqnarray" columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>Y</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>o</mml:mi>
                      <mml:mi>u</mml:mi>
                      <mml:mi>t</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>=</mml:mo>
                  <mml:mi>c</mml:mi>
                  <mml:mi>o</mml:mi>
                  <mml:mi>n</mml:mi>
                  <mml:mi>c</mml:mi>
                  <mml:mi>a</mml:mi>
                  <mml:mi>t</mml:mi>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>Y</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>a</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>n</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msubsup>
                      <mml:mo>,</mml:mo>
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>Y</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>a</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>n</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                        </mml:mrow>
                      </mml:msubsup>
                      <mml:mo>,</mml:mo>
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>Y</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>a</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>n</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>3</mml:mn>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                  <mml:mo>.</mml:mo>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <p>Finally, the output <italic>O</italic> of the network is obtained through a FC layer,</p>
        <disp-formula id="E7">
          <label>(7)</label>
          <mml:math id="M12">
            <mml:mtable class="eqnarray" columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:mi>O</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mi>S</mml:mi>
                  <mml:mi>o</mml:mi>
                  <mml:mi>f</mml:mi>
                  <mml:mi>t</mml:mi>
                  <mml:mi>M</mml:mi>
                  <mml:mi>a</mml:mi>
                  <mml:mi>x</mml:mi>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:mi>g</mml:mi>
                      <mml:mrow>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mrow>
                          <mml:msub>
                            <mml:mrow>
                              <mml:mi>W</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>f</mml:mi>
                              <mml:mi>c</mml:mi>
                            </mml:mrow>
                          </mml:msub>
                          <mml:mo>*</mml:mo>
                          <mml:msub>
                            <mml:mrow>
                              <mml:mi>Y</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>o</mml:mi>
                              <mml:mi>u</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:mrow>
                          </mml:msub>
                          <mml:mo>+</mml:mo>
                          <mml:msub>
                            <mml:mrow>
                              <mml:mi>b</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>f</mml:mi>
                              <mml:mi>c</mml:mi>
                            </mml:mrow>
                          </mml:msub>
                        </mml:mrow>
                        <mml:mo stretchy="false">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                  <mml:mo>.</mml:mo>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
      </sec>
    </sec>
  </sec>
  <sec id="s3">
    <title>3. Experimental Results</title>
    <sec>
      <title>3.1. Experimental Settings</title>
      <p>The DeCban model has three branches, and the sizes of their convolution kernels are 3, 5, 7, respectively. Each branch has three convolutional layers and each layer has 100 filters. The initial parameters of each attention module are randomly generated with normal distribution. We use Adam optimizer with learning rate of 0.001 to optimize the model. The number of early stopping rounds is set to 10, and the training-to-test ratio is 7:3.</p>
    </sec>
    <sec>
      <title>3.2. Baseline Methods</title>
      <p>To assess the performance of DeCban, we compare it with not only the existing predictor for RNA-protein interactions but also mainstream deep neural networks, including a recent method called CRIP (Zhang et al., <xref rid="B31" ref-type="bibr">2019</xref>), recurrent neural networks (RNNs), and convolutional neural networks (CNNs). Note that CRIP performs prediction on short fragments (i.e., 101-nt), thus for a full-length RNA sequence, we first divide it into fragments and use CRIP to predict for each fragment, and then merge the results to get the prediction for the whole sequence. The other baseline models fall into five groups. Each group contains three methods with the same backbone model but different feature representations, namely RNA embeddings, peptide embeddings, and double embeddings. In addition, the performance of DeCban working with RNA or peptide embeddings alone is evaluated. The specification of baseline models is as follows.</p>
      <list list-type="bullet">
        <list-item>
          <p>Group 1—LSTM: a vanilla long short-term memory network (Hochreiter and Schmidhuber, <xref rid="B10" ref-type="bibr">1997</xref>).</p>
        </list-item>
        <list-item>
          <p>Group 2—BiLSTM with attention: a bidirectional LSTM with attention mechanism (Zhou et al., <xref rid="B33" ref-type="bibr">2016</xref>)<xref ref-type="fn" rid="fn0001"><sup>1</sup></xref>.</p>
        </list-item>
        <list-item>
          <p>Group 3—TextCNN: a TextCNN (Kim, <xref rid="B13" ref-type="bibr">2014</xref>) model.</p>
        </list-item>
        <list-item>
          <p>Group 4—ResNet18 base: a basic ResNet18 model (He et al., <xref rid="B9" ref-type="bibr">2016</xref>).</p>
        </list-item>
        <list-item>
          <p>Group 5—ResNet18 small: a simplified ResNet18 model, which has the same architecture as ResNet18 but fewer convolutional kernels on each layer.</p>
        </list-item>
        <list-item>
          <p>CRIP: a CNN-RNN hybrid model for the prediction of RBP-bindings sites on RNAs (Zhang et al., <xref rid="B31" ref-type="bibr">2019</xref>).</p>
        </list-item>
      </list>
    </sec>
    <sec>
      <title>3.3. Experimental Results and Analysis</title>
      <p>For a comprehensive comparison, we consider not only the prediction accuracy but also computational efficiency. The accuracy is evaluated by the common metrics of machine learning models, F<sub>1</sub> and AUC score (Area under the ROC Curve). The efficiency is assessed by the number of parameters and speedup.</p>
      <p>First, we compare the AUC scores of DeCban and CRIP on all 37 data sets. The ROC curves are shown in <xref ref-type="fig" rid="F3">Figures 3</xref>, <xref ref-type="fig" rid="F4">4</xref>, respectively. The AUC scores range from 0.819 to 0.970, and the average AUC is 0.905. The lowest, highest, and average AUCs of these two methods are 0.819 vs. 0.734, 0.970 vs. 0.917, and 0.905 vs. 0.821, respectively. DeCban has an obvious advantage over CRIP.</p>
      <fig id="F3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>The ROC curves obtained by DeCban for 37 circRNA data sets.</p>
        </caption>
        <graphic xlink:href="fgene-11-632861-g0003"/>
      </fig>
      <fig id="F4" position="float">
        <label>Figure 4</label>
        <caption>
          <p>The ROC curves obtained by CRIP for 37 circRNA data sets.</p>
        </caption>
        <graphic xlink:href="fgene-11-632861-g0004"/>
      </fig>
      <p>Second, we compare the F<sub>1</sub> scores for all baseline models. <xref rid="T2" ref-type="table">Table 2</xref> shows the average F<sub>1</sub>, number of parameters and speedup. As can be seen, DeCban achieves the highest average F<sub>1</sub> of 0.841, and the second best model is BiLSTM with attention, whose average F<sub>1</sub> is 0.827. The detailed scores for all 37 data sets are listed in <xref ref-type="supplementary-material" rid="SM1">Supplementary Table 1</xref>. DeCban obtains the highest F<sub>1</sub> scores on all of the datasets. Meanwhile, DeCban has a lightweight architecture. Compared with the second best model BiLSTM, DeCban has a significant reduction on model parameters. The detailed comparison results are discussed in sections 3.3.1–3.3.5.</p>
      <table-wrap id="T2" position="float">
        <label>Table 2</label>
        <caption>
          <p>Experimental results of different models<xref ref-type="table-fn" rid="TN1"><sup>a</sup></xref>.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Model</bold>
              </th>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Param<xref ref-type="table-fn" rid="TN2"><sup><bold>b</bold></sup></xref></bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Avg F<xref ref-type="table-fn" rid="TN1"><sup><bold>a</bold></sup></xref></bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Speedup<xref ref-type="table-fn" rid="TN3"><sup><bold>c</bold></sup></xref></bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="middle" align="left" rowspan="3" colspan="1">LSTM-base</td>
              <td valign="top" align="left" rowspan="1" colspan="1">RNA</td>
              <td valign="top" align="center" rowspan="1" colspan="1">118 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.685</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.8x</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Peptide</td>
              <td valign="top" align="center" rowspan="1" colspan="1">132 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.685</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2.0x</td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td valign="top" align="left" rowspan="1" colspan="1">Double</td>
              <td valign="top" align="center" rowspan="1" colspan="1">183 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.692</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3.3x</td>
            </tr>
            <tr>
              <td valign="middle" align="left" rowspan="3" colspan="1">BiLSTM-attention</td>
              <td valign="top" align="left" rowspan="1" colspan="1">RNA</td>
              <td valign="top" align="center" rowspan="1" colspan="1">647 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.817</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6.4x</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Peptide</td>
              <td valign="top" align="center" rowspan="1" colspan="1">676 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.810</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6.4x</td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td valign="top" align="left" rowspan="1" colspan="1">Double</td>
              <td valign="top" align="center" rowspan="1" colspan="1">778 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.827</td>
              <td valign="top" align="center" rowspan="1" colspan="1">8.2x</td>
            </tr>
            <tr>
              <td valign="middle" align="left" rowspan="3" colspan="1">CNN-base</td>
              <td valign="top" align="left" rowspan="1" colspan="1">RNA</td>
              <td valign="top" align="center" rowspan="1" colspan="1">26 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.796</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.0x</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Peptide</td>
              <td valign="top" align="center" rowspan="1" colspan="1">30 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.793</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.2x</td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td valign="top" align="left" rowspan="1" colspan="1">Double</td>
              <td valign="top" align="center" rowspan="1" colspan="1">46 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.806</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2.3x</td>
            </tr>
            <tr>
              <td valign="middle" align="left" rowspan="3" colspan="1">ResNet-18-base</td>
              <td valign="top" align="left" rowspan="1" colspan="1">RNA</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3,914 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.811</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2.7x</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Peptide</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3,927 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.803</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2.6x</td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td valign="top" align="left" rowspan="1" colspan="1">Double</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3,972 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.814</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3.7x</td>
            </tr>
            <tr>
              <td valign="middle" align="left" rowspan="3" colspan="1">ResNet-18-small</td>
              <td valign="top" align="left" rowspan="1" colspan="1">RNA</td>
              <td valign="top" align="center" rowspan="1" colspan="1">254 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.770</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.7x</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Peptide</td>
              <td valign="top" align="center" rowspan="1" colspan="1">255 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.761</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.8x</td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td valign="top" align="left" rowspan="1" colspan="1">Double</td>
              <td valign="top" align="center" rowspan="1" colspan="1">261 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.773</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2.7x</td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td valign="middle" align="left" rowspan="1" colspan="1">CRIP</td>
              <td valign="top" align="left" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">900 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.766</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.7x</td>
            </tr>
            <tr>
              <td valign="middle" align="left" rowspan="4" colspan="1">DeCban</td>
              <td valign="top" align="left" rowspan="1" colspan="1">One-hot</td>
              <td valign="top" align="center" rowspan="1" colspan="1">33 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.822</td>
              <td valign="top" align="center" rowspan="1" colspan="1">9.6x</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">RNA</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.833</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.8x</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Peptide</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.826</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2.0x</td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td valign="top" align="left" rowspan="1" colspan="1">Double</td>
              <td valign="top" align="center" rowspan="1" colspan="1">141 K</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.841</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3.2x</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="TN1">
            <label>a</label>
            <p>RNA, Peptide, Double denote the RNA embedding, Peptide embedding, and double embedding, respectively.</p>
          </fn>
          <fn id="TN2">
            <label>b</label>
            <p>Param denotes the number of parameters in the model.</p>
          </fn>
          <fn id="TN3">
            <label>c</label>
            <p><italic>Speedup measures the relative performance of two methods processing the same problem in terms of speed. We use CNN-base with RNA embedding as the basic reference, i.e., its speedup is 1.0x</italic>.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <sec>
        <title>3.3.1. Comparison of the Sequence Encoding Methods</title>
        <p>From <xref rid="T2" ref-type="table">Table 2</xref>, it can be observed that double embeddings can improve the performance of both baseline models and DeCban. Compared to original RNA embeddings, double embeddings increase F<sub>1</sub> by around 1%. In the meantime, using double embeddings do not significantly increase the complexity of the model. The total number of parameters of DeCban using double embeddings has the same order of magnitude as that of the model with RNA embeddings or amino acid embeddings. Taking ResNet-18-base as an example, the number of parameters is increased by &lt;1.5%, while the average F<sub>1</sub> on 37 data sets is increased by nearly 1 percentage.</p>
        <p>The results suggest that the combination of RNA information and pseudo-peptide information can improve the data representation ability, although the “peptides” are not biological meaningful. A major reason for the performance improvement is the enlarged feature space. Moreover, the new encoding method traverses the RNA <italic>k</italic>-mers sequentially in an overlapping manner, thus retaining some local context information, which may be helpful for capturing the dependency relationship of nucleotides.</p>
        <p>In addition, we replace the double embedding encoding with the traditional one-hot encoding for comparison. The average <italic>F</italic><sub>1</sub> on 37 data sets is 0.822, and the training speed is significantly slower than double embedding. This result shows the advantages of double embedding over traditional one-hot encoding.</p>
      </sec>
      <sec>
        <title>3.3.2. Comparison of Model Architectures</title>
        <p>As DeCban is a convolutional neural network, we compare it with the state-of-the-art CNN model, ResNet-18. The number of layers and parameters of ResNet is much larger than that of DeCban. Specifically, the parameter amount of ResNet-18-base is 28 times of DeCban, while the F<sub>1</sub> score is 2.5% lower than DeCban. Considering that ResNet might overfit the data due to the large model size, we implement a lightweight version of ResNet-18, namely the ResNet-18-small, by reducing the number of convolutional kernels for each layer, then the amount of parameters is at the same order of magnitude as DeCban. However, after the simplification, the prediction accuracy drops significantly. Comparing with ResNet-18-base, the F<sub>1</sub> scores of three embedding methods are decreased by 0.038, 0.043, and 0.044, respectively. By contrast, benefitting from the multi-branch and self-attention mechanism, DeCban can extract features of different scales, and achieve better accuracy with much higher efficiency. Even using only RNA word embeddings, DeCban outperforms all baseline models, demonstrating the superiority of the new model architecture.</p>
        <p>Besides CNN models, we also consider the widely-used RNN model, LSTM. Although LSTM was designed to address the gradient vanishing issue and long-term dependencies, it is still difficult for LSTM to handle very long sequences. It can be seen from the experimental results that the performance of vanilla LSTM is poor. When using the double embeddings, the average F<sub>1</sub> on 37 datasets is 0.692, which is much lower than that of basic CNN (0.806). The gap of performance between these two kinds of models may be attributed to the large difference in the sequence length.</p>
        <p>As the input sequences vary greatly in length, a large number of meaningless zeros are filled at the end of short sequences. The padding operation affects the training of LSTM, while CNN has more flexibility in extracting features from sequences with varying length. In this case, attention mechanism becomes a necessary part to enable the model focus on informative regions, thus the BiLSTM model with attention improves the performance of LSTM significantly, even better than basic CNNs and ResNets.</p>
        <p>As for the training speed, RNN models generally need longer training time compared with CNN-based models. BiLSTM-attention becomes the most time-consuming model. By contrast, although ResNet-18 has the most parameters, it takes only less than half of the training time of BiLSTM-attention. Thus, the CNN-based DeCban model also achieves high efficiency. Taking the DeCban using double embedding as an example, the parameters are only one fifth of those of BiLSTM-attention, but the average F<sub>1</sub> value is increased by 1.4%, which shows that the proposed network can achieve better performance with less computing resources.</p>
      </sec>
    </sec>
    <sec>
      <title>3.4. Comparison With the Latest Models</title>
      <p>In addition to the baseline models with common model architectures, we compare DeCban with the existing predictors for RBP-RNA interactions. Currently, the predictors for circRNAs are very few. CRIP (Zhang et al., <xref rid="B31" ref-type="bibr">2019</xref>) and PASSION (Jia et al., <xref rid="B11" ref-type="bibr">2020</xref>) are two recently developed models. We compare them with DeCban in terms of feature extraction, model architecture, and input, as described in the following.</p>
      <p>CRIP also uses the 3-nucleotide codons to convert RNAs into pseudo-amino acids, i.e., the stacked-codon encoding scheme. However, CRIP presents the pseudo-amino acids as one-hot vectors, while DeCban uses word embeddings for both original RNAs and the converted pseudo-amino acids. PASSION incorporates some traditional statistical features in addition to CRIP's features. Therefore, a major difference between DeCban and the previous studies is using continuous dense feature encoding instead of sparse discrete features. Besides, the double embeddings contain the information of both RNA segments and pseudo-peptides, so as to strengthen the representation of raw sequences.</p>
      <p>As for the model architecture, CRIP adopts a CNN-LSTM hybrid network, and PASSION proposes an ensemble classifier, which combines the hybrid network with an artificial neural network (consisting of fully-connected layers). DeCban is a CNN-based multi-branch attention network. As shown in <xref rid="T2" ref-type="table">Table 2</xref>, the parameter quantity of CRIP is 900 K, and PASSION has more parameters due to the ensemble nature; while DeCban with double embedding uses only one seventh of the parameters of CRIP.</p>
      <p>Finally, both CRIP and PASSION perform prediction on short fragments, i.e., 101-nt segments. The incomplete sequences may lose some characteristics of original RNA molecules and lead to more false positive predictions, as mentioned in Zhang et al. (<xref rid="B31" ref-type="bibr">2019</xref>), while DeCban handles full-length sequences. <xref ref-type="fig" rid="F4">Figure 4</xref> shows the ROC curve of CRIP. The average AUC value of the CRIP model on 37 data sets is 0.821, while DeCban is 0.905. DeCban gets significantly higher AUC value than that of CRIP on nearly all datasets. And, according to the results reported in Jia et al. (<xref rid="B11" ref-type="bibr">2020</xref>), PASSION's AUC is about 0.01 higher than that of CRIP. As both these two methods' inputs are short fragments with balanced positive-to-negative ratio, they may have close performance when handling full-length circRNAs.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>4. Discussion</title>
    <p>Circular RNAs are a special kind of non-coding RNAs, which play an important role in gene regulation and disease development. Studying the interactions between circRNAs and RBPs can reveal the functions of circRNAs. However, the prediction of binding sites on circRNAs faces many challenges.</p>
    <p>First, the length range of circRNA sequences is very large, from tens to over 100,000 nt, which adds great difficulty to the learning models. Thus, it is important to design a network to adapt to the large variance of input sequences. The multi-branch design of DeCban aims to extract features from different ranges of sequence regions, as the branches differ in kernel sizes, leading to different receptive fields. For instance, assume that step length is 3, with 0 padding and 0 dilation. When the convolution kernel size is 3, the receptive field sizes of the features output by the first and second layers are 3 and 5, respectively. When the convolution kernel size is 5, the receptive field sizes of the features output by the first and second layers are 5 and 9. Thus, different convolution kernel sizes can extract features of different scales.</p>
    <p>The second challenge is that RBP-binding sites are extremely sparsely located in the whole RNA sequences, i.e., the number of binding sites are few and the binding regions are very short compared to full-length sequences. Thus, this is a severely imbalanced learning task, as most of the regions have no binding affinity. The attention mechanism in DeCban can alleviate this problem to a certain extent, which enables the model focus on key regions in long sequences.</p>
    <p>The third challenge arises from the data side. Compared with linear RNAs, domain knowledge or information sources other than sequences are lacked. By utilizing the codon-based mapping between RNA and peptides, and performing large-scale pre-training of word embeddings for both RNA segments and peptides, we propose a new feature representation method for circRNAs, called double embeddings. Experiments show that this method effectively improves the representation ability for raw sequences.</p>
    <p>Compared with the existing circRNA-RBP prediction methods, DeCban has the following advantages:</p>
    <list list-type="order">
      <list-item>
        <p>The prediction can be performed on full-length circRNA sequences instead of short segments.</p>
      </list-item>
      <list-item>
        <p>The model is highly efficient, whose training has a low cost on computation resources.</p>
      </list-item>
      <list-item>
        <p>The high prediction accuracy makes it a useful tool for studying circRNA-RBP interactions.</p>
      </list-item>
    </list>
  </sec>
  <sec sec-type="conclusions" id="s5">
    <title>5. Conclusion</title>
    <p>In this study, we propose a method called DeCban to predict the binding relationship between RNA-binding-proteins and circRNAs. Different from the existing tools which can only handle short segments of circRNAs, DeCban is able to predict whether a binding site is present on full-length circRNAs. In order to solve the problem of large length span and sparse distribution of binding sites, we design a multi-branch and multi-layer convolutional neural network with an attention module. Moreover, to enhance the input data representation, we propose the double embedding encoding scheme, which is superior to the traditional single RNA embedding due to the introduction of amino-acid-level sequence information. We perform experiments on 37 data sets, corresponding to 37 RBPs. The experimental results show that our method achieves the best results compared with a variety of advanced deep learning structures. DeCban will be a useful tool for studying the interactions between RBP and circRNA.</p>
  </sec>
  <sec sec-type="data-availability" id="s6">
    <title>Data Availability Statement</title>
    <p>The original contributions presented in the study are included in the article/<xref ref-type="supplementary-material" rid="SM1">Supplementary Materials</xref>, further inquiries can be directed to the corresponding author/s.</p>
  </sec>
  <sec id="s7">
    <title>Author Contributions</title>
    <p>LY and YY designed the model, analyzed the results, and wrote the manuscript. LY conducted the experiments. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="fn0001">
      <p><sup>1</sup>An advanced model structure based on LSTM, which has achieved state of art results on multiple NLP tasks.</p>
    </fn>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding.</bold> This work was supported by the National Natural Science Foundation of China (No. 61972251).</p>
    </fn>
  </fn-group>
  <sec sec-type="supplementary-material" id="s8">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fgene.2020.632861/full#supplementary-material">https://www.frontiersin.org/articles/10.3389/fgene.2020.632861/full#supplementary-material</ext-link></p>
    <supplementary-material content-type="local-data" id="SM1">
      <media xlink:href="Table_1.XLSX">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alipanahi</surname><given-names>B.</given-names></name><name><surname>Delong</surname><given-names>A.</given-names></name><name><surname>Weirauch</surname><given-names>M. T.</given-names></name><name><surname>Frey</surname><given-names>B. J.</given-names></name></person-group> (<year>2015</year>). <article-title>Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning</article-title>. <source>Nat. Biotechnol</source>. <volume>33</volume>:<fpage>831</fpage>. <pub-id pub-id-type="doi">10.1038/nbt.3300</pub-id><?supplied-pmid 26213851?><pub-id pub-id-type="pmid">26213851</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>W.</given-names></name><name><surname>Lei</surname><given-names>T. Y.</given-names></name><name><surname>Jin</surname><given-names>D. C.</given-names></name><name><surname>Lin</surname><given-names>H.</given-names></name><name><surname>Chou</surname><given-names>K. C.</given-names></name></person-group> (<year>2014</year>). <article-title>Pseknc: a flexible web server for generating pseudo k-tuple nucleotide composition</article-title>. <source>Anal. Biochem</source>. <volume>456</volume>:<fpage>53</fpage>. <pub-id pub-id-type="doi">10.1016/j.ab.2014.04.001</pub-id><?supplied-pmid 24732113?><pub-id pub-id-type="pmid">24732113</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clauwaert</surname><given-names>J.</given-names></name><name><surname>Waegeman</surname><given-names>W.</given-names></name></person-group> (<year>2019</year>). <article-title>Novel transformer networks for improved sequence labeling in genomics</article-title>. <source>bioRxiv [Preprint]</source>. 836163. <pub-id pub-id-type="doi">10.1101/836163</pub-id><?supplied-pmid 33125335?><pub-id pub-id-type="pmid">33125335</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devlin</surname><given-names>J.</given-names></name><name><surname>Chang</surname><given-names>M. W.</given-names></name><name><surname>Lee</surname><given-names>K.</given-names></name><name><surname>Toutanova</surname><given-names>K.</given-names></name></person-group> (<year>2018</year>). <article-title>Bert: Pre-training of deep bidirectional transformers for language understanding</article-title>. <source>arXiv preprint</source> arXiv:1810.04805.</mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Du</surname><given-names>W. W.</given-names></name><name><surname>Zhang</surname><given-names>C.</given-names></name><name><surname>Yang</surname><given-names>W.</given-names></name><name><surname>Yong</surname><given-names>T.</given-names></name><name><surname>Awan</surname><given-names>F. M.</given-names></name><name><surname>Yang</surname><given-names>B. B.</given-names></name></person-group> (<year>2017</year>). <article-title>Identifying and characterizing circRNA-protein interaction</article-title>. <source>Theranostics</source>
<volume>7</volume>:<fpage>4183</fpage>. <pub-id pub-id-type="doi">10.7150/thno.21299</pub-id><?supplied-pmid 29158818?><pub-id pub-id-type="pmid">29158818</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dudekula</surname><given-names>D. B.</given-names></name><name><surname>Panda</surname><given-names>A. C.</given-names></name><name><surname>Grammatikakis</surname><given-names>I.</given-names></name><name><surname>De</surname><given-names>S.</given-names></name><name><surname>Abdelmohsen</surname><given-names>K.</given-names></name><name><surname>Gorospe</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>). <article-title>Circinteractome: a web tool for exploring circular RNAs and their interacting proteins and microRNAs</article-title>. <source>RNA Biol</source>. <volume>13</volume>, <fpage>34</fpage>–<lpage>42</lpage>. <pub-id pub-id-type="doi">10.1080/15476286.2015.1128065</pub-id><?supplied-pmid 26669964?><pub-id pub-id-type="pmid">26669964</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>L.</given-names></name><name><surname>Niu</surname><given-names>B.</given-names></name><name><surname>Zhu</surname><given-names>Z.</given-names></name><name><surname>Wu</surname><given-names>S.</given-names></name><name><surname>Li</surname><given-names>W.</given-names></name></person-group> (<year>2012</year>). <article-title>CD-hit</article-title>. <source>Bioinformatics</source>
<volume>28</volume>, <fpage>3150</fpage>–<lpage>3152</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bts565</pub-id><?supplied-pmid 23060610?><pub-id pub-id-type="pmid">23060610</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hansen</surname><given-names>T. B.</given-names></name><name><surname>Kjems</surname><given-names>J.</given-names></name><name><surname>Damgaard</surname><given-names>C. K.</given-names></name></person-group> (<year>2013</year>). <article-title>Circular RNA and MIR-7 in cancer</article-title>. <source>Cancer Res</source>. <volume>73</volume>, <fpage>5609</fpage>–<lpage>5612</lpage>. <pub-id pub-id-type="doi">10.1158/0008-5472.CAN-13-1568</pub-id><?supplied-pmid 24014594?><pub-id pub-id-type="pmid">24014594</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). <article-title>Deep residual learning for image recognition</article-title>, in <source>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>, Doha. <pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S.</given-names></name><name><surname>Schmidhuber</surname><given-names>J.</given-names></name></person-group> (<year>1997</year>). <article-title>Long short-term memory</article-title>. <source>Neural Comput</source>. <volume>9</volume>, <fpage>1735</fpage>–<lpage>1780</lpage>. <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><pub-id pub-id-type="pmid">9377276</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>C.</given-names></name><name><surname>Bi</surname><given-names>Y.</given-names></name><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Leier</surname><given-names>A.</given-names></name><name><surname>Li</surname><given-names>F.</given-names></name><name><surname>Song</surname><given-names>J.</given-names></name></person-group> (<year>2020</year>). <article-title>PASSION: an ensemble neural network approach for identifying the binding sites of RBPs on circRNAs</article-title>. <source>Bioinformatics</source>
<volume>36</volume>, <fpage>4276</fpage>–<lpage>4282</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa522</pub-id><pub-id pub-id-type="pmid">32426818</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ju</surname><given-names>Y.</given-names></name><name><surname>Yuan</surname><given-names>L.</given-names></name><name><surname>Yang</surname><given-names>Y.</given-names></name><name><surname>Zhao</surname><given-names>H.</given-names></name></person-group> (<year>2019</year>). <article-title>Circslnn: Identifying rbp-binding sites on circrnas via sequence labeling neural networks</article-title>. <source>Front. Genet</source>. <volume>10</volume>:<fpage>1184</fpage>. <pub-id pub-id-type="doi">10.3389/fgene.2019.01184</pub-id><?supplied-pmid 31824574?><pub-id pub-id-type="pmid">31824574</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>Y.</given-names></name></person-group> (<year>2014</year>). <article-title>Convolutional neural networks for sentence classification</article-title>. <source>arXiv preprint arXiv:1408.5882</source>. <pub-id pub-id-type="doi">10.3115/v1/D14-1181</pub-id><?supplied-pmid 31070725?><pub-id pub-id-type="pmid">31070725</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J.-H.</given-names></name><name><surname>Liu</surname><given-names>S.</given-names></name><name><surname>Zhou</surname><given-names>H.</given-names></name><name><surname>Qu</surname><given-names>L.-H.</given-names></name><name><surname>Yang</surname><given-names>J.-H.</given-names></name></person-group> (<year>2013</year>). <article-title>starbase v2. 0: decoding miRNA-ceRNA, miRNA-ncRNA and protein-RNA interaction networks from large-scale clip-seq data</article-title>. <source>Nucl. Acids Res</source>. <volume>42</volume>, <fpage>D92</fpage>–<lpage>D97</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkt1248</pub-id><?supplied-pmid 24297251?><pub-id pub-id-type="pmid">24297251</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Yang</surname><given-names>L.</given-names></name><name><surname>Chen</surname><given-names>L.-L.</given-names></name></person-group> (<year>2018</year>). <article-title>The biogenesis, functions, and challenges of circular RNAs</article-title>. <source>Mol. Cell</source>
<volume>71</volume>, <fpage>428</fpage>–<lpage>442</lpage>. <pub-id pub-id-type="doi">10.1016/j.molcel.2018.06.034</pub-id><?supplied-pmid 30057200?><pub-id pub-id-type="pmid">30057200</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Zheng</surname><given-names>Q.</given-names></name><name><surname>Bao</surname><given-names>C.</given-names></name><name><surname>Li</surname><given-names>S.</given-names></name><name><surname>Guo</surname><given-names>W.</given-names></name><name><surname>Zhao</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>Circular RNA is enriched and stable in exosomes: a promising biomarker for cancer diagnosis</article-title>. <source>Cell Res</source>. <volume>25</volume>:<fpage>981</fpage>. <pub-id pub-id-type="doi">10.1038/cr.2015.82</pub-id><?supplied-pmid 26138677?><pub-id pub-id-type="pmid">26138677</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meher</surname><given-names>P. K.</given-names></name><name><surname>Sahu</surname><given-names>T. K.</given-names></name><name><surname>Gahoi</surname><given-names>S.</given-names></name><name><surname>Satpathy</surname><given-names>S.</given-names></name><name><surname>Rao</surname><given-names>A. R.</given-names></name></person-group> (<year>2019</year>). <article-title>Evaluating the performance of sequence encoding schemes and machine learning methods for splice sites recognition</article-title>. <source>Gene</source>
<volume>705</volume>, <fpage>113</fpage>–<lpage>126</lpage>. <pub-id pub-id-type="doi">10.1016/j.gene.2019.04.047</pub-id><?supplied-pmid 31009682?><pub-id pub-id-type="pmid">31009682</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mikolov</surname><given-names>T.</given-names></name><name><surname>Chen</surname><given-names>K.</given-names></name><name><surname>Corrado</surname><given-names>G.</given-names></name><name><surname>Dean</surname><given-names>J.</given-names></name></person-group> (<year>2013</year>). <article-title>Efficient estimation of word representations in vector space</article-title>. <source>Comput. Sci. arXiv preprint</source> arXiv:1301.3781.</mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pamudurti</surname><given-names>N. R.</given-names></name><name><surname>Bartok</surname><given-names>O.</given-names></name><name><surname>Jens</surname><given-names>M.</given-names></name><name><surname>Ashwalfluss</surname><given-names>R.</given-names></name><name><surname>Stottmeister</surname><given-names>C.</given-names></name><name><surname>Ruhe</surname><given-names>L.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Translation of circrnas</article-title>. <source>Mol. Cell</source><volume>66</volume>, <fpage>9</fpage>–<lpage>21</lpage>.e7. <pub-id pub-id-type="doi">10.1016/j.molcel.2017.02.021</pub-id><?supplied-pmid 28344080?><pub-id pub-id-type="pmid">28344080</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>X.</given-names></name><name><surname>Rijnbeek</surname><given-names>P.</given-names></name><name><surname>Yan</surname><given-names>J.</given-names></name><name><surname>Shen</surname><given-names>H.-B.</given-names></name></person-group> (<year>2018</year>). <article-title>Prediction of rna-protein sequence and structure binding preferences using deep convolutional and recurrent neural networks</article-title>. <source>BMC Genomics</source>
<volume>17</volume>:<fpage>582</fpage>. <pub-id pub-id-type="doi">10.1186/s12864-018-4889-1</pub-id><?supplied-pmid 29970003?><pub-id pub-id-type="pmid">29970003</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>X.</given-names></name><name><surname>Shen</surname><given-names>H.-B.</given-names></name></person-group> (<year>2017</year>). <article-title>RNA-protein binding motifs mining with a new hybrid deep learning based cross-domain knowledge integration approach</article-title>. <source>BMC Bioinformatics</source>
<volume>18</volume>:<fpage>136</fpage>. <pub-id pub-id-type="doi">10.1186/s12859-017-1561-8</pub-id><?supplied-pmid 28245811?><pub-id pub-id-type="pmid">28245811</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>X.</given-names></name><name><surname>Shen</surname><given-names>H.-B.</given-names></name></person-group> (<year>2018</year>). <article-title>Predicting RNA-protein binding sites and motifs through combining local and global deep convolutional neural networks</article-title>. <source>Bioinformatics</source>
<volume>34</volume>, <fpage>3427</fpage>–<lpage>3436</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bty364</pub-id><?supplied-pmid 29722865?><pub-id pub-id-type="pmid">29722865</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>X.</given-names></name><name><surname>Yang</surname><given-names>Y.</given-names></name><name><surname>Xia</surname><given-names>C.-Q.</given-names></name><name><surname>Mirza</surname><given-names>A. H.</given-names></name><name><surname>Shen</surname><given-names>H.-B.</given-names></name></person-group> (<year>2019</year>). <article-title>Recent methodology progress of deep learning for RNA-protein interaction prediction</article-title>. <source>Wiley Interdisc. Rev</source>. <volume>10</volume>:<fpage>e1544</fpage>. <pub-id pub-id-type="doi">10.1002/wrna.1544</pub-id><?supplied-pmid 31067608?><pub-id pub-id-type="pmid">31067608</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pennington</surname><given-names>J.</given-names></name><name><surname>Socher</surname><given-names>R.</given-names></name><name><surname>Manning</surname><given-names>C.</given-names></name></person-group> (<year>2014</year>). <article-title>Glove: Global vectors for word representation</article-title>, in <source>Proceedings of EMNLP</source>, <volume>Doha</volume>, <fpage>1532</fpage>–<lpage>1543</lpage>. <pub-id pub-id-type="doi">10.3115/v1/D14-1162</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peters</surname><given-names>M. E.</given-names></name><name><surname>Neumann</surname><given-names>M.</given-names></name><name><surname>Iyyer</surname><given-names>M.</given-names></name><name><surname>Gardner</surname><given-names>M.</given-names></name><name><surname>Clark</surname><given-names>C.</given-names></name><name><surname>Lee</surname><given-names>K.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Deep contextualized word representations</article-title>. <source>arXiv preprint</source> arXiv:1802.05365. <pub-id pub-id-type="doi">10.18653/v1/N18-1202</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radford</surname><given-names>A.</given-names></name><name><surname>Narasimhan</surname><given-names>K.</given-names></name><name><surname>Salimans</surname><given-names>T.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name></person-group> (<year>2018</year>). Improving language understanding by generative pre-training.</mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Lei</surname><given-names>X.</given-names></name></person-group> (<year>2020</year>). <article-title>Matrix factorization with neural network for predicting circrna-rbp interactions</article-title>. <source>BMC Bioinformatics</source>
<volume>21</volume>:<fpage>229</fpage>. <pub-id pub-id-type="doi">10.1186/s12859-020-3514-x</pub-id><?supplied-pmid 32503474?><pub-id pub-id-type="pmid">32503474</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>J.</given-names></name><name><surname>Friedrich</surname><given-names>S.</given-names></name><name><surname>Kurgan</surname><given-names>L.</given-names></name></person-group> (<year>2016</year>). <article-title>A comprehensive comparative review of sequence-based predictors of DNA-and RNA-binding residues</article-title>. <source>Brief. Bioinformatics</source>
<volume>17</volume>, <fpage>88</fpage>–<lpage>105</lpage>. <pub-id pub-id-type="doi">10.1093/bib/bbv023</pub-id><?supplied-pmid 25935161?><pub-id pub-id-type="pmid">25935161</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Y.-C. T.</given-names></name><name><surname>Di</surname><given-names>C.</given-names></name><name><surname>Hu</surname><given-names>B.</given-names></name><name><surname>Zhou</surname><given-names>M.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Song</surname><given-names>N.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>CLIPdb: a CLIP-seq database for protein-RNA interactions</article-title>. <source>BMC Genomics</source><volume>16</volume>:<fpage>51</fpage>. <pub-id pub-id-type="doi">10.1186/s12864-015-1273-2</pub-id><?supplied-pmid 25652745?><pub-id pub-id-type="pmid">25652745</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zang</surname><given-names>J.</given-names></name><name><surname>Lu</surname><given-names>D.</given-names></name><name><surname>Xu</surname><given-names>A.</given-names></name></person-group> (<year>2020</year>). <article-title>The interaction of circRNAs and RNA binding proteins: an important part of circRNA maintenance and function</article-title>. <source>J. Neurosci. Res</source>. <volume>98</volume>, <fpage>87</fpage>–<lpage>97</lpage>. <pub-id pub-id-type="doi">10.1002/jnr.24356</pub-id><?supplied-pmid 30575990?><pub-id pub-id-type="pmid">30575990</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>K.</given-names></name><name><surname>Pan</surname><given-names>X.</given-names></name><name><surname>Yang</surname><given-names>Y.</given-names></name><name><surname>Shen</surname><given-names>H.-B.</given-names></name></person-group> (<year>2019</year>). <article-title>Crip: predicting circRNA-RBP interaction sites using a codon-based encoding and hybrid deep neural networks</article-title>. <source>RNA</source> 25:rna.070565.119. <pub-id pub-id-type="doi">10.1261/rna.070565.119</pub-id><?supplied-pmid 31537716?><pub-id pub-id-type="pmid">31537716</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Kang</surname><given-names>L.</given-names></name></person-group> (<year>2011</year>). <article-title>A k-mer scheme to predict pirnas and characterize locust piRNAs</article-title>. <source>Bioinformatics</source>
<volume>27</volume>:<fpage>771</fpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btr016</pub-id><?supplied-pmid 21224287?><pub-id pub-id-type="pmid">21224287</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>P.</given-names></name><name><surname>Shi</surname><given-names>W.</given-names></name><name><surname>Tian</surname><given-names>J.</given-names></name><name><surname>Qi</surname><given-names>Z.</given-names></name><name><surname>Li</surname><given-names>B.</given-names></name><name><surname>Hao</surname><given-names>H.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Attention-based bidirectional long short-term memory networks for relation classification</article-title>, in <source>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</source>, Doha. <pub-id pub-id-type="doi">10.18653/v1/P16-2034</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
