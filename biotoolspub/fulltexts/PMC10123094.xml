<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Nucleic Acids Res</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nucleic Acids Res</journal-id>
    <journal-id journal-id-type="publisher-id">nar</journal-id>
    <journal-title-group>
      <journal-title>Nucleic Acids Research</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0305-1048</issn>
    <issn pub-type="epub">1362-4962</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10123094</article-id>
    <article-id pub-id-type="pmid">36796796</article-id>
    <article-id pub-id-type="doi">10.1093/nar/gkad055</article-id>
    <article-id pub-id-type="publisher-id">gkad055</article-id>
    <article-categories>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00010</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>Narese/7</subject>
        <subject>Narese/24</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>NAR Breakthrough Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepBIO: an automated and interpretable deep-learning platform for high-throughput biological sequence prediction, functional annotation and visualization analysis</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Ruheng</given-names>
        </name>
        <aff><institution>School of Software, Shandong University</institution>, <addr-line>Jinan</addr-line>, <country country="CN">China</country></aff>
        <aff><institution>Joint SDU-NTU Centre for Artificial Intelligence Research (C-FAIR), Shandong University</institution>, <addr-line>Jinan</addr-line>, <country country="CN">China</country></aff>
        <xref rid="FN1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jiang</surname>
          <given-names>Yi</given-names>
        </name>
        <aff><institution>School of Software, Shandong University</institution>, <addr-line>Jinan</addr-line>, <country country="CN">China</country></aff>
        <aff><institution>Joint SDU-NTU Centre for Artificial Intelligence Research (C-FAIR), Shandong University</institution>, <addr-line>Jinan</addr-line>, <country country="CN">China</country></aff>
        <xref rid="FN1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jin</surname>
          <given-names>Junru</given-names>
        </name>
        <aff><institution>School of Software, Shandong University</institution>, <addr-line>Jinan</addr-line>, <country country="CN">China</country></aff>
        <aff><institution>Joint SDU-NTU Centre for Artificial Intelligence Research (C-FAIR), Shandong University</institution>, <addr-line>Jinan</addr-line>, <country country="CN">China</country></aff>
        <xref rid="FN1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yin</surname>
          <given-names>Chenglin</given-names>
        </name>
        <aff><institution>School of Software, Shandong University</institution>, <addr-line>Jinan</addr-line>, <country country="CN">China</country></aff>
        <aff><institution>Joint SDU-NTU Centre for Artificial Intelligence Research (C-FAIR), Shandong University</institution>, <addr-line>Jinan</addr-line>, <country country="CN">China</country></aff>
        <xref rid="FN1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yu</surname>
          <given-names>Haoqing</given-names>
        </name>
        <aff><institution>School of Software, Shandong University</institution>, <addr-line>Jinan</addr-line>, <country country="CN">China</country></aff>
        <aff><institution>Joint SDU-NTU Centre for Artificial Intelligence Research (C-FAIR), Shandong University</institution>, <addr-line>Jinan</addr-line>, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Fengsheng</given-names>
        </name>
        <aff><institution>School of Software, Shandong University</institution>, <addr-line>Jinan</addr-line>, <country country="CN">China</country></aff>
        <aff><institution>Joint SDU-NTU Centre for Artificial Intelligence Research (C-FAIR), Shandong University</institution>, <addr-line>Jinan</addr-line>, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Feng</surname>
          <given-names>Jiuxin</given-names>
        </name>
        <aff><institution>School of Software, Shandong University</institution>, <addr-line>Jinan</addr-line>, <country country="CN">China</country></aff>
        <aff><institution>Joint SDU-NTU Centre for Artificial Intelligence Research (C-FAIR), Shandong University</institution>, <addr-line>Jinan</addr-line>, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-5922-0364</contrib-id>
        <name>
          <surname>Su</surname>
          <given-names>Ran</given-names>
        </name>
        <aff><institution>College of Intelligence and Computing, Tianjin University</institution>, <addr-line>Tianjin</addr-line>, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-8721-8883</contrib-id>
        <name>
          <surname>Nakai</surname>
          <given-names>Kenta</given-names>
        </name>
        <aff><institution>Human Genome Center, Institute of Medical Science, University of Tokyo</institution>, <addr-line>Tokyo</addr-line>, <country country="JP">Japan</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-6406-1142</contrib-id>
        <name>
          <surname>Zou</surname>
          <given-names>Quan</given-names>
        </name>
        <!--zouquan@nclab.net-->
        <aff><institution>Institute of Fundamental and Frontier Sciences, University of Electronic Science and Technology of China</institution>, <addr-line>Chengdu</addr-line>, <country country="CN">China</country></aff>
        <xref rid="COR2" ref-type="corresp"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-1444-190X</contrib-id>
        <name>
          <surname>Wei</surname>
          <given-names>Leyi</given-names>
        </name>
        <!--weileyi@sdu.edu.cn-->
        <aff><institution>School of Software, Shandong University</institution>, <addr-line>Jinan</addr-line>, <country country="CN">China</country></aff>
        <aff><institution>Joint SDU-NTU Centre for Artificial Intelligence Research (C-FAIR), Shandong University</institution>, <addr-line>Jinan</addr-line>, <country country="CN">China</country></aff>
        <xref rid="COR1" ref-type="corresp"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="COR1">To whom correspondence should be addressed. Email: <email>weileyi@sdu.edu.cn</email></corresp>
      <corresp id="COR2">Correspondence may also be addressed to Quan Zou. Email: <email>zouquan@nclab.net</email></corresp>
      <fn id="FN1">
        <p>The authors wish it to be known that, in their opinion, the first four authors should be regarded as Joint First Authors.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="collection">
      <day>24</day>
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2023-02-17">
      <day>17</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>17</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <volume>51</volume>
    <issue>7</issue>
    <fpage>3017</fpage>
    <lpage>3029</lpage>
    <history>
      <date date-type="accepted">
        <day>19</day>
        <month>1</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>19</day>
        <month>12</month>
        <year>2022</year>
      </date>
      <date date-type="received">
        <day>17</day>
        <month>11</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023. Published by Oxford University Press on behalf of Nucleic Acids Research.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact <email>journals.permissions@oup.com</email></license-p>
      </license>
    </permissions>
    <self-uri xlink:href="gkad055.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>Here, we present DeepBIO, the first-of-its-kind automated and interpretable deep-learning platform for high-throughput biological sequence functional analysis. DeepBIO is a one-stop-shop web service that enables researchers to develop new deep-learning architectures to answer any biological question. Specifically, given any biological sequence data, DeepBIO supports a total of 42 state-of-the-art deep-learning algorithms for model training, comparison, optimization and evaluation in a fully automated pipeline. DeepBIO provides a comprehensive result visualization analysis for predictive models covering several aspects, such as model interpretability, feature analysis and functional sequential region discovery. Additionally, DeepBIO supports nine base-level functional annotation tasks using deep-learning architectures, with comprehensive interpretations and graphical visualizations to validate the reliability of annotated sites. Empowered by high-performance computers, DeepBIO allows ultra-fast prediction with up to million-scale sequence data in a few hours, demonstrating its usability in real application scenarios. Case study results show that DeepBIO provides an accurate, robust and interpretable prediction, demonstrating the power of deep learning in biological sequence functional analysis. Overall, we expect DeepBIO to ensure the reproducibility of deep-learning biological sequence analysis, lessen the programming and hardware burden for biologists and provide meaningful functional insights at both the sequence level and base level from biological sequences alone. DeepBIO is publicly available at <ext-link xlink:href="https://inner.wei-group.net/DeepBIO" ext-link-type="uri">https://inner.wei-group.net/DeepBIO</ext-link>.</p>
    </abstract>
    <abstract abstract-type="graphical">
      <title>Graphical Abstract</title>
      <p>
        <fig position="float" id="ga1">
          <label>Graphical Abstract</label>
          <caption>
            <p>DeepBIO is an automated and interpretable deep-learning platform for high-throughput biological sequence level prediction, functional site annotation, and comprehensive visualization analysis.</p>
          </caption>
          <graphic xlink:href="gkad055figgra1" position="float"/>
        </fig>
      </p>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>62250028</award-id>
        <award-id>62071278</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="13"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="SEC1">
    <title>INTRODUCTION</title>
    <p>The development of next-generation sequencing techniques has led to an exponential increase in the amount of biological sequence data accessible, including genomic, transcriptomic and proteomic sequences. It naturally poses an important challenge—how to build the relationships from sequences to structures and functions (<xref rid="B1" ref-type="bibr">1</xref>). Given such large-scale data, traditional wet lab experimental methods are laborious, time-consuming and high cost for functional analysis. Alternatively, the booming development of machine-learning approaches has paved a new way to understand the complex mapping from biological sequences to their structures and functional mechanisms. Over the past few decades, data-driven machine-learning approaches have emerged as powerful methods to enable the automated and fast function prediction <italic toggle="yes">ab initio</italic> from sequences alone, providing a new perspective on studying the functional aspects of biological sequences (<xref rid="B2" ref-type="bibr">2–5</xref>).</p>
    <p>The increasing use of machine-learning workflows in biological sequence analysis and the willingness to disseminate trained machine-learning models have pushed computer scientists to design more user-friendly solutions. In recent years, there have been an increasing number of web servers and software packages developed for this purpose. For instance, BioSeq-Analysis was the first platform to analyze various biological sequences via machine-learning approaches (<xref rid="B6" ref-type="bibr">6</xref>). Later, Liu <italic toggle="yes">et al.</italic> established BioSeq-Analysis2.0 (<xref rid="B7" ref-type="bibr">7</xref>) to automatically generate various predictors for biological sequence analysis at both the residue level and sequence level. Additionally, iLearnplus is a popular platform that provides a pipeline for biological sequence analysis, which comprises feature extraction and selection, model construction and analysis of prediction results (<xref rid="B8" ref-type="bibr">8</xref>). More recently, Liu <italic toggle="yes">et al.</italic> developed BioSeq-BLM, a web platform that introduces different biological language models for DNA, RNA and protein sequence analysis (<xref rid="B9" ref-type="bibr">9</xref>). In addition, other representative tools include iFeatureOmega (<xref rid="B10" ref-type="bibr">10</xref>), Rcpi (<xref rid="B11" ref-type="bibr">11</xref>) and protr (<xref rid="B12" ref-type="bibr">12</xref>). The platforms and tools have boosted the use of machine-learning solutions for biological sequence analysis tasks in biology. However, these traditional machine-learning workflows have some drawbacks. For instance, to train a good model, strong professional knowledge is usually required in terms of designing feature descriptors, selecting machine-learning algorithms as well as setting up the model parameters, which limit their usability in real applications to some extent; on the other hand, they cannot support large-scale prediction and analysis. Recently, deep learning has played a complementary role to traditional machine learning due to its excellent scalability and adaptivity. Therefore, some deep-learning analysis tools have been developed, such as Kipoi (<xref rid="B13" ref-type="bibr">13</xref>), Pysster (<xref rid="B14" ref-type="bibr">14</xref>) and Selene (<xref rid="B15" ref-type="bibr">15</xref>). Kipoi (<xref rid="B13" ref-type="bibr">13</xref>) is a repository of ready-to-use trained models for genomic data analysis. Pysster (<xref rid="B14" ref-type="bibr">14</xref>) is a Python package that supports training deep-learning models only with convolutional neural networks on biological sequence data. More recently, Chen <italic toggle="yes">et al.</italic> developed Selene, a PyTorch-based deep-learning library for quick and simple development, training and application of deep-learning model architectures for biological sequence data (<xref rid="B15" ref-type="bibr">15</xref>).</p>
    <p>These tools make the use of deep learning more convenient for data-driven biological sequence analysis to some extent. However, there remain some key challenges that need to be addressed. First, despite the availability of deep-learning tools, they are not completely automated. Running the tools still faces some technical challenges for non-expert researchers. On the one hand, they require strong professional knowledge and programming skills to set up sophisticated software, which directly limits the generic use of deep learning in non-expert communities (i.e. researchers without any computer science training). On the other hand, it is extremely difficult for researchers to train deep-learning models with the enormous scale of data space, since the major downside of deep learning is its computational intensity, requiring high-performance computational resources and a long training time. Accordingly, a web platform that enables an automated deep-learning pipeline is greatly needed. Second, most of the existing tools cannot meet the high demand of the research community, since they provide few deep-learning architectures (seen in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>), such as convolutional neural networks, for model construction. In fact, there are some state-of-the-art deep-learning models with successful applications in bioinformatics problems. For example, Graph Neural Networks (GNNs) demonstrated an ability to address complex biological problems, for example, the prediction of microRNA–disease interaction (<xref rid="B16" ref-type="bibr">16</xref>). Large-scale language pre-trained models such as DNABERT and ProtBERT showed strong capacities in discriminative feature learning in biological sequence analysis (<xref rid="B5" ref-type="bibr">5</xref>,<xref rid="B17" ref-type="bibr">17</xref>). Thus, it is necessary to enable the use of state-of-the-art deep-learning approaches in biology. Ultimately, existing tools lack comprehensive result analysis, which might limit users’ understanding of what the deep-learning models learn; how reliable the deep-learning predictions are; and why the deep learning performs well.</p>
    <p>In this work, we present DeepBIO, an automated deep-learning platform for biological sequence prediction, functional annotation and result visualization analysis. To be specific, our DeepBIO distinguishes itself from other platforms with the following unique advantages. (i) One-stop-shop service. DeepBIO is the first-of-its-kind platform as a code-free web portal to ensure the reproducibility of deep-learning biological sequence analysis and to lessen the programming burden for biologists. (ii) Pure deep-learning platform. DeepBIO is a purely deep-learning platform that integrates &gt;40 state-of-the-art mainstream deep-learning algorithms, including convolutional neural networks, advanced natural language processing models and GNNs (detailed in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>), which enables researchers who are interested to train, compare and evaluate different architectures on any biological sequence data. (iii) Two predictive modules. DeepBIO is the first platform that supports not only sequence-level function prediction for any biological sequence data, but also allows nine base-level functional annotation tasks using pre-trained deep-learning architectures, covering DNA methylation, RNA methylation and protein binding specificity. Besides, we further offer an in-depth comparative analysis between the predictions by our models and by experimental data to validate the reliability of the predictions. Notably, empowered by high-performance computers, we demonstrate that DeepBIO supports fast prediction with up to million-scale sequence data, demonstrating its usability in real application scenarios. (iv) Comprehensive result visualization analysis. Aiming to help researchers to delve into more insights from the model predictions, DeepBIO offers a comprehensive visualization result analysis with a variety of interactive figures and tables, covering the following five aspects: statistical analysis of input data; evaluation and comparison of the performance of the prediction models; feature importance and interpretable analysis; model parameter analysis; and sequence conservation analysis. In particular, integrating interpretable mechanisms (e.g. attention heatmap and motif discovery) into deep-learning frameworks enables researchers to analyze which sequential regions are important for the predictions, addressing the issue of the ‘black box’ in deep learning and effectively building the relationship between biological sequences and functions.</p>
  </sec>
  <sec sec-type="materials|methods" id="SEC2">
    <title>MATERIALS AND METHODS</title>
    <sec id="SEC2-1">
      <title>The overall framework of DeepBIO</title>
      <p>The DeepBIO platform fully automates the model training process and applies &gt;40 deep-learning approaches for sequence classification prediction and functional site analysis for genomic, transcriptomic, and proteomic sequence data. Figure <xref rid="F1" ref-type="fig">1A</xref> illustrates the overall framework of the proposed DeepBIO, which consists of four modules: (i) data input module; (ii) sequence-level functional prediction module; (iii) base-level functional annotation module; and (iv) result report module. The workflow of DeepBIO is described as follows. Firstly, DeepBIO takes biological sequence data from the data input module, where it can handle three biological sequence types: DNA, RNA and protein. Next, there are two functional modules for the two types of tasks: one is the sequence-level functional prediction module for binary classification tasks and the other is the base-level functional annotation module for functional annotation tasks. The detailed web techniques and the overview of the interaction between the front end and back end are shown in Figure <xref rid="F1" ref-type="fig">1B</xref>. In the sequence-level functional prediction module, we support users in automatically training, evaluating and comparing the deep-learning models with their input data. Specifically, there are four main steps (seen in Figure <xref rid="F1" ref-type="fig">1C</xref>) in this module: (i) data pre-processing; (ii) model construction; (iii) model evaluation; and (iv) visualization analysis. In the base-level functional annotation module, we provide the base-level functional annotation using deep-learning approaches, such as DNA methylation, RNA methylation and protein binding specificity prediction. Similarly, we also curated four steps for this module (seen in Figure <xref rid="F1" ref-type="fig">1C</xref>): (i) data selection; (ii) task selection; (iii) model loading; and (iv) result visualization. Ultimately, in the result report module, we provide a series of visualization analysis results with various kinds of data formats. Below, we emphasize the details of the four modules.</p>
      <fig position="float" id="F1">
        <label>Figure 1.</label>
        <caption>
          <p>The overview of the DeepBIO platform. (<bold>A</bold>) The architecture of the DeepBIO platform. DeepBIO contains four modules, comprising the data input module, the sequence-level functional prediction module, the base-level functional annotation module and the result report module. DeepBIO accepts DNA, RNA and protein sequence data from the data input module, with some additional user-selectable settings (e.g. sequence similarity setting, imbalanced data processing and data augmentation). The sequence-level functional prediction module provides the deep-learning biological sequence classification and analysis, while the base-level functional annotation module supports the prediction of functional sites for user-provided sequences. In the result report module, DeepBIO provides user-friendly, interactive and interpretable visualization representations to deliver results of the above two functional modules as a download-free report. (<bold>B</bold>) The web technique of DeepBIO. (<bold>C</bold>) The workflow of the two main functional modules. The sequence-level functional prediction module consists of four steps: (i) data pre-processing; (ii) model construction; (iii) model evaluation; and (iv) visualization analysis. DeepBIO outputs classification predictions of input sequences based on the four steps and displays the result reports with interactive graphs and tables. The base-level functional annotation module includes the following steps: (i) data selection; (ii) task selection; (iii) model loading; and (iv) result visualization. DeepBIO offers multiple methylation annotations for DNA and RNA sequences while supporting ligand-binding site recognition for protein sequences.</p>
        </caption>
        <graphic xlink:href="gkad055fig1" position="float"/>
      </fig>
    </sec>
    <sec id="SEC2-2">
      <title>Data input module</title>
      <p>DeepBIO serves as an automatically integrated analysis platform based on biological sequence data, including for DNA, RNA and protein, which can be entered in the online input box or uploaded with the standard file format. In addition, users can select deep-learning models and different settings (e.g. whether to turn on the data enhancement option). After obtaining the user's input data, DeepBIO further performs data cleaning to ensure that the user input data are ready and legal to pass into the next module and run properly.</p>
    </sec>
    <sec id="SEC2-3">
      <title>Sequence-level functional prediction module</title>
      <sec id="SEC2-3-1">
        <title>Step 1. Data pre-processing</title>
        <p>This step is an optimization-based data-processing phase that uses a variety of approaches to handle and obtain the input sequence data that are suitable for model training, ensuring the robustness of the final results and improving the success rate of our service. We design four sections to pre-process the input data: (i) sequence legitimacy testing; (ii) sequence similarity setting; (iii) imbalanced data processing; and (iv) data augmentation. In Section (i), for the input sequence data, we first examine whether the input data have the legal data format (e.g. FASTA), and then check if the input sequences contain illegal characters (e.g. characters other than A, T, C and G in DNA sequences). In Section (ii), considering that the high sequence similarity in the input data might lead to a performance evaluation bias, we provide a commonly used tool named CD-HIT (<xref rid="B18" ref-type="bibr">18</xref>) to reduce the sequence similarity in the input data. The similarity threshold ranges from 0.7 to 1. In Section (iii), considering that the data imbalance issue might exist in a majority of datasets in real scenarios (i.e. the number of the positives and negatives are extremely imbalanced), we further provide several imbalanced data processing strategies that are commonly used in the machine-learning field, including Focal Loss (<xref rid="B19" ref-type="bibr">19</xref>), Synthetic Minority Oversampling Technique (SMOTE) (<xref rid="B20" ref-type="bibr">20</xref>) and Adaptive Synthetic (ADASYN) (<xref rid="B21" ref-type="bibr">21</xref>). By doing so, we can prevent the biased prediction of models due to the imbalanced data distribution and perhaps achieve an improvement in the overall predictive performance. Finally, in Section (iv), to deal with the few sample learning problem, we further provide some data augmentation methods, such as sequence replacement, sequence flipping and sequence cropping, which can improve the robustness and generalization ability of the deep-learning models.</p>
      </sec>
      <sec id="SEC2-3-2">
        <title>Step 2. Model construction</title>
        <p>The pre-processed sequence data are then fed into user-selected models to train and make the binary prediction. For a scenario in which users do not provide sequence data with training and testing labels, we randomly split the dataset into the training and testing set at a ratio of 8:2 for model training and evaluation. Table <xref rid="tbl1" ref-type="table">1</xref> summarizes all the state-of-the-art deep-learning models in our platform. There are a total of 42 deep-learning models, covering classic deep-learning methods [e.g. Deep Neural Networks (DNN) and Gate Recurrent Unit (GRU)], natural language processing (NLP) methods [e.g. Transformer and Bidirectional Encoder Representations from Transformers (BERT)] and GNN methods [e.g. Graph Convolutional Network (GCN)]. Notably, each of the above models can be exploited to deal with all three types (DNA, RNA and protein) of sequence analysis tasks. In addition, DeepBIO permits users to select multiple models simultaneously, and train and compare their performance on the same datasets. It is worth noting that we have also deployed several deep-learning models (e.g. DNABERT, RNABERT and ProtBERT) that are well pre-trained on large-scale biological background sequences. They can be utilized for fine-tuning the models on small datasets, if necessary, to alleviate the overfitting problem and increase the models' generalization capability.</p>
        <table-wrap position="float" id="tbl1">
          <label>Table 1.</label>
          <caption>
            <p>Deep-learning models in DeepBIO</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Category</th>
                <th rowspan="1" colspan="1">Methods</th>
                <th rowspan="1" colspan="1">References</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">Classic deep-learning methods</td>
                <td rowspan="1" colspan="1">DNN</td>
                <td rowspan="1" colspan="1">(<xref rid="B22" ref-type="bibr">22</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">RNN</td>
                <td rowspan="1" colspan="1">(<xref rid="B23" ref-type="bibr">23</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">LSTM</td>
                <td rowspan="1" colspan="1">(<xref rid="B24" ref-type="bibr">24</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">BiLSTM</td>
                <td rowspan="1" colspan="1">(<xref rid="B25" ref-type="bibr">25</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">LSTM-Attention</td>
                <td rowspan="1" colspan="1">(<xref rid="B26" ref-type="bibr">26</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">GRU</td>
                <td rowspan="1" colspan="1">(<xref rid="B27" ref-type="bibr">27</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">TextCNN</td>
                <td rowspan="1" colspan="1">(<xref rid="B28" ref-type="bibr">28</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">TextRCNN</td>
                <td rowspan="1" colspan="1">(<xref rid="B29" ref-type="bibr">29</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">VDCNN</td>
                <td rowspan="1" colspan="1">(<xref rid="B30" ref-type="bibr">30</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">RNN-CNN</td>
                <td rowspan="1" colspan="1">(<xref rid="B31" ref-type="bibr">31</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Natural language processing methods</td>
                <td rowspan="1" colspan="1">Transformer</td>
                <td rowspan="1" colspan="1">(<xref rid="B32" ref-type="bibr">32</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Reformer</td>
                <td rowspan="1" colspan="1">(<xref rid="B33" ref-type="bibr">33</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Performer</td>
                <td rowspan="1" colspan="1">(<xref rid="B34" ref-type="bibr">34</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Linformer</td>
                <td rowspan="1" colspan="1">(<xref rid="B35" ref-type="bibr">35</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">RoutingTransformer</td>
                <td rowspan="1" colspan="1">(<xref rid="B36" ref-type="bibr">36</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">DNABERT, RNABERT, ProtBERT</td>
                <td rowspan="1" colspan="1">(<xref rid="B5" ref-type="bibr">5</xref>,<xref rid="B17" ref-type="bibr">17</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">BERT-Base</td>
                <td rowspan="1" colspan="1">(<xref rid="B37" ref-type="bibr">37</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">BERT-CNN</td>
                <td rowspan="1" colspan="1">(<xref rid="B38" ref-type="bibr">38</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">BERT-DPCNN</td>
                <td rowspan="1" colspan="1">(<xref rid="B39" ref-type="bibr">39</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">BERT-RCNN</td>
                <td rowspan="1" colspan="1">(<xref rid="B40" ref-type="bibr">40</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">BERT-RNN</td>
                <td rowspan="1" colspan="1">(<xref rid="B41" ref-type="bibr">41</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ERNIE</td>
                <td rowspan="1" colspan="1">(<xref rid="B42" ref-type="bibr">42</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Graph neural network methods</td>
                <td rowspan="1" colspan="1">GCN</td>
                <td rowspan="1" colspan="1">(<xref rid="B43" ref-type="bibr">43</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">TextGCN</td>
                <td rowspan="1" colspan="1">(<xref rid="B44" ref-type="bibr">44</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">GIN</td>
                <td rowspan="1" colspan="1">(<xref rid="B45" ref-type="bibr">45</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">GAT</td>
                <td rowspan="1" colspan="1">(<xref rid="B46" ref-type="bibr">46</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">GraphSage</td>
                <td rowspan="1" colspan="1">(<xref rid="B47" ref-type="bibr">47</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ChebGCN</td>
                <td rowspan="1" colspan="1">(<xref rid="B48" ref-type="bibr">48</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">RECT-L</td>
                <td rowspan="1" colspan="1">(<xref rid="B49" ref-type="bibr">49</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">LightGCN</td>
                <td rowspan="1" colspan="1">(<xref rid="B50" ref-type="bibr">50</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">GNN-FILM</td>
                <td rowspan="1" colspan="1">(<xref rid="B51" ref-type="bibr">51</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">HYPER-Conv</td>
                <td rowspan="1" colspan="1">(<xref rid="B52" ref-type="bibr">52</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">HYPER-Attention</td>
                <td rowspan="1" colspan="1">(<xref rid="B53" ref-type="bibr">53</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">APPNP</td>
                <td rowspan="1" colspan="1">(<xref rid="B54" ref-type="bibr">54</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">TextRGNN</td>
                <td rowspan="1" colspan="1">(<xref rid="B55" ref-type="bibr">55</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">TextSGC</td>
                <td rowspan="1" colspan="1">(<xref rid="B56" ref-type="bibr">56</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">BertGCN</td>
                <td rowspan="1" colspan="1">(<xref rid="B57" ref-type="bibr">57</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">BertGAT</td>
                <td rowspan="1" colspan="1">(<xref rid="B58" ref-type="bibr">58</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">RoBERTaGCN</td>
                <td rowspan="1" colspan="1">(<xref rid="B59" ref-type="bibr">59</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">RoBERTaGAT</td>
                <td rowspan="1" colspan="1">(<xref rid="B60" ref-type="bibr">60</xref>)</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec id="SEC2-3-3">
        <title>Step 3. Model evaluation</title>
        <p>For the performance comparison of deep-learning models, we chose several commonly used evaluation metrics, including sensitivity (SN), specificity (SP), accuracy (ACC) and Matthews’ correlation coefficient (MCC). The formulas of these metrics are as follows:</p>
        <disp-formula id="M1">
          <label>(1)</label>
          <tex-math id="M0001" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*}\rm {\rm{SN\ = \ }}\frac{{{\rm{TP}}}}{{{\rm{TP + FN}}}}\end{equation*}$$\end{document}</tex-math>
        </disp-formula>
        <disp-formula id="M2">
          <label>(2)</label>
          <tex-math id="M0001a" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*}\rm SP\ = \frac{{TN}}{{TN{\rm{ + }}FP}}\ \end{equation*}$$\end{document}</tex-math>
        </disp-formula>
        <disp-formula id="M3">
          <label>(3)</label>
          <tex-math id="M0002" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*}\rm {\rm{ACC\ = }}\frac{{{\rm{TP + TN}}}}{{{\rm{TP + TN + }}FP{\rm{ + }}FN}}\ \end{equation*}$$\end{document}</tex-math>
        </disp-formula>
        <disp-formula id="M4">
          <label>(4)</label>
          <tex-math id="M0003" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*}\rm{\rm{MCC\ = }}\frac{{TP{\rm{ \times TN - FP \times FN}}}}{{\sqrt {\left( {TP{\rm{ + }}FP} \right)\left( {TP{\rm{ + }}FN} \right)\left( {TN{\rm{ + }}FP} \right)\left( {TN{\rm{ + }}FN} \right)} }}\ \end{equation*}$$\end{document}</tex-math>
        </disp-formula>
        <p>where TP, FP, TN and FN represent the numbers of true positives, false positives, true negatives and false negatives, respectively. For the comprehensive performance comparison of different models, the area under the receiver operating characteristic (ROC) curve (AUC) and the area under the precision recall (PR) curve (AP), which range from 0 to 1, are calculated based on the ROC curve and the PR curve, respectively. The ROC curve shows the proportion of true positives versus false positives, in which the AUC equals the probability of ranking a randomly chosen true target higher than a randomly chosen decoy target. The PR curve is used more frequently to evaluate the performance of a model on imbalanced datasets (<xref rid="B61" ref-type="bibr">61</xref>). Similarly, we can calculate the AP, which equals the average of the interpolated precisions. The higher the AUC and AP values, the better the predictive performance of the underlying model.</p>
      </sec>
      <sec id="SEC2-3-4">
        <title>Step 4. Visualization analysis</title>
        <p>DeepBIO combines the prediction results and intermediate data analysis generated from above, and helps users to better understand the input data through statistical analysis and the model results through various types of visualization analyses. To provide users with intuitive and comprehensive result analysis, we have designed and illustrated multiple visual presentation formats, including the pie plot, histogram, sequence statistics graph, ROC and PR curves, kernel density plot and scatter plot. In Table <xref rid="tbl2" ref-type="table">2</xref>, we present the four sections of visualization analysis, which are dataset statistical analysis, result analysis, feature analysis and parameter optimization analysis. The details of each section are described as follows.</p>
        <table-wrap position="float" id="tbl2">
          <label>Table 2.</label>
          <caption>
            <p>Result visualization analysis in the prediction module</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Category</th>
                <th rowspan="1" colspan="1">Type</th>
                <th rowspan="1" colspan="1">Purpose</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">Statistical analysis</td>
                <td rowspan="1" colspan="1">Pie plot</td>
                <td rowspan="1" colspan="1">Sequence composition analysis</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Sequence statistics graph</td>
                <td rowspan="1" colspan="1">Sequence statistical analysis from the datasets</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Histogram</td>
                <td rowspan="1" colspan="1">Sequence length distribution analysis</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Result analysis</td>
                <td rowspan="1" colspan="1">ROC and PR curve</td>
                <td rowspan="1" colspan="1">Performance of model prediction</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Line chart</td>
                <td rowspan="1" colspan="1">Performance of different epochs of models</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Kernel density plot</td>
                <td rowspan="1" colspan="1">Prediction confidence by different models</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Venn diagram and Upset plot</td>
                <td rowspan="1" colspan="1">Overlap of prediction results of different models</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Feature analysis</td>
                <td rowspan="1" colspan="1">ROC and PR curve</td>
                <td rowspan="1" colspan="1">Feature performance comparison between hand-crafted features and the features learned by selected models</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">UMAP plot</td>
                <td rowspan="1" colspan="1">Feature analysis result by UMAP</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">SHAP plot</td>
                <td rowspan="1" colspan="1">Feature importance analysis by SHAP</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Attention heatmap</td>
                <td rowspan="1" colspan="1">Feature representation attention analysis</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Motif discovery</td>
                <td rowspan="1" colspan="1">The discovery of conservative sequence motifs by models</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Parameter optimization analysis</td>
                <td rowspan="1" colspan="1">ROC and PR curve, histogram</td>
                <td rowspan="1" colspan="1">The effect of different parameters (sequence similarity setting, data augmentation and imbalanced data processing) on predictive performance</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>For dataset statistical analysis, we summarize the overall status of the input datasets to assist users in better comprehending the datasets themselves. There are the following three statistical analyses, namely sequence composition analysis, sequence motif analysis and the sequence length distribution analysis. In sequence composition analysis, DeepBIO plots the sequence composition histogram for the samples in the positive and negative dataset. In sequence motif analysis, users can obtain the conservation of each position along the sequences. Ultimately, the sequence length distribution analysis provides users with the length preference of the sequences in the dataset.</p>
        <p>For result analysis, the DeepBIO visualization analysis includes plots considering all the evaluation metrics presented in step 3, to conduct detailed performance comparisons among different models, such as ROC and PR curves, the performance of different epochs and density distribution of model prediction confidence. In addition, we offer a Venn diagram and an Upset plot to illustrate the relationship among different models from the overlapping prediction results. It can be concluded that in this part users can obtain an intuitive comparison of prediction performance for different deep-learning models.</p>
        <p>For feature analysis, we use a dimensionality reduction tool called Uniform Manifold Approximation and Projection (UMAP) (<xref rid="B62" ref-type="bibr">62</xref>) for feature space analysis, and a commonly used tool Shapley additive explanation (SHAP) (<xref rid="B63" ref-type="bibr">63</xref>) for feature importance analysis. UMAP can map and visualize the high-dimensional features learned from the deep-learning models to the low-dimensional space. It intuitively illustrates how good the learned features are. On the other hand, SHAP gives a good explanation of the feature importance, which quantitively measures the impact of each of the learned features on the predictive performance. In addition, for some specific models such as DNABERT and RNABERT, to further explore what the model learns, we employ the attention heatmap to visualize the information the model captures from either global or local perspectives. Furthermore, the motif discovery enables users to know the conservative sequential determinants which the models learn from the datasets. The details of the motif discovery process are described as follows. Firstly, we apply the attention mechanism of BERT-based models to identify key regions from the sequences. Then, for those identified regions, we further extract and visualize the corresponding motifs using the attention scores obtained from the model. In this way, we highlight the concerns of different models for different features and enhance the interpretability of the deep-learning models.</p>
        <p>For parameter optimization analysis, we provide users with some parameter options to make their own choice to optimize the model training, including the effect of different sequence similarities in datasets, the effect of different data augmentation strategies and the effect of different imbalanced data processing methods on predictive performance. We also give some tabular information, including a summary of the input datasets and the performance of deep-learning models. This helps the users to compare the results of the same model with different parameters.</p>
      </sec>
    </sec>
    <sec id="SEC2-4">
      <title>Base-level functional annotation module</title>
      <p>The base-level functional annotation module is to predict the functions of the biological sequences at base level using deep learning. In this module, we provide nine annotation tasks, including the methylation annotation for DNA and RNA sequences, and the ligand-binding site recognition for protein sequences. The module comprises four steps: data selection, task selection, model loading and result visualization. The details of this module are as follows.</p>
      <sec id="SEC2-4-1">
        <title>Step 1. Data selection</title>
        <p>In this step, DeepBIO enables users to upload the sequences they want to annotate. In particular, for the DNA methylation site prediction task, we support two ways to input sequence data, i.e. (i) uploading sequence data by themselves and (ii) selecting a segment of our pre-set human genome by choosing a specific cell line and chromosome.</p>
      </sec>
      <sec id="SEC2-4-2">
        <title>Step 2. Task selection</title>
        <p>There are three types of functional annotation tasks: DNA methylation site prediction, RNA methylation site prediction and protein–ligand binding site prediction. For DNA methylation site prediction, there are DNA <italic toggle="yes">N</italic><sup>4</sup>-methylcytosine (4mC) site prediction, DNA 5-hydroxymethylcytosine (5hmC) site prediction and DNA <italic toggle="yes">N</italic><sup>6</sup>-methyladenine (6mA) site prediction. For RNA methylation site prediction, there are RNA 5-hydroxymethylcytosine (5hmC) site prediction, RNA <italic toggle="yes">N</italic><sup>6</sup>-methyladenosine (m6A) site prediction and RNA <italic toggle="yes">N</italic><sup>4</sup>-methylcytidine (m4C) site prediction. For protein–ligand binding site prediction, there are DNA-binding site prediction, RNA-binding site prediction and peptide-binding site prediction.</p>
      </sec>
      <sec id="SEC2-4-3">
        <title>Step 3. Model loading</title>
        <p>In this step, there are two modes for deep-learning model selection. One is the default and the other is customized. For the default mode, we provide several deep-learning models that are well pre-trained with large-scale biological data, such as DNABERT. The well-pre-trained models generally have good generalization ability and achieve good performance for different downstream tasks. For the customized mode, we allow users to upload the models they trained for the same tasks in the sequence-level functional prediction module.</p>
      </sec>
      <sec id="SEC2-4-4">
        <title>Step 4. Result visualization</title>
        <p>In this part, we provide users with the in-depth annotation result analysis based on the model predictions and biological experiment data (e.g. histone modification signals and protein 3D structural information). Specifically, for DNA, we designed two visualization sections, namely functional site annotation and integrative analysis. For base-level functional annotation, we predict the functional sites that match the selected task type (e.g. DNA 4mC site prediction) and perform the model-predicted confidence of the corresponding position. In the integrative analysis, we perform statistical analysis between DNA methylation predictions and different histone modification signals. For RNA, due to the constraint of data, we only perform the functional site annotation section the same as in DNA. In addition, for protein, to intuitively show the binding site annotation results, we visualize and annotate the binding residues on the 3D structure of a given protein sequence with PDB ID.</p>
      </sec>
    </sec>
    <sec id="SEC2-5">
      <title>Result report module</title>
      <p>We list the analysis results in the form of reports, which involve various kinds of data formats from the results of the two main functional modules. Users can view or download freely all of the result data and charts. Notably, each submitted task is immediately notified to the corresponding user once it is completed.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="SEC3">
    <title>RESULTS AND DISCUSSION</title>
    <sec id="SEC3-1">
      <title>Case study</title>
      <p>We showcase real-world applications of DeepBIO with two bioinformatic scenarios: (i) the prediction of DNA 6mA methylation and (ii) the prediction of protein toxicity. We emphasize that the underlying objective is to illustrate how to use our platform for two such diverse applications, rather than securing the top predictive performance compared with the state-of-the-art methods.</p>
      <p>DNA methylation is closely associated with a variety of key processes, such as genomic imprinting and carcinogenesis (<xref rid="B64" ref-type="bibr">64</xref>). Therefore, for the task of the DNA 6mA methylation prediction, we use the <italic toggle="yes">Drosophila melanogaster</italic> dataset with 11 191 methylation sequences (as positives) and 11 191 non-methylation sequences (as negatives) as an example dataset (<xref rid="B65" ref-type="bibr">65</xref>) for DNA sequence functional analysis. In addition, protein toxicity is closely related to most neurodegenerative diseases, and accurate toxicity prediction leads to significant promotion in new drug design. We choose a protein toxicity benchmark dataset (<xref rid="B66" ref-type="bibr">66</xref>) with 3379 toxic animal protein sequences (as positives) and 5464 non-toxic animal protein sequences (as negatives) for protein functional analysis. With the two example datasets, we first randomly split them into the training set and testing set and then submit them to our online platform to demonstrate the data analysis and various functions of DeepBIO. More details of the visualization results of the two cases are described below.</p>
    </sec>
    <sec id="SEC3-2">
      <title>Dataset statistics</title>
      <p>Figure <xref rid="F2" ref-type="fig">2</xref> illustrates the dataset statistics of the protein toxicity dataset. The proportion and number of bases or amino acids in the dataset are counted and illustrated as a pie chart and composition histogram, respectively (Figure <xref rid="F2" ref-type="fig">2A</xref>), from which we can clearly see the difference between the positives and the negatives from the compositional perspective. In addition, we calculate and show the sequence length distribution (Figure <xref rid="F2" ref-type="fig">2B</xref>), which intuitively gives users the sequence length preference in the datasets. For example, it can be seen from Figure <xref rid="F2" ref-type="fig">2B</xref> that positive samples are mainly distributed in the range of 50–150 amino acids, while the negative samples have a relatively even distribution. Furthermore, we use a motif analysis tool called Weblogo to generate the sequence motifs of the datasets, which enables users to analyze the composition and conservation at the sequence level (Figure <xref rid="F2" ref-type="fig">2C</xref>).</p>
      <fig position="float" id="F2">
        <label>Figure 2.</label>
        <caption>
          <p>The statistical analysis on the protein toxicity dataset. (<bold>A</bold>) Sequence compositions of positive and negative samples in the training and testing sets. (<bold>B</bold>) Sequence length distribution of positive and negative samples in the training and testing sets. (<bold>C</bold>) Motifs of sequence statistics from the training and testing sets.</p>
        </caption>
        <graphic xlink:href="gkad055fig2" position="float"/>
      </fig>
    </sec>
    <sec id="SEC3-3">
      <title>Model prediction analysis</title>
      <p>DeepBIO provides detailed analysis for users to interpret and show comprehensive performance comparisons among different deep-learning models. Here, we select four models (DNABERT, DNN, LSTM and RNN) to train and compare their prediction performance on the DNA methylation dataset. Figure <xref rid="F3" ref-type="fig">3A</xref> shows the performances of the compared models in terms of ACC, sensitivity, specificity, AUC and MCC. It can be seen that DNABERT achieves the best performance with the ACC, sensitivity, specificity, AUC and MCC of 0.921, 0.896, 0.946, 0.967 and 0.844, respectively. For a better evaluation of models, Figure <xref rid="F3" ref-type="fig">3B</xref> illustrates the ROC and PR curves of the compared models. As seen in Figure <xref rid="F3" ref-type="fig">3B</xref>, DNABERT achieved the highest AUC and AP values of 0.968 and 0.973, respectively, which further confirms that DNABERT is better than the other compared models. To further study the generalization ability of the compared models, we draw the epoch plot in Figure <xref rid="F3" ref-type="fig">3C</xref> to show the trends of accuracy and calculation loss for each model during the training process. From Figure <xref rid="F3" ref-type="fig">3C</xref>, it is easy to see that the accuracy of three models (DNABERT, LSTM and RNN), but not DNN, increases rapidly at first and then the accuracy curves become smooth gradually. Meanwhile, their calculation losses converge to lower values, indicating that these models are well trained and their predictions are reliable. It is worth noting that during the training process, DNN always achieves relatively low accuracy, however with high calculation loss on the example dataset (seen in Figure <xref rid="F3" ref-type="fig">3C</xref>). This indicates that it is not a good and robust model after training. In addition, to intuitively show the relationship among different models from their prediction results, we further provide an Upset plot and a Venn diagram in Figure <xref rid="F3" ref-type="fig">3D</xref> and <xref rid="F3" ref-type="fig">E</xref>. From Figure <xref rid="F3" ref-type="fig">3D</xref> and <xref rid="F3" ref-type="fig">E</xref>, we can see that most positive samples are successfully predicted from the overlapping predictions of the compared models. Moreover, to clearly understand the prediction preference for each model, we visualize the density distribution of the prediction confidence of different deep-learning models in Figure <xref rid="F3" ref-type="fig">3F</xref>. As can be seen, DNN mainly focuses its prediction confidence on ∼0.5, which is the center part of the <italic toggle="yes">x</italic>-axis, illustrating its poor discrimination ability between positive and negative samples on the example dataset. In contrast, the other three models (DNABERT, LSTM and RNN) achieve higher prediction confidence on both positive and negative samples, indicating that they possess better classification ability on the example dataset. The comparison results for this part on the dataset of protein toxicity can be seen in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figures S1–S6</xref>.</p>
      <fig position="float" id="F3">
        <label>Figure 3.</label>
        <caption>
          <p>The prediction result analysis from different models using DeepBIO. (<bold>A</bold>) Performance comparison among different models in terms of ACC, sensitivity, specificity, AUC and MCC. (<bold>B</bold>) ROC and PR curves of different deep-learning models. (<bold>C</bold>) The trends of performance and calculation loss with each epoch, showing ACC and the loss change process on different models. (<bold>D</bold> and <bold>E</bold>) Upset plot and Venn diagram to express the relationships of prediction results for different models. (<bold>F</bold>) Density distribution of the prediction confidence for different deep-learning models.</p>
        </caption>
        <graphic xlink:href="gkad055fig3" position="float"/>
      </fig>
    </sec>
    <sec id="SEC3-4">
      <title>Feature analysis and visualization</title>
      <p>To compare the features automatically learned by deep-learning models with the traditional hand-crafted features, we select some commonly used feature encodings, including accumulated nucleotide frequency (ANF) (<xref rid="B67" ref-type="bibr">67</xref>), binary (<xref rid="B68" ref-type="bibr">68</xref>), composition of K-spaced nucleic acid pairs (CKSNAP) (<xref rid="B69" ref-type="bibr">69</xref>) and dinucleotide composition (DNC) (<xref rid="B70" ref-type="bibr">70</xref>,<xref rid="B71" ref-type="bibr">71</xref>) for nucleotide sequences, and show the prediction performance in terms of different metrics in Figure <xref rid="F4" ref-type="fig">4A</xref> and <xref rid="F4" ref-type="fig">B</xref>. It is worth noting that in Figure <xref rid="F4" ref-type="fig">4A</xref> and <xref rid="F4" ref-type="fig">B</xref>, the compared deep-learning models, with the exception of DNN, outperform most of the traditional feature encodings, allowing us to easily conclude that the features learnt from the deep-learning methods generally are superior to hand-crafted features.</p>
      <fig position="float" id="F4">
        <label>Figure 4.</label>
        <caption>
          <p>The visualization of feature analysis and model interpretation using DeepBIO. (<bold>A</bold>) Prediction performance comparison between deep-learning models and traditional hand-crafted feature-based methods. (<bold>B</bold>) ROC and PR curves of deep-learning models and traditional hand-crafted feature-based methods. (<bold>C</bold>) UMAP feature visualization results of different deep-learning models. (<bold>D</bold>) SHAP feature importance analysis of different deep-learning models. (<bold>E</bold>) Attention heatmap of the user-chosen biological sequence. (<bold>F</bold>) Motif discovery by deep-learning models.</p>
        </caption>
        <graphic xlink:href="gkad055fig4" position="float"/>
      </fig>
      <p>In addition, to intuitively evaluate the feature representations learnt from different deep-learning models, we employ UMAP to conduct the dimension reduction and feature visualization (Figure <xref rid="F4" ref-type="fig">4C</xref>). In Figure <xref rid="F4" ref-type="fig">4C</xref>, each point represents a sample in the dataset; the positive and negative samples are assigned with different colors. In this case, we can get a better sense of the classification ability of each model from the UMAP plot. Specifically, Figure <xref rid="F4" ref-type="fig">4C</xref> shows that compared with the other three models (DNABERT, LSTM and RNN), the samples belonging to different classes in the feature space of DNN are almost connected, making it difficult to distinguish the positives from the negatives. Moreover, the positives and negatives are distributed more clearly in two clusters the feature space of DNABERT than in that of RNN and LSTM. This demonstrates that DNABERT has stronger prediction ability than the other models. In addition, we also provide the SHAP plot (Figure <xref rid="F4" ref-type="fig">4D</xref>), assigning each feature a value to represent its importance for the model prediction. Each row represents the SHAP value distributions of a feature, and the <italic toggle="yes">x</italic>-axis refers to the SHAP value, where the value of SHAP &gt;0 shows that the prediction favors the positive class, and a value &lt;0 indicates that the prediction tends to be the negative class. The color of sample points in Figure <xref rid="F4" ref-type="fig">4D</xref> indicates the corresponding feature value: redder points mean higher feature importance values, while bluer points indicate lower feature values. The features are sorted according to the sum of SHAP values incorporating all the samples in the dataset. Therefore, the SHAP plot shows the relationship between the value of features and their impact on the model prediction. From Figure <xref rid="F4" ref-type="fig">4D</xref>, we can see that the high-dimension features play a predominant role for DNABERT; the top-ranked feature values in DNABERT are more easily distinguished in the color as compared with other models, indicating that DNABERT performs better in feature representation learning.</p>
      <p>To improve the interpretability of the deep-learning models, in Figure <xref rid="F4" ref-type="fig">4E</xref> we calculate the importance of the base at each position on the user-given sequence from the attention matrix in the model and visualize it with normalized attention scores, enabling us to analyze which sequential region the model considers is more important for the prediction. To further study what the models learn from the training process, we extract the important regions with high attention scores through the attention mechanism of the models to find conservative sequence motifs in Figure <xref rid="F4" ref-type="fig">4F</xref> from the datasets, which might be related to biological functions. The detailed feature analysis results on the dataset of protein toxicity can be seen in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figures S7–S9</xref>.</p>
    </sec>
    <sec id="SEC3-5">
      <title>Functional annotation analysis</title>
      <p>To intuitively illustrate in-depth annotation result analysis of biological sequences, we provide users with functional site prediction and result visualization analysis as illustrated in Figure <xref rid="F5" ref-type="fig">5</xref>. As seen in Figure <xref rid="F5" ref-type="fig">5A</xref>, we can predict DNA methylation sites with corresponding positions and prediction confidence based on user-provided DNA sequences or a segment of our pre-set human genome. In addition, if users select a segment from our pre-set human genome by choosing a specific cell line and chromosome, we will display the statistical analysis between our DNA methylation site predictions and relevant histone modification signals queried from the database (Figure <xref rid="F5" ref-type="fig">5B</xref>), assisting in mining the potential relationships between them. Moreover, for base-level annotation on protein sequences, we predict and visualize the annotation results on the 3D structure of the given protein sequence with its PDB ID. Specifically, Figure <xref rid="F5" ref-type="fig">5C</xref> shows the protein-peptide binding site prediction results of an example protein (PDB ID 1a81A), and the residues in red color on the protein 3D structure indicate the protein-peptide binding sites.</p>
      <fig position="float" id="F5">
        <label>Figure 5.</label>
        <caption>
          <p>The biological sequence functional annotation analysis using DeepBIO. (<bold>A</bold>) Prediction results of DNA methylation annotation. (<bold>B</bold>) Comparison and analysis between the DNA methylation site prediction results and relevant histone modification signals. (<bold>C</bold>) Prediction results of protein-peptide binding site annotation on an example protein (1a81A).</p>
        </caption>
        <graphic xlink:href="gkad055fig5" position="float"/>
      </fig>
    </sec>
    <sec id="SEC3-6">
      <title>Webserver implementation</title>
      <p>DeepBIO is empowered by the high-performance graphic processing architecture with million-scale calculation capability. Specifically, DeepBIO is deployed with the Ubuntu 18.04 Linux system, multiple Intel Xeon Silver 4210R CPUs, 256 GB of RAM, hundreds of TB Solid State Drives and NVIDIA RTX 3090 GPU clusters. Unlike other online platforms that use CPU for training models and making predictions, the models we provide are all trained and evaluated based on GPU, which significantly reduces the computational time. It can be seen from <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S3</xref> that we spent less time in optimizing deep-learning models and enable fast and accurate predictions. In addition, <xref rid="sup1" ref-type="supplementary-material">Supplementary Figures S10–S15</xref> show that the front-end pages are carefully designed to give users an interactive visualization of the results while they can download each result graph freely. To be specific, we use React framework to set up the user interface, Spring Boot framework for server implementation in the back end, Python and R programming languages for the model's construction and visualization, and the MySQL database to manage the data storage. In addition, to provide a better Web experience for users, we also construct interactive graphs to visualize the analysis results on the front page. Our platform can be run stably on many browsers including Internet Explorer (≥v.7.0), Mozilla Firefox, Microsoft Edge, Safari and Google Chrome. In conclusion, with our DeepBIO platform, data scientists and researchers limited by equipment resources can now employ high-performance computers to tackle their challenging work.</p>
    </sec>
  </sec>
  <sec sec-type="data-availability" id="SEC4">
    <title>DATA AVAILABILITY</title>
    <p>As an online platform for biological sequence analysis, DeepBIO can be freely accessed without registration via <ext-link xlink:href="https://inner.wei-group.net/DeepBIO" ext-link-type="uri">https://inner.wei-group.net/DeepBIO</ext-link>. All code used in data analysis and preparation of the manuscript, alongside a description of necessary steps for reproducing results, can be found in a GitHub repository accompanying this manuscript: <ext-link xlink:href="https://github.com/WLYLab/DeepBIO" ext-link-type="uri">https://github.com/WLYLab/DeepBIO</ext-link>. The source code for DeepBIO is also freely available at Zenodo website (DOI: 10.5281/zenodo.7547847).</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>gkad055_Supplemental_File</label>
      <media xlink:href="gkad055_supplemental_file.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ACK1">
    <title>ACKNOWLEDGEMENTS</title>
    <p>We would like to thank Dr. Martin de Jesus LOZA-LOPEZ from University of Tokyo for his valuable suggestions regarding the improvement of our platform.</p>
    <p><italic toggle="yes">Author contributions</italic>: L.W. conceived the basic idea and designed the DeepBIO framework. R.W., Y.J, J.J., C.Y., H.Y., F.W. and J.F. set up the server and visualized the result analysis. Y.J., R.W., L.W. and C.Y. designed the interface and tested the server. R.W., C.Y., J.J., Q.Z. and L.W. wrote the manuscript. K.N., R.S., Q.Z. and L.W. revised the manuscript.</p>
  </ack>
  <sec id="SEC5">
    <title>SUPPLEMENTARY DATA</title>
    <p><ext-link xlink:href="https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkad055#supplementary-data" ext-link-type="uri">Supplementary Data</ext-link> are available at NAR Online.</p>
  </sec>
  <sec id="SEC6">
    <title>FUNDING</title>
    <p>The work was supported by the Natural Science Foundation of China [62250028 and 62071278].</p>
    <p><italic toggle="yes">Conflict of interest statement</italic>. None declared.</p>
  </sec>
  <ref-list id="REF1">
    <title>REFERENCES</title>
    <ref id="B1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Larranaga</surname><given-names>P.</given-names></string-name>, <string-name><surname>Calvo</surname><given-names>B.</given-names></string-name>, <string-name><surname>Santana</surname><given-names>R.</given-names></string-name>, <string-name><surname>Bielza</surname><given-names>C.</given-names></string-name>, <string-name><surname>Galdiano</surname><given-names>J.</given-names></string-name>, <string-name><surname>Inza</surname><given-names>I.</given-names></string-name>, <string-name><surname>Lozano</surname><given-names>J.A.</given-names></string-name>, <string-name><surname>Armananzas</surname><given-names>R.</given-names></string-name>, <string-name><surname>Santafe</surname><given-names>G.</given-names></string-name>, <string-name><surname>Perez</surname><given-names>A.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Machine learning in bioinformatics</article-title>. <source>Brief .Bioinform.</source><year>2006</year>; <volume>7</volume>:<fpage>86</fpage>–<lpage>112</lpage>.<pub-id pub-id-type="pmid">16761367</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>R.</given-names></string-name>, <string-name><surname>Jin</surname><given-names>J.</given-names></string-name>, <string-name><surname>Zou</surname><given-names>Q.</given-names></string-name>, <string-name><surname>Nakai</surname><given-names>K.</given-names></string-name>, <string-name><surname>Wei</surname><given-names>L.</given-names></string-name></person-group><article-title>Predicting protein–peptide binding residues via interpretable deep learning</article-title>. <source>Bioinformatics</source>. <year>2022</year>; <volume>38</volume>:<fpage>3351</fpage>–<lpage>3360</lpage>.<pub-id pub-id-type="pmid">35604077</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <label>3.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Jiang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>R.</given-names></string-name>, <string-name><surname>Feng</surname><given-names>J.</given-names></string-name>, <string-name><surname>Jin</surname><given-names>J.</given-names></string-name>, <string-name><surname>Liang</surname><given-names>S.</given-names></string-name>, <string-name><surname>Li</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Yu</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Ma</surname><given-names>A.</given-names></string-name>, <string-name><surname>Su</surname><given-names>R.</given-names></string-name>, <string-name><surname>Zou</surname><given-names>Q.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Explainable deep graph learning accurately modeling the peptide secondary structure prediction</article-title>. <year>2022</year>; <comment>bioRxiv doi:</comment><comment>10 August 2022, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.1101/2022.06.09.495580</pub-id>.</mixed-citation>
    </ref>
    <ref id="B4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jin</surname><given-names>J.</given-names></string-name>, <string-name><surname>Yu</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>R.</given-names></string-name>, <string-name><surname>Zeng</surname><given-names>X.</given-names></string-name>, <string-name><surname>Pang</surname><given-names>C.</given-names></string-name>, <string-name><surname>Jiang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Li</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Dai</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Su</surname><given-names>R.</given-names></string-name>, <string-name><surname>Zou</surname><given-names>Q.</given-names></string-name></person-group><article-title>iDNA-ABF: multi-scale deep biological language learning model for the interpretable prediction of DNA methylations</article-title>. <source>Genome Biol.</source><year>2022</year>; <volume>23</volume>:<fpage>1</fpage>–<lpage>23</lpage>.<pub-id pub-id-type="pmid">34980209</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Elnaggar</surname><given-names>A.</given-names></string-name>, <string-name><surname>Heinzinger</surname><given-names>M.</given-names></string-name>, <string-name><surname>Dallago</surname><given-names>C.</given-names></string-name>, <string-name><surname>Rehawi</surname><given-names>G.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Jones</surname><given-names>L.</given-names></string-name>, <string-name><surname>Gibbs</surname><given-names>T.</given-names></string-name>, <string-name><surname>Feher</surname><given-names>T.</given-names></string-name>, <string-name><surname>Angerer</surname><given-names>C.</given-names></string-name>, <string-name><surname>Steinegger</surname><given-names>M.</given-names></string-name><etal>et al</etal>.</person-group><article-title>ProtTrans: towards cracking the language of lifes code through self-supervised deep learning and high performance computing</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2021</year>; <volume>44</volume>:<fpage>7112</fpage>–<lpage>7127</lpage>.</mixed-citation>
    </ref>
    <ref id="B6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B.</given-names></string-name></person-group><article-title>BioSeq-Analysis: a platform for DNA, RNA and protein sequence analysis based on machine learning approaches</article-title>. <source>Brief. Bioinform.</source><year>2019</year>; <volume>20</volume>:<fpage>1280</fpage>–<lpage>1294</lpage>.<pub-id pub-id-type="pmid">29272359</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B.</given-names></string-name>, <string-name><surname>Gao</surname><given-names>X.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>H.</given-names></string-name></person-group><article-title>BioSeq-Analysis2. 0: an updated platform for analyzing DNA, RNA and protein sequences at sequence level and residue level based on machine learning approaches</article-title>. <source>Nucleic Acids Res.</source><year>2019</year>; <volume>47</volume>:<fpage>e127</fpage>.<pub-id pub-id-type="pmid">31504851</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>P.</given-names></string-name>, <string-name><surname>Li</surname><given-names>C.</given-names></string-name>, <string-name><surname>Li</surname><given-names>F.</given-names></string-name>, <string-name><surname>Xiang</surname><given-names>D.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>Y.Z.</given-names></string-name>, <string-name><surname>Akutsu</surname><given-names>T.</given-names></string-name>, <string-name><surname>Daly</surname><given-names>R.J.</given-names></string-name>, <string-name><surname>Webb</surname><given-names>G.I.</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>Q.</given-names></string-name><etal>et al</etal>.</person-group><article-title>iLearnPlus: a comprehensive and automated machine-learning platform for nucleic acid and protein sequence analysis, prediction and visualization</article-title>. <source>Nucleic Acids Res.</source><year>2021</year>; <volume>49</volume>:<fpage>e60</fpage>.<pub-id pub-id-type="pmid">33660783</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>H.-L.</given-names></string-name>, <string-name><surname>Pang</surname><given-names>Y.-H.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>B.</given-names></string-name></person-group><article-title>BioSeq-BLM: a platform for analyzing DNA, RNA and protein sequences based on biological language models</article-title>. <source>Nucleic Acids Res.</source><year>2021</year>; <volume>49</volume>:<fpage>e129</fpage>.<pub-id pub-id-type="pmid">34581805</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>X.</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>P.</given-names></string-name>, <string-name><surname>Li</surname><given-names>C.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Li</surname><given-names>F.</given-names></string-name>, <string-name><surname>Akutsu</surname><given-names>T.</given-names></string-name>, <string-name><surname>Bain</surname><given-names>C.</given-names></string-name>, <string-name><surname>Gasser</surname><given-names>R.B.</given-names></string-name>, <string-name><surname>Li</surname><given-names>J.</given-names></string-name><etal>et al</etal>.</person-group><article-title>iFeatureOmega: an integrative platform for engineering, visualization and analysis of features from molecular sequences, structural and ligand data sets</article-title>. <source>Nucleic Acids Res.</source><year>2022</year>; <volume>50</volume>:<fpage>W434</fpage>–<lpage>W447</lpage>.<pub-id pub-id-type="pmid">35524557</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cao</surname><given-names>D.-S.</given-names></string-name>, <string-name><surname>Xiao</surname><given-names>N.</given-names></string-name>, <string-name><surname>Xu</surname><given-names>Q.-S.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>A.F.</given-names></string-name></person-group><article-title>Rcpi: R/Bioconductor package to generate various descriptors of proteins, compounds and their interactions</article-title>. <source>Bioinformatics</source>. <year>2015</year>; <volume>31</volume>:<fpage>279</fpage>–<lpage>281</lpage>.<pub-id pub-id-type="pmid">25246429</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xiao</surname><given-names>N.</given-names></string-name>, <string-name><surname>Cao</surname><given-names>D.-S.</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>M.-F.</given-names></string-name>, <string-name><surname>Xu</surname><given-names>Q.-S.</given-names></string-name></person-group><article-title>protr/ProtrWeb: R package and web server for generating various numerical representation schemes of protein sequences</article-title>. <source>Bioinformatics</source>. <year>2015</year>; <volume>31</volume>:<fpage>1857</fpage>–<lpage>1859</lpage>.<pub-id pub-id-type="pmid">25619996</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Avsec</surname><given-names>Ž.</given-names></string-name>, <string-name><surname>Kreuzhuber</surname><given-names>R.</given-names></string-name>, <string-name><surname>Israeli</surname><given-names>J.</given-names></string-name>, <string-name><surname>Xu</surname><given-names>N.</given-names></string-name>, <string-name><surname>Cheng</surname><given-names>J.</given-names></string-name>, <string-name><surname>Shrikumar</surname><given-names>A.</given-names></string-name>, <string-name><surname>Banerjee</surname><given-names>A.</given-names></string-name>, <string-name><surname>Kim</surname><given-names>D.S.</given-names></string-name>, <string-name><surname>Beier</surname><given-names>T.</given-names></string-name>, <string-name><surname>Urban</surname><given-names>L.</given-names></string-name></person-group><article-title>The Kipoi repository accelerates community exchange and reuse of predictive models for genomics</article-title>. <source>Nat. Biotechnol.</source><year>2019</year>; <volume>37</volume>:<fpage>592</fpage>–<lpage>600</lpage>.<pub-id pub-id-type="pmid">31138913</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Budach</surname><given-names>S.</given-names></string-name>, <string-name><surname>Marsico</surname><given-names>A.</given-names></string-name></person-group><article-title>Pysster: classification of biological sequences by learning sequence and structure motifs with convolutional neural networks</article-title>. <source>Bioinformatics</source>. <year>2018</year>; <volume>34</volume>:<fpage>3035</fpage>–<lpage>3037</lpage>.<pub-id pub-id-type="pmid">29659719</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>K.M.</given-names></string-name>, <string-name><surname>Cofer</surname><given-names>E.M.</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>J.</given-names></string-name>, <string-name><surname>Troyanskaya</surname><given-names>O.G.</given-names></string-name></person-group><article-title>Selene: a PyTorch-based deep learning library for sequence data</article-title>. <source>Nat. Methods</source>. <year>2019</year>; <volume>16</volume>:<fpage>315</fpage>–<lpage>318</lpage>.<pub-id pub-id-type="pmid">30923381</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>C.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>H.</given-names></string-name>, <string-name><surname>Hu</surname><given-names>Q.</given-names></string-name>, <string-name><surname>Que</surname><given-names>J.</given-names></string-name>, <string-name><surname>Yao</surname><given-names>J.</given-names></string-name></person-group><article-title>A novel computational model for predicting microRNA–disease associations based on heterogeneous graph convolutional networks</article-title>. <source>Cells</source>. <year>2019</year>; <volume>8</volume>:<fpage>977</fpage>.<pub-id pub-id-type="pmid">31455028</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ji</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>H.</given-names></string-name>, <string-name><surname>Davuluri</surname><given-names>R.V.</given-names></string-name></person-group><article-title>DNABERT: pre-trained bidirectional encoder representations from transformers model for DNA-language in genome</article-title>. <source>Bioinformatics</source>. <year>2021</year>; <volume>37</volume>:<fpage>2112–</fpage>–<lpage>2120</lpage>.<pub-id pub-id-type="pmid">33538820</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>W.</given-names></string-name>, <string-name><surname>Godzik</surname><given-names>A.</given-names></string-name></person-group><article-title>Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences</article-title>. <source>Bioinformatics</source>. <year>2006</year>; <volume>22</volume>:<fpage>1658</fpage>–<lpage>1659</lpage>.<pub-id pub-id-type="pmid">16731699</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <label>19.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>T.-Y.</given-names></string-name>, <string-name><surname>Goyal</surname><given-names>P.</given-names></string-name>, <string-name><surname>Girshick</surname><given-names>R.</given-names></string-name>, <string-name><surname>He</surname><given-names>K.</given-names></string-name>, <string-name><surname>Dollár</surname><given-names>P.</given-names></string-name></person-group><article-title>Focal loss for dense object detection</article-title>. <source>Proceedings of the IEEE international conference on computer vision</source>. <year>2017</year>; <fpage>2980</fpage>–<lpage>2988</lpage>.</mixed-citation>
    </ref>
    <ref id="B20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chawla</surname><given-names>N.V.</given-names></string-name>, <string-name><surname>Bowyer</surname><given-names>K.W.</given-names></string-name>, <string-name><surname>Hall</surname><given-names>L.O.</given-names></string-name>, <string-name><surname>Kegelmeyer</surname><given-names>W.P.</given-names></string-name></person-group><article-title>SMOTE: synthetic minority over-sampling technique</article-title>. <source>J. Artificial Intelligence Res.</source><year>2002</year>; <volume>16</volume>:<fpage>321</fpage>–<lpage>357</lpage>.</mixed-citation>
    </ref>
    <ref id="B21">
      <label>21.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>He</surname><given-names>H.</given-names></string-name>, <string-name><surname>Bai</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Garcia</surname><given-names>E.A.</given-names></string-name>, <string-name><surname>Li</surname><given-names>S.</given-names></string-name></person-group><article-title>ADASYN: Adaptive synthetic sampling approach for imbalanced learning</article-title>. <source>IEEE international joint conference on neural networks (IEEE world congress on computational intelligence)</source>. <year>2008</year>; <fpage>1322</fpage>–<lpage>1328</lpage>.</mixed-citation>
    </ref>
    <ref id="B22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>LeCun</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Hinton</surname><given-names>G.</given-names></string-name></person-group><article-title>Deep learning</article-title>. <source>Nature</source>. <year>2015</year>; <volume>521</volume>:<fpage>436</fpage>–<lpage>444</lpage>.<pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rumelhart</surname><given-names>D.E.</given-names></string-name>, <string-name><surname>Hinton</surname><given-names>G.E.</given-names></string-name>, <string-name><surname>Williams</surname><given-names>R.J.</given-names></string-name></person-group><article-title>Learning representations by back-propagating errors</article-title>. <source>Nature</source>. <year>1986</year>; <volume>323</volume>:<fpage>533</fpage>–<lpage>536</lpage>.</mixed-citation>
    </ref>
    <ref id="B24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hochreiter</surname><given-names>S.</given-names></string-name>, <string-name><surname>Schmidhuber</surname><given-names>J.</given-names></string-name></person-group><article-title>Long short-term memory</article-title>. <source>Neural Comput.</source><year>1997</year>; <volume>9</volume>:<fpage>1735</fpage>–<lpage>1780</lpage>.<pub-id pub-id-type="pmid">9377276</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Graves</surname><given-names>A.</given-names></string-name>, <string-name><surname>Schmidhuber</surname><given-names>J.</given-names></string-name></person-group><article-title>Framewise phoneme classification with bidirectional LSTM and other neural network architectures</article-title>. <source>Neural Netw.</source><year>2005</year>; <volume>18</volume>:<fpage>602</fpage>–<lpage>610</lpage>.<pub-id pub-id-type="pmid">16112549</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>Q.</given-names></string-name>, <string-name><surname>Hao</surname><given-names>Y.</given-names></string-name></person-group><article-title>ALSTM: an attention-based long short-term memory framework for knowledge base reasoning</article-title>. <source>Neurocomputing</source>. <year>2020</year>; <volume>399</volume>:<fpage>342</fpage>–<lpage>351</lpage>.</mixed-citation>
    </ref>
    <ref id="B27">
      <label>27.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Dey</surname><given-names>R.</given-names></string-name>, <string-name><surname>Salem</surname><given-names>F.M.</given-names></string-name></person-group><article-title>Gate-variants of gated recurrent unit (GRU) neural networks</article-title>. <source>2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS)</source>. <year>2017</year>; <fpage>1597</fpage>–<lpage>1600</lpage>.</mixed-citation>
    </ref>
    <ref id="B28">
      <label>28.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>dos Santos</surname><given-names>C.</given-names></string-name>, <string-name><surname>Gatti</surname><given-names>M</given-names></string-name></person-group><article-title>Deep convolutional neural networks for sentiment analysis of short texts</article-title>. <source>Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</source>. <year>2014</year>; <fpage>69</fpage>–<lpage>78</lpage>.</mixed-citation>
    </ref>
    <ref id="B29">
      <label>29.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Lai</surname><given-names>S.</given-names></string-name>, <string-name><surname>Xu</surname><given-names>L.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>K.</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>J.</given-names></string-name></person-group><article-title>Recurrent convolutional neural networks for text classification</article-title>. <source>Twenty-ninth AAAI conference on artificial intelligence</source>. <year>2015</year>.</mixed-citation>
    </ref>
    <ref id="B30">
      <label>30.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Simonyan</surname><given-names>K.</given-names></string-name>, <string-name><surname>Zisserman</surname><given-names>A.</given-names></string-name></person-group><article-title>Very deep convolutional networks for large-scale image recognition</article-title>. <year>2015</year>; <comment>arXiv doi:</comment><comment>10 April 2015, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</uri>.</mixed-citation>
    </ref>
    <ref id="B31">
      <label>31.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>J.</given-names></string-name>, <string-name><surname>Yang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Mao</surname><given-names>J.</given-names></string-name>, <string-name><surname>Huang</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Huang</surname><given-names>C.</given-names></string-name>, <string-name><surname>Xu</surname><given-names>W.</given-names></string-name></person-group><article-title>Cnn-rnn: A unified framework for multi-label image classification</article-title>. <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>. <year>2016</year>; <fpage>2285</fpage>–<lpage>2294</lpage>.</mixed-citation>
    </ref>
    <ref id="B32">
      <label>32.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Vaswani</surname><given-names>A.</given-names></string-name>, <string-name><surname>Shazeer</surname><given-names>N.</given-names></string-name>, <string-name><surname>Parmar</surname><given-names>N.</given-names></string-name>, <string-name><surname>Uszkoreit</surname><given-names>J.</given-names></string-name>, <string-name><surname>Jones</surname><given-names>L.</given-names></string-name>, <string-name><surname>Gomez</surname><given-names>A.N.</given-names></string-name>, <string-name><surname>Kaiser</surname><given-names>Ł.</given-names></string-name>, <string-name><surname>Polosukhin</surname><given-names>I.</given-names></string-name></person-group><article-title>Attention is all you need</article-title>. <source>Advances in neural information processing systems</source>. <year>2017</year>; <fpage>5998</fpage>–<lpage>6008</lpage>.</mixed-citation>
    </ref>
    <ref id="B33">
      <label>33.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Kitaev</surname><given-names>N.</given-names></string-name>, <string-name><surname>Kaiser</surname><given-names>Ł.</given-names></string-name>, <string-name><surname>Levskaya</surname><given-names>A.</given-names></string-name></person-group><article-title>Reformer: The efficient transformer</article-title>. <source>Proceedings of ICLR</source>. <year>2020</year>.</mixed-citation>
    </ref>
    <ref id="B34">
      <label>34.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Choromanski</surname><given-names>K.</given-names></string-name>, <string-name><surname>Likhosherstov</surname><given-names>V.</given-names></string-name>, <string-name><surname>Dohan</surname><given-names>D.</given-names></string-name>, <string-name><surname>Song</surname><given-names>X.</given-names></string-name>, <string-name><surname>Gane</surname><given-names>A.</given-names></string-name>, <string-name><surname>Sarlos</surname><given-names>T.</given-names></string-name>, <string-name><surname>Hawkins</surname><given-names>P.</given-names></string-name>, <string-name><surname>Davis</surname><given-names>J.</given-names></string-name>, <string-name><surname>Mohiuddin</surname><given-names>A.</given-names></string-name>, <string-name><surname>Kaiser</surname><given-names>L.</given-names></string-name></person-group><article-title>Rethinking attention with performers</article-title>. <year>2022</year>; <comment>arXiv doi:</comment><comment>19 November 2022, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/abs/2009.14794">https://arxiv.org/abs/2009.14794</uri>.</mixed-citation>
    </ref>
    <ref id="B35">
      <label>35.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>S.</given-names></string-name>, <string-name><surname>Li</surname><given-names>B.Z.</given-names></string-name>, <string-name><surname>Khabsa</surname><given-names>M.</given-names></string-name>, <string-name><surname>Fang</surname><given-names>H.</given-names></string-name>, <string-name><surname>Ma</surname><given-names>H.</given-names></string-name></person-group><article-title>Linformer: self-attention with linear complexity</article-title>. <year>2020</year>; <comment>arXiv doi:</comment><comment>14 June 2020, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/abs/2006.04768">https://arxiv.org/abs/2006.04768</uri>.</mixed-citation>
    </ref>
    <ref id="B36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roy</surname><given-names>A.</given-names></string-name>, <string-name><surname>Saffar</surname><given-names>M.</given-names></string-name>, <string-name><surname>Vaswani</surname><given-names>A.</given-names></string-name>, <string-name><surname>Grangier</surname><given-names>D</given-names></string-name></person-group><article-title>Efficient content-based sparse attention with routing transformers</article-title>. <source>Transactions of the Association for Computational Linguistics</source>. <year>2021</year>; <volume>9</volume>:<fpage>53</fpage>–<lpage>68</lpage>.</mixed-citation>
    </ref>
    <ref id="B37">
      <label>37.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Devlin</surname><given-names>J.</given-names></string-name>, <string-name><surname>Chang</surname><given-names>M.-W.</given-names></string-name>, <string-name><surname>Lee</surname><given-names>K.</given-names></string-name>, <string-name><surname>Toutanova</surname><given-names>K.</given-names></string-name></person-group><article-title>BERT: Pre-training of deep bidirectional transformers for language understanding</article-title>. <source>Proceedings of NAACL</source>. <year>2019</year>; <fpage>4171</fpage>–<lpage>4186</lpage>.</mixed-citation>
    </ref>
    <ref id="B38">
      <label>38.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Safaya</surname><given-names>A.</given-names></string-name>, <string-name><surname>Abdullatif</surname><given-names>M.</given-names></string-name>, <string-name><surname>Yuret</surname><given-names>D</given-names></string-name></person-group><article-title>Kuisail at semeval-2020 task 12: Bert-cnn for offensive speech identification in social media</article-title>. <source>Proceedings of the Fourteenth Workshop on Semantic Evaluation</source>. <year>2020</year>; <fpage>2054</fpage>–<lpage>2059</lpage>.</mixed-citation>
    </ref>
    <ref id="B39">
      <label>39.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>Y.-J.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>H.-J.</given-names></string-name>, <string-name><surname>Pan</surname><given-names>W.-M.</given-names></string-name>, <string-name><surname>Feng</surname><given-names>R.-J.</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>Z.-Y.</given-names></string-name></person-group><source>Artificial intelligence in China</source>. <year>2021</year>; <publisher-name>Springer</publisher-name><fpage>524</fpage>–<lpage>530</lpage>.</mixed-citation>
    </ref>
    <ref id="B40">
      <label>40.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Nguyen</surname><given-names>Q.T.</given-names></string-name>, <string-name><surname>Nguyen</surname><given-names>T.L.</given-names></string-name>, <string-name><surname>Luong</surname><given-names>N.H.</given-names></string-name>, <string-name><surname>Ngo</surname><given-names>Q.H.</given-names></string-name></person-group><article-title>Fine-tuning bert for sentiment analysis of vietnamese reviews</article-title>. <source>2020 7th NAFOSTED Conference on Information and Computer Science (NICS)</source>. <year>2020</year>; <fpage>302</fpage>–<lpage>307</lpage>.</mixed-citation>
    </ref>
    <ref id="B41">
      <label>41.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Huang</surname><given-names>P.</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>H.</given-names></string-name>, <string-name><surname>Zheng</surname><given-names>L.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>Y.</given-names></string-name></person-group><article-title>Text Sentiment Analysis based on BERT and Convolutional Neural Networks</article-title>. <source>2021 5th International Conference on Natural Language Processing and Information Retrieval (NLPIR)</source>. <year>2021</year>; <fpage>1</fpage>–<lpage>7</lpage>.</mixed-citation>
    </ref>
    <ref id="B42">
      <label>42.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Han</surname><given-names>X.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Jiang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Sun</surname><given-names>M.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>Q.</given-names></string-name></person-group><article-title>ERNIE: Enhanced language representation with informative entities</article-title>. <source>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</source>. <year>2019</year>; <fpage>1441</fpage>–<lpage>1451</lpage>.</mixed-citation>
    </ref>
    <ref id="B43">
      <label>43.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Kipf</surname><given-names>T.N.</given-names></string-name>, <string-name><surname>Welling</surname><given-names>M.</given-names></string-name></person-group><article-title>Semi-supervised classification with graph convolutional networks</article-title>. <source>International conference on learning representations (ICLR ’17)</source>. <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="B44">
      <label>44.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Zhu</surname><given-names>J.</given-names></string-name>, <string-name><surname>Cui</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Sun</surname><given-names>H.</given-names></string-name>, <string-name><surname>Li</surname><given-names>X.</given-names></string-name>, <string-name><surname>Pelger</surname><given-names>M.</given-names></string-name>, <string-name><surname>Yang</surname><given-names>T.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>L.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>R.</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>H.</given-names></string-name></person-group><article-title>Textgnn: Improving text encoder via graph neural network in sponsored search</article-title>. <source>Proceedings of the Web Conference 2021</source>. <year>2021</year>; <fpage>2848</fpage>–<lpage>2857</lpage>.</mixed-citation>
    </ref>
    <ref id="B45">
      <label>45.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>J.</given-names></string-name>, <string-name><surname>Xie</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>K.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>Z.H.</given-names></string-name>, <string-name><surname>Lahoti</surname><given-names>G.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>C.</given-names></string-name>, <string-name><surname>Vannan</surname><given-names>M.A.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>B.</given-names></string-name>, <string-name><surname>Qian</surname><given-names>Z.</given-names></string-name></person-group><article-title>Generative invertible networks (GIN): Pathophysiology-interpretable feature mapping and virtual patient generation</article-title>. <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source>. <year>2018</year>; <publisher-name>Springer</publisher-name><fpage>537</fpage>–<lpage>545</lpage>.</mixed-citation>
    </ref>
    <ref id="B46">
      <label>46.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>K.</given-names></string-name>, <string-name><surname>Shen</surname><given-names>W.</given-names></string-name>, <string-name><surname>Yang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Quan</surname><given-names>X.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>R.</given-names></string-name></person-group><article-title>Relational graph attention network for aspect-based sentiment analysis</article-title>. <source>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</source>. <year>2020</year>; <fpage>3229</fpage>–<lpage>3238</lpage>.</mixed-citation>
    </ref>
    <ref id="B47">
      <label>47.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Hamilton</surname><given-names>W.</given-names></string-name>, <string-name><surname>Ying</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Leskovec</surname><given-names>J.</given-names></string-name></person-group><article-title>Inductive representation learning on large graphs</article-title>. <source>NIPS'17: Proceedings of the 31st International Conference on Neural Information Processing Systems</source>. <year>2017</year>; <fpage>1025</fpage>–<lpage>1035</lpage>.</mixed-citation>
    </ref>
    <ref id="B48">
      <label>48.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Defferrard</surname><given-names>M.</given-names></string-name>, <string-name><surname>Bresson</surname><given-names>X.</given-names></string-name>, <string-name><surname>Vandergheynst</surname><given-names>P.</given-names></string-name></person-group><article-title>Convolutional neural networks on graphs with fast localized spectral filtering</article-title>. <source>NIPS'16: Proceedings of the 30th International Conference on Neural Information Processing Systems</source>. <year>2016</year>; <fpage>3844</fpage>–<lpage>3852</lpage>.</mixed-citation>
    </ref>
    <ref id="B49">
      <label>49.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Ye</surname><given-names>X.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>C.</given-names></string-name>, <string-name><surname>Cui</surname><given-names>J.</given-names></string-name>, <string-name><surname>Philip</surname><given-names>S.Y.</given-names></string-name></person-group><article-title>Network embedding with completely-imbalanced labels</article-title>. <source>IEEE Trans. Knowl. Data Eng.</source><year>2020</year>; <volume>33</volume>:<fpage>3634</fpage>–<lpage>3647</lpage>.</mixed-citation>
    </ref>
    <ref id="B50">
      <label>50.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>He</surname><given-names>X.</given-names></string-name>, <string-name><surname>Deng</surname><given-names>K.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Li</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>M.</given-names></string-name></person-group><article-title>Lightgcn: Simplifying and powering graph convolution network for recommendation</article-title>. <source>Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</source>. <year>2020</year>; <fpage>639</fpage>–<lpage>648</lpage>.</mixed-citation>
    </ref>
    <ref id="B51">
      <label>51.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Brockschmidt</surname><given-names>M.</given-names></string-name></person-group><article-title>Gnn-film: Graph neural networks with feature-wise linear modulation</article-title>. <source>International Conference on Machine Learning</source>. <year>2020</year>; <fpage>1144</fpage>–<lpage>1152</lpage>.</mixed-citation>
    </ref>
    <ref id="B52">
      <label>52.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ma</surname><given-names>T.</given-names></string-name>, <string-name><surname>Dalca</surname><given-names>A.V.</given-names></string-name>, <string-name><surname>Sabuncu</surname><given-names>M.R.</given-names></string-name></person-group><article-title>Hyper-convolution networks for biomedical image segmentation</article-title>. <source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source>. <year>2022</year>; <fpage>1933</fpage>–<lpage>1942</lpage>.</mixed-citation>
    </ref>
    <ref id="B53">
      <label>53.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Duan</surname><given-names>W.</given-names></string-name>, <string-name><surname>He</surname><given-names>X.</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Rao</surname><given-names>H.</given-names></string-name>, <string-name><surname>Thiele</surname><given-names>L.</given-names></string-name></person-group><article-title>Injecting descriptive meta-information into pre-trained language models with hypernetworks</article-title>. <source>Interspeech 2021</source>. <year>2021</year>; <fpage>3216</fpage>–<lpage>3220</lpage>.</mixed-citation>
    </ref>
    <ref id="B54">
      <label>54.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Klicpera</surname><given-names>J.</given-names></string-name>, <string-name><surname>Bojchevski</surname><given-names>A.</given-names></string-name>, <string-name><surname>Günnemann</surname><given-names>S.</given-names></string-name></person-group><article-title>Predict then propagate: Graph neural networks meet personalized pagerank</article-title>. <source>7th International Conference on Learning Representations</source>. <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="B55">
      <label>55.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>J.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>B.</given-names></string-name>, <string-name><surname>Xu</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>M.</given-names></string-name></person-group><article-title>TextRGNN: residual Graph Neural Networks for Text Classification</article-title>. <year>2021</year>; <comment>arXiv doi:</comment><comment>30 December 2021, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/abs/2112.15060">https://arxiv.org/abs/2112.15060</uri>.</mixed-citation>
    </ref>
    <ref id="B56">
      <label>56.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Wu</surname><given-names>F.</given-names></string-name>, <string-name><surname>Souza</surname><given-names>A.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>T.</given-names></string-name>, <string-name><surname>Fifty</surname><given-names>C.</given-names></string-name>, <string-name><surname>Yu</surname><given-names>T.</given-names></string-name>, <string-name><surname>Weinberger</surname><given-names>K.</given-names></string-name></person-group><article-title>Simplifying graph convolutional networks</article-title>. <source>International conference on machine learning</source>. <year>2019</year>; <publisher-name>PMLR</publisher-name><fpage>6861</fpage>–<lpage>6871</lpage>.</mixed-citation>
    </ref>
    <ref id="B57">
      <label>57.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Meng</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Sun</surname><given-names>X.</given-names></string-name>, <string-name><surname>Han</surname><given-names>Q.</given-names></string-name>, <string-name><surname>Kuang</surname><given-names>K.</given-names></string-name>, <string-name><surname>Li</surname><given-names>J.</given-names></string-name>, <string-name><surname>Wu</surname><given-names>F.</given-names></string-name></person-group><article-title>BertGCN: Transductive Text Classification by Combining GCN and BERT</article-title>. <source>Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</source>. <year>2021</year>; <fpage>1456</fpage>–<lpage>1462</lpage>.</mixed-citation>
    </ref>
    <ref id="B58">
      <label>58.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Veličković</surname><given-names>P.</given-names></string-name>, <string-name><surname>Cucurull</surname><given-names>G.</given-names></string-name>, <string-name><surname>Casanova</surname><given-names>A.</given-names></string-name>, <string-name><surname>Romero</surname><given-names>A.</given-names></string-name>, <string-name><surname>Lio</surname><given-names>P.</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y.</given-names></string-name></person-group><article-title>Graph attention networks</article-title>. <source>International conference on learning representations</source>. <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="B59">
      <label>59.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Wei</surname><given-names>M.</given-names></string-name>, <string-name><surname>He</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Q.</given-names></string-name></person-group><article-title>Robust layout-aware IE for visually rich documents with pre-trained language models</article-title>. <source>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</source>. <year>2020</year>; <fpage>2367</fpage>–<lpage>2376</lpage>.</mixed-citation>
    </ref>
    <ref id="B60">
      <label>60.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Chandra</surname><given-names>S.</given-names></string-name>, <string-name><surname>Mishra</surname><given-names>P.</given-names></string-name>, <string-name><surname>Yannakoudakis</surname><given-names>H.</given-names></string-name>, <string-name><surname>Nimishakavi</surname><given-names>M.</given-names></string-name>, <string-name><surname>Saeidi</surname><given-names>M.</given-names></string-name>, <string-name><surname>Shutova</surname><given-names>E.</given-names></string-name></person-group><article-title>Graph-based modeling of online communities for fake news detection</article-title>. <year>2020</year>; <comment>arXiv doi:</comment><comment>23 November 2020, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/abs/2008.06274">https://arxiv.org/abs/2008.06274</uri>.</mixed-citation>
    </ref>
    <ref id="B61">
      <label>61.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saito</surname><given-names>T.</given-names></string-name>, <string-name><surname>Rehmsmeier</surname><given-names>M.</given-names></string-name></person-group><article-title>The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets</article-title>. <source>PLoS One</source>. <year>2015</year>; <volume>10</volume>:<fpage>e0118432</fpage>.<pub-id pub-id-type="pmid">25738806</pub-id></mixed-citation>
    </ref>
    <ref id="B62">
      <label>62.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Becht</surname><given-names>E.</given-names></string-name>, <string-name><surname>McInnes</surname><given-names>L.</given-names></string-name>, <string-name><surname>Healy</surname><given-names>J.</given-names></string-name>, <string-name><surname>Dutertre</surname><given-names>C.-A.</given-names></string-name>, <string-name><surname>Kwok</surname><given-names>I.W.</given-names></string-name>, <string-name><surname>Ng</surname><given-names>L.G.</given-names></string-name>, <string-name><surname>Ginhoux</surname><given-names>F.</given-names></string-name>, <string-name><surname>Newell</surname><given-names>E.W.</given-names></string-name></person-group><article-title>Dimensionality reduction for visualizing single-cell data using UMAP</article-title>. <source>Nat. Biotechnol.</source><year>2019</year>; <volume>37</volume>:<fpage>38</fpage>–<lpage>44</lpage>.</mixed-citation>
    </ref>
    <ref id="B63">
      <label>63.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Lundberg</surname><given-names>S.M.</given-names></string-name>, <string-name><surname>Lee</surname><given-names>S.-I.</given-names></string-name></person-group><article-title>A unified approach to interpreting model predictions</article-title>. <source>Proceedings of the 31st international conference on neural information processing systems</source>. <year>2017</year>; <fpage>4768</fpage>–<lpage>4777</lpage>.</mixed-citation>
    </ref>
    <ref id="B64">
      <label>64.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Richardson</surname><given-names>B.C.</given-names></string-name></person-group><article-title>Role of DNA methylation in the regulation of cell function: autoimmunity, aging and cancer</article-title>. <source>J. Nutr.</source><year>2002</year>; <volume>132</volume>:<fpage>2401S</fpage>–<lpage>2405S</lpage>.<pub-id pub-id-type="pmid">12163700</pub-id></mixed-citation>
    </ref>
    <ref id="B65">
      <label>65.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lv</surname><given-names>H.</given-names></string-name>, <string-name><surname>Dao</surname><given-names>F.-Y.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>D.</given-names></string-name>, <string-name><surname>Guan</surname><given-names>Z.-X.</given-names></string-name>, <string-name><surname>Yang</surname><given-names>H.</given-names></string-name>, <string-name><surname>Su</surname><given-names>W.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>M.-L.</given-names></string-name>, <string-name><surname>Ding</surname><given-names>H.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>W.</given-names></string-name>, <string-name><surname>Lin</surname><given-names>H.</given-names></string-name></person-group><article-title>iDNA-MS: an integrated computational tool for detecting DNA modification sites in multiple genomes</article-title>. <source>Iscience</source>. <year>2020</year>; <volume>23</volume>:<fpage>100991</fpage>.<pub-id pub-id-type="pmid">32240948</pub-id></mixed-citation>
    </ref>
    <ref id="B66">
      <label>66.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pan</surname><given-names>X.</given-names></string-name>, <string-name><surname>Zuallaert</surname><given-names>J.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Shen</surname><given-names>H.-B.</given-names></string-name>, <string-name><surname>Campos</surname><given-names>E.P.</given-names></string-name>, <string-name><surname>Marushchak</surname><given-names>D.O.</given-names></string-name>, <string-name><surname>De Neve</surname><given-names>W.</given-names></string-name></person-group><article-title>ToxDL: deep learning using primary structure and domain embeddings for assessing protein toxicity</article-title>. <source>Bioinformatics</source>. <year>2020</year>; <volume>36</volume>:<fpage>5159</fpage>–<lpage>5168</lpage>.</mixed-citation>
    </ref>
    <ref id="B67">
      <label>67.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname><given-names>H.</given-names></string-name>, <string-name><surname>Hu</surname><given-names>R.</given-names></string-name>, <string-name><surname>Jia</surname><given-names>P.</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>Z.</given-names></string-name></person-group><article-title>6mA-Finder: a novel online tool for predicting DNA N6-methyladenine sites in genomes</article-title>. <source>Bioinformatics</source>. <year>2020</year>; <volume>36</volume>:<fpage>3257</fpage>–<lpage>3259</lpage>.<pub-id pub-id-type="pmid">32091591</pub-id></mixed-citation>
    </ref>
    <ref id="B68">
      <label>68.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>Y.-Z.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.-F.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>C.</given-names></string-name>, <string-name><surname>Yan</surname><given-names>R.-X.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Z.</given-names></string-name></person-group><article-title>Prediction of ubiquitination sites by using the composition of k-spaced amino acid pairs</article-title>. <source>PLoS One</source>. <year>2011</year>; <volume>6</volume>:<fpage>e22930</fpage>.<pub-id pub-id-type="pmid">21829559</pub-id></mixed-citation>
    </ref>
    <ref id="B69">
      <label>69.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>P.</given-names></string-name>, <string-name><surname>Li</surname><given-names>F.</given-names></string-name>, <string-name><surname>Marquez-Lago</surname><given-names>T.T.</given-names></string-name>, <string-name><surname>Leier</surname><given-names>A.</given-names></string-name>, <string-name><surname>Revote</surname><given-names>J.</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Powell</surname><given-names>D.R.</given-names></string-name>, <string-name><surname>Akutsu</surname><given-names>T.</given-names></string-name>, <string-name><surname>Webb</surname><given-names>G.I.</given-names></string-name></person-group><article-title>iLearn: an integrated platform and meta-learner for feature engineering, machine-learning analysis and modeling of DNA, RNA and protein sequence data</article-title>. <source>Brief. Bioinform.</source><year>2020</year>; <volume>21</volume>:<fpage>1047</fpage>–<lpage>1057</lpage>.<pub-id pub-id-type="pmid">31067315</pub-id></mixed-citation>
    </ref>
    <ref id="B70">
      <label>70.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>F.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>J.</given-names></string-name>, <string-name><surname>Fang</surname><given-names>L.</given-names></string-name>, <string-name><surname>Chou</surname><given-names>K.-C.</given-names></string-name></person-group><article-title>Pse-in-One: a web server for generating various modes of pseudo components of DNA, RNA, and protein sequences</article-title>. <source>Nucleic Acids Res.</source><year>2015</year>; <volume>43</volume>:<fpage>W65</fpage>–<lpage>W71</lpage>.<pub-id pub-id-type="pmid">25958395</pub-id></mixed-citation>
    </ref>
    <ref id="B71">
      <label>71.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>F.</given-names></string-name>, <string-name><surname>Fang</surname><given-names>L.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Chou</surname><given-names>K.-C.</given-names></string-name></person-group><article-title>repDNA: a Python package to generate various modes of feature vectors for DNA sequences by incorporating user-defined physicochemical properties and sequence-order effects</article-title>. <source>Bioinformatics</source>. <year>2015</year>; <volume>31</volume>:<fpage>1307</fpage>–<lpage>1309</lpage>.<pub-id pub-id-type="pmid">25504848</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
