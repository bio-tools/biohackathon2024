<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8275335</article-id>
    <article-id pub-id-type="pmid">34252932</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btab312</article-id>
    <article-id pub-id-type="publisher-id">btab312</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Macromolecular Sequence, Structure, and Function</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Predicting MHC-peptide binding affinity by differential boundary tree</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Feng</surname>
          <given-names>Peiyuan</given-names>
        </name>
        <aff><institution>Institute for Interdisciplinary Information Sciences, Tsinghua University</institution>, Beijing, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Zeng</surname>
          <given-names>Jianyang</given-names>
        </name>
        <aff><institution>Institute for Interdisciplinary Information Sciences, Tsinghua University</institution>, Beijing, <country country="CN">China</country></aff>
        <aff><institution>MOE Key Laboratory of Bioinformatics, Tsinghua University</institution>, Beijing, <country country="CN">China</country></aff>
        <xref rid="btab312-cor1" ref-type="corresp"/>
        <!--zengjy321@tsinghua.edu.cn-->
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Ma</surname>
          <given-names>Jianzhu</given-names>
        </name>
        <aff><institution>Institute for Artificial Intelligence, Peking University</institution>, <country country="CN">China</country></aff>
        <xref rid="btab312-cor1" ref-type="corresp"/>
        <!--majianzhu@pku.edu.cn-->
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btab312-cor1">To whom correspondence should be addressed. <email>zengjy321@tsinghua.edu.cn</email> or <email>majianzhu@pku.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2021-07-12">
      <day>12</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>12</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <volume>37</volume>
    <issue>Suppl 1</issue>
    <issue-title>ISMB/ECCB 2021 Proceedings</issue-title>
    <fpage>i254</fpage>
    <lpage>i261</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2021. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btab312.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>The prediction of the binding between peptides and major histocompatibility complex (MHC) molecules plays an important role in neoantigen identification. Although a large number of computational methods have been developed to address this problem, they produce high false-positive rates in practical applications, since in most cases, a single residue mutation may largely alter the binding affinity of a peptide binding to MHC which cannot be identified by conventional deep learning methods.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We developed a differential boundary tree-based model, named DBTpred, to address this problem. We demonstrated that DBTpred can accurately predict MHC class I binding affinity compared to the state-of-art deep learning methods. We also presented a parallel training algorithm to accelerate the training and inference process which enables DBTpred to be applied to large datasets. By investigating the statistical properties of differential boundary trees and the prediction paths to test samples, we revealed that DBTpred can provide an intuitive interpretation and possible hints in detecting important residue mutations that can largely influence binding affinity.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The DBTpred package is implemented in Python and freely available at: <ext-link xlink:href="https://github.com/fpy94/DBT" ext-link-type="uri">https://github.com/fpy94/DBT</ext-link>.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
          <!-- oupReleaseDelayRemoved from OA Article (00|0) -->
        </funding-source>
        <award-id>61872216</award-id>
        <award-id>81630103</award-id>
        <award-id>31900862</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Turing AI Institute of Nanjing and the Zhongguancun Haihua Institute for Frontier Information Technology</institution>
          </institution-wrap>
          <!-- oupReleaseDelayRemoved from OA Article (00|0) -->
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Cytotoxic T lymphocytes destroy cancer or virus infected cells by expressing T-cell receptors (TCRs) that can recognize specific antigens. In most cases, an antigen is a linear peptide arising from mutant proteins in cancer cells or extracellular infected viruses that can bind to major histocompatibility complex (MHC) molecules. After the recognition by the MHC, antigens are brought to the surface of cells where they are identified by specific TCRs, which then initiates the downstream immune response. The binding between the peptides and MHC molecules plays an essential role in this complicated biological process.</p>
    <p>In the last decades, there have been lots of interests in developing computational methods that can accurately predict the binding affinity between peptides and MHC molecules. These methods can be mainly divided into two types: allele-specific and pan-allele approaches. The allele-specific models usually take only peptide sequences as input to train one model for each allele (<xref rid="btab312-B1" ref-type="bibr">Andreatta and Nielsen, 2016</xref>; <xref rid="btab312-B3" ref-type="bibr">Han and Kim, 2017</xref>; <xref rid="btab312-B14" ref-type="bibr">Nielsen and Andreatta, 2016</xref>), while pan-specific models take both peptide sequences and MHC pseudo-sequences as input to train a single model for all alleles (<xref rid="btab312-B3" ref-type="bibr">Han and Kim, 2017</xref>; <xref rid="btab312-B5" ref-type="bibr">Hoof <italic toggle="yes">et al.</italic>, 2009</xref>; <xref rid="btab312-B6" ref-type="bibr">Hu <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btab312-B8" ref-type="bibr">Jurtz <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btab312-B9" ref-type="bibr">Karosiene <italic toggle="yes">et al.</italic>, 2012</xref>; <xref rid="btab312-B12" ref-type="bibr">Liu <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btab312-B14" ref-type="bibr">Nielsen and Andreatta, 2016</xref>; <xref rid="btab312-B15" ref-type="bibr">2017</xref>). Recently, deep neural networks (DNNs) have been proved to be powerful models and demonstrated the state-of-the-art performance in pan-specific tasks. For instance, NetMHCpan uses only a single layer neural network to predict the binding affinity between peptides and MHC molecules and achieves high prediction accuracy on both peptide-MHC binding affinity data and mass spectrometry data (<xref rid="btab312-B8" ref-type="bibr">Jurtz <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btab312-B14" ref-type="bibr">Nielsen and Andreatta, 2016</xref>). More carefully designed and precise neural networks have also been developed to address this problem. ConvMHC uses convolutional neural networks as a feature extractor of input sequences (<xref rid="btab312-B3" ref-type="bibr">Han and Kim, 2017</xref>) that can reliably predict the peptide binding of most HLA-A and -B alleles. ACME (Attention-based convolutional neural network for MHC epitope binding prediction) provides a novel sequence encoding method with a sophisticated network architecture to achieve a higher prediction accuracy (<xref rid="btab312-B6" ref-type="bibr">Hu <italic toggle="yes">et al.</italic>, 2019</xref>). It uses a head-to-head and tail-to-tail sequence encoding method and concatenates the output of the intermediate layer as the input to the fully-connected layer before the final output, which thus enables the information flow from the shallow layer to the deep (<xref rid="btab312-B6" ref-type="bibr">Hu <italic toggle="yes">et al.</italic>, 2019</xref>). These methods have been widely used in the discovery of tumor neoantigens to narrow down the lists of epitopes by providing high-quality rankings for candidates. However, they suffer from a high false-positive rate of predicted epitopes (<xref rid="btab312-B21" ref-type="bibr">“The Problem with Neoantigen Prediction”, 2017</xref>). In addition, these DNN based methods are biased toward learning the most determinant sequence motifs by smoothing the neighborhood signals and often fail in identifying mutation-induced tumor neoantigens since these neoantigens typically have only one single amino acid mutations that may significantly influence the MHC-peptide binding (<xref rid="btab312-B7" ref-type="bibr">Jiang <italic toggle="yes">et al.</italic>, 2019</xref>). Another limitation of these DNN models is their model interpretation. Most of these convolutional neural network based models provide multiple sequence motifs represented by a position-specific scoring matrix as model interpretation through examining the model weights of the convolutional layers (<xref rid="btab312-B3" ref-type="bibr">Han and Kim, 2017</xref>; <xref rid="btab312-B6" ref-type="bibr">Hu <italic toggle="yes">et al.</italic>, 2019</xref>). This type of interpretation is able to propose the informative regions on the protein sequences when the algorithm takes complete protein sequences as input. However, experimental results now have already identified a significant amount of residues whose mutations play essential roles in the immune response (<xref rid="btab312-B2" ref-type="bibr">Castle <italic toggle="yes">et al.</italic>, 2019</xref>). Therefore, more important biological knowledge in comparison to simple sequence motifs can help understand the logic of how the machine learning models make predictions based on these key residues and how these key residues interact with each other to determine the MHC-peptide affinity. In this work, we develop a new machine learning model, named DBTpred (<xref rid="btab312-F1" ref-type="fig">Fig. 1</xref>), based on a differential boundary tree (DBT) (<xref rid="btab312-B25" ref-type="bibr">Zoran <italic toggle="yes">et al.</italic>, 2017</xref>), which can predict MHC-peptide binding affinity and at the same time provide a relatively transparent decision process as the model interpretation. The central idea of DBT is to organize all the training samples as a tree based on a distance metric calculated by a neural network model. For two connecting nodes (training samples) in the DBT, we require that their features are similar but their labels have to be significantly different. The prediction process is simply to search for the nearest neighbors along the tree from the root to the leaves. The intuition of this process is that we keep crossing the decision boundaries of the machine learning model to search for the nearest neighbor of the query sample. This model is specially designed to address the MHC-peptide binding problem in which proteins with similar sequences might have different binding affinities.</p>
    <p>
      <boxed-text id="btab312-BOX1" position="float">
        <sec>
          <title>Algorithm 1: Building the Boundary tree</title>
          <p> /* <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mo>ϵ</mml:mo></mml:math></inline-formula> determines the difference of labels */</p>
          <p> /* <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mi>k</mml:mi></mml:math></inline-formula> is the maximum number of children per node */</p>
          <p> /* <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> represents the label of node <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mi>v</mml:mi></mml:math></inline-formula> */</p>
          <p> <bold>1 Function</bold> Building BT(<inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mi mathvariant="script">D</mml:mi></mml:math></inline-formula>):</p>
          <p>
            <inline-graphic xlink:href="btab312ilf1.jpg"/>
          </p>
          <p> 11 <bold>End Function</bold></p>
        </sec>
      </boxed-text>
    </p>
    <fig position="float" id="btab312-F1">
      <label>Fig. 1.</label>
      <caption>
        <p>Overview of the DBTpred framework. DBTpred predicts the binding of peptides to MHC molecules by constructing a differential boundary tree</p>
      </caption>
      <graphic xlink:href="btab312f1" position="float"/>
    </fig>
    <p>To build the DBT model, we jointly construct the DBT structure and train the weights of the neural network. The major technical challenge is that the DBT structure search problem is a computationally demanding optimization problem which significantly limits both the scalability and training/test speed of the neural network model. To address this problem, we develop a new parallel algorithm to simultaneously train a deep neural network and construct a DBT by transforming the entire tree structure search process into multiple matrix operations. We demonstrated that our model achieved a superior performance in comparison to the state of art methods. We evaluated our model on the IEDB MHC class I binding affinity benchmark datasets (<xref rid="btab312-B10" ref-type="bibr">Kim, 2014</xref>; <xref rid="btab312-B23" ref-type="bibr">Vita <italic toggle="yes">et al.</italic>, 2015</xref>) and found that DBTpred can achieve better performance compared to the state-of-the-art MHC-peptide binding prediction methods. In addition, DBTpred can demonstrate the inner working logic to generate the final prediction, which enables us to identify important mutations that significantly alter the binding affinity.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Boundary tree</title>
      <p>Boundary tree (BT) is a special type of k-nearest neighbor (KNN) model, which was first proposed by <xref rid="btab312-B13" ref-type="bibr">Mathy <italic toggle="yes">et al.</italic> (2015)</xref>. Compared to conventional KNN based models, BT has very appealing properties in reducing the computational and memory requirements of KNN methods. More importantly, BT can generate a decision path for the final prediction, which thus provides valuable insights to understand the logic of the model prediction. A BT model selects a subset of data instances to represent the whole training dataset by organizing these instances into a tree. To construct the tree structure, the algorithm randomly samples a data instance as the root of BT and then iterates all the training instances to search for the nearest neighbor by traversing the entire tree from the root based on a particular distance metric. At each step, if the label of the nearest neighbor is different from the query node (label difference <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:mo>&gt;</mml:mo><mml:mo>ϵ</mml:mo></mml:mrow></mml:math></inline-formula> for a regression problem), the query node is added as the child of the current node of the tree. Thus, for two connecting nodes in a BT, their features should be similar while their labels are distinct. The algorithm details of building a BT is shown in Algorithm 1. To make a prediction for a query (test sample), if the query is much closer to the current node than any of its children, we assign the current node as the closest node and then transfer its label as the final prediction. Otherwise, the current node is updated to the child node that is closest to the query node, and then this process continues until reaching the leaf. The purpose is to search for the nearest neighbor but the search must follow the path of the tree from the root to leaves. The algorithmic details of querying a BT are shown in Algorithm 2.</p>
      <p>
        <boxed-text id="btab312-BOX2" position="float">
          <sec>
            <title>Algorithm 2<bold>:</bold> Querying the Boundary tree</title>
            <p> /* <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mo>ϵ</mml:mo></mml:math></inline-formula> determines the difference of labels */</p>
            <p> /* <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">child</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> ={Child node set of <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mi>v</mml:mi></mml:math></inline-formula>} */</p>
            <p> <bold>1 Function</bold> QueryBT (<inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mi mathvariant="script">T</mml:mi></mml:math></inline-formula>,<inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mi>q</mml:mi></mml:math></inline-formula>) :</p>
            <p>
              <inline-graphic xlink:href="btab312ilf2.jpg"/>
            </p>
          </sec>
        </boxed-text>
      </p>
    </sec>
    <sec>
      <title>2.2 Differential boundary tree</title>
      <p>Usually, BT requires an accurate metric to quantify the distance between training samples. To achieve this goal, <xref rid="btab312-B25" ref-type="bibr">Zoran <italic toggle="yes">et al.</italic> (2017)</xref> developed a new computational framework, named Differential Boundary Tree (DBT), which includes a deep neural network (DNN) to represent the node features in a low compact dimensional space. In particular, DBT implements a differentiable cost function enabling the simultaneous construction of the boundary tree and the weight optimization of the DNN model. In this work, we modify the training process of DBT to a relatively much simpler process. In particular, at each training epoch, we construct a temporal boundary tree on a batch of random sampled data points and then measure the loss function between the ground truth labels and their predictions from the temporal boundary tree. Another non-overlapping batch of training examples are sampled and then queried on the temporal boundary tree. Unlike the training algorithm proposed in <xref rid="btab312-B25" ref-type="bibr">Zoran <italic toggle="yes">et al.</italic> (2017)</xref> which maximized the log-likelihood of all the data collected from the transition paths, here we adopt the weighted summation of the labels of the nearest neighbor and its siblings on the temporal boundary tree as the final prediction of the query. Specifically, let <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> stand for the nearest neighbor of query node <italic toggle="yes">x</italic>. Then the final prediction of <italic toggle="yes">x</italic> is
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">d</italic> represents the Euclidean distance function, <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denotes the sibling nodes of <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> itself, <italic toggle="yes">y<sub>i</sub></italic> stands for the label of <italic toggle="yes">x<sub>i</sub></italic> and <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> stands for the neural network with weights <italic toggle="yes">θ</italic>. Then, the loss function that we try to minimize is the mean square error loss defined as follows,
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">query</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">N</italic> stands for the number of nodes in set <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">query</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> represents the label of <italic toggle="yes">y</italic>. We optimize the parameter in the neural network using the Adam algorithm (<xref rid="btab312-B10" ref-type="bibr">Kim, 2014</xref>) in each epoch. In the conventional error backpropagation training algorithm, the forward function makes the prediction and the backward function propagates the error to calculate the gradient for each weight. Here, DBT simply replaces the prediction from a forward function based on the prediction from a boundary tree. The algorithm details are demonstrated in Algorithm 3. When Algorithm 3 converges, we re-run Algorithm 1 on all the training samples to create the final DBT which is then used for making final prediction.</p>
      <p>
        <boxed-text id="btab312-BOX3" position="float">
          <sec>
            <title>Algorithm 3<bold>:</bold> Training the differential boundary tree (DBT)</title>
            <p> /* <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mi mathvariant="script">D</mml:mi></mml:math></inline-formula> is the training dataset */</p>
            <p> /* <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mi>T</mml:mi></mml:math></inline-formula> is the number of iterations */</p>
            <p> <bold>1</bold> Initialize the neural network <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> with random weights.</p>
            <p> <bold>2 while</bold> <italic toggle="yes">not converged</italic> <bold>do</bold></p>
            <p>
              <inline-graphic xlink:href="btab312ilf3.jpg"/>
            </p>
            <p> <bold>13 end</bold></p>
          </sec>
        </boxed-text>
      </p>
    </sec>
    <sec>
      <title>2.3 A parallel DBT training algorithm</title>
      <p>The major problem of DBT and BT is that their training process is generally quite slow. Training one model for a MHC-peptide binding prediction task with 10 000 samples needs about an hour. The training process is slow since it both involves a series of node querying operations on the temporal boundary trees and can only update the model parameters after collecting a number of traverse paths for all the samples in a sequential way, which cannot benefit from the modern parallelization frameworks such as Math Kernel Library (MKL) (<xref rid="btab312-B24" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2014</xref>) and Graphics Processing Units (GPUs).</p>
      <p>In this work, we also develop a new parallel training/test algorithm to accelerate this query process by transferring the path search process into a matrix multiplication problem, which can be easily parallelized through using the GPUs. First, we decompose the intermediate nodes of the boundary tree into different subtrees where each subtree contains one intermediate node and all of its children. Then we construct a query matrix <italic toggle="yes">Q</italic> in which rows represent query samples within the same batch and columns represent all the subtrees from the decomposition. For each query node, we searched for its closest neighbor in each subtree based on the Euclidean distance of neural network outputs. The closest neighbor is labeled as 1 and otherwise as 0 in the matrix <italic toggle="yes">Q</italic>. For instance, Suppose that the closest nodes of <italic toggle="yes">s</italic><sub>2</sub> in the three subtrees are <italic toggle="yes">n</italic><sub>2</sub>, <italic toggle="yes">n</italic><sub>4</sub>, <italic toggle="yes">n</italic><sub>5</sub>, respectively, then the values corresponding to the <italic toggle="yes">n</italic><sub>2</sub>, <italic toggle="yes">n</italic><sub>4</sub>, <italic toggle="yes">n</italic><sub>5</sub> columns are labeled as 1 in row <italic toggle="yes">s</italic><sub>2</sub> (<xref rid="btab312-F2" ref-type="fig">Fig. 2</xref>, left). Note that each subtree is independent of each other and this construction can be fully parallelized on different computational resources. Next, we construct a traverse matrix <italic toggle="yes">H</italic> in which rows represent subtrees and columns represent all the paths of boundary trees from the root (<xref rid="btab312-F2" ref-type="fig">Fig. 2</xref>, right). For each edge (e.g. <italic toggle="yes">n</italic><sub>1</sub> to <italic toggle="yes">n</italic><sub>2</sub>) in each path (column <italic toggle="yes">p</italic><sub>3</sub>), we assign a score <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> to it, where <italic toggle="yes">d</italic> is the depth of the reachable node (<italic toggle="yes">n</italic><sub>2</sub>) of the edge and <italic toggle="yes">α</italic> is a constant between 0 and 1 (which is set to 0.5 in practice). The score for a path is equal to the summation of all the scores on its edges. Note that the same edge can be assigned with different scores as it can belong to different paths. For a path stopping at non-leaf nodes, its score is equal to the mean of <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mo>α</mml:mo><mml:mi>i</mml:mi></mml:msup></mml:math></inline-formula>, that is,
<disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mo>α</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mo>α</mml:mo><mml:mi>i</mml:mi></mml:msup><mml:mo stretchy="true">)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>where <italic toggle="yes">M</italic> stands for the maximum depth of the tree (which is 4 in <xref rid="btab312-F2" ref-type="fig">Fig. 2</xref>). Note that this construction can be easily implemented by the matrix multiplication using the feature matrix of query and tree nodes. Then for each query sample <italic toggle="yes">s<sub>i</sub></italic>, and corresponding row <italic toggle="yes">Q<sub>i</sub></italic> and each column <italic toggle="yes">H<sub>j</sub></italic>, we select the path of <italic toggle="yes">s<sub>i</sub></italic> based on the following criterion,
<disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">path</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:msubsup><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <fig position="float" id="btab312-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>The illustration of a parallel algorithm for querying boundary trees</p>
        </caption>
        <graphic xlink:href="btab312f2" position="float"/>
      </fig>
      <p>It is easy to see that for a query sample <italic toggle="yes">s<sub>i</sub></italic>, the inner product between <italic toggle="yes">Q<sub>i</sub></italic> and <italic toggle="yes">H<sub>j</sub></italic> results in the sum over all the edges scores of <italic toggle="yes">s<sub>i</sub></italic> on the path <italic toggle="yes">j</italic>. Given the above definition, we introduce the following main theorem about our fast training algorithm, below<statement id="mthst1"><label>Theorem 1.</label><p><italic toggle="yes">The optimal path found by Algorithm 2 is the same as that derived from Equation 4, when</italic> <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:mo>α</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p></statement></p>
      <p>Proof 1. <italic toggle="yes">To prove this theorem, we first define some notation. Let S(n) represent the assigned score for a reachable node n, and S(P) represent the total score of a path P. Then for an arbitrary path <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where n<sub>i</sub> is the i-th node on the path, then total score of path P is <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:munder><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We first prove the following lemma.</italic><statement id="mthst2"><label>Lemma 1.</label><p><inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">parent</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">parent</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> represents the paths that start from the parent of <italic toggle="yes">n<sub>i</sub></italic> but do not pass <italic toggle="yes">n<sub>i</sub></italic>. That is, the assigned score of node <italic toggle="yes">n<sub>i</sub></italic> is larger than those of paths starting from the parent of <italic toggle="yes">n<sub>i</sub></italic> but not passing through <italic toggle="yes">n<sub>i</sub></italic>.</p></statement></p>
      <p>As shown in <xref rid="btab312-F2" ref-type="fig">Figure 2</xref>, if a query node passes an edge <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> in a subtree, the scores of <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">parent</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be divided into following cases:
</p>
      <list list-type="order">
        <list-item>
          <p>For a path that ends at a leaf node, the maximum score it can get is <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mo>α</mml:mo><mml:mi>i</mml:mi></mml:msup></mml:math></inline-formula>, where <italic toggle="yes">d</italic> is the depth of node <italic toggle="yes">n<sub>i</sub></italic>, and <italic toggle="yes">M</italic> is the maximum depth of the tree.</p>
        </list-item>
        <list-item>
          <p>For a path that ends at a child node of the node <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, the maximum score is <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mo>α</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mo>α</mml:mo><mml:mi>i</mml:mi></mml:msup><mml:mo stretchy="true">)</mml:mo></mml:math></inline-formula>, where <italic toggle="yes">d</italic> is the depth of node <italic toggle="yes">n<sub>i</sub></italic>, and <italic toggle="yes">M</italic> is the maximum depth of the tree.</p>
        </list-item>
        <list-item>
          <p>For a path that ends at a non-leaf node which is not a child node of the node <inline-formula id="IE35"><mml:math id="IM35" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, the maximum score is <inline-formula id="IE36"><mml:math id="IM36" display="inline" overflow="scroll"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mo>α</mml:mo><mml:mi>i</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mo>α</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mo>α</mml:mo><mml:mi>i</mml:mi></mml:msup><mml:mo stretchy="true">)</mml:mo></mml:math></inline-formula>, where <italic toggle="yes">g</italic> is the depth of a non-leaf node that the path stops, <italic toggle="yes">d</italic> is the depth of node <italic toggle="yes">n<sub>i</sub></italic> and <italic toggle="yes">M</italic> is the maximum depth of the tree.</p>
        </list-item>
      </list>
      <p>Considering all these three conditions, the maximum value of <inline-formula id="IE37"><mml:math id="IM37" display="inline" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">parent</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is <inline-formula id="IE38"><mml:math id="IM38" display="inline" overflow="scroll"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mo>α</mml:mo><mml:mi>i</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mo>α</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mo>α</mml:mo><mml:mi>i</mml:mi></mml:msup><mml:mo stretchy="true">)</mml:mo></mml:math></inline-formula>. Meanwhile, it is easy to see that <inline-formula id="IE39"><mml:math id="IM39" display="inline" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Then we can see that the maximum of <inline-formula id="IE40"><mml:math id="IM40" display="inline" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">parent</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is smaller than <inline-formula id="IE41"><mml:math id="IM41" display="inline" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> since,
<disp-formula id="E5"><label>(5)</label><mml:math id="M5" display="block" overflow="scroll"><mml:msup><mml:mo>α</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>≥</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mo>α</mml:mo><mml:mi>i</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mo>α</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mo>α</mml:mo><mml:mi>i</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mo>α</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mo>α</mml:mo><mml:mi>i</mml:mi></mml:msup><mml:mo stretchy="true">)</mml:mo></mml:math></disp-formula></p>
      <p>This equation holds when <inline-formula id="IE42"><mml:math id="IM42" display="inline" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi><mml:mo>≥</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE43"><mml:math id="IM43" display="inline" overflow="scroll"><mml:mrow><mml:mi>g</mml:mi><mml:mo>≥</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> which are easily satisfied in real world applications. Therefore, Lemma 1 has been proved.</p>
      <p>Finally, from Lemma 1 we have <inline-formula id="IE44"><mml:math id="IM44" display="inline" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:munder><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:munder><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">parent</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, which means that at each node <inline-formula id="IE45"><mml:math id="IM45" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>≥</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> of the path <italic toggle="yes">P</italic>, the score of any other branch paths that starts from <inline-formula id="IE46"><mml:math id="IM46" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and does not pass <italic toggle="yes">n<sub>i</sub></italic> are smaller than <italic toggle="yes">S</italic>(<italic toggle="yes">P</italic>). Thus, Theorem 1 has been proved.</p>
    </sec>
    <sec>
      <title>2.4 Architecture of deep neural network</title>
      <p>The peptides and MHC pseudo-sequences were first encoded as in <xref rid="btab312-B6" ref-type="bibr">Hu <italic toggle="yes">et al.</italic> (2019)</xref>. As shown in <xref rid="btab312-F3" ref-type="fig">Figure 3</xref>, the protein sequences were first encoded by a BLOSUM50 scoring matrix (<xref rid="btab312-B4" ref-type="bibr">Henikoff and Henikoff, 1992</xref>) into encoded vectors and then passed to 1–2 convolutional layers. In particular, we use one convolutional layer to model the peptide sequence and two convolutional layers to model the MHC protein sequences. The outputs of the two branches are then concatenated as the input to the three fully-connected layers. In the end, the outputs of fully-connected layers are concatenated and pass through another fully-connected layer to output the final representation features.</p>
      <fig position="float" id="btab312-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>Neural network architecture of DBTpred. The input data first passes 1–2 convolutional layers (Conv) and then the outputs of two branches are concatenated as the input to the three fully-connected layers (FC). Finally, the outputs of fully-connected layers are concatenated and pass through another fully-connected layer to produce the final representation features</p>
        </caption>
        <graphic xlink:href="btab312f3" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.5 Saliency map scores</title>
      <p>As a query node traverses along the boundary tree, it continually searches for the nearest tree node based on the distance metric derived from the feature representation of deep neural networks. Specifically, let <inline-formula id="IE47"><mml:math id="IM47" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>20</mml:mn><mml:mo>×</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> be the one-hot encoded sequence of an arbitrary tree node on the transition path and <inline-formula id="IE48"><mml:math id="IM48" display="inline" overflow="scroll"><mml:mrow><mml:mi>q</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>20</mml:mn><mml:mo>×</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> be the one-hot encoded sequence of query node, where <italic toggle="yes">L</italic> is the length of sequence, we can obtain the distance between <italic toggle="yes">x</italic> and <italic toggle="yes">q</italic> as
<disp-formula id="E6"><label>(6)</label><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">B</italic> is the BLOSUM50 matrix, and <inline-formula id="IE49"><mml:math id="IM49" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> is the deep neural network. To illustrate the important residues that significantly influence the distance <italic toggle="yes">d</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">q</italic>), we calculated the saliency map (<xref rid="btab312-B19" ref-type="bibr">Simonyan <italic toggle="yes">et al</italic>., 2013</xref>) of <italic toggle="yes">x</italic> and <italic toggle="yes">q</italic> with respect to <italic toggle="yes">d</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">q</italic>), that is,
<disp-formula id="E7"><label>(7)</label><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mi>q</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>Then, we multiplied <italic toggle="yes">w<sub>x</sub></italic> and <italic toggle="yes">w<sub>q</sub></italic> by their one-hot encoded matrices <italic toggle="yes">x</italic> and <italic toggle="yes">q</italic>, respectively to get the derivatives of the actual residues of sequences, noted as <inline-formula id="IE50"><mml:math id="IM50" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE51"><mml:math id="IM51" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mi>q</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>, respectively. Finally, we normalized the saliency map by dividing its maximum absolute values, that is <inline-formula id="IE52"><mml:math id="IM52" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mrow><mml:mi>max</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE53"><mml:math id="IM53" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mi>q</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mrow><mml:mi>max</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mi>q</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. Thus, the saliency map scores demonstrate the important residues between the distance of query and tree nodes along the traverse path in the boundary tree.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Performance of DBTpred</title>
      <p>To evaluate the performance of DBTpred, we conducted a five-fold cross-validation and compared DBTpred to the state-of-the-art method NetMHCpan 3.0 (<xref rid="btab312-B14" ref-type="bibr">Nielsen and Andreatta, 2016</xref>), which is a pan-specific method and also trained on the IEDB MHC class I binding affinity dataset (<xref rid="btab312-B10" ref-type="bibr">Kim, 2014</xref>; <xref rid="btab312-B23" ref-type="bibr">Vita <italic toggle="yes">et al.</italic>, 2015</xref>). NetMHCpan 3.0 is probably the most widely used method in the prediction of MHC-peptide binding affinity and also provides detailed performance for different alleles and peptide lengths. Therefore, we trained our DBTpred model on the same IEDB dataset and investigated the performance of our method on the same alleles and peptide lengths (i.e., 9-mer, 10-mer and 11-mer).</p>
      <p>The prediction performance is measured in terms of Pearson correlation coefficient (PCC). As shown in <xref rid="btab312-F4" ref-type="fig">Figure 4a</xref>, DBTpred increased the PCC by 2.8%, 3.7% and 6.9% in comparison to NetMHCpan 3.0 for 9-mers, 10-mer and 11-mers, respectively. We also investigated the performance in terms of the area under receiver operating characteristic curve (AUROC). As shown in <xref rid="btab312-F4" ref-type="fig">Figure 4b</xref>, DBTpred increased AUROC by 0.91%, 2.0% and 4.1% compared to NetMHCpan 3.0 for 9-mers, 10-mers and 11-mers, respectively. When investigating the performance of individual alleles in peptide lengths, we found that DBTpred outperformed NetMHCpan 3.0 in the vast majority of alleles. More specifically, for 88 alleles of 11-mers, the performance of DBTpred was larger than NetMHCpan 3.0 in 21 out of 29 alleles. Among them, 5 alleles achieved over 20% improvements in PCC. For 10-mers and 9-mers, the performance in 33 out of 40 alleles and 70 out of 88 alleles achieved performance improvement compared with NetMHCpan 3.0. For 9-mers, among 19 alleles DBTpred improved over 5% in PCC and for 10-mers, among 15 alleles DBTpred improved above 5% over the baseline (<xref rid="btab312-F4" ref-type="fig">Fig. 4c</xref>).</p>
      <fig position="float" id="btab312-F4">
        <label>Fig. 4.</label>
        <caption>
          <p>Performance comparison in five-fold cross-validation between DBTpred and different baselines in distinct peptide lengths in terms of (<bold>a</bold>) Pearson correlation coefficient (PCC), and (<bold>b</bold>) area under receiver operating characteristic curve (AUROC). (<bold>c</bold>) The performance comparison between DBTpred and NetMHCpan 3.0 for different alleles. *<inline-formula id="IE54"><mml:math id="IM54" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.05</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, **<inline-formula id="IE55"><mml:math id="IM55" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.001</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>, ***<italic toggle="yes">P </italic>&lt;<italic toggle="yes"> </italic>0.001, <sup>-</sup><italic toggle="yes">P </italic>&gt;<italic toggle="yes"> </italic>0.1 (one-sample <italic toggle="yes">t</italic>-test based on the allele performance between DBTpred and NetMHCpan3.0 and Wilcoxon rank-sum test based on the allele performance between DBTpred and ACME)</p>
        </caption>
        <graphic xlink:href="btab312f4" position="float"/>
      </fig>
      <p>To evaluate the contribution of DBT, we also compared the performance of DBTpred to ACME (<xref rid="btab312-B6" ref-type="bibr">Hu <italic toggle="yes">et al.</italic>, 2019</xref>), which used the same neural network architectures as in DBTpred. We found that DBTpred still achieved better performance compared with ACME for most of the cases although in 11-mer DBTpred decreased a little in terms of PCC (<xref rid="btab312-F4" ref-type="fig">Fig. 4</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S1</xref>). These results revealed that DBTpred can accurately predict the binding affinity of peptides and MHC class I molecules.</p>
      <p>To further demonstrate the generalizability of DBTpred, we tested its performance on an independent MHC class I binding affinity benchmarking dataset (<xref rid="btab312-B22" ref-type="bibr">Trolle <italic toggle="yes">et al.</italic>, 2015</xref>), which contains over 30 000 MHC class I binding peptides. In our study, we tested the performance on the peptides with IC50 binding affinity measurement in the benchmarking dataset. We trained DBTpred on the training dataset and compared its performance to that of other state-of-the-art methods. The prediction results of these state-of-the-art methods were downloaded from the benchmarking website (<ext-link xlink:href="http://tools.immuneepitope.org/auto_bench/mhci/weekly/" ext-link-type="uri">http://tools.immuneepitope.org/auto_bench/mhci/weekly/</ext-link>). As shown in <xref rid="btab312-F5" ref-type="fig">Figure 5</xref>, DBTpred outperformed all the other benchmarking methods. Compared to the NetMHCpan3.0, DBTpred achieved an increase of 4.3% in PCC. We also retrained the ACME (<xref rid="btab312-B6" ref-type="bibr">Hu <italic toggle="yes">et al.</italic>, 2019</xref>) with the same training dataset and compared its performance to that of DBTpred. We found that DBTpred still obtained better performance compared to ACME.</p>
      <fig position="float" id="btab312-F5">
        <label>Fig. 5.</label>
        <caption>
          <p>Performance comparison between DBTpred and other state-of-the-art methods obtained from website (<ext-link xlink:href="http://tools.immuneepitope.org/auto_bench/mhci/weekly/" ext-link-type="uri">http://tools.immuneepitope.org/auto_bench/mhci/weekly/</ext-link>) on an independent test dataset (<xref rid="btab312-B22" ref-type="bibr">Trolle <italic toggle="yes">et al.</italic>, 2015</xref>) in terms of Pearson correlation coefficient (PCC)</p>
        </caption>
        <graphic xlink:href="btab312f5" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.2 Training DBTpred</title>
      <p>During the training of DBTpred, the model iteratively built a boundary tree with a subset of training data, and traversed this tree using another subset of training data to update the parameters of the backbone neural network. When building the boundary tree, we ensured that the absolute difference between the query node and its closest node was larger than a threshold (denoted as <inline-formula id="IE56"><mml:math id="IM56" display="inline" overflow="scroll"><mml:mo>ϵ</mml:mo></mml:math></inline-formula>), otherwise it was be discarded. This process is slow when trained on a large dataset (<xref rid="btab312-B25" ref-type="bibr">Zoran <italic toggle="yes">et al.</italic>, 2017</xref>). To address this problem, we first pre-trained a conventional neural network (only one epoch) and then used the weights except for the last output layer as the initiation to our backbone. Next, we used our parallel training algorithm to accelerate the query process of the boundary tree, which was the most time-consuming step in the whole training process. In our study, we found that a good initiation of backbone enabled the training process to converge fast. <xref rid="btab312-F6" ref-type="fig">Figure 6a</xref> illustrates how the performance changed with the number of training epochs in DBTpred. We found that the label difference parameter should be larger than 0.1 to ensure a model with good training performance. When was too small, the performance of DBTpred fluctuated, since there were less samples in the boundary tree to represent the whole dataset.</p>
      <fig position="float" id="btab312-F6">
        <label>Fig. 6.</label>
        <caption>
          <p>Performance of the training process. (<bold>a</bold>) The test performance of DBTpred vs. the number of training epochs. (<bold>b</bold>) The ratio of the number of tree nodes to the total number of training examples vs. the number of training epochs</p>
        </caption>
        <graphic xlink:href="btab312f6" position="float"/>
      </fig>
      <p>Another observation was that the larger, the less number of nodes was left in the tree. As shown in <xref rid="btab312-F6" ref-type="fig">Figure 6b</xref>, all the boundary trees with small contained more nodes than those with large. In addition, the ratio of the number of tree nodes to the total number of training samples decreased as the number of training epochs in DBTpred. This result indicated that the training process enabled the model to learn good feature representation such that the boundary tree can represent the whole dataset with less nodes. For example, after 25 training epochs, DBTpred only needed 40% training examples to achieve over 0.8 of test performance in terms of PCC.</p>
      <p>In addition, our parallel algorithm transferred the discrete query process into matrix multiplications, which can be easily calculated on GPUs. As illustrated in <xref rid="btab312-F7" ref-type="fig">Figure 7</xref>, compared to the original query algorithm (<xref rid="btab312-B13" ref-type="bibr">Mathy <italic toggle="yes">et al.</italic>, 2015</xref>), whose training time was linearly proportional to the number of samples, our method exhibited almost a constant growth rate, which thus can greatly speed up the training process.</p>
      <fig position="float" id="btab312-F7">
        <label>Fig. 7.</label>
        <caption>
          <p>Comparison of computational time between the conventional query algorithm of boundary tree (<xref rid="btab312-B13" ref-type="bibr">Mathy <italic toggle="yes">et al.</italic>, 2015</xref>) and our parallel querying algorithm</p>
        </caption>
        <graphic xlink:href="btab312f7" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.3 Feature representation</title>
      <p>We investigated how many samples are generally needed in DBTpred for each allele. Intriguingly, we found that for each allele, if there were more samples of the allele left in the boundary tree, the smaller was the performance of this allele. As shown in <xref rid="btab312-F8" ref-type="fig">Figure 8</xref>, most alleles can be represented by only 30–40% of all the samples using the boundary tree and achieved over 0.7 of test performance, while some alleles needed to be represented by over 50% samples in the boundary tree and exhibited less than 0.5 of test performance. The sequence patterns of these alleles were not completely learned by the model since although they occupied large amounts of samples in the boundary tree, they did not achieve a good performance compared with those of other alleles. Notably, we also found that there were some alleles which left small amounts of samples in the boundary tree while still achieving good performance (e.g. alleles in bottom right corner in <xref rid="btab312-F8" ref-type="fig">Fig. 8</xref>), which indicated that the features of these alleles may be easy to learn. <xref rid="btab312-F8" ref-type="fig">Figure 8</xref> also showed the label complexity of individual alleles (measured as the entropy of the allele labels). In fact, these alleles with less tree nodes while achieving high performance were those with low label complexity (e.g. most of the labels are 0). Thus, the boundary tree needed a very small amount of samples to represent the features of these alleles. In summary, the statistical properties of the boundary tree demonstrated how well the model was trained for the task. In some cases, the ratio of the number of samples left in trees to the total number of samples actually indicated the complexity of the learning.</p>
      <fig position="float" id="btab312-F8">
        <label>Fig. 8.</label>
        <caption>
          <p>Correlation between the ratio of the number of allele tree nodes (allele samples left in boundary tree) to the total number of allele samples and the performance of individual alleles</p>
        </caption>
        <graphic xlink:href="btab312f8" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.4 Model interpretation</title>
      <p>Most deep learning models are end-to-end methods which input test samples into a deep neural network and then output prediction results. Although deep learning has exhibited strong predictive power in lots of biological problems, researchers are interested in analyzing the interpretability of models which may provide useful biological understandings. There have been many developed methods in analyzing deep neural networks such as LIME (<xref rid="btab312-B16" ref-type="bibr">Ribeiro <italic toggle="yes">et al.</italic>, 2016</xref>), saliency map (<xref rid="btab312-B19" ref-type="bibr">Simonyan <italic toggle="yes">et al.</italic>, 2013</xref>), GradCam (<xref rid="btab312-B17" ref-type="bibr">Selvaraju <italic toggle="yes">et al.</italic>, 2017</xref>) and DeepLIFT <xref rid="btab312-B18" ref-type="bibr">Shrikumar <italic toggle="yes">et al.</italic> (2017)</xref>. These methods are able to decipher which part of input plays an important role in the final prediction. However, deeply interpreting a conventional deep learning method is still a popular research direction (<xref rid="btab312-B11" ref-type="bibr">Lipton, 2018</xref>; <xref rid="btab312-B19" ref-type="bibr">Simonyan <italic toggle="yes">et al.</italic>, 2013</xref>).</p>
      <p>DBTpred takes advantage of the strong learning power of deep neural networks to learn a good representation of datasets, through organizing the datasets into a tree structure. Thus, when predicting a test sample, DBTpred can provide an interpretable decision path of the tree, which may provide useful interpretation. <xref rid="btab312-F9" ref-type="fig">Figure 9</xref> illustrates a simple example of the interpretation of how DBTpred predicted a test sample. First, we calculated the saliency map (<xref rid="btab312-B19" ref-type="bibr">Simonyan <italic toggle="yes">et al.</italic>, 2013</xref>) in terms of the Euclidean distance between the test sample and each tree node. The saliency map scores pointed out the important residues in determining the sample distance. DBTpred clearly demonstrated how the test sample traversed to its closest node step by step. In particular, the model first searched for the last valine in the peptide which occupied the largest saliency map score (step 1 to step 3), and then it searched for cysteine and two valines in the first, fifth and the last positions of peptide (step 3 to step 7). These three residues are the key residues in determining the prediction of binding affinity of the test sample. In addition, apart from these three residues, the path also displayed several possible point mutations which may strongly alter the binding affinity of the test sample that can be smoothed out by the deep neural network, e.g. the mutation between valine and isoleucine on the second residue of the peptide (marked as crimson in <xref rid="btab312-F9" ref-type="fig">Fig. 9</xref>). Moreover, since the peptide sequences and MHC pseudo-sequences are both passed through the deep neural network, we can also calculate the saliency scores along the MHC sequence to demonstrate the important residues influencing the final prediction of binding affinities (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S2</xref>). In summary, DBTpred enabled an easy interpretation of how the model makes a prediction of peptide-MHC binding affinity and also provided possible mutation residues that may influence the binding affinity.</p>
      <fig position="float" id="btab312-F9">
        <label>Fig. 9.</label>
        <caption>
          <p>Illustration of a query path in DBTpred. The normalized saliency map scores are shown on the bottom</p>
        </caption>
        <graphic xlink:href="btab312f9" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <p>In this article, we demonstrated that DBTpred can achieve an accurate prediction about MHC class I binding affinity. We developed a parallel training algorithm to accelerate the most time-consuming step of the differential boundary tree and made it possible to apply DBTpred into a large dataset. We also demonstrated that the traverse path of the test sample can provide an interpretable visualization of how the model predicted the final output. Meanwhile, DBTpred detected the possible residue mutations that can largely influence the binding affinity of the test sample which was missed by conventional deep neural networks. In addition, like other KNN based methods, DBTpred searches for the nearest training samples for queries and uses the weighted summation of the labels of the training examples and their children as the final prediction. This means that it is easy for DBTpred to evaluate the reliability of its prediction based on the distance between queries and their closest nodes (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S3</xref>). However, although we have vastly increased the training and inference speed of DBTpred, it was still quite slow compared to conventional neural networks. The training of DBTpred on the total dataset still requires about two days. When building the boundary tree, the training samples were randomly sampled from the training dataset, which may influence the structure of the boundary tree. Thus, the paths for the test samples may be different in the boundary trees. In practice, we built several boundary trees and query the test samples in these trees to choose a path with good interpretation. We also calculated the averaged cosine distance of the saliency map between the test sample and the traverse nodes as the interpretable score for the path. In summary, DBTpred provides a novel method that can accurately predict the MHC class I binding affinity and provide detect useful mutation residues affecting the binding affinity.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btab312_Supplementary_Data</label>
      <media xlink:href="btab312_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>The authors thank Mr Yan Hu for helpful discussions about this work.</p>
    <sec>
      <title>Funding</title>
      <p>This work is supported in part by the National Natural Science Foundation of China (61872216, 81630103, 31900862), the Turing AI Institute of Nanjing and the Zhongguancun Haihua Institute for Frontier Information Technology.</p>
      <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btab312-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Andreatta</surname><given-names>M.</given-names></string-name>, <string-name><surname>Nielsen</surname><given-names>M.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Gapped sequence alignment using artificial neural networks: application to the MHC class I system</article-title>. <source>Bioinformatics</source>, <volume>32</volume>, <fpage>511</fpage>–<lpage>517</lpage>.<pub-id pub-id-type="pmid">26515819</pub-id></mixed-citation>
    </ref>
    <ref id="btab312-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Castle</surname><given-names>J.C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Mutation-derived neoantigens for cancer immunotherapy</article-title>. <source>Front. Immunol</source>., <volume>10</volume>, <fpage>1856</fpage>.<pub-id pub-id-type="pmid">31440245</pub-id></mixed-citation>
    </ref>
    <ref id="btab312-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Han</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Kim</surname><given-names>D.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Deep convolutional neural networks for pan-specific peptide-MHC class I binding prediction</article-title>. <source>BMC Bioinformatics</source>, <volume>18</volume>, <fpage>585</fpage>.<pub-id pub-id-type="pmid">29281985</pub-id></mixed-citation>
    </ref>
    <ref id="btab312-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Henikoff</surname><given-names>S.</given-names></string-name>, <string-name><surname>Henikoff</surname><given-names>J.G.</given-names></string-name></person-group> (<year>1992</year>) <article-title>Amino acid substitution matrices from protein blocks</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>89</volume>, <fpage>10915</fpage>–<lpage>10919</lpage>.<pub-id pub-id-type="pmid">1438297</pub-id></mixed-citation>
    </ref>
    <ref id="btab312-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoof</surname><given-names>I.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2009</year>) <article-title>NetMHCpan, a method for MHC class I binding prediction beyond humans</article-title>. <source>Immunogenetics</source>, <volume>61</volume>, <fpage>1</fpage>–<lpage>13</lpage>.<pub-id pub-id-type="pmid">19002680</pub-id></mixed-citation>
    </ref>
    <ref id="btab312-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hu</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Acme: pan-specific peptide–MHC class I binding prediction through attention-based deep neural networks</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>4946</fpage>–<lpage>4954</lpage>.<pub-id pub-id-type="pmid">31120490</pub-id></mixed-citation>
    </ref>
    <ref id="btab312-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jiang</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Tumor neoantigens: from basic research to clinical applications</article-title>. <source>J. Hematol. Oncol</source>., <volume>12</volume>, <fpage>93</fpage>.<pub-id pub-id-type="pmid">31492199</pub-id></mixed-citation>
    </ref>
    <ref id="btab312-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jurtz</surname><given-names>V.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>NetMHCpan-4.0: improved Peptide-MHC class I interaction predictions integrating eluted ligand and peptide binding affinity data</article-title>. <source>J. Immunol</source>., <volume>199</volume>, <fpage>3360</fpage>–<lpage>3368</lpage>.<pub-id pub-id-type="pmid">28978689</pub-id></mixed-citation>
    </ref>
    <ref id="btab312-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Karosiene</surname><given-names>E.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2012</year>) <article-title>NetMHCcons: a consensus method for the major histocompatibility complex class I predictions</article-title>. <source>Immunogenetics</source>, <volume>64</volume>, <fpage>177</fpage>–<lpage>186</lpage>.<pub-id pub-id-type="pmid">22009319</pub-id></mixed-citation>
    </ref>
    <ref id="btab312-B10">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kim</surname><given-names>Y.</given-names></string-name></person-group> (<year>2014</year>). Convolutional neural networks for sentence classification. In <italic toggle="yes">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</italic>, pages 1746–1751. Doha, Qatar. Association for Computational Linguistics.</mixed-citation>
    </ref>
    <ref id="btab312-B11">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Lipton</surname><given-names>Z.C.</given-names></string-name></person-group> (<year>2018</year>). The mythos of model interpretability: In machine learning, theconcept of interpretability is both important and slippery. <italic toggle="yes">Queue</italic>, <bold>16</bold>(3), <fpage>31</fpage>–<lpage>57</lpage>.</mixed-citation>
    </ref>
    <ref id="btab312-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>G.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>PSSMHCpan: a novel PSSM-based software for predicting class I peptide-HLA binding affinity</article-title>. <source>Gigascience</source>, <volume>6</volume>, <fpage>1</fpage>–<lpage>11</lpage>.</mixed-citation>
    </ref>
    <ref id="btab312-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathy</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) <article-title>The boundary forest algorithm for online supervised and unsupervised learning</article-title>. <source>AAAI</source>, <volume>29</volume>(1).</mixed-citation>
    </ref>
    <ref id="btab312-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nielsen</surname><given-names>M.</given-names></string-name>, <string-name><surname>Andreatta</surname><given-names>M.</given-names></string-name></person-group> (<year>2016</year>) <article-title>NetMHCpan-3.0; improved prediction of binding to MHC class I molecules integrating information from multiple receptor and peptide length datasets</article-title>. <source>Genome Med</source>., <volume>8</volume>, <fpage>33</fpage>.<pub-id pub-id-type="pmid">27029192</pub-id></mixed-citation>
    </ref>
    <ref id="btab312-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nielsen</surname><given-names>M.</given-names></string-name>, <string-name><surname>Andreatta</surname><given-names>M.</given-names></string-name></person-group> (<year>2017</year>) <article-title>NNAlign: a platform to construct and evaluate artificial neural network models of receptor-ligand interactions</article-title>. <source>Nucleic Acids Res</source>., <volume>45</volume>, <fpage>W344</fpage>–<lpage>W349</lpage>.<pub-id pub-id-type="pmid">28407117</pub-id></mixed-citation>
    </ref>
    <ref id="btab312-B16">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ribeiro</surname><given-names>M.T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>). Why should i trust you?: Explaining the predictions of any classifier. In: <italic toggle="yes">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</italic>, pp. <fpage>1135</fpage>–<lpage>1144</lpage>.</mixed-citation>
    </ref>
    <ref id="btab312-B17">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Selvaraju</surname><given-names>R.R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>). Grad-CAM: visual explanations from deep networks via gradient-based localization. In <italic toggle="yes">Proceedings of the IEEE international conferenceon computer vision</italic>, pages <fpage>618</fpage>–<lpage>626</lpage>.</mixed-citation>
    </ref>
    <ref id="btab312-B18">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Shrikumar</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>). Learning important features through propagating activation differences. In: Precup, D. and Teh, Y. W. (eds), <italic toggle="yes">Proceedings of the 34th International Conference on Machine Learning</italic>, volume 70 of <italic toggle="yes">Proceedings of Machine Learning Research</italic>. International Convention Centre, Sydney, Australia. PMLR, pp. <fpage>3145</fpage>–<lpage>3153</lpage>.</mixed-citation>
    </ref>
    <ref id="btab312-B19">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Simonyan</surname><given-names>K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2013</year>). Deep inside convolutional networks: visualising image classification models and saliency maps. <italic toggle="yes">arXiv preprint arXiv:1312.6034.</italic></mixed-citation>
    </ref>
    <ref id="btab312-B21">
      <mixed-citation publication-type="journal">“The Problem with Neoantigen Prediction” (<year>2017</year>) <article-title>The problem with neoantigen prediction</article-title>. <source>Nat. Biotechnol</source>, <volume>35</volume>, <fpage>97</fpage>.<pub-id pub-id-type="pmid">28178261</pub-id></mixed-citation>
    </ref>
    <ref id="btab312-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Trolle</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) <article-title>Automated benchmarking of peptide-MHC class I binding predictions</article-title>. <source>Bioinformatics</source>, <volume>31</volume>, <fpage>2174</fpage>–<lpage>2181</lpage>.<pub-id pub-id-type="pmid">25717196</pub-id></mixed-citation>
    </ref>
    <ref id="btab312-B23">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Vita</surname><given-names>R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>). The immune epitope database (IEDB) 3.0. <italic toggle="yes">Nucleic Acids Res</italic>., 43, D405–D412.</mixed-citation>
    </ref>
    <ref id="btab312-B24">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>E.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>). Intel math kernel library.</mixed-citation>
    </ref>
    <ref id="btab312-B25">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zoran</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>). Learning deep nearest neighbor representations using differentiable boundary trees. <italic toggle="yes">arXiv preprint arXiv:1702.08833</italic>.</mixed-citation>
    </ref>
  </ref-list>
</back>
