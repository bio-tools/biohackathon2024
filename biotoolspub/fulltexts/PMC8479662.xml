<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8479662</article-id>
    <article-id pub-id-type="pmid">33734318</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btab185</article-id>
    <article-id pub-id-type="publisher-id">btab185</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Data and Text Mining</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>GPDBN: deep bilinear network integrating both genomic data and pathological images for breast cancer prognosis prediction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-2379-1709</contrib-id>
        <name>
          <surname>Wang</surname>
          <given-names>Zhiqin</given-names>
        </name>
        <aff><institution>School of Information Science and Technology, University of Science and Technology of China</institution>, Hefei AH230027, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Ruiqing</given-names>
        </name>
        <aff><institution>School of Information Science and Technology, University of Science and Technology of China</institution>, Hefei AH230027, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Wang</surname>
          <given-names>Minghui</given-names>
        </name>
        <xref rid="btab185-cor1" ref-type="corresp"/>
        <aff><institution>School of Information Science and Technology, University of Science and Technology of China</institution>, Hefei AH230027, <country country="CN">China</country></aff>
        <aff><institution>Centers for Biomedical Engineering, University of Science and Technology of China</institution>, Hefei AH230027, <country country="CN">China</country></aff>
        <!--mhwang@ustc.edu.cn-->
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Li</surname>
          <given-names>Ao</given-names>
        </name>
        <xref rid="btab185-cor1" ref-type="corresp"/>
        <aff><institution>School of Information Science and Technology, University of Science and Technology of China</institution>, Hefei AH230027, <country country="CN">China</country></aff>
        <aff><institution>Centers for Biomedical Engineering, University of Science and Technology of China</institution>, Hefei AH230027, <country country="CN">China</country></aff>
        <!--aoli@ustc.edu.cn-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Lu</surname>
          <given-names>Zhiyong</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btab185-cor1">To whom correspondence should be addressed. <email>mhwang@ustc.edu.cn</email> or <email>aoli@ustc.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>15</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2021-03-18">
      <day>18</day>
      <month>3</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>18</day>
      <month>3</month>
      <year>2021</year>
    </pub-date>
    <volume>37</volume>
    <issue>18</issue>
    <fpage>2963</fpage>
    <lpage>2970</lpage>
    <history>
      <date date-type="received">
        <day>03</day>
        <month>12</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>07</day>
        <month>2</month>
        <year>2021</year>
      </date>
      <date date-type="editorial-decision">
        <day>11</day>
        <month>3</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>16</day>
        <month>3</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btab185.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Breast cancer is a very heterogeneous disease and there is an urgent need to design computational methods that can accurately predict the prognosis of breast cancer for appropriate therapeutic regime. Recently, deep learning-based methods have achieved great success in prognosis prediction, but many of them directly combine features from different modalities that may ignore the complex inter-modality relations. In addition, existing deep learning-based methods do not take intra-modality relations into consideration that are also beneficial to prognosis prediction. Therefore, it is of great importance to develop a deep learning-based method that can take advantage of the complementary information between intra-modality and inter-modality by integrating data from different modalities for more accurate prognosis prediction of breast cancer.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We present a novel unified framework named genomic and pathological deep bilinear network (GPDBN) for prognosis prediction of breast cancer by effectively integrating both genomic data and pathological images. In GPDBN, an inter-modality bilinear feature encoding module is proposed to model complex inter-modality relations for fully exploiting intrinsic relationship of the features across different modalities. Meanwhile, intra-modality relations that are also beneficial to prognosis prediction, are captured by two intra-modality bilinear feature encoding modules. Moreover, to take advantage of the complementary information between inter-modality and intra-modality relations, GPDBN further combines the inter- and intra-modality bilinear features by using a multi-layer deep neural network for final prognosis prediction. Comprehensive experiment results demonstrate that the proposed GPDBN significantly improves the performance of breast cancer prognosis prediction and compares favorably with existing methods.</p>
      </sec>
      <sec id="s3">
        <title>Availabilityand implementation</title>
        <p>GPDBN is freely available at <ext-link xlink:href="https://github.com/isfj/GPDBN" ext-link-type="uri">https://github.com/isfj/GPDBN</ext-link>.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>61971393</award-id>
        <award-id>61871361</award-id>
        <award-id>61571414</award-id>
        <award-id>61471331</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>The most common malignancy among females is breast cancer, which is one of the leading causes of cancer-related deaths in the world (<xref rid="btab185-B14" ref-type="bibr">Hortobagyi <italic toggle="yes">et al.</italic>, 2005</xref>). As reported by WHO, more than 1.3 million new cases of breast cancer are diagnosed, and the death toll is as high as 458 000 each year (<xref rid="btab185-B21" ref-type="bibr">Nguyen <italic toggle="yes">et al.</italic>, 2013</xref>). As a very heterogeneous disease, breast cancer has been reported with distinct prognoses on the basis of morphological and molecular stratifications (<xref rid="btab185-B16" ref-type="bibr">Huang <italic toggle="yes">et al.</italic>, 2019</xref>), which represents the major hurdle for accurate diagnosis and curative therapy. Therefore, there is an urgent need to design computational methods to accurately predict the prognosis of breast cancer, which may further assist clinicians to prescribe the most appropriate therapeutic regime (<xref rid="btab185-B27" ref-type="bibr">Sun <italic toggle="yes">et al.</italic>, 2018a</xref>,<xref rid="btab185-B28" ref-type="bibr">b</xref>).</p>
    <p>There have been many studies in predicting the prognosis of breast cancer that assign breast cancer patients into a poor and a good prognosis group based on genomic data and/or pathological images (<xref rid="btab185-B13" ref-type="bibr">Gevaert <italic toggle="yes">et al.</italic>, 2006</xref>; <xref rid="btab185-B27" ref-type="bibr">Sun <italic toggle="yes">et al.</italic>, 2018a</xref>,<xref rid="btab185-B28" ref-type="bibr">b</xref>; <xref rid="btab185-B35" ref-type="bibr">Yu <italic toggle="yes">et al.</italic>, 2016</xref>). Genomic data, especially gene expression profiles (<xref rid="btab185-B33" ref-type="bibr">Xu <italic toggle="yes">et al.</italic>, 2012</xref>) obtained from high-throughput platforms, has been widely adopted in breast cancer prognosis prediction (<xref rid="btab185-B19" ref-type="bibr">Moon <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btab185-B27" ref-type="bibr">Sun <italic toggle="yes">et al.</italic>, 2018a</xref>,<xref rid="btab185-B28" ref-type="bibr">b</xref>; <xref rid="btab185-B31" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2005</xref>). The initial studies aim to discover genes that can separate patients with good prognosis from those with poor prognosis. After that, there are plenty of studies generate a multigene predictor based on a hypothesis developed from in vivo or in vitro experiments and then apply it to breast cancer samples (<xref rid="btab185-B23" ref-type="bibr">Reis-Filho and Pusztai, 2011</xref>). For example, <xref rid="btab185-B30" ref-type="bibr">Van De Vijver <italic toggle="yes">et al.</italic> (2002)</xref> recognize a 70-gene prognostic signature from 117 patients with breast cancer by correlating candidate genes with the outcome and determine the optimal set of genes by using leave-one-out cross validation. <xref rid="btab185-B31" ref-type="bibr">Wang et al. (2005)</xref> further identify a 76-gene signature representing a strong prognostic factor that can be applied to identify patients who have a favorable prognosis. These studies undoubtedly have not only contributed to our understanding of the heterogeneity and complexity of breast cancer behavior but also provided computational methods to distinguish between patients with good and poor prognosis.</p>
    <p>Besides aforementioned prognosis prediction methods using genomic data, the continued importance of pathological analysis of tumors should be emphasized, which has been confirmed to provide independent prognostic information of breast cancer (<xref rid="btab185-B23" ref-type="bibr">Reis-Filho and Pusztai, 2011</xref>). With the emergence of digital whole-slide images, comprehensive computational methods for pathological image analysis have demonstrated promising capability to improve efficiency, accuracy and consistency compared with human examination (<xref rid="btab185-B5" ref-type="bibr">Cheng <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btab185-B8" ref-type="bibr">Courtiol <italic toggle="yes">et al.</italic>, 2019</xref>). With rapid growth of computing resources, many computational pathological methods have been proposed for predicting the prognosis of breast cancer (<xref rid="btab185-B32" ref-type="bibr">Xu <italic toggle="yes">et al.</italic>, 2016</xref>) and a considerable number of other cancers such as lung (<xref rid="btab185-B35" ref-type="bibr">Yu <italic toggle="yes">et al.</italic>, 2016</xref>; <xref rid="btab185-B39" ref-type="bibr">Zhu <italic toggle="yes">et al.</italic>, 2016</xref>) and kidney (<xref rid="btab185-B4" ref-type="bibr">Cheng <italic toggle="yes">et al.</italic>, 2018</xref>) cancers. In this way, hundreds of pathological image features, characterizing cell size, shape, distribution and texture of nuclei, can be extracted from pathological images, which can provide important prognostic information for further investigation.</p>
    <p>In addition to aforementioned studies using either genomic data or pathological images, there is an increasing interest in combining both of them that may contribute to more accurate cancer prognosis prediction (<xref rid="btab185-B5" ref-type="bibr">Cheng <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btab185-B22" ref-type="bibr">Ning <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab185-B26" ref-type="bibr">Shao <italic toggle="yes">et al.</italic>, 2020</xref>). With the fast development of computer-aided technology, machine learning methods are widely applied to predict cancer prognosis. For example, by using SVM with a Gaussian radial basis kernel, <xref rid="btab185-B37" ref-type="bibr">Yuan et al. (2012)</xref> integrate both genomic data and pathological images to improve the performance of breast cancer prognosis prediction. <xref rid="btab185-B27" ref-type="bibr">Sun et al. (2018a</xref>,<xref rid="btab185-B28" ref-type="bibr">b</xref>) develop a multiple kernel learning approach named GPMKL, which employs heterogeneous features extracted from genomic data and pathological images. Besides breast cancer, <xref rid="btab185-B5" ref-type="bibr">Cheng et al. (2017)</xref> predict the survival outcomes of renal cell carcinoma patients by combining quantitative image features derived from pathological images and eigengenes derived from gene expression profiles. These studies clearly show that genomic data and pathological images are complementary to each other and can acquire better performance of prognosis of patients when employed together.</p>
    <p>Recently, deep learning-based approaches for the integration of data from different modalities have been proposed (<xref rid="btab185-B20" ref-type="bibr">Ngiam <italic toggle="yes">et al.</italic>, 2011</xref>) and successfully applied in cancer prognosis prediction (<xref rid="btab185-B1" ref-type="bibr">Cheerla and Gevaert, 2019</xref>; <xref rid="btab185-B18" ref-type="bibr">Mobadersany <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btab185-B34" ref-type="bibr">Yao <italic toggle="yes">et al.</italic>, 2017</xref>), which are highly flexible and can interpret the complexity of data in a non-linear manner (<xref rid="btab185-B16" ref-type="bibr">Huang <italic toggle="yes">et al.</italic>, 2019</xref>). For example, <xref rid="btab185-B18" ref-type="bibr">Mobadersany et al. (2018)</xref> develop a unified framework named genomic survival convolutional neural network (GSCNN) to predict time-to-event outcomes of patients diagnosed with glioma. To derive highly predictive prognostic features, GSCNN incorporate features embedded within pathological images with genomic features by feeding them into a multi-layer neural network that can add additional non-linear transformations to extracted features. Although the prediction accuracy of GSCNN exceeds human experts according to the current clinical standard, the direct combination of image and genomic features in GSCNN is suboptimal as it may ignore the intrinsic relationship of the features across different modalities (<xref rid="btab185-B25" ref-type="bibr">Shao <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btab185-B34" ref-type="bibr">Yao <italic toggle="yes">et al.</italic>, 2017</xref>). Alternatively, <xref rid="btab185-B34" ref-type="bibr">Yao et al. (2017)</xref> introduce an efficient correlation loss function for training deep learning-based survival model by explicitly maximizing the correlation among image and genomic features, and report quite promising performances for lung and brain cancer. Also, <xref rid="btab185-B1" ref-type="bibr">Cheerla and Gevaert (2019)</xref> exploit similarities between image and genomic features from the same patient via maximizing a cosine similarity-based loss function. The success of these studies highlights the importance of developing sophisticated deep learning-based methods for more accurate breast cancer prognosis prediction by efficiently leveraging both genomic data and pathological images.</p>
    <p>In this study, we propose a novel unified framework named genomic and pathological deep bilinear network (GPDBN) for integrating genomic data and pathological images to predict breast cancer prognosis. Our GPDBN framework includes an inter-modality bilinear feature encoding module (Inter-BFEM) and two intra-modality bilinear feature encoding modules (Intra-BFEMs) to achieve effective information between and within the genomic data and pathological images. Given pre-processed genomic data and pathological images, Inter-BFEM generates inter-modality bilinear features to model complex inter-modality relations for fully exploiting intrinsic relationship of the features across different modalities. In addition, Intra-BFEMs calculate the intra-modality bilinear features within each modality for capturing intra-modality relations. We argue that the intra-modality relations play an important role for breast cancer prognosis prediction, as they can provide valuable information complementary to that included in inter-modality relations (<xref rid="btab185-B11" ref-type="bibr">Gao <italic toggle="yes">et al.</italic>, 2019</xref>). Therefore, GPDBN further combines the intra- and inter-modality bilinear features by using a multi-layer deep neural network. The experimental results verify that our proposed GPDBN framework enhances the performance of breast cancer prognosis prediction and outperforms existing approaches using both genomic data and pathological images.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Dataset and pre-process</title>
      <p>Breast cancer patient samples adopted in this study include matched digital whole-slide images and gene expression profiles (totally 20 436 genes), which are acquired from the Cancer Genome Atlas (TCGA) data portal (<xref rid="btab185-B40" ref-type="bibr">Zhu <italic toggle="yes">et al.</italic>, 2014</xref>). By following previous work (<xref rid="btab185-B5" ref-type="bibr">Cheng <italic toggle="yes">et al.</italic>, 2017</xref>), patients with missing follow-up are excluded and finally 345 patients are enrolled in this study. These patients are further categorized into longer-term or shorter-term survivors by the criterion of 5-year survival (<xref rid="btab185-B13" ref-type="bibr">Gevaert <italic toggle="yes">et al.</italic>, 2006</xref>; <xref rid="btab185-B21" ref-type="bibr">Nguyen <italic toggle="yes">et al.</italic>, 2013</xref>). Accordingly, shorter-term survivors are labeled as 1 (i.e. poor prognosis) while the longer-term survivors are labeled as 0 (i.e. good prognosis). In consistent with previous prognosis prediction studies (<xref rid="btab185-B7" ref-type="bibr">Ching <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btab185-B16" ref-type="bibr">Huang <italic toggle="yes">et al.</italic>, 2019</xref>), 5-fold cross-validation is conducted on the dataset with 80% of the data in each fold used for training and 20% for testing.</p>
      <p>Similar to <xref rid="btab185-B9" ref-type="bibr">Ding et al. (2016)</xref>, the genes with missing values (NA) in more than 10% patients are deleted. After that, gene expression profiles with 19 006 genes are normalized with <italic toggle="yes">z</italic>-score by standardizing the distribution and then discretized into under-expression (-1), over-expression (1) and baseline (0) with <italic toggle="yes">z</italic>-score threshold of -1 and 1 by following <xref rid="btab185-B13" ref-type="bibr">Gevaert et al. (2006)</xref>. For pathological images, the whole-slide images with <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula>40 magnification are tiled into overlapping 1000 <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mo>×</mml:mo></mml:math></inline-formula> 1000 pixels by adopting bftools in the open microscopy environment (<xref rid="btab185-B27" ref-type="bibr">Sun <italic toggle="yes">et al.</italic>, 2018a</xref>,<xref rid="btab185-B28" ref-type="bibr">b</xref>; <xref rid="btab185-B35" ref-type="bibr">Yu <italic toggle="yes">et al.</italic>, 2016</xref>). Next, we select the 10 densest images of each image series ranked by image density defined as the percentage of non-white (all of the red, green and blue values are below 200 in the 24-bit RGB color space) pixels by following previous study (<xref rid="btab185-B35" ref-type="bibr">Yu <italic toggle="yes">et al.</italic>, 2016</xref>). Moreover, according to <xref rid="btab185-B27" ref-type="bibr">Sun et al. (2018a</xref>,<xref rid="btab185-B28" ref-type="bibr">b</xref>), by using CellProfiler we obtain 2343 image features from pathological images, which contain nucleus size, shape, distribution of pixel intensity and texture of nuclei. The aforementioned normalization and discretization steps are also applied to image features to mitigate the heterogeneity gap between different modalities. After that, the pre-processed genomic and pathological image features are fed into the FSelector package (<xref rid="btab185-B6" ref-type="bibr">Cheng <italic toggle="yes">et al.</italic>, 2012</xref>) implemented by R, respectively, which performs automatic feature selection from the input by ranking informative features. Finally, from the results of FSelector we choose the top 32 genomic and pathological image features, respectively, for further study (<xref rid="btab185-B27" ref-type="bibr">Sun <italic toggle="yes">et al.</italic>, 2018a</xref>,<xref rid="btab185-B28" ref-type="bibr">b</xref>; <xref rid="btab185-B35" ref-type="bibr">Yu <italic toggle="yes">et al.</italic>, 2016</xref>).</p>
    </sec>
    <sec>
      <title>2.2 GPDBN</title>
      <p>The workflow of GPDBN is shown in <xref rid="btab185-F1" ref-type="fig">Figure 1</xref>.Inter-modality bilinear features are firstly generated by Inter-BFEM to model complex inter-modality relations for fully exploiting intrinsic relationship of the features derived from genomic data and pathological images. Meanwhile, intra-modality bilinear features within each modality that are beneficial to prognosis prediction are also extracted by Intra-BFEMs. Finally, the intra- and inter-modality bilinear features are combined by multi-layer deep neural network to take advantage of the complementary information between intra-modality and inter-modality relations. The following subsections describe GPDBN in detail.</p>
      <fig position="float" id="btab185-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>Illustration of the proposed GPDBN framework</p>
        </caption>
        <graphic xlink:href="btab185f1" position="float"/>
      </fig>
      <sec>
        <title>2.2.1 Inter-BFEM</title>
        <p>Previous deep learning-based studies for cancer prognosis prediction have used direct feature combination (<xref rid="btab185-B18" ref-type="bibr">Mobadersany <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btab185-B16" ref-type="bibr">Huang <italic toggle="yes">et al.</italic>, 2019</xref>) or score level fusion (<xref rid="btab185-B24" ref-type="bibr">Sahasrabudhe <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab185-B27" ref-type="bibr">Sun <italic toggle="yes">et al.</italic>, 2018a</xref>,<xref rid="btab185-B28" ref-type="bibr">b</xref>) to integrate data from different modalities, which may not be sufficient to capture the complex inter-modality relations. To address this issue, in GPDBN we propose Inter-BFEM that is inspired by recent achievements in bilinear model (<xref rid="btab185-B15" ref-type="bibr">Hou <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btab185-B36" ref-type="bibr">Yu <italic toggle="yes">et al.</italic>, 2017</xref>). It is of note that this idea is similar to the recent work by <xref rid="btab185-B2" ref-type="bibr">Chen et al. (2020a</xref>,<xref rid="btab185-B3" ref-type="bibr">b</xref>) in modeling inter-modality relations between genomic data and pathological images for diagnosis and prognosis of glioma. Specifically, given <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mi>L</mml:mi></mml:math></inline-formula>-dimensional input vectors <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> derived from genomic data and pathological images, respectively, Inter-BFEM utilizes a bilinear function of <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">g</mml:mi></mml:math></inline-formula> and <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">p</mml:mi></mml:math></inline-formula> via fully connected layer followed by ReLU as the activation function. Accordingly, the <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mi>K</mml:mi></mml:math></inline-formula>-dimensional inter-modality bilinear feature <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">inter</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> generated by Inter-BFEM can be formulated as:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">inter</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="italic">ReLU</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula>where <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mi mathvariant="italic">vec</mml:mi><mml:mo>(</mml:mo><mml:mo>.</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula> is the vectorization operator that converts a matrix to a vector, <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">inter</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is the k-th value of <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">inter</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are learnable weight matrix and bias term of Inter-BFEM, respectively. Also, <xref rid="E1" ref-type="disp-formula">Eq. (1)</xref> can be written by the general form of bilinear model (<xref rid="btab185-B29" ref-type="bibr">Tenenbaum and Freeman, 2000</xref>):
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">inter</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi mathvariant="italic">ReLU</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ijk</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal"> </mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula>where <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ijk</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the entry of <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mi>i</mml:mi></mml:math></inline-formula>-th, <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mi>j</mml:mi></mml:math></inline-formula>-th value of <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">g</mml:mi></mml:math></inline-formula> and <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">p</mml:mi></mml:math></inline-formula>, respectively. Notice that the number of corresponding parameters of Inter-BFEM is <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>, which induce substantial computational burdens and require a huge amount of training data to fit when <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mi>K</mml:mi></mml:math></inline-formula> and <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mi>L</mml:mi></mml:math></inline-formula> are both large. Therefore, <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mi>K</mml:mi></mml:math></inline-formula> and <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mi>L</mml:mi></mml:math></inline-formula> are set to 20 and 32, respectively, in this study.</p>
      </sec>
      <sec>
        <title>2.2.2 Intra-BFEMs</title>
        <p>Besides inter-modality relations, there are also relations within each modality that may play a crucial role in prognosis prediction. For example, genomic data encompasses all ongoing biological processes in a cell or tissue, where multiple factors do not work independently of each other but are intertwined in a complex and entangled fashion (<xref rid="btab185-B2" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2020a</xref>,<xref rid="btab185-B3" ref-type="bibr">b</xref>). Therefore, in GPDBN two Intra-BFEMs are developed to capture intra-modality relations within genomic and image features, respectively. Specifically, intra-modality bilinear features denoted as <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">intra</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">intra</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> are extracted as follows:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">intra</mml:mi><mml:mo>_</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi mathvariant="italic">ReLU</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="italic">ReLU</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ijm</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal"> </mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:math></disp-formula>
 <disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">intra</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi mathvariant="italic">ReLU</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="italic">ReLU</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ijm</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal"> </mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:math></disp-formula>where <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> are the weight matrix and bias term of Intra-BFEM for genomic features, <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ijm</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is the entry of <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> are the weight matrix and bias term of Intra-BFEM for pathological image features, <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ijm</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is the entry of <inline-formula id="IE35"><mml:math id="IM35" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula id="IE36"><mml:math id="IM36" display="inline" overflow="scroll"><mml:mi>M</mml:mi></mml:math></inline-formula> is the dimension of intra-modality bilinear features and is set to 20 in this study. Accordingly, the output vectors of Intra-BFEMs for genomic and pathological image features are denoted as <inline-formula id="IE37"><mml:math id="IM37" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">intra</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">intra</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and can be defined as follows:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5" display="block" overflow="scroll"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="italic">intra</mml:mi><mml:mo>_</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mo>⊕</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">intra</mml:mi><mml:mo>_</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msup></mml:math></disp-formula>
 <disp-formula id="E6"><label>(6)</label><mml:math id="M6" display="block" overflow="scroll"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="italic">intra</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mo>⊕</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">intra</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math></disp-formula>where<inline-formula id="IE38"><mml:math id="IM38" display="inline" overflow="scroll"><mml:mo>⊕</mml:mo></mml:math></inline-formula>denotes concatenation of vectors.</p>
      </sec>
      <sec>
        <title>2.2.3 Prognosis prediction</title>
        <p>Considering the intra-modality relations are complementary to the inter-modality relations, GPDBN adopts a multi-layer deep neural network that takes combined features <inline-formula id="IE39"><mml:math id="IM39" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> as input to improve the predictive performance of breast cancer prognosis, and <inline-formula id="IE40"><mml:math id="IM40" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">h</mml:mi></mml:math></inline-formula> can be defined as follows:
<disp-formula id="E7"><label>(7)</label><mml:math id="M7" display="block" overflow="scroll"><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="italic">intra</mml:mi><mml:mo>_</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msup><mml:mo>⊕</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">inter</mml:mi></mml:mrow></mml:msup><mml:mo>⊕</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="italic">intra</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        <p>The multi-layer deep neural network consists of an input layer, <inline-formula id="IE41"><mml:math id="IM41" display="inline" overflow="scroll"><mml:mi>S</mml:mi></mml:math></inline-formula> fully connected layers and an output layer. The abstract features of the last fully connected layer are fed into the output layer to generate the final predictive scores of shorter-term and longer-term survivors. To realize the non-linear transformation, we employ ReLU and softmax as the activation functions for the fully connected layers and output layer, respectively. Mathematically, it can be described as follows:
<disp-formula id="E8"><label>(8)</label><mml:math id="M8" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="italic">ReLU</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula>
 <disp-formula id="E9"><label>(9)</label><mml:math id="M9" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="italic">ReLU</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mn>2</mml:mn><mml:mo>≤</mml:mo><mml:mi>s</mml:mi><mml:mo>≤</mml:mo><mml:mi>S</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula>
 <disp-formula id="E10"><label>(10)</label><mml:math id="M10" display="block" overflow="scroll"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="italic">softmax</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula>
 <disp-formula id="E11"><label>(11)</label><mml:math id="M11" display="block" overflow="scroll"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:mfenced></mml:math></disp-formula>where <inline-formula id="IE42"><mml:math id="IM42" display="inline" overflow="scroll"><mml:mi>S</mml:mi></mml:math></inline-formula> is set to 4, <inline-formula id="IE43"><mml:math id="IM43" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="IE44"><mml:math id="IM44" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> refer to parameter matrices, bias item and output of the <inline-formula id="IE45"><mml:math id="IM45" display="inline" overflow="scroll"><mml:mi>s</mml:mi></mml:math></inline-formula>-th fully connected layer, respectively, <inline-formula id="IE46"><mml:math id="IM46" display="inline" overflow="scroll"><mml:mi>y</mml:mi></mml:math></inline-formula> represents the corresponding prognosis label. The hyperparameters of the multi-layer deep neural network are listed in <xref rid="btab185-T1" ref-type="table">Table 1</xref>.</p>
        <table-wrap position="float" id="btab185-T1">
          <label>Table 1.</label>
          <caption>
            <p>Details of multi-layer deep neural network</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="(" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th rowspan="1" colspan="1">Layer</th>
                <th rowspan="1" colspan="1">Details</th>
                <th rowspan="1" colspan="1">Output size</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="10" colspan="1">Multi-layer deep neural network</td>
                <td rowspan="1" colspan="1">Input combined features</td>
                <td rowspan="1" colspan="1">concatenate the output of Inter-BFEM and Intra-BFEMs as input</td>
                <td rowspan="1" colspan="1">(124)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Fully connected layer (+ReLU)</td>
                <td rowspan="1" colspan="1">500 neurons</td>
                <td rowspan="1" colspan="1">(500)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Dropout layer</td>
                <td rowspan="1" colspan="1"><italic toggle="yes">P</italic> = 0.3</td>
                <td rowspan="1" colspan="1">(500)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Fully connected layer (+ReLU)</td>
                <td rowspan="1" colspan="1">256 neurons</td>
                <td rowspan="1" colspan="1">(256)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Dropout layer</td>
                <td rowspan="1" colspan="1"><italic toggle="yes">P</italic> = 0.3</td>
                <td rowspan="1" colspan="1">(256)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Fully connected layer (+ReLU)</td>
                <td rowspan="1" colspan="1">128 neurons</td>
                <td rowspan="1" colspan="1">(128)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Dropout layer</td>
                <td rowspan="1" colspan="1"><italic toggle="yes">P</italic> = 0.1</td>
                <td rowspan="1" colspan="1">(128)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Fully connected layer (+ReLU)</td>
                <td rowspan="1" colspan="1">32 neurons</td>
                <td rowspan="1" colspan="1">(32)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Dropout layer</td>
                <td rowspan="1" colspan="1"><italic toggle="yes">P</italic> = 0.1</td>
                <td rowspan="1" colspan="1">(32)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Softmax output unit</td>
                <td rowspan="1" colspan="1">2 neurons</td>
                <td rowspan="1" colspan="1">(2)</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
    </sec>
    <sec>
      <title>2.3 Training</title>
      <p>GPDBN is a unified learning framework and is trained to classify breast cancer patients into two classes: longer-term survivors or shorter-term survivors (<xref rid="btab185-B13" ref-type="bibr">Gevaert <italic toggle="yes">et al.</italic>, 2006</xref>; <xref rid="btab185-B21" ref-type="bibr">Nguyen <italic toggle="yes">et al.</italic>, 2013</xref>). Accordingly, Inter-BFEM, Intra-BFEMs and the multi-layer deep neural network in GPDBN are trained with a binary cross entropy objective function defined as follows:
<disp-formula id="E12"><label>(12)</label><mml:math id="M12" display="block" overflow="scroll"><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>]</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE47"><mml:math id="IM47" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> represent input vectors of the <inline-formula id="IE48"><mml:math id="IM48" display="inline" overflow="scroll"><mml:mi>n</mml:mi></mml:math></inline-formula>th training sample, <inline-formula id="IE49"><mml:math id="IM49" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> represents its corresponding longer-term or shorter-term class label and <inline-formula id="IE50"><mml:math id="IM50" display="inline" overflow="scroll"><mml:mi>N</mml:mi></mml:math></inline-formula> is the total number of patients in training set. The weights and biases of fully connected layers in Inter-BFEM, Intra-BFEMs and the multi-layer neural network in GPDBN are parameters to be estimated. We train the model with Adam optimizer that is a widely used stochastic gradient descent algorithm. Meanwhile, mini-batch training strategy is adopted in this study by randomly dividing small proportions of the training samples in each iteration to optimizer loops. We preset learning rate to 4e-4, the number of training epochs to 150 and the optimal value for the batch size adjusted on the cross-validation set is 16. We use Keras, a high-level neural network API written in python, to implement GPDBN under Linux with CPU Intel Xeon 4110 @ 2.10 GHz, GPU NVIDIA GeForce RTX 2080 Ti, and 192GB of RAM.</p>
    </sec>
    <sec>
      <title>2.4 Evaluation metrics</title>
      <p>To evaluate the performance of our proposed GPDBN, in this study we employ several commonly used metrics, containing sensitivity (Sn), specificity (Sp), overall accuracy (Acc), precision (Pre) and F1 scores. The detailed definitions are:
<disp-formula id="E13"><label>(13)</label><mml:math id="M13" display="block" overflow="scroll"><mml:mi>S</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula>
 <disp-formula id="E14"><label>(14)</label><mml:math id="M14" display="block" overflow="scroll"><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula>
 <disp-formula id="E15"><label>(15)</label><mml:math id="M15" display="block" overflow="scroll"><mml:mi mathvariant="italic">Acc</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula>
 <disp-formula id="E16"><label>(16)</label><mml:math id="M16" display="block" overflow="scroll"><mml:mi mathvariant="italic">Pre</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula>
 <disp-formula id="E17"><label>(17)</label><mml:math id="M17" display="block" overflow="scroll"><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="italic">Pre</mml:mi><mml:mo>×</mml:mo><mml:mi>S</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Pre</mml:mi><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula>where TP, TN, FP and FN refer to true positives, true negatives, false positives and false negatives, respectively. Furthermore, we apply area under the ROC curve (AUC) and concordance index (C-index) as described in <xref rid="btab185-B27" ref-type="bibr">Sun <italic toggle="yes">et al.</italic> (2018a</xref>,<xref rid="btab185-B28" ref-type="bibr">b</xref>) to assess the overall performance. C-index is widely applied to assess the performance of prognosis prediction methods, which quantifies the ranking quality of rankings and is valued from 0 to 1. C-index = 0.5 suggests the model makes ineffective prediction and a higher C-index &gt; 0.5 suggests a better prognosis method. All of the metrics are evaluated on the test splits of the 5-fold cross validation. Specifically, we concatenate the predictive scores from all of the test splits in the 5-fold cross validation and calculate the evaluation metrics and plot ROC curves.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Performance evaluation of GPDBN</title>
      <p>To verify the effectiveness of our proposed GPDBN in contributing to prognosis prediction of breast cancer, we conduct ablation study comparing different model configurations in 5-fold cross validation. First, we train four unimodal models to evaluate the performance of Intra-BFEMs: (i) BaselineG: a multi-layer deep neural network using genomic features as input, (ii) Intra-BFEMG: an Intra-BFEM using genomic features as input, followed by a multi-layer deep neural network, (iii) BaselineP: a multi-layer deep neural network using pathological image features as input, (iv) Intra-BFEMP: an Intra-BFEM using pathological image features as input, followed by a multi-layer deep neural network. After that, we train three multimodal models to evaluate the performance of Inter-BFEM: (i) BaselineGP: a multi-layer deep neural network using directly concatenated genomic and pathological image features as input, (ii) Inter-BFEM*: an Inter-BFEM using genomic and pathological image features as input, followed by a multi-layer deep neural network, (iii) GPDBN: our proposed deep bilinear network using genomic and pathological image features as input. It is of note that all aforementioned multi-layer deep neural networks have the same hyperparameters as described in <xref rid="btab185-T1" ref-type="table">Table 1</xref>.</p>
      <p><xref rid="btab185-T2" ref-type="table">Table 2</xref> shows the C-index and AUC values of different models. We can find that Intra-BFEM<sub>G</sub> and Intra-BFEM<sub>P</sub> achieve consistently better overall performance than Baseline<sub>G</sub> and Baseline<sub>P</sub>, respectively. For example, the C-index and AUC values of Intra-BFEM<sub>G</sub> are 0.695 and 0.779, while the corresponding C-index and AUC values of Baseline<sub>G</sub> are 0.674 and 0.734, respectively. It is of note that the average C-index and AUC values over 5-fold cross validation (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>) show similar performance. Besides, we plot ROC curves to compare the predictive performance in <xref rid="btab185-F2" ref-type="fig">Figure 2</xref>, which suggest that our proposed Intra-BFEMs have great efficiency on both genomic data and pathological images. Furthermore, we calculate the corresponding Sn, Acc, Pre and F1 of all compared methods with Sp equal to 90.0% or 95.0% and the histograms of these methods are shown in <xref rid="btab185-F3" ref-type="fig">Figure 3</xref>. From these results, we can see that when Sp is equal to 95.0%, by adopting Intra-BFEM Sn is improved by 6.2% and 5.1% on pathological images and genomic data, respectively. Meanwhile, compared with Baseline<sub>P</sub> and Baseline<sub>G</sub>, the Pre values of Intra-BFEM<sub>P</sub> and Intra-BFEM<sub>G</sub> have an improvement of 16.7% and 3.2% respectively. The above results verify the advantage of using Intra-BFEMs for prognosis prediction of breast cancer in our proposed GPDBN framework by capturing the intra-modality relations within genomic data and pathological images.</p>
      <fig position="float" id="btab185-F2">
        <label>Fig.2.</label>
        <caption>
          <p>ROC curves of GPDBN for breast cancer prognosis prediction</p>
        </caption>
        <graphic xlink:href="btab185f2" position="float"/>
      </fig>
      <fig position="float" id="btab185-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>The values of Sn, Acc, Pre and F1 of GPDBN for breast cancer prognosis prediction at stringent levels of Sp = 90.0% (left) and Sp = 95.0% (right)</p>
        </caption>
        <graphic xlink:href="btab185f3" position="float"/>
      </fig>
      <table-wrap position="float" id="btab185-T2">
        <label>Table 2.</label>
        <caption>
          <p>C-index and AUC values of GPDBN for breast cancer prognosis prediction</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Data</th>
              <th rowspan="1" colspan="1">Method</th>
              <th rowspan="1" colspan="1">C-index</th>
              <th rowspan="1" colspan="1">AUC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="2" colspan="1">Genomic data</td>
              <td rowspan="1" colspan="1">Baseline<sub>G</sub></td>
              <td rowspan="1" colspan="1">0.674</td>
              <td rowspan="1" colspan="1">0.734</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Intra-BFEM<sub>G</sub></td>
              <td rowspan="1" colspan="1">0.695</td>
              <td rowspan="1" colspan="1">0.779</td>
            </tr>
            <tr>
              <td rowspan="2" colspan="1">Pathological images</td>
              <td rowspan="1" colspan="1">Baseline<sub>P</sub></td>
              <td rowspan="1" colspan="1">0.569</td>
              <td rowspan="1" colspan="1">0.571</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Intra-BFEM<sub>P</sub></td>
              <td rowspan="1" colspan="1">0.578</td>
              <td rowspan="1" colspan="1">0.585</td>
            </tr>
            <tr>
              <td rowspan="3" colspan="1">Genomic data + Pathological images</td>
              <td rowspan="1" colspan="1">Baseline<sub>GP</sub></td>
              <td rowspan="1" colspan="1">0.703</td>
              <td rowspan="1" colspan="1">0.775</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Inter-BFEM*</td>
              <td rowspan="1" colspan="1">0.708</td>
              <td rowspan="1" colspan="1">0.793</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GPDBN</td>
              <td rowspan="1" colspan="1">
                <bold>0.723</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.817</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p>The bold values show the performances of proposed GPDBN method.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>As indicated in <xref rid="btab185-T2" ref-type="table">Table 2</xref>, Intra-BFEM<sub>G</sub> outperforms Intra-BFEM<sub>P</sub> with an improvement of more than 11.2% in C-index, which reflects the importance of genomic data for prognosis prediction. Although the prognostic performance on pathological images is worse than genomic data, we can also see its effect in improving performance when integrated with genomic data. For example, the C-index value of Baseline<sub>GP</sub> is 0.703, which has 13.4% and 2.9% improvement over Baseline<sub>P</sub> and Baseline<sub>G</sub>, respectively. Meanwhile, we also find that Inter-BFEM* can obtain higher C-index and AUC values than Baseline<sub>GP</sub>. For example, the AUC value achieved by Inter-BFEM* is 0.793, compared with 0.775 obtained by Baseline<sub>GP</sub>. This demonstrates the power of Inter-BFEM in capturing complex inter-modality relations for prognosis prediction. More importantly, by adopting Intra-BFEM and Inter-BFEM simultaneously, GPDBN obtains the best performance with remarkable improvements on both C-index and AUC values in comparison with other methods. For example, the C-index value achieved by GPDBN is 0.723, which has an improvement of 2.0% and 1.5% over Baseline<sub>GP</sub> and Inter-BFEM*, respectively. Furthermore, we calculate the Sn, Acc, Pre and F1 of all compared methods, and the histograms these methods are displayed in <xref rid="btab185-F4" ref-type="fig">Figure 4</xref>. From these results, we can find that GPDBN consistently yields the best performance on all metrics. Taken together, these results demonstrate that our proposed GPDBN can effectively integrate genomic data and pathological images by taking advantage of the complementary information between inter-modality and intra-modality relations obtained by Inter-BFEM and Intra-BFEM, respectively.</p>
      <fig position="float" id="btab185-F4">
        <label>Fig. 4.</label>
        <caption>
          <p>The values of Sn, Acc, Pre and F1 of different methods for breast cancer prognosis prediction at stringent levels of Sp = 90.0% (left) and Sp = 95.0% (right)</p>
        </caption>
        <graphic xlink:href="btab185f4" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.2 Comparison with existing methods</title>
      <p>To further evaluate the performance of our proposed GPDBN, we compare it with several existing methods, namely BoostCI, PCRM, superPC, En-Cox, LASSO-Cox, MDNNMD and DeepCorrSurv, and the C-index values of different methods are listed in <xref rid="btab185-T3" ref-type="table">Table 3</xref>. It is obvious that all these methods have satisfying performance by using genomic data. For example, the C-index values obtained by BoostCI and LASSO-Cox are 0.677 and 0.690, respectively, meanwhile deep learning-based MDNNMD and GPDBN have similar results of 0.691 and 0.695, respectively. These results further corroborate the conclusion that genomic data plays a crucial role in prognosis prediction of breast cancer. By using pathological images, we can find that deep learning-based approaches generally exhibit better performance than other methods evaluated in this study. For example, with a C-index value of 0.570, MDNNMD outperforms all other non-deep learning methods except superPC. At the same time, GPDBN achieves the best performance among all investigated methods and obtains a C-index improvement of 1.3% compared with MDNNMD, which indicates the effectiveness of GPDBN in breast cancer prognosis prediction.</p>
      <table-wrap position="float" id="btab185-T3">
        <label>Table 3.</label>
        <caption>
          <p>Performance comparison of the proposed GPDBN and other methods using C-index value</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">Genomic data</th>
              <th rowspan="1" colspan="1">Pathological images</th>
              <th rowspan="1" colspan="1">Genomic data + pathological images</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">BoostCI</td>
              <td rowspan="1" colspan="1">0.677</td>
              <td rowspan="1" colspan="1">0.435</td>
              <td rowspan="1" colspan="1">0.681</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PCRM</td>
              <td rowspan="1" colspan="1">0.569</td>
              <td rowspan="1" colspan="1">0.569</td>
              <td rowspan="1" colspan="1">0.688</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">superPC</td>
              <td rowspan="1" colspan="1">0.696</td>
              <td rowspan="1" colspan="1">0.573</td>
              <td rowspan="1" colspan="1">0.696</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">En-Cox</td>
              <td rowspan="1" colspan="1">0.688</td>
              <td rowspan="1" colspan="1">0.560</td>
              <td rowspan="1" colspan="1">0.697</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LASSO-Cox</td>
              <td rowspan="1" colspan="1">0.690</td>
              <td rowspan="1" colspan="1">0.564</td>
              <td rowspan="1" colspan="1">0.700</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MDNNMD</td>
              <td rowspan="1" colspan="1">0.691</td>
              <td rowspan="1" colspan="1">0.570</td>
              <td rowspan="1" colspan="1">0.704</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepCorrSurv</td>
              <td align="center" rowspan="1" colspan="1">NA</td>
              <td align="center" rowspan="1" colspan="1">NA</td>
              <td rowspan="1" colspan="1">0.694</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GPDBN</td>
              <td rowspan="1" colspan="1">0.695</td>
              <td rowspan="1" colspan="1">0.583</td>
              <td rowspan="1" colspan="1">0.723</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>From <xref rid="btab185-T3" ref-type="table">Table 3</xref>, we can also observe that compared with using single-modality data alone, the application of both genomic data and pathological images enhances the performance for most of the methods. For example, the C-index value of LASSO-Cox using both genomic data and pathological images is increased by 1.0% and 13.6% compared with those obtained by using only genomic data and pathological images, respectively. These suggest that pathological images can provide valuable predictive information additional to those provided by genomic data. Importantly, GPDBN outperforms all non-deep learning methods with remarkable improvement in C-index value, which indicates that sophisticated deep learning-based methods can be advantageous in integrating data from different modalities. At the same time, GPDBN achieves the highest C-index value of 0.723 among all the deep learning-based methods, which demonstrates that compared with MDNNMD and DeepCorrSurv, the proposed GPDBN can efficiently integrate genomic data and pathological images. In addition to C-index value, the ROC curves of different methods using both genomic data and pathological images are plotted in <xref rid="btab185-F5" ref-type="fig">Figure 5</xref>, in which GPDBN also achieves the best AUC value of 0.817 among all investigated methods. Taken together, these results suggest that our proposed GPDBN can take advantage of not only inter- but also intra-modality relations to significantly improve the performance of breast cancer prognosis prediction. Moreover, we conduct survival analysis on TCGA breast cancer dataset, in which GPDBN is supervised with the Cox objective function (<xref rid="btab185-B2" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2020a</xref>,<xref rid="btab185-B3" ref-type="bibr">b</xref>) and the detailed information is provided in <xref rid="sup1" ref-type="supplementary-material">Supplementary Methods</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>. As shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S3</xref>, we can find that the average C-index value of GPDBN compares favorably with Pathomic Fusion (<xref rid="btab185-B2" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2020a</xref>,<xref rid="btab185-B3" ref-type="bibr">b</xref>) for survival analysis.</p>
      <fig position="float" id="btab185-F5">
        <label>Fig. 5.</label>
        <caption>
          <p>ROC curves of GPDBN and other methods by employing both genomic data and pathological images</p>
        </caption>
        <graphic xlink:href="btab185f5" position="float"/>
      </fig>
      <p>To further evaluate the performance of GPDBN, Kaplan-Meier curves of different methods are plotted and displayed in <xref rid="btab185-F6" ref-type="fig">Figure 6</xref>. We can observe that for non-deep learning-based methods, LASSO-Cox provides better prognostic prediction with a log-rank test <inline-formula id="IE51"><mml:math id="IM51" display="inline" overflow="scroll"><mml:mi>P</mml:mi></mml:math></inline-formula> value of 6.117E-8 than other methods such as BoostCI (log-rank test <inline-formula id="IE52"><mml:math id="IM52" display="inline" overflow="scroll"><mml:mi>P</mml:mi></mml:math></inline-formula> value = 0.000010) and superPC (log-rank test <inline-formula id="IE53"><mml:math id="IM53" display="inline" overflow="scroll"><mml:mi>P</mml:mi></mml:math></inline-formula> value = 4.4754E-7). In comparison with these non-deep learning methods, deep learning-based DeepCorrSurv provides competitive performance (log-rank test <inline-formula id="IE54"><mml:math id="IM54" display="inline" overflow="scroll"><mml:mi>P</mml:mi></mml:math></inline-formula> value = 2.4049E-11) by considering intrinsic relationship of the features across different modalities in deep learning architecture. Moreover, by effectively integrating genomic data and pathological images, our proposed GPDBN can successfully distinguish shorter-term survivors from longer-term survivors and achieve superior performance (log-rank test <inline-formula id="IE55"><mml:math id="IM55" display="inline" overflow="scroll"><mml:mi>P</mml:mi></mml:math></inline-formula> value = 6.802E-15) over all other methods. In addition, Kaplan-Meier curves of GPDBN supervised with the Cox objective function and Pathomic Fusion are plotted in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figures S1 and S2</xref>, and GPDBN provides better survival prediction with a log-rank test <italic toggle="yes">P</italic> value of 2.0452E-10 than Pathomic Fusion (log-rank test <italic toggle="yes">P</italic> value = 0.000117). These results further confirm the effectiveness of GPDBN in breast cancer prognosis prediction.</p>
      <fig position="float" id="btab185-F6">
        <label>Fig. 6.</label>
        <caption>
          <p>Performance comparison of the proposed GPDBN and other methods using Kaplan–Meier curve</p>
        </caption>
        <graphic xlink:href="btab185f6" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.3 Visualization of features</title>
      <p>To visualize the superiority of the proposed method in prognosis prediction, a popular algorithm t-SNE is used to squeeze abstract features extracted by Intra-BFEM and GPDBFN into a 2-D space. As shown in <xref rid="btab185-F7" ref-type="fig">Figure 7</xref>, original genomic and pathological image features are overlapped in mixture, while combined genomic and pathological image features show separate trends, which suggests that the integration of data from different modalities can contribute to more accurate prognosis prediction of breast cancer. Furthermore, we can observe that compared with using original genomic and pathological image features, using abstract features extracted by Intra-BFEM shows an obvious improvement in separating shorter-term survivors and longer-term survivors, which become even more evident when using combined features extracted by GPDBFN. These results show that original genomic and pathological image features can be transformed into meaningful representations by Intra-BFEM, respectively, and GPDBFN can generate better combined representation with stronger discriminant power in distinguishing shorter-term and longer-term survivors.</p>
      <fig position="float" id="btab185-F7">
        <label>Fig. 7.</label>
        <caption>
          <p>Visualization of original genomic, pathological image, combined features and the corresponding abstract features extracted by our proposed method. The red star represents shorter-term survivors, and the blue dot represents longer-term survivors</p>
        </caption>
        <graphic xlink:href="btab185f7" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <p>In this study, we propose a novel deep learning-based GPDBN framework to effectively integrate both genomic data and pathological images for more accurate breast cancer prognosis prediction. Our findings suggest that prognosis prediction methods based on data from different modalities outperform those using single-modality data and our proposed Intra-BFEM and Inter-BFEM can efficiently capture complex relations not only within each modality but also between different modalities. Meanwhile, the comprehensive experimental results show that by using both intra- and inter-modality bilinear features as input, GPDBN compares favorably with many existing methods. Furthermore, the visualization results also indicate its powerful capability in distinguishing shorter-term survivors from longer-term survivors. The main contributions of this work are as follows: (i) we design Inter-BFEM to generate inter-modality bilinear features that can fully exploit intrinsic relationship of the features across different modalities, (ii) two efficient deep bilinear encoding modules named Intra-BFEMs are proposed to calculate intra-modality bilinear features for capturing relations within each modality that are also beneficial for breast cancer prognosis prediction, (iii) by taking advantage of the complementary information between intra-modality and inter-modality relations, the proposed GPDBN shows great strength in integrating data from different modalities, and achieves remarkable performance for predicting the prognosis of breast cancer.</p>
    <p>Although GPDBN has enhanced the predictive performance of breast cancer prognosis, there is still considerable room for further expansion and improvement. Firstly, limited by available genomic data and pathological images, this work can further improve prognostic performance by introducing more breast cancer patients. Secondly, other genomic data (e.g. copy number, miRNA expression) have also been reported to be beneficial for cancer prognosis prediction (<xref rid="btab185-B16" ref-type="bibr">Huang <italic toggle="yes">et al.</italic>, 2019</xref>). To integrate more modalities, computational approaches such as tensor fusion (<xref rid="btab185-B2" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2020a</xref>,<xref rid="btab185-B3" ref-type="bibr">b</xref>; <xref rid="btab185-B38" ref-type="bibr">Zadeh <italic toggle="yes">et al.</italic>, 2017</xref>) can be incorporated into the proposed GPDBN framework in future work. Meanwhile, other computationally efficient bilinear models (<xref rid="btab185-B10" ref-type="bibr">Fukui <italic toggle="yes">et al.</italic>, 2016</xref>; <xref rid="btab185-B12" ref-type="bibr">Gao <italic toggle="yes">et al.</italic>, 2016</xref>) can also be explored in the future to reduce memory consumption and computation times when more modalities are adopted. Thirdly, it is possible to further improve our framework by building a pancancer model for prognosis that also predicts prognosis of breast cancer (<xref rid="btab185-B1" ref-type="bibr">Cheerla and Gevaert, 2019</xref>). Finally, it is important to develop deep learning-based prognosis prediction method with improved interpretability (<xref rid="btab185-B17" ref-type="bibr">Ma <italic toggle="yes">et al.</italic>, 2018</xref>). In conclusion, we present a novel deep bilinear network for breast cancer prognosis prediction, which has the potential to be extended to integrate data from different modalities for other predictive tasks and provides clues for further cancer prognosis research.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the National Natural Science Foundation of China [61971393, 61871361, 61571414, 61471331].</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btab185_Supplementary_Data</label>
      <media xlink:href="btab185_supplementary_data.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btab185-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cheerla</surname><given-names>A.</given-names></string-name>, <string-name><surname>Gevaert</surname><given-names>O.</given-names></string-name></person-group> (<year>2019</year>) 
<article-title>Deep learning with multimodal representation for pancancer prognosis prediction</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>i446</fpage>–<lpage>i454</lpage>.<pub-id pub-id-type="pmid">31510656</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020a</year>) 
<article-title>Deep-learning approach to identifying cancer subtypes using high-dimensional genomic data</article-title>. <source>Bioinformatics</source>, <volume>36</volume>, <fpage>1476</fpage>–<lpage>1483</lpage>.<pub-id pub-id-type="pmid">31603461</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>R.J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020b</year>) 
<article-title>Pathomic fusion: an integrated framework for fusing histopathology and genomic features for cancer diagnosis and prognosis</article-title>. <source>IEEE Trans. Med. Imag</source>., <volume>99</volume>, <fpage>1</fpage>–<lpage>1</lpage>.</mixed-citation>
    </ref>
    <ref id="btab185-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cheng</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Identification of topological features in renal tumor microenvironment associated with patient survival</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>1024</fpage>–<lpage>1030</lpage>.<pub-id pub-id-type="pmid">29136101</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cheng</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) 
<article-title>Integrative analysis of histopathological images and genomic data predicts clear cell renal cell carcinoma prognosis</article-title>. <source>Cancer Res</source>., <volume>77</volume>, <fpage>e91</fpage>–<lpage>e100</lpage>.<pub-id pub-id-type="pmid">29092949</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cheng</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2012</year>) 
<article-title>FSelector: a Ruby gem for feature selection</article-title>. <source>Bioinformatics</source>, <volume>28</volume>, <fpage>2851</fpage>–<lpage>2852</lpage>.<pub-id pub-id-type="pmid">22942017</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ching</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Cox-nnet: an artificial neural network method for prognosis prediction of high-throughput omics data</article-title>. <source>PLoS Comput. Biol</source>., <volume>14</volume>, <fpage>e1006076</fpage>.<pub-id pub-id-type="pmid">29634719</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Courtiol</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>Deep learning-based classification of mesothelioma improves prediction of patient outcome</article-title>. <source>Nat. Med</source>., <volume>25</volume>, <fpage>1519</fpage>–<lpage>1525</lpage>.<pub-id pub-id-type="pmid">31591589</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ding</surname><given-names>Z.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) 
<article-title>Evaluating the molecule-based prediction of clinical drug responses in cancer</article-title>. <source>Bioinformatics</source>, <volume>32</volume>, <fpage>2891</fpage>–<lpage>2895</lpage>.<pub-id pub-id-type="pmid">27354694</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B10">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Fukui</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) Multimodal compact bilinear pooling for visual question answering and visual grounding. In <italic toggle="yes">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</italic>, pp. <fpage>457</fpage>–<lpage>468</lpage>. Austin, Texas, USA.</mixed-citation>
    </ref>
    <ref id="btab185-B11">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Gao</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) Dynamic fusion with intra-and inter-modality attention flow for visual question answering. In <italic toggle="yes">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>, pp. <fpage>6639</fpage>–<lpage>6648</lpage>. Long Beach, CA, USA.</mixed-citation>
    </ref>
    <ref id="btab185-B12">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Gao</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) Compact bilinear pooling. In <italic toggle="yes">Proceedings of the IEEE conference on computer vision and pattern recognition</italic>, pp. <fpage>317</fpage>–<lpage>326</lpage>. Las Vegas, NV, USA.</mixed-citation>
    </ref>
    <ref id="btab185-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gevaert</surname><given-names>O.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2006</year>) 
<article-title>Predicting the prognosis of breast cancer by integrating clinical and microarray data with Bayesian networks</article-title>. <source>Bioinformatics</source>, <volume>22</volume>, <fpage>e184</fpage>–<lpage>e190</lpage>.<pub-id pub-id-type="pmid">16873470</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hortobagyi</surname><given-names>G.N.</given-names></string-name></person-group>  <etal>et al</etal>; ABREAST Investigators. (<year>2005</year>) 
<article-title>The global breast cancer burden: variations in epidemiology and survival</article-title>. <source>Clin. Breast Cancer</source>, <volume>6</volume>, <fpage>391</fpage>–<lpage>401</lpage>.<pub-id pub-id-type="pmid">16381622</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hou</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>Deep multimodal multilinear fusion with high-order polynomial pooling</article-title>. <source>Adv. Neural Inf. Process. Syst</source>.,32, <fpage>12136</fpage>–<lpage>12145</lpage>.</mixed-citation>
    </ref>
    <ref id="btab185-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname><given-names>Z.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>SALMON: survival analysis learning with multi-omics neural networks on breast cancer</article-title>. <source>Front. Genet</source>., <volume>10</volume>, <fpage>166</fpage>.<pub-id pub-id-type="pmid">30906311</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ma</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Using deep learning to model the hierarchical structure and function of a cell</article-title>. <source>Nat. Methods</source>, <volume>15</volume>, <fpage>290</fpage>–<lpage>298</lpage>.<pub-id pub-id-type="pmid">29505029</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mobadersany</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Predicting cancer outcomes from histology and genomics using convolutional networks</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>115</volume>, <fpage>E2970</fpage>–<lpage>E2979</lpage>.<pub-id pub-id-type="pmid">29531073</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moon</surname><given-names>W.K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) 
<article-title>Computer-aided prediction of axillary lymph node status in breast cancer using tumor surrounding tissue features in ultrasound images</article-title>. <source>Comput. Methods Programs Biomed</source>., <volume>146</volume>, <fpage>143</fpage>–<lpage>150</lpage>.<pub-id pub-id-type="pmid">28688484</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B20">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ngiam</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2011</year>) Multimodal deep learning. In <italic toggle="yes">Proceedings of the 28th International Conference on Machine Learning</italic>, pp. <fpage>689</fpage>–<lpage>696</lpage>. Bellevue, WA, USA.</mixed-citation>
    </ref>
    <ref id="btab185-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nguyen</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2013</year>) Random forest classifier combined with feature selection for breast cancer diagnosis and prognostic. Journal of B<italic toggle="yes">iomedical Science &amp; Engineering</italic>, <volume>06</volume>(<issue>5</issue>), <fpage>551</fpage>–<lpage>560</lpage>.</mixed-citation>
    </ref>
    <ref id="btab185-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ning</surname><given-names>Z.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) 
<article-title>Integrative analysis of cross-modal features for the prognosis prediction of clear cell renal cell carcinoma</article-title>. <source>Bioinformatics</source>, <volume>36</volume>, <fpage>2888</fpage>–<lpage>2895</lpage>.<pub-id pub-id-type="pmid">31985775</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reis-Filho</surname><given-names>J.S.</given-names></string-name>, <string-name><surname>Pusztai</surname><given-names>L.</given-names></string-name></person-group> (<year>2011</year>) 
<article-title>Gene expression profiling in breast cancer: classification, prognostication, and prediction</article-title>. <source>Lancet</source>, <volume>378</volume>, <fpage>1812</fpage>–<lpage>1823</lpage>.<pub-id pub-id-type="pmid">22098854</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sahasrabudhe</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) 
<article-title>Deep multi-instance learning using multi-modal data for diagnosis of lymphocytosis</article-title>. <source>IEEE J. Biomed. Health Inf</source>., <volume>99</volume>, <fpage>1</fpage>–<lpage>1</lpage>.</mixed-citation>
    </ref>
    <ref id="btab185-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shao</surname><given-names>W.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>Integrative analysis of pathological images and multi-dimensional genomic data for early-stage cancer prognosis</article-title>. <source>IEEE Trans. Med. Imag</source>., <volume>39</volume>, <fpage>99</fpage>–<lpage>110</lpage>.</mixed-citation>
    </ref>
    <ref id="btab185-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shao</surname><given-names>W.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) 
<article-title>Multi-task multi-modal learning for joint diagnosis and prognosis of human cancers</article-title>. <source>Med. Image Anal</source>., <volume>65</volume>, <fpage>101795</fpage>.<pub-id pub-id-type="pmid">32745975</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sun</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018a</year>) 
<article-title>Integrating genomic data and pathological images to effectively predict breast cancer clinical outcome</article-title>. <source>Comput. Methods Programs Biomed</source>., <volume>161</volume>, <fpage>45</fpage>–<lpage>53</lpage>.<pub-id pub-id-type="pmid">29852967</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sun</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018b</year>) 
<article-title>A multimodal deep neural network for human breast cancer prognosis prediction by integrating multi-dimensional data</article-title>. <source>IEEE/ACM Trans. Comput. Biol. Bioinf</source>., <volume>16</volume>, <fpage>841</fpage>–<lpage>850</lpage>.</mixed-citation>
    </ref>
    <ref id="btab185-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tenenbaum</surname><given-names>J.B.</given-names></string-name>, <string-name><surname>Freeman</surname><given-names>W.T.</given-names></string-name></person-group> (<year>2000</year>) 
<article-title>Separating style and content with bilinear models</article-title>. <source>Neural Comput</source>., <volume>12</volume>, <fpage>1247</fpage>–<lpage>1283</lpage>.<pub-id pub-id-type="pmid">10935711</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van De Vijver</surname><given-names>M.J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2002</year>) 
<article-title>A gene-expression signature as a predictor of survival in breast cancer</article-title>. <source>N. Engl. J. Med</source>., <volume>347</volume>, <fpage>1999</fpage>–<lpage>2009</lpage>.<pub-id pub-id-type="pmid">12490681</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2005</year>) 
<article-title>Gene-expression profiles to predict distant metastasis of lymph-node-negative primary breast cancer</article-title>. <source>Lancet</source>, <volume>365</volume>, <fpage>671</fpage>–<lpage>679</lpage>.<pub-id pub-id-type="pmid">15721472</pub-id></mixed-citation>
    </ref>
    <ref id="btab185-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) 
<article-title>Stacked sparse autoencoder (SSAE) for nuclei detection on breast cancer histopathology images</article-title>. <source>IEEE Trans. Med. Imag</source>., <volume>35</volume>, <fpage>119</fpage>–<lpage>130</lpage>.</mixed-citation>
    </ref>
    <ref id="btab185-B33">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Xu</surname><given-names>X.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2012</year>) A gene signature for breast cancer prognosis using support vector machine. In: <italic toggle="yes">2012 5th International Conference on BioMedical Engineering and Informatics</italic>, pp. <fpage>928</fpage>–<lpage>931</lpage>. Chongqing, China.</mixed-citation>
    </ref>
    <ref id="btab185-B34">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Yao</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) Deep correlational learning for survival prediction from multi-modality data. In: <italic toggle="yes">International Conference on Medical Image Computing and Computer-Assisted Intervention</italic>, pp. <fpage>406</fpage>–<lpage>414</lpage>. Quebec City, QC, Canada.</mixed-citation>
    </ref>
    <ref id="btab185-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname><given-names>K.-H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) 
<article-title>Predicting non-small cell lung cancer prognosis by fully automated microscopic pathology image features</article-title>. <source>Nat. Commun</source>., <volume>7</volume>, <fpage>1</fpage>–<lpage>10</lpage>.</mixed-citation>
    </ref>
    <ref id="btab185-B36">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Yu</surname><given-names>Z.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) Multi-modal factorized bilinear pooling with co-attention learning for visual question answering. In <italic toggle="yes">Proceedings of the IEEE International Conference on Computer Vision</italic>, pp. <fpage>1821</fpage>–<lpage>1830</lpage>. Venice, Italy.</mixed-citation>
    </ref>
    <ref id="btab185-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yuan</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2012</year>) 
<article-title>Quantitative image analysis of cellular heterogeneity in breast tumors complements genomic profiling</article-title>. <source>Sci. Transl. Med</source>., <volume>4</volume>, <fpage>157ra143</fpage>–<lpage>157ra143</lpage>.</mixed-citation>
    </ref>
    <ref id="btab185-B38">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zadeh</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) Tensor fusion network for multimodal sentiment analysis. In <italic toggle="yes">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</italic>, pp. <fpage>1114</fpage>–<lpage>1125</lpage>. Copenhagen, Denmark.</mixed-citation>
    </ref>
    <ref id="btab185-B39">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zhu</surname><given-names>X.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) Deep convolutional neural network for survival analysis with pathological images. In: <italic toggle="yes">2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</italic>, pp. <fpage>544</fpage>–<lpage>547</lpage>. Shenzhen, China.</mixed-citation>
    </ref>
    <ref id="btab185-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhu</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) 
<article-title>TCGA-assembler: open-source software for retrieving and processing TCGA data</article-title>. <source>Nat. Methods</source>, <volume>11</volume>, <fpage>599</fpage>–<lpage>600</lpage>.<pub-id pub-id-type="pmid">24874569</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
