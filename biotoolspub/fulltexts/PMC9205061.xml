<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Genomics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Genomics</journal-id>
    <journal-title-group>
      <journal-title>BMC Genomics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2164</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9205061</article-id>
    <article-id pub-id-type="publisher-id">8648</article-id>
    <article-id pub-id-type="doi">10.1186/s12864-022-08648-9</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Sequence-based drug-target affinity prediction using weighted graph neural networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Jiang</surname>
          <given-names>Mingjian</given-names>
        </name>
        <address>
          <email>jiangmingjian@qut.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Shuang</given-names>
        </name>
        <address>
          <email>wangshuang@upc.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Shugang</given-names>
        </name>
        <address>
          <email>zhangshugang@hotmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhou</surname>
          <given-names>Wei</given-names>
        </name>
        <address>
          <email>zhouwei@qut.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Yuanyuan</given-names>
        </name>
        <address>
          <email>yyzhang1217@163.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Li</surname>
          <given-names>Zhen</given-names>
        </name>
        <address>
          <email>lizhen0130@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.412609.8</institution-id><institution-id institution-id-type="ISNI">0000 0000 8977 2197</institution-id><institution>School of Information and Control Engineering, </institution><institution>Qingdao University of Technology, </institution></institution-wrap>Qingdao, 266525 China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.497420.c</institution-id><institution-id institution-id-type="ISNI">0000 0004 1798 1132</institution-id><institution>College of Computer Science and Technology, </institution><institution>China University of Petroleum, </institution></institution-wrap>Qingdao, 266580 China </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.4422.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 2152 3263</institution-id><institution>College of Computer Science and Technology, </institution><institution>Ocean University of China, </institution></institution-wrap>Qingdao, 266100 China </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.410645.2</institution-id><institution-id institution-id-type="ISNI">0000 0001 0455 0905</institution-id><institution>College of Computer Science and Technology, </institution><institution>Qingdao University, </institution></institution-wrap>Qingdao, 266071 China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>17</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>17</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>23</volume>
    <elocation-id>449</elocation-id>
    <history>
      <date date-type="received">
        <day>24</day>
        <month>1</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>23</day>
        <month>5</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Affinity prediction between molecule and protein is an important step of virtual screening, which is usually called drug-target affinity (DTA) prediction. Its accuracy directly influences the progress of drug development. Sequence-based drug-target affinity prediction can predict the affinity according to protein sequence, which is fast and can be applied to large datasets. However, due to the lack of protein structure information, the accuracy needs to be improved.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">The proposed model which is called WGNN-DTA can be competent in drug-target affinity (DTA) and compound-protein interaction (CPI) prediction tasks. Various experiments are designed to verify the performance of the proposed method in different scenarios, which proves that WGNN-DTA has the advantages of simplicity and high accuracy. Moreover, because it does not need complex steps such as multiple sequence alignment (MSA), it has fast execution speed, and can be suitable for the screening of large databases.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par3">We construct protein and molecular graphs through sequence and SMILES that can effectively reflect their structures. To utilize the detail contact information of protein, graph neural network is used to extract features and predict the binding affinity based on the graphs, which is called weighted graph neural networks drug-target affinity predictor (WGNN-DTA). The proposed method has the advantages of simplicity and high accuracy.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Sequence representation</kwd>
      <kwd>Drug-protein affinity prediction</kwd>
      <kwd>Graph neural network</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100007129</institution-id>
            <institution>Natural Science Foundation of Shandong Province</institution>
          </institution-wrap>
        </funding-source>
        <award-id>ZR2021QF023</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100012226</institution-id>
            <institution>Fundamental Research Funds for the Central Universities</institution>
          </institution-wrap>
        </funding-source>
        <award-id>21CX06018A</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id>
            <institution>National Natural Science Foundation of China</institution>
          </institution-wrap>
        </funding-source>
        <award-id>61902430</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Shandong Key Science and Technology Innovation Project</institution>
        </funding-source>
        <award-id>2021CXGC011003</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par16">The drug-target affinity prediction is a key task in virtual screening, which has been studied for decades. The prediction can be used to determine whether the small molecule can bind to a target, which could be further applied to screen lead compounds to speed up drug research and development. Sequence-based method and structure-based method are two commonly used methods in drug-target affinity prediction. The difference is that whether the protein structure is provided or not. For a target with known structure, we can use molecular docking and molecular dynamics simulation to predict the binding conformation and binding strength between molecule and protein, which can obtain a relatively accurate result. There are several programs that can be implemented for molecular docking, such as DOCK [<xref ref-type="bibr" rid="CR1">1</xref>] and AutoDock [<xref ref-type="bibr" rid="CR2">2</xref>]. However, structure-based method has many limitations. On the one hand, molecular docking requires conformational searching, which is time-consuming, and it is expensive to screen a large database containing hundreds of millions of small molecules. On the other hand, with the development of proteomics, protein sequencing is very fast, but its structure is still difficult to obtain, which means that there are still many targets without structural information. Sequence is the only information for these targets that can be used to predict binding affinities with small molecules, which is called as sequence-based affinity prediction.</p>
    <p id="Par17">Sequence-based method has always been a research hotspot in computational biology. Various neural networks have been tried to extract the feature of protein sequence and molecule SMILES (simplified molecular input line entry specification) [<xref ref-type="bibr" rid="CR3">3</xref>]. Sequence-based method can be further divided into two types according to different demands, including compound-protein interaction (CPI) prediction and drug-target affinity (DTA) prediction. CPI prediction is a simplified DTA prediction, which is a binary classification task and could predict whether the drug can bind to the target. For example, TransformerCPI [<xref ref-type="bibr" rid="CR4">4</xref>] uses a transformer neural network [<xref ref-type="bibr" rid="CR5">5</xref>] to mine sequence information for the CPI prediction. Liu et al. [<xref ref-type="bibr" rid="CR6">6</xref>] builds up highly credible negative samples and combines various sources such as chemical expression profiles, sequences information and protein functional annotations into a systematic screening framework to get the CPI prediction. However, it is need to generate many profiles for the pre-processing, which is time consuming. DeepScreen [<xref ref-type="bibr" rid="CR7">7</xref>] is an individual predictor for a specific target using a deep convolutional neural network. NeoDTI [<xref ref-type="bibr" rid="CR8">8</xref>] integrates various sources from heterogeneous network data and uses topology-preserving representations of drugs and targets to implement interaction prediction. Hu et al. [<xref ref-type="bibr" rid="CR9">9</xref>] proposes a CNN-based method for drug-target interaction prediction, which takes 1D, 2D structural descriptors of drug and sequence of protein as the network inputs. To achieve a more accurate prediction, decision tree and kernel ridge regression [<xref ref-type="bibr" rid="CR10">10</xref>] are used for feature dimensionality reduction and ensemble learning.</p>
    <p id="Par18">Different from CPI prediction, DTA prediction can predict the detailed binding affinity between drug and target, it is usually a regression task and has aroused great interest in recent years. For instance, DeepDTA [<xref ref-type="bibr" rid="CR11">11</xref>] is made up of two convolutional neural networks (CNN), which are used to learn the latent vectors of protein sequences and drug SMILES respectively to predict their affinity. WideDTA [<xref ref-type="bibr" rid="CR12">12</xref>] improves DeepDTA by adding two extra CNNs to represent the additional protein domains and motifs (PDM) and ligand maximum common substructures (LMCS). DeepPurpose [<xref ref-type="bibr" rid="CR13">13</xref>] utilizes two encoders to represents the SMILES and sequence, which are composed of CNN, recurrent neural network (RNN) and Transformer. GANsDTA [<xref ref-type="bibr" rid="CR14">14</xref>] uses a semi-supervised generative adversarial networks (GAN) for feature extraction to predict binding affinity. Shim et al. [<xref ref-type="bibr" rid="CR15">15</xref>] proposes a method based on CNN, which involves the outer products between column vectors of two similarity matrices for the drugs and targets to predict the affinity. For molecule representation, molecular fingerprinting has always been an effective way, which includes extended connectivity fingerprints (ECFPs) [<xref ref-type="bibr" rid="CR16">16</xref>], atom-environment fingerprints (MOLPRINT2D) [<xref ref-type="bibr" rid="CR17">17</xref>] and molecular access system keys (MACCS) [<xref ref-type="bibr" rid="CR18">18</xref>]. Because the obtained fingerprinting is a vector composed of 0 and 1, it can be easily learned by neural networks. Moreover, MSTG [<xref ref-type="bibr" rid="CR19">19</xref>] utilizes a substructure tree to describe molecule, which is used for generating molecule in drug design and achieves a good performance.</p>
    <p id="Par19">The molecule could be easily represented using a graph, so graph neural network (GNN) [<xref ref-type="bibr" rid="CR20">20</xref>] is suitable for extracting feature of the molecule. GNN could obtain the local and global structural information by using neighbor node features to update feature. Through the transmission of multi-layer networks, the feature of the whole data can be extracted. It has been successfully applied in CPI and DTA prediction task. For example, Tsubaki et al. [<xref ref-type="bibr" rid="CR21">21</xref>] develops a novel CPI prediction method by combining graph neural network (GNN) and convolution neural network (CNN) for compounds and proteins representation respectively. GraphDTA [<xref ref-type="bibr" rid="CR22">22</xref>], MCN-CPI [<xref ref-type="bibr" rid="CR23">23</xref>] and PADME [<xref ref-type="bibr" rid="CR24">24</xref>] also construct graphs to describe molecules and apply GNN for the feature extraction in DTA prediction. The achievements of these methods demonstrate that the GNN could effectively characterize the small molecule.</p>
    <p id="Par20">Due to the simplicity of the molecule, most existing methods can effectively mine their structural information, but structural information contained in protein sequence is always ignored. Moreover, as we stated above, there are still many targets without structure. To obtain structural information from sequence, the commonly used method is protein structure prediction. After decades of development, the accuracy of protein structure prediction has gradually increased. Especially with the emergence of AlphaFold [<xref ref-type="bibr" rid="CR25">25</xref>], a breakthrough has been made in this field. Protein structure prediction usually needs to predict the interaction between different residue pairs, which is called contact map. It is a two-dimensional matrix, in which each element in the matrix represents the distance or interaction probability between residues. Because proteins are formed by the interaction of residues, contact map can reflect the spatial structure of the whole protein. Thus, if the contact map can be obtained according to the sequence in a fast way, its structural information can be obtained more quickly, which is useful for the affinity prediction.</p>
    <p id="Par21">To improve the performance of DTA and CPI predictions, we proposed a sequence-based method using weighted graph neural networks, the contributions of this paper are listed as below: 
<list list-type="bullet"><list-item><p id="Par22">Protein structure is determined by its residue interaction, which is caused by molecular forces. Due to the different distance between residues, the interactions are different. Only using a binary value in contact map is not accurate to depict the interaction between residues. A weighted protein graph construction method is proposed in this paper, which could provide more detailed information of the residue interaction.</p></list-item><list-item><p id="Par23">In our previous work, we proposed DGraphDTA [<xref ref-type="bibr" rid="CR26">26</xref>]model, which uses contact map to construct protein graph, and achieved good performance for DTA prediction. However, the MSA process used in DGraphDTA for structure prediction and feature generation is time-consuming. Protein structure prediction provides key information for CPI and DTA prediction, so how to select a fast and accurate method to improve the efficiency is another issue to be solved. In this work, the evolutionary scale modeling (ESM) [<xref ref-type="bibr" rid="CR27">27</xref>] is used, which could keep the accuracy rate stable while significantly increasing the computing speed. Moreover, to solve the sequence length limitation, a subsequence merging method is proposed.</p></list-item><list-item><p id="Par24">Finally, a weighted graph neural networks model that could be suitable for the proposed weighted protein graph is established. Different experiments show that the proposed method has high accuracy for CPI and DTA prediction and is faster than our previous method.</p></list-item></list></p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <p id="Par25">We propose a novel method for protein sequence representation using weighted graph, and a model is constructed based on it, which is called as WGNN-DTA. In order to comprehensively test the accuracy of the proposed WGNN-DTA, various experiments are designed, including CPI and DTA prediction performance validation, contact map efficiency comparison.</p>
    <sec id="Sec3">
      <title>Datasets</title>
      <p id="Par26">Based on a variety of designed experiments, several datasets are involved. The datasets are described in detail as follows.</p>
      <p id="Par27">Davis and KIBA datasets: Davis [<xref ref-type="bibr" rid="CR28">28</xref>] and KIBA [<xref ref-type="bibr" rid="CR29">29</xref>] datasets are used to verify the DTA prediction performance of WGNN-DTA. It is a benchmark for DTA prediction used in DeepDTA [<xref ref-type="bibr" rid="CR11">11</xref>] and is published. Davis dataset is obtained by screening some kinase proteins and their related inhibitors, and its binding affinity is the corresponding dissociation constant <italic>K</italic><sub><italic>d</italic></sub>. The KIBA dataset is constructed from kinase family proteins and their inhibitors, and the binding KIBA score is calculated based on different affinity (<italic>K</italic><sub><italic>i</italic></sub>,<italic>K</italic><sub><italic>d</italic></sub> and <italic>I</italic><italic>C</italic><sub>50</sub>). The detailed information of the two datasets shown in Table <xref rid="Tab1" ref-type="table">1</xref>. For Davis dataset, the affinities are processed in the same way of DeepDTA, which is calculated using Eq. (<xref rid="Equ1" ref-type="">1</xref>). 
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {pK}_{d}=-\log_{10}{\frac{K_{d}}{10^{9}}} \end{aligned}$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="italic">pK</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mo>log</mml:mo><mml:mn>10</mml:mn></mml:msub><mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:msup><mml:mn>10</mml:mn><mml:mn>9</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12864_2022_8648_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula><table-wrap id="Tab1"><label>Table 1</label><caption><p>Davis and KIBA datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Number of proteins</th><th align="left">Number of drugs</th><th align="left">Binding entries</th></tr></thead><tbody><tr><td align="left">Davis</td><td align="left">442</td><td align="left">68</td><td align="left">30056</td></tr><tr><td align="left">KIBA</td><td align="left">229</td><td align="left">2111</td><td align="left">118254</td></tr></tbody></table></table-wrap></p>
      <p id="Par28">Human and C.elegans datasets: the human and C.elegans of CPI datasets were created by Liu et al. [<xref ref-type="bibr" rid="CR6">6</xref>] in 2015 and used by Masashi et al. [<xref ref-type="bibr" rid="CR21">21</xref>] to verify the CPI prediction performance of their proposed method. The dataset includes highly credible negative samples of compound-protein pairs obtained by using a systematic screening framework. The positive samples of the dataset are obtained from two manually managed databases which are DrugBank [<xref ref-type="bibr" rid="CR30">30</xref>] and matador [<xref ref-type="bibr" rid="CR31">31</xref>]. The human dataset contains 3369 positive interactions between 1052 unique compounds and 852 unique proteins; The Caenorhabditis elegans (C.elegans) dataset contains 4000 positive interactions between 1434 unique compounds and 2504 unique proteins.</p>
      <p id="Par29">DUD-E dataset: DUD-E [<xref ref-type="bibr" rid="CR32">32</xref>] is designed for benchmark molecular docking programs by providing challenging decoys. There are 102 targets in the dataset, and each target is provided with several active molecules and decoys, which constitutes the CPI dataset of positive samples and negative samples. The dataset contains the structure information of proteins, so it is used in our work to test whether the proposed WGNN-DTA can be extended to the structure-based prediction.</p>
    </sec>
    <sec id="Sec4">
      <title>Performance of compound-protein interaction (CPI) prediction with WGNN-DTA</title>
      <p id="Par30">Compound-protein interaction (CPI) prediction is an important task of virtual screening. In order to verify the performance of WGNN-DTA proposed in our work, two CPI prediction experiments are introduced.</p>
      <p id="Par31">First, a five-fold cross validation experiment is implemented on two datasets including human and C.elegans datasets [<xref ref-type="bibr" rid="CR6">6</xref>], where every dataset is randomly divided into five parts with the same size and every part is used to verify the performance of the model trained by the other four parts in turn. In Masashi’s implementation [<xref ref-type="bibr" rid="CR21">21</xref>], they verified the performances for their method with a ratio of 1:1, 1:3 and 1:5 for positive and negative samples. We also adopted the same setting, and the same measures are used for the performance comparison, which consists of area under curve (AUC), precision and recall. Precision and recall are commonly used measures to evaluate the binary classification, which are calculated through the Eqs. <xref rid="Equ2" ref-type="">2</xref> and <xref rid="Equ3" ref-type="">3</xref>, where TP, FP and FN means the number of true positive predictions, false positive predictions and false negative predictions. In addition, F1-score is also involved to measure the proposed method which is calculated through Eq. <xref rid="Equ4" ref-type="">4</xref>. The used hyperparameters for our model are listed in Table <xref rid="Tab2" ref-type="table">2</xref> and the five-fold cross validation results for the two datasets are shown in Figs. <xref rid="Fig1" ref-type="fig">1</xref>, <xref rid="Fig2" ref-type="fig">2</xref>, <xref rid="Fig3" ref-type="fig">3</xref>, <xref rid="Fig4" ref-type="fig">4</xref>. The black line in the histogram indicates the standard deviation. 
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Precision=\frac{TP}{TP+FP} \end{aligned}$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12864_2022_8648_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><fig id="Fig1"><label>Fig. 1</label><caption><p>Five-fold cross validation of CPI prediction performance on human dataset with GCN implementation</p></caption><graphic xlink:href="12864_2022_8648_Fig1_HTML" id="MO1"/></fig><fig id="Fig2"><label>Fig. 2</label><caption><p>Five-fold cross validation of CPI prediction performance on human dataset with GAT implementation</p></caption><graphic xlink:href="12864_2022_8648_Fig2_HTML" id="MO2"/></fig><fig id="Fig3"><label>Fig. 3</label><caption><p>Five-fold cross validation of CPI prediction performance on C.elegans dataset with GCN implementation</p></caption><graphic xlink:href="12864_2022_8648_Fig3_HTML" id="MO3"/></fig><fig id="Fig4"><label>Fig. 4</label><caption><p>Five-fold cross validation of CPI prediction performance on C.elegans dataset with GAT implementation</p></caption><graphic xlink:href="12864_2022_8648_Fig4_HTML" id="MO4"/></fig><table-wrap id="Tab2"><label>Table 2</label><caption><p>Hyperparameters used in the experiment</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Number</th><th align="left">Name</th><th align="left">Setting</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">The number of GNN layers</td><td align="left">3</td></tr><tr><td align="left">2</td><td align="left">The type of GNN pooling layer</td><td align="left">Global mean pooling</td></tr><tr><td align="left">3</td><td align="left">Activation function</td><td align="left">Sigmoid</td></tr><tr><td align="left">4</td><td align="left">Optimizer</td><td align="left">Adam</td></tr><tr><td align="left">5</td><td align="left">Learning rate</td><td align="left">0.001</td></tr><tr><td align="left">6</td><td align="left">Loss function</td><td align="left">Binary cross entropy loss</td></tr><tr><td align="left">7</td><td align="left">Epochs</td><td align="left">1000</td></tr><tr><td align="left">8</td><td align="left">Batch size</td><td align="left">512</td></tr></tbody></table></table-wrap></p>
      <p id="Par32">
        <disp-formula id="Equ3">
          <label>3</label>
          <alternatives>
            <tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Recall=\frac{TP}{TP+FN} \end{aligned}$$\end{document}</tex-math>
            <mml:math id="M6" display="block">
              <mml:mrow>
                <mml:mtable>
                  <mml:mtr>
                    <mml:mtd columnalign="right">
                      <mml:mrow>
                        <mml:mi>R</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>c</mml:mi>
                        <mml:mi>a</mml:mi>
                        <mml:mi>l</mml:mi>
                        <mml:mi>l</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mfrac>
                          <mml:mrow>
                            <mml:mi mathvariant="italic">TP</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>T</mml:mi>
                            <mml:mi>P</mml:mi>
                            <mml:mo>+</mml:mo>
                            <mml:mi>F</mml:mi>
                            <mml:mi>N</mml:mi>
                          </mml:mrow>
                        </mml:mfrac>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="12864_2022_8648_Article_Equ3.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p id="Par33">
        <disp-formula id="Equ4">
          <label>4</label>
          <alternatives>
            <tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} F1-score=\frac{2\times(Precision+Recall)}{Precision \times Recall} \end{aligned}$$\end{document}</tex-math>
            <mml:math id="M8" display="block">
              <mml:mrow>
                <mml:mtable>
                  <mml:mtr>
                    <mml:mtd columnalign="right">
                      <mml:mrow>
                        <mml:mi>F</mml:mi>
                        <mml:mn>1</mml:mn>
                        <mml:mo>-</mml:mo>
                        <mml:mi>s</mml:mi>
                        <mml:mi>c</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mfrac>
                          <mml:mrow>
                            <mml:mn>2</mml:mn>
                            <mml:mo>×</mml:mo>
                            <mml:mo stretchy="false">(</mml:mo>
                            <mml:mi>P</mml:mi>
                            <mml:mi>r</mml:mi>
                            <mml:mi>e</mml:mi>
                            <mml:mi>c</mml:mi>
                            <mml:mi>i</mml:mi>
                            <mml:mi>s</mml:mi>
                            <mml:mi>i</mml:mi>
                            <mml:mi>o</mml:mi>
                            <mml:mi>n</mml:mi>
                            <mml:mo>+</mml:mo>
                            <mml:mi>R</mml:mi>
                            <mml:mi>e</mml:mi>
                            <mml:mi>c</mml:mi>
                            <mml:mi>a</mml:mi>
                            <mml:mi>l</mml:mi>
                            <mml:mi>l</mml:mi>
                            <mml:mo stretchy="false">)</mml:mo>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>P</mml:mi>
                            <mml:mi>r</mml:mi>
                            <mml:mi>e</mml:mi>
                            <mml:mi>c</mml:mi>
                            <mml:mi>i</mml:mi>
                            <mml:mi>s</mml:mi>
                            <mml:mi>i</mml:mi>
                            <mml:mi>o</mml:mi>
                            <mml:mi>n</mml:mi>
                            <mml:mo>×</mml:mo>
                            <mml:mi>R</mml:mi>
                            <mml:mi>e</mml:mi>
                            <mml:mi>c</mml:mi>
                            <mml:mi>a</mml:mi>
                            <mml:mi>l</mml:mi>
                            <mml:mi>l</mml:mi>
                          </mml:mrow>
                        </mml:mfrac>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="12864_2022_8648_Article_Equ4.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p id="Par34">The four figures illustrate that the performance of WGNN-DTA achieve a high performance whether using GCN or using GAT for the model construction, and the AUC measure on two datasets are all above 0.9 in the five-fold cross validation. When the ratio of positive and negative samples is 1:1, the performance is the best. With the proportion of negative samples increases, the performance decreases, and the standard deviation also increases gradually, which means that the stability of the model decreases. This is because unbalanced data will lead to poor fitting of the model. But even the ratio is 1:5, WGNN-DTA could still have a good capability, where the AUC measure reaches 0.969 and 0.985 with GCN implementation and reaches 0.972 and 0.989 with GAT implementation on the two datasets.</p>
      <p id="Par35">To further validate the performance of WGNN-DTA, Masashi’s method [<xref ref-type="bibr" rid="CR21">21</xref>] is involved for comparison. The same CPI datasets as Masashi are used, and the same dataset division is implemented, in which the dataset is randomly divided into training set, validation set and test set with a proportion of 0.8, 0.1 and 0.1. The dataset settings with ratios of 1:1, 1:3 and 1:5 for positive and negative samples are also adopted, and the same measures are used for the performance comparison, which consists of AUC, recall and precision. In addition, F1-score measure is introduced. We compare WGNN-DTA with Masashi’s method and other traditional machine learning methods including k-NN, random forest (RF), L2 logistic (L2) and support vector machine (SVM), experimental results of which are referenced from the Masashi’s paper [<xref ref-type="bibr" rid="CR21">21</xref>]. The comparison results are shown in Figs. <xref rid="Fig5" ref-type="fig">5</xref> and <xref rid="Fig6" ref-type="fig">6</xref>.
<fig id="Fig5"><label>Fig. 5</label><caption><p>CPI prediction performance on human dataset</p></caption><graphic xlink:href="12864_2022_8648_Fig5_HTML" id="MO5"/></fig><fig id="Fig6"><label>Fig. 6</label><caption><p>CPI prediction performance on C.elegans dataset</p></caption><graphic xlink:href="12864_2022_8648_Fig6_HTML" id="MO6"/></fig></p>
      <p id="Par36">It is illustrated from the figures that the performance of WGNN-DTA has a high accuracy on two CPI datasets. For the human dataset, WGNN-DTA achieve a better prediction on balanced data with AUC scores of 0.986 and 0.995, and the prediction performance declines on unbalanced data. But for C.elegans dataset, whether GCN or GAT is used, WGNN-DTA has a superior prediction performance on all ratios, which is better than other methods. The AUC scores are both more than 0.990 on balanced and unbalanced data with GAT implementation.</p>
      <p id="Par37">So it can be concluded from the experimental results that the performance of WGNN-DTA for CPI prediction is superior to other methods. This is because the structural information of proteins and small molecules are fully represented by constructing weighted protein graph and molecular graph, and through the mining of the GNN model, the factors affecting the binding can be accurately taken into account, which leads to a better prediction performance.</p>
    </sec>
    <sec id="Sec5">
      <title>Performance of drug-target affinity (DTA) prediction with WGNN-DTA</title>
      <p id="Par38">Compared with CPI prediction, the drug-target affinity (DTA) prediction is more complex. CPI prediction only needs to determine whether a molecule can bind the protein or not, while DTA prediction needs to predict the detailed binding affinity.</p>
      <p id="Par39">Similar to CPI experiment, a five-fold cross validation experiment is firstly implemented on Davis [<xref ref-type="bibr" rid="CR28">28</xref>] and KIBA [<xref ref-type="bibr" rid="CR29">29</xref>] datasets to test the stability of the model. The published datasets have already divide each dataset into five parts and one test part randomly, and the five parts in training set are used for the validation in the experiment. Mean square error (MSE), concordance index (CI) and the extra Pearson correlation coefficient (Pearson) are introduced for performance measurement, where a smaller MSE or a higher CI and Pearson of the results means a better performance of the model. The used hyperparameters are listed in Table <xref rid="Tab3" ref-type="table">3</xref> and the validation results are shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref>. It is shown that WGNN-DTA with GCN implementation performs well on KIBA dataset, which could reach 0.149 with MSE measurement, the performance of GCN implementation and GAT implementation is almost the same on Davis dataset with a MSE of 0.214 and 0.215. In addition, the standard deviation is small, which means that WGNN-DTA has good stability on the two datasets.
<fig id="Fig7"><label>Fig. 7</label><caption><p>Five-fold cross validation of DTA prediction performance with WGNN-DTA on Davis and KIBA datasets</p></caption><graphic xlink:href="12864_2022_8648_Fig7_HTML" id="MO7"/></fig><table-wrap id="Tab3"><label>Table 3</label><caption><p>Some hyperparameters used in the experiment</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Number</th><th align="left">Name</th><th align="left">Setting</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">The number of GNN layers</td><td align="left">3</td></tr><tr><td align="left">2</td><td align="left">The type of GNN pooling layer</td><td align="left">Global mean pooling</td></tr><tr><td align="left">3</td><td align="left">Activation function</td><td align="left">Not used</td></tr><tr><td align="left">4</td><td align="left">optimizer</td><td align="left">Adam</td></tr><tr><td align="left">5</td><td align="left">Learning rate</td><td align="left">0.001</td></tr><tr><td align="left">6</td><td align="left">Loss function</td><td align="left">Mean squared error loss</td></tr><tr><td align="left">7</td><td align="left">Epochs</td><td align="left">2000</td></tr><tr><td align="left">8</td><td align="left">Batch size</td><td align="left">512</td></tr></tbody></table></table-wrap></p>
      <p id="Par40">To comprehensively evaluate the performance of the proposed method on DTA prediction, we compare our work with DeepDTA [<xref ref-type="bibr" rid="CR11">11</xref>], GraphDTA [<xref ref-type="bibr" rid="CR22">22</xref>] and GANsDTA [<xref ref-type="bibr" rid="CR14">14</xref>] using the same benchmark datasets including Davis [<xref ref-type="bibr" rid="CR28">28</xref>] and KIBA [<xref ref-type="bibr" rid="CR29">29</xref>] datasets. The same training set and test set are implemented, as well as the same performance measures are introduced for evaluation. All other method prediction results are referenced from their papers. The performance comparison is illustrated in Figs. <xref rid="Fig8" ref-type="fig">8</xref> and <xref rid="Fig9" ref-type="fig">9</xref>.
<fig id="Fig8"><label>Fig. 8</label><caption><p>Comparison of DTA prediction performance on Davis dataset</p></caption><graphic xlink:href="12864_2022_8648_Fig8_HTML" id="MO8"/></fig><fig id="Fig9"><label>Fig. 9</label><caption><p>Comparison of DTA prediction performance on KIBA dataset</p></caption><graphic xlink:href="12864_2022_8648_Fig9_HTML" id="MO9"/></fig></p>
      <p id="Par41">Compared with other methods, the performance of DTA prediction is improved with WGNN-DTA. No matter using GCN or GAT, the proposed method has excellent performance with all measures used for evaluation. WGNN-DTA could reach 0.208 and 0.130 on Davis and KIBA datasets with MSE measure, which is smaller than other methods and suggests a better performance. In addition, the CI score of the predictions for WGNN-DTA is also higher than other methods, which indicates the comprehensively excellent performance of the model. Moreover, the Pearson correlation coefficient measure used in WideDTA is calculated to evaluate WGNN-DTA, which is illustrated in Table <xref rid="Tab4" ref-type="table">4</xref>. It can be seen from the table that WGNN-DTA performs well with Pearson correlation coefficient measure evaluation.
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Performance of DTA prediction with Pearson correlation coefficient measure on two datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">Davis</th><th align="left">KIBA</th></tr></thead><tbody><tr><td align="left">WideDTA</td><td align="left">0.820</td><td align="left">0.856</td></tr><tr><td align="left">WGNN-DTA (GCN)</td><td align="left">0.862</td><td align="left">0.891</td></tr><tr><td align="left">WGNN-DTA (GAT)</td><td align="left"><bold>0.863</bold></td><td align="left"><bold>0.900</bold></td></tr></tbody></table><table-wrap-foot><p>The values in boldface represent the best prediction performances with the corresponding measures</p></table-wrap-foot></table-wrap></p>
      <p id="Par42">Molecular SMILES and protein sequence contain a wealth of structural information, especially for proteins, which include the function and binding site information. By constructing the graphs for proteins and drugs, these features can be encoded effectively, and the representations further mined by the GNN model. The hidden structural and binding sources can be fully obtained, which plays an important role in the final affinity prediction.</p>
    </sec>
    <sec id="Sec6">
      <title>Performance of edge weights</title>
      <p id="Par43">WGNN-DTA constructs a weighted protein graph based on the contact probability. It could show the interaction between different residues more accurately, and could comprehensively describe the protein structure. To demonstrate whether the added edge weights improve the prediction performance or not, the protein graphs are constructed with and without weights, and the performances of different graph construction methods are compared. In the experiment, CPI and DTA prediction are all involved, and Human [<xref ref-type="bibr" rid="CR6">6</xref>] balanced (positive samples: negative samples= 1:1) dataset and Davis [<xref ref-type="bibr" rid="CR28">28</xref>] dataset are used respectively. The prediction results are shown in Tables <xref rid="Tab5" ref-type="table">5</xref> and <xref rid="Tab6" ref-type="table">6</xref>.
<table-wrap id="Tab5"><label>Table 5</label><caption><p>Performance of CPI prediction with and without edge weights on human dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">AUC</th><th align="left">Precision</th><th align="left">Recall</th></tr></thead><tbody><tr><td align="left">WGNN-DTA (GCN)</td><td align="left">0.986</td><td align="left">0.972</td><td align="left">0.960</td></tr><tr><td align="left">WGNN-DTA (GAT)</td><td align="left"><bold>0.995</bold></td><td align="left"><bold>0.980</bold></td><td align="left"><bold>0.970</bold></td></tr><tr><td align="left">Without Weight (GCN)</td><td align="left">0.967</td><td align="left">0.927</td><td align="left">0.869</td></tr><tr><td align="left">Without Weight (GAT)</td><td align="left">0.970</td><td align="left">0.930</td><td align="left">0.877</td></tr></tbody></table><table-wrap-foot><p>The values in boldface represent the best prediction performances with the corresponding measures</p></table-wrap-foot></table-wrap><table-wrap id="Tab6"><label>Table 6</label><caption><p>Performance of DTA prediction with and without edge weights on Davis dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">AUC</th><th align="left">Precision</th><th align="left">Recall</th></tr></thead><tbody><tr><td align="left">WGNN-DTA (GCN)</td><td align="left"><bold>0.208</bold></td><td align="left"><bold>0.902</bold></td><td align="left">0.862</td></tr><tr><td align="left">WGNN-DTA (GAT)</td><td align="left"><bold>0.208</bold></td><td align="left">0.898</td><td align="left"><bold>0.863</bold></td></tr><tr><td align="left">Without Weight (GCN)</td><td align="left">0.215</td><td align="left">0.893</td><td align="left">0.856</td></tr><tr><td align="left">Without Weight (GAT)</td><td align="left">0.224</td><td align="left">0.892</td><td align="left">0.850</td></tr></tbody></table><table-wrap-foot><p>The values in boldface represent the best prediction performances with the corresponding measures</p></table-wrap-foot></table-wrap></p>
      <p id="Par44">It is illustrated that the introduction of edge weights improved the prediction performance for both CPI and DTA prediction. In the folding structure of protein, even if the residues can generate an interaction, the interaction strength may be different, which is a reflection of molecular forces such as hydrogen bond. The closer the residues are, the more likely they can interact with each other. WGNN-DTA takes the probability prediction of interaction as the edge weight of the constructed protein graph, which can take the interaction strengths between different residues into account, thus it could represent the protein structure more comprehensively and thoroughly, and improves the binding prediction performance with small molecule.</p>
    </sec>
    <sec id="Sec7">
      <title>Performance of contact map prediction</title>
      <p id="Par45">The first step in the proposed method is to predict contact map according to the corresponding sequence, then the weighted protein graph can be constructed based on it. The performance of the contact map prediction will greatly influence the accuracy and efficiency of CPI and DTA prediction. Therefore, another experiment is set up to verify the contact map prediction performance and the speed of WGNN-DTA.</p>
      <p id="Par46">In our previous work, our proposed DGraphDTA uses PconsC4 [<xref ref-type="bibr" rid="CR33">33</xref>] for the contact map prediction, which needs to implement the MSA for every protein at first. To compare the accuracy of WGNN-DTA and our previous MSA-based method in DTA prediction, the test set of Davis dataset is introduced for the comparison. The predictions of the two methods are run on a server with a single core mode of an Intel(R) Xeon(R) E5-2620 CPU and a GTX 1080Ti GPU. The performance of DGraphDTA and WGNN-DTA are shown in Table <xref rid="Tab7" ref-type="table">7</xref>.
<table-wrap id="Tab7"><label>Table 7</label><caption><p>Performance comparison of WGNN-DTA with DGraphDTA</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">MSE</th><th align="left">CI</th><th align="left">Pearson</th></tr></thead><tbody><tr><td align="left">DGraphDTA</td><td align="left"><bold>0.202</bold></td><td align="left"><bold>0.904</bold></td><td align="left"><bold>0.867</bold></td></tr><tr><td align="left">WGNN-DTA (GCN)</td><td align="left">0.208</td><td align="left">0.902</td><td align="left">0.862</td></tr></tbody></table><table-wrap-foot><p>The values in boldface represent the best prediction performances with the corresponding measures</p></table-wrap-foot></table-wrap></p>
      <p id="Par47">It can be seen from the table that the performance degradation of the proposed method is limited, which achieve an MSE of 0.208. Therefore, although WGNN-DTA removes the complex MSA processing, the constructed protein graph could still describe the structural information of protein effectively, which results in a high accuracy.</p>
      <p id="Par48">Moreover, two other methods are introduced for the speed comparison, which includes DeepDTA and GraphDTA. The prediction time of the two methods together with WGNN-DTA and DGraphDTA are calculated for the test set. The time includes the time of data processing and model prediction. The results are illustrated in Table <xref rid="Tab8" ref-type="table">8</xref>.
<table-wrap id="Tab8"><label>Table 8</label><caption><p>Speed comparison of WGNN-DTA with other methods</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">Total time (min)</th></tr></thead><tbody><tr><td align="left">DGraphDTA</td><td align="left">15900</td></tr><tr><td align="left">WGNN-DTA (GCN)</td><td align="left">50</td></tr><tr><td align="left">DeepDTA</td><td align="left"><bold>0.5</bold></td></tr><tr><td align="left">GrapDTA</td><td align="left">2.5</td></tr></tbody></table><table-wrap-foot><p>The values in boldface represent the best prediction performances with the corresponding measures</p></table-wrap-foot></table-wrap></p>
      <p id="Par49">It can be concluded that DeepDTA has a faster speed for the prediction. DeepDTA utilizes CNN directly to the SMILES and sequence, and there is no other additional process for the prediction. GraphDTA also encodes sequence using CNN, but it introduces molecular graph processing, which takes a little more time. WGNN-DTA construct graphs for both molecule and protein, especially when constructing weighted protein graph, the ESM model is implemented for contact map prediction, which takes up most of the time. Obtaining structural information requires more calculations, so WGNN-DTA takes more time on this to improve the accuracy. But even so, for the whole test set formed by more than 400 proteins, it only takes a total of 50 minutes. Compared with the previous DGraphDTA, it improves the efficiency greatly. MSA processing have to generate a list of similar sequences for the target, and the database searching is a large time consuming process. The proposed WGNN-DTA no longer needs MSA profiles as input, which saves a lot of time and still maintain high accuracy. Thus, WGNN-DTA could improves DTA prediction efficiency.</p>
    </sec>
    <sec id="Sec8">
      <title>Performance of contact map prediction</title>
      <p id="Par50">From the above experiments, it can be seen that the proposed WGNN-DTA has high performance for the sequence-based DTA and CPI prediction. In order to verify whether the proposed method can be extended to the structure-based prediction, an additional experiment is implemented on data with structural information, which could obtain their contact map directly. We construct the corresponding weighted protein graph based on its real contact map with a threshold of 8Å, and use the distance between different pair of residues as the edge weights. The WGNN-DTA is used for feature extraction to predict the binding interaction. In this experiment, DUD-E [<xref ref-type="bibr" rid="CR32">32</xref>] is introduced for the validation. We select 200 active molecules and decoys for each protein, which yield total of around 35000 molecule-protein pairs, random 60% proteins (61 proteins) and their binding molecules and decoys are used as the training set and the remain 40% proteins (41 proteins) are used as the test set. The detailed experimental results are shown in Table <xref rid="Tab9" ref-type="table">9</xref>.
<table-wrap id="Tab9"><label>Table 9</label><caption><p>CPI prediction performance on DUD-E dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">Protein rep.</th><th align="left">Compound rep.</th><th align="left">AUC</th><th align="left">Precision</th><th align="left">Recall</th></tr></thead><tbody><tr><td align="left">WGNN-DTA (GCN)</td><td align="left">GCN</td><td align="left">GCN</td><td align="left">0.963</td><td align="left">0.949</td><td align="left">0.789</td></tr><tr><td align="left">WGNN-DTA (GAT)</td><td align="left">GAT</td><td align="left">GAT</td><td align="left">0.962</td><td align="left">0.896</td><td align="left">0.867</td></tr></tbody></table></table-wrap></p>
      <p id="Par51">It is illustrated that the when WGNN-DTA applying to structure-based CPI prediction, the performance still remains quite better, which can achieve 0.963 and 0.962 AUC scores with GCN and GAT. It is shown that the constructed weighted protein graph and molecular graph can indeed represent the binding information and structural information of proteins and small molecules.</p>
    </sec>
  </sec>
  <sec id="Sec9" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par52">In order to improve the performance of DTA prediction, the WGNN-DTA method is proposed in this paper, which could well characterize small molecule and protein sequence by constructing molecular graph and weighted protein graph based on contact map. By using GNN for further feature extraction, the obtained latent vectors can represent proteins and molecules in a more sufficient way. Moreover, the introduction of contact map prediction method, ESA model, which removes the complex multiple sequence alignment process, improves the efficiency of the method. So the proposed method could be applied to virtual screening of large datasets.</p>
    <p id="Par53">With the accumulation of actual biological data, many relevant databases used for simulation and validation have been published. With the help of deep learning to assist analysis, more accurate simulation can be realized. At present, there are many research fields and many excellent cases have emerged. In our work, a novel weight graph is constructed to represent the protein sequence, and the graph neural network is used to construct the model to realize the predictions of DTA and CPI. The corresponding graphs of small molecule SMILES and protein sequence constructed according to the proposed method can be input into the model in practical application, then the binding prediction result could be obtained, which provides a powerful mean for the virtual screening of target proteins and assists the discovery of lead compounds.</p>
    <p id="Par54">Based on various experiments, it is demonstrated that WGNN-DTA can not only be applied to DTA and CPI prediction, but also has good performance for the prediction extended to the structure-based prediction.</p>
  </sec>
  <sec id="Sec10">
    <title>Method</title>
    <sec id="Sec11">
      <title>WGNN-DTA architecture</title>
      <p id="Par55">The process of the proposed WGNN-DTA is shown in Fig. <xref rid="Fig10" ref-type="fig">10</xref>. The input are protein sequence and molecule SMILES. A weighted protein graph is constructed based on the contact map, which could comprehensively describe the protein structure. Instead of binary value used in DGraphDTA, the interaction probability is used as edge weight, which could provide more detailed structural information. In addition, molecular graph is introduced to describe molecule, the atoms are used as node and bonds are used as edge. Finally, a method called weighted GNN drug-target affinity predictor (WGNN-DTA) is proposed, which uses two types of GNNs including graph convolutional network (GCN) [<xref ref-type="bibr" rid="CR34">34</xref>] and graph attention network (GAT) [<xref ref-type="bibr" rid="CR35">35</xref>] to extract the latent vectors of protein and molecular graphs, and the affinity prediction can be achieved base on the latent vectors. Some node features that can be quickly generated are selected to improve the performance of the method.
<fig id="Fig10"><label>Fig. 10</label><caption><p>The process of WGNN-DTA for affinity prediction</p></caption><graphic xlink:href="12864_2022_8648_Fig10_HTML" id="MO10"/></fig></p>
    </sec>
    <sec id="Sec12">
      <title>Weighted protein graph and latent vector extraction</title>
      <p id="Par56">The first important step is to construct weighted protein graph, which will directly influence the accuracy of the prediction. Protein sequence is a string composed of about 20 symbols, but it contains rich evolutionary and structure information. How to extract the information from the sequence is the key factor to promote the prediction accuracy.</p>
      <p id="Par57">Contact map is the result of protein structure prediction. There are many structure prediction methods, such as AlphaFold [<xref ref-type="bibr" rid="CR25">25</xref>]. However, most existing structure prediction methods need to carry out sequence alignment by scanning the sequence database, which spends lots of time and greatly weaken the efficiency of affinity prediction. Therefore, the evolutionary scale modeling (ESM) model proposed by Rao et al. [<xref ref-type="bibr" rid="CR27">27</xref>] is involved in our work for contact map prediction, which could obtain relatively accurate results without sequence alignment, and the prediction of a protein sequence can be completed in less than a minute. In addition, we verified that the contact prediction was relatively accurate for DTA prediction, and the detailed validation is described in the experiment part.</p>
      <p id="Par58">After obtaining the contact map, the weighted graph for protein is constructed. The obtained contact map is a probability matrix, which indicates the interaction probability of different residue pairs, and the range of probability is [0,1]. Normally, a threshold is used to convert the probability matrix to a binary matrix to indicate whether two residues are connected or not. The weighted protein graph is constructed according to contact map with residue as node and interaction as edge, the value of 0.5 is set as the threshold which means there will be an edge between residues with interaction probability exceeding 0.5. Different with the protein graph proposed in our previous DGraphDTA, we do not only use binary matrix as edge, but also use the probability value as the edge weight. The detailed construction process is shown in Fig. <xref rid="Fig11" ref-type="fig">11</xref>, the intensity of the contact map indicating the weight of the corresponding edge.
<fig id="Fig11"><label>Fig. 11</label><caption><p>Construction of weighted protein graph</p></caption><graphic xlink:href="12864_2022_8648_Fig11_HTML" id="MO11"/></fig></p>
      <p id="Par59">In the constructed weighted protein graph, the residue is used as the node, so it is need to describe the features for different residue nodes. The selected residue features are shown in Table <xref rid="Tab10" ref-type="table">10</xref>. Due to the different R group of residues, the residues show different properties, and the properties further influence their interactions. It is noted that in order to speed up the processing, we remove the position-specific scoring matrix (PSSM) profile introduced in our previous work, because the PSSM needs to be calculated based on MSA processing.
<table-wrap id="Tab10"><label>Table 10</label><caption><p>Residue node feature</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature name</th><th align="left">Feature description</th><th align="left">Dimension</th></tr></thead><tbody><tr><td align="left">Residue type</td><td align="left">One-hot encoding of the residue</td><td align="left">21</td></tr><tr><td align="left">Residue aliphatic</td><td align="left">Whether the residue is aliphatic</td><td align="left">1</td></tr><tr><td align="left">Residue aromatic</td><td align="left">Whether the residue is aromatic</td><td align="left">1</td></tr><tr><td align="left">Residue polar</td><td align="left">Whether the residue is polar neutral</td><td align="left">1</td></tr><tr><td align="left">Residue Acidic polar</td><td align="left">Whether the residue is acidic charged</td><td align="left">1</td></tr><tr><td align="left">Residue basic polar</td><td align="left">Whether the residue is basic charged</td><td align="left">1</td></tr><tr><td align="left">Residue weight</td><td align="left">The molecular weight of the residue</td><td align="left">1</td></tr><tr><td align="left">−<italic>C</italic><italic>O</italic><italic>O</italic><italic>H</italic> property</td><td align="left">Dissociation constant for the −<italic>C</italic><italic>O</italic><italic>O</italic><italic>H</italic> group [<xref ref-type="bibr" rid="CR36">36</xref>]</td><td align="left">1</td></tr><tr><td align="left">−<italic>N</italic><italic>H</italic><sub>3</sub> property</td><td align="left">Dissociation constant for the −<italic>N</italic><italic>H</italic><sub>3</sub> group [<xref ref-type="bibr" rid="CR36">36</xref>]</td><td align="left">1</td></tr><tr><td align="left">Other groups property</td><td align="left">Dissociation constant for any other group in the molecule [<xref ref-type="bibr" rid="CR36">36</xref>]</td><td align="left">1</td></tr><tr><td align="left">Residue isoelectric</td><td align="left">The pH at the isoelectric point [<xref ref-type="bibr" rid="CR36">36</xref>]</td><td align="left">1</td></tr><tr><td align="left">Hydrophobicity 1</td><td align="left">Hydrophobicity of residue (pH = 2) [<xref ref-type="bibr" rid="CR37">37</xref>]</td><td align="left">1</td></tr><tr><td align="left">Hydrophobicity 2</td><td align="left">Hydrophobicity of residue (pH = 7) [<xref ref-type="bibr" rid="CR37">37</xref>]</td><td align="left">1</td></tr><tr><td align="left">All</td><td align="left">All features of the residue</td><td align="left">33</td></tr></tbody></table></table-wrap></p>
      <p id="Par60">The length of input sequence of ESM model is required to be within 1024, so it is need to find a way to deal with longer sequences. It is well-known that the close residues in the sequence usually generate an interaction, the value on the diagonal of contact map is what we focus on. Thus, based on the above observations, for proteins with long sequence (more than 1000), we use intercepting and splicing to reconstruct the contact map, and the process is shown in Fig. <xref rid="Fig12" ref-type="fig">12</xref>. The whole sequence is cut into multiple subsequences, the length and step of which is set as L (set as 1000 in our work) and L/2. The contact map of each subsequence is calculated in turn, and the final contact map is reconstructed according to them. For the overlapping part of the subsequence, the contact map is obtained by averaging them (blue area in Fig. <xref rid="Fig12" ref-type="fig">12</xref>). Through experiment verification, we find that the contact map obtained by this method can still maintain high accuracy.
<fig id="Fig12"><label>Fig. 12</label><caption><p>Intercepting and splicing of the contact map prediction for large proteins (with a sequence length over 1000)</p></caption><graphic xlink:href="12864_2022_8648_Fig12_HTML" id="MO12"/></fig></p>
      <p id="Par61">In addition, since adjacent residues are connected by peptide bonds in principle, the value of any adjacent residues is set to 1, and a self-loop is also added, which means each residue connect with itself.</p>
      <p id="Par62">The weighted protein graph can initially represent the proteins, so a neural network needs to be established to deeply mine the latent feature. Graph neural network (GNN) is applied to extract the features of the constructed graph in WGNN-DTA. GNN is a deep learning model gradually used in recent years. The traditional convolution neural network can only extract the features of Euclidean structure data with fixed sizes, which limits its application. But GNN could handle non-Euclidean structure data such as graph, which ensure it could be widely used. The commonly used GNN models include graph convolution neural network (GCN) and graph attention network (GAT), which is also introduced to establish the proposed WGNN-DTA.</p>
      <p id="Par63">For GCN, each layer will execute the following equation: 
<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} H^{l+1}=f\left(H^{l},A\right)=\sigma\left(\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}H^{l}W^{l+1}\right) \end{aligned}$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:msup><mml:mi>H</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:msup><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:msup><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mi>H</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12864_2022_8648_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par64">where <italic>A</italic> is the adjacency matrix of the graph with shape of (<italic>n</italic>,<italic>n</italic>), and <italic>n</italic> is the number of nodes of the graph. <inline-formula id="IEq1"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat {A}=A+I$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12864_2022_8648_Article_IEq1.gif"/></alternatives></inline-formula>, and <italic>I</italic> is the identity matrix. <inline-formula id="IEq2"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat {D}$$\end{document}</tex-math><mml:math id="M14"><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12864_2022_8648_Article_IEq2.gif"/></alternatives></inline-formula> is the degree matrix calculated according to <italic>A</italic>, which has the same shape as <italic>A</italic>. <italic>W</italic><sup><italic>l</italic>+1</sup> is the weight matrix of layer <italic>l</italic>+1, and it can be learned during training. <italic>H</italic><sup><italic>l</italic></sup> is the output of the last layer with a shape of (<italic>n</italic>,<italic>F</italic><sup><italic>l</italic></sup>). <italic>F</italic><sup><italic>l</italic></sup> is the number of output channels of layer <italic>l</italic>, and <italic>H</italic><sup>0</sup>=<italic>X</italic>, <italic>H</italic><sup>0</sup>=<italic>X</italic> is the input feature matrix of the graph node.</p>
      <p id="Par65">The node features of each layer for GAT are calculated as: 
<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} h_{i}=\sigma\left(\sum\limits_{j\in N(i)}\alpha_{ij}{WX}_{j}\right) \end{aligned}$$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="italic">WX</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12864_2022_8648_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par66">
        <disp-formula id="Equ7">
          <label>7</label>
          <alternatives>
            <tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \alpha_{ij}=\frac{e^{a\left(h_{i},h_{j}\right)}}{\sum\limits_{k\in N(i)}a(h_{k}h_{i})} \end{aligned}$$\end{document}</tex-math>
            <mml:math id="M18" display="block">
              <mml:mrow>
                <mml:mtable>
                  <mml:mtr>
                    <mml:mtd columnalign="right">
                      <mml:mrow>
                        <mml:msub>
                          <mml:mi>α</mml:mi>
                          <mml:mrow>
                            <mml:mi mathvariant="italic">ij</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mo>=</mml:mo>
                        <mml:mfrac>
                          <mml:msup>
                            <mml:mi>e</mml:mi>
                            <mml:mrow>
                              <mml:mi>a</mml:mi>
                              <mml:mfenced close=")" open="(">
                                <mml:msub>
                                  <mml:mi>h</mml:mi>
                                  <mml:mi>i</mml:mi>
                                </mml:msub>
                                <mml:mo>,</mml:mo>
                                <mml:msub>
                                  <mml:mi>h</mml:mi>
                                  <mml:mi>j</mml:mi>
                                </mml:msub>
                              </mml:mfenced>
                            </mml:mrow>
                          </mml:msup>
                          <mml:mrow>
                            <mml:munder>
                              <mml:mo movablelimits="false">∑</mml:mo>
                              <mml:mrow>
                                <mml:mi>k</mml:mi>
                                <mml:mo>∈</mml:mo>
                                <mml:mi>N</mml:mi>
                                <mml:mo stretchy="false">(</mml:mo>
                                <mml:mi>i</mml:mi>
                                <mml:mo stretchy="false">)</mml:mo>
                              </mml:mrow>
                            </mml:munder>
                            <mml:mi>a</mml:mi>
                            <mml:mrow>
                              <mml:mo stretchy="false">(</mml:mo>
                              <mml:msub>
                                <mml:mi>h</mml:mi>
                                <mml:mi>k</mml:mi>
                              </mml:msub>
                              <mml:msub>
                                <mml:mi>h</mml:mi>
                                <mml:mi>i</mml:mi>
                              </mml:msub>
                              <mml:mo stretchy="false">)</mml:mo>
                            </mml:mrow>
                          </mml:mrow>
                        </mml:mfrac>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="12864_2022_8648_Article_Equ7.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p id="Par67">where, <italic>N</italic>(<italic>i</italic>) is the set of neighbor nodes of node <italic>i</italic>, <italic>W</italic> is the weight matrix, and <italic>X</italic><sub><italic>j</italic></sub> is the input feature matrix of node <italic>j</italic>, <italic>α</italic><sub><italic>ij</italic></sub> is the attention coefficient after regularization through Eq. (<xref rid="Equ7" ref-type="">7</xref>). <italic>a</italic>(<italic>x</italic>) is a mapping function <inline-formula id="IEq3"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R^{F^{l}}\times R^{F^{l}}\rightarrow R$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:msup><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:msup></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:msup><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:msup></mml:msup><mml:mo stretchy="false">→</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12864_2022_8648_Article_IEq3.gif"/></alternatives></inline-formula>, which can calculate the non-regularization coefficients of a pair of node <italic>i</italic> and <italic>j</italic>.</p>
      <p id="Par68">The detailed WGNN-DTA structure is shown in Fig. <xref rid="Fig13" ref-type="fig">13</xref>. Because the contact probability is used as the edge weight in the construction of weighted protein graph, weighed GNN is used to extract protein features. In implementation, when using GAT to establish WGNN-DTA, it is difficult for GAT to involve the weights, so we first use a layer of GCN to obtain the weight information of the graph, and then use GAT layers for the further construction of the model.
<fig id="Fig13"><label>Fig. 13</label><caption><p>The detail neural network using for weighted protein graph feature extraction in WGNN-DTA</p></caption><graphic xlink:href="12864_2022_8648_Fig13_HTML" id="MO13"/></fig></p>
      <p id="Par69">There is rich structure information in protein sequence, which determines its function and contains the mechanism of drug-target binding. The protein structure is then determined by the residue interactions, and the proposed weighted protein graph can present the residue interaction in the way of edge weight, which effectively shows the hidden structure information of protein. At the same time, the properties of residue will be represented in the form of node features, which can achieve a more accurate and comprehensive sequence representation.</p>
    </sec>
    <sec id="Sec13">
      <title>Molecular graph and latent vector extraction</title>
      <p id="Par70">In sequence-based affinity prediction, molecule is usually represented by SMILES, which could convert the original atoms and covalent bonds into an ASCII character sequence. It is easy to restore SMILES back to the form of molecular structure with common molecular processing softwares such as RDKit [<xref ref-type="bibr" rid="CR38">38</xref>]. Then the molecular graph is constructed with atom as node and chemical bond as edge. More specifically, when constructing the molecular graph, all atoms in the molecule are set as graph nodes, and if there is a bond between two atoms, then an edge will be added between the corresponding atom nodes. The construction process is shown in Fig. <xref rid="Fig14" ref-type="fig">14</xref>. Moreover, in the same way as the construction of protein graph, self-loop is also introduced when constructing molecular graph, in which each atom node is connected to itself.
<fig id="Fig14"><label>Fig. 14</label><caption><p>Construction of molecular graph</p></caption><graphic xlink:href="12864_2022_8648_Fig14_HTML" id="MO14"/></fig></p>
      <p id="Par71">Similar to protein graph construction, in addition to nodes and edges, the features of atom nodes should be described to distinguish them. So it is necessary to determine the node feature for different atoms. Atoms are the smallest particles in chemical reaction. Due to their different sizes and charges, different atoms show different chemical properties. The selected features of atom node are shown in Table <xref rid="Tab11" ref-type="table">11</xref>. By characterizing the features of different atom nodes, the chemical properties and binding properties of small molecule can be expressed more comprehensively. Therefore, the factors influencing the binding of molecule is involved, which can promote the prediction performance.
<table-wrap id="Tab11"><label>Table 11</label><caption><p>Atom node feature</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature name</th><th align="left">Feature description</th><th align="left">Dimension</th></tr></thead><tbody><tr><td align="left">Atom type</td><td align="justify">One-hot encoding of the atom</td><td align="left">44</td></tr><tr><td align="left">Atom neighbors</td><td align="justify">One-hot encoding of the degree of the atom in the molecule, which is the number of directly-bonded neighbors</td><td align="left">11</td></tr><tr><td align="left">Number of hydrogens</td><td align="justify">One-hot encoding of the total number of H bound to the atom</td><td align="left">11</td></tr><tr><td align="left">Number of implicit hydrogens</td><td align="justify">One-hot encoding of the number of implicit H bound to the atom</td><td align="left">11</td></tr><tr><td align="left">All</td><td align="justify">All features of the atom</td><td align="left">78</td></tr></tbody></table></table-wrap></p>
      <p id="Par72">GCN or GAT is also utilized to extract features after the molecular graph is constructed, and processed by global pooling, molecules with different sizes are extracted into latent vectors with the same size. The connections and atom features will eventually influence the molecule properties and its binding with the target protein.</p>
      <p id="Par73">After feature extraction of GNN, the latent vectors of the weighted protein graph and molecular graph are obtained. Then the two latent vectors are concatenated and further extracted by the neural network to realize the final binding prediction, which is illustrated in Fig. <xref rid="Fig15" ref-type="fig">15</xref>. Whether it is used for CPI or DTA prediction, WGNN-DTA can be competent. The only difference is whether to add a sigmoid activation function to the output.
<fig id="Fig15"><label>Fig. 15</label><caption><p>Affinity and interaction prediction in WGNN-DTA</p></caption><graphic xlink:href="12864_2022_8648_Fig15_HTML" id="MO15"/></fig></p>
      <p id="Par74">At the same time, because the contact map prediction does not need sequence alignment and the generation of node feature can be quickly calculated by sequences, the prediction is fast and easy to be implemented. Thus, the proposed WGNN-DTA is suitable for the virtual screening of large databases, which improves the efficiency of sequence-based DTA prediction.</p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>DTA</term>
        <def>
          <p id="Par4">Drug-target affinity</p>
        </def>
      </def-item>
      <def-item>
        <term>CPI</term>
        <def>
          <p id="Par5">Compound-protein interaction</p>
        </def>
      </def-item>
      <def-item>
        <term>CNN</term>
        <def>
          <p id="Par6">Convolutional neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>GNN</term>
        <def>
          <p id="Par7">Graph neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>WGNN-DTA</term>
        <def>
          <p id="Par8">Weighted graph neural work drug-target affinity predictor</p>
        </def>
      </def-item>
      <def-item>
        <term>GCN</term>
        <def>
          <p id="Par9">Graph convolutional network</p>
        </def>
      </def-item>
      <def-item>
        <term>GAT</term>
        <def>
          <p id="Par10">Graph attention network</p>
        </def>
      </def-item>
      <def-item>
        <term>SMILES</term>
        <def>
          <p id="Par11">Simplified molecular input line entry specification</p>
        </def>
      </def-item>
      <def-item>
        <term>AUC</term>
        <def>
          <p id="Par12">Area under curve</p>
        </def>
      </def-item>
      <def-item>
        <term>MSE</term>
        <def>
          <p id="Par13">Mean square error</p>
        </def>
      </def-item>
      <def-item>
        <term>CI</term>
        <def>
          <p id="Par14">Concordance index</p>
        </def>
      </def-item>
      <def-item>
        <term>MSA</term>
        <def>
          <p id="Par15">Multiple sequence alignment</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>Not applicable.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>Mingjian Jiang designed the method and wrote the manuscript, Shuang Wang and Shugang Zhang designed the experiments and collected the data. Wei Zhou and Yuanyuan Zhang analyzed the results. Zhen Li revised the manuscript. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work has been supported by Shandong Key Science and Technology Innovation Project [2021CXGC011003], the National Natural Science Foundation of China [61902430], the Shandong Provincial Natural Science Foundation [ZR2021QF023] and the Fundamental Research Funds for the Central Universities [21CX06018A].</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The code and data are provided at <ext-link ext-link-type="uri" xlink:href="https://github.com/595693085/WGNN-DTA">https://github.com/595693085/WGNN-DTA</ext-link>.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1">
      <title>Ethics approval and consent to participate</title>
      <p id="Par75">Not applicable.</p>
    </notes>
    <notes id="FPar2">
      <title>Consent for publication</title>
      <p id="Par76">Not applicable.</p>
    </notes>
    <notes id="FPar3" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par77">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lang</surname>
            <given-names>PT</given-names>
          </name>
          <name>
            <surname>Brozell</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>Mukherjee</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Pettersen</surname>
            <given-names>EF</given-names>
          </name>
          <name>
            <surname>Meng</surname>
            <given-names>EC</given-names>
          </name>
          <name>
            <surname>Thomas</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Rizzo</surname>
            <given-names>RC</given-names>
          </name>
          <name>
            <surname>Case</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>James</surname>
            <given-names>TL</given-names>
          </name>
          <name>
            <surname>Kuntz</surname>
            <given-names>ID</given-names>
          </name>
        </person-group>
        <article-title>Dock 6: Combining techniques to model rna–small molecule complexes</article-title>
        <source>Rna</source>
        <year>2009</year>
        <volume>15</volume>
        <issue>6</issue>
        <fpage>1219</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="doi">10.1261/rna.1563609</pub-id>
        <pub-id pub-id-type="pmid">19369428</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Morris</surname>
            <given-names>GM</given-names>
          </name>
          <name>
            <surname>Huey</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lindstrom</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Sanner</surname>
            <given-names>MF</given-names>
          </name>
          <name>
            <surname>Belew</surname>
            <given-names>RK</given-names>
          </name>
          <name>
            <surname>Goodsell</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Olson</surname>
            <given-names>AJ</given-names>
          </name>
        </person-group>
        <article-title>Autodock4 and autodocktools4: Automated docking with selective receptor flexibility</article-title>
        <source>J Comput Chem</source>
        <year>2009</year>
        <volume>30</volume>
        <issue>16</issue>
        <fpage>2785</fpage>
        <lpage>91</lpage>
        <pub-id pub-id-type="doi">10.1002/jcc.21256</pub-id>
        <pub-id pub-id-type="pmid">19399780</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Weininger</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules</article-title>
        <source>J Chem Inf Comput Sci</source>
        <year>1988</year>
        <volume>28</volume>
        <issue>1</issue>
        <fpage>31</fpage>
        <lpage>36</lpage>
        <pub-id pub-id-type="doi">10.1021/ci00057a005</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Tan</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Zhong</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Transformercpi: improving compound–protein interaction prediction by sequence-based deep learning with self-attention mechanism and label reversal experiments</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <issue>16</issue>
        <fpage>4406</fpage>
        <lpage>14</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa524</pub-id>
        <pub-id pub-id-type="pmid">32428219</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. In: Advances in Neural Information Processing Systems. New York: Curran Associates, Inc.; 2017. p. 5998–6008.  <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Guan</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Improving compound–protein interaction prediction by building up highly credible negative samples</article-title>
        <source>Bioinformatics</source>
        <year>2015</year>
        <volume>31</volume>
        <issue>12</issue>
        <fpage>221</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btv256</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rifaioglu</surname>
            <given-names>AS</given-names>
          </name>
          <name>
            <surname>Nalbat</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Atalay</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Cetin-Atalay</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Doğan</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Deepscreen: high performance drug–target interaction prediction with convolutional neural networks using 2-d structural compound representations</article-title>
        <source>Chem Sci</source>
        <year>2020</year>
        <volume>11</volume>
        <issue>9</issue>
        <fpage>2531</fpage>
        <lpage>57</lpage>
        <pub-id pub-id-type="doi">10.1039/C9SC03414E</pub-id>
        <pub-id pub-id-type="pmid">33209251</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wan</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Hong</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Neodti: neural integration of neighbor information from a heterogeneous network for discovering new drug–target interactions</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>35</volume>
        <issue>1</issue>
        <fpage>104</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty543</pub-id>
        <pub-id pub-id-type="pmid">30561548</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Gu</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Predicting drug-target interactions from drug structure and protein sequence using novel convolutional neural networks</article-title>
        <source>BMC Bioinformatics</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>25</issue>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="pmid">30606105</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ezzat</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X-L</given-names>
          </name>
          <name>
            <surname>Kwoh</surname>
            <given-names>C-K</given-names>
          </name>
        </person-group>
        <article-title>Drug-target interaction prediction using ensemble learning and dimensionality reduction</article-title>
        <source>Methods</source>
        <year>2017</year>
        <volume>129</volume>
        <fpage>81</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ymeth.2017.05.016</pub-id>
        <pub-id pub-id-type="pmid">28549952</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Öztürk</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Özgür</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ozkirimli</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Deepdta: deep drug–target binding affinity prediction</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <issue>17</issue>
        <fpage>821</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty593</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Öztürk H, Ozkirimli E, Özgür A. Widedta: prediction of drug-target binding affinity. arXiv preprint arXiv:1902.04166. 2019. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1902.04166">https://arxiv.org/abs/1902.04166</ext-link>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1807.09741">https://arxiv.org/abs/1807.09741</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Glass</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Zitnik</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Deeppurpose: a deep learning library for drug–target interaction prediction</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <issue>22-23</issue>
        <fpage>5545</fpage>
        <lpage>7</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa1005</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Pang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Gansdta: Predicting drug-target binding affinity using gans</article-title>
        <source>Front Genet</source>
        <year>2020</year>
        <volume>10</volume>
        <fpage>1243</fpage>
        <pub-id pub-id-type="doi">10.3389/fgene.2019.01243</pub-id>
        <pub-id pub-id-type="pmid">31993067</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shim</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hong</surname>
            <given-names>Z-Y</given-names>
          </name>
          <name>
            <surname>Sohn</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Hwang</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Prediction of drug–target binding affinity using similarity-based convolutional neural network</article-title>
        <source>Sci Rep</source>
        <year>2021</year>
        <volume>11</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1038/s41598-020-79139-8</pub-id>
        <pub-id pub-id-type="pmid">33414495</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rogers</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Hahn</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Extended-connectivity fingerprints</article-title>
        <source>J Chem Inf Model</source>
        <year>2010</year>
        <volume>50</volume>
        <issue>5</issue>
        <fpage>742</fpage>
        <lpage>54</lpage>
        <pub-id pub-id-type="doi">10.1021/ci100050t</pub-id>
        <pub-id pub-id-type="pmid">20426451</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bender</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mussa</surname>
            <given-names>HY</given-names>
          </name>
          <name>
            <surname>Glen</surname>
            <given-names>RC</given-names>
          </name>
          <name>
            <surname>Reiling</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Similarity searching of chemical databases using atom environment descriptors (molprint 2d): evaluation of performance</article-title>
        <source>J Chem Inf Comput Sci</source>
        <year>2004</year>
        <volume>44</volume>
        <issue>5</issue>
        <fpage>1708</fpage>
        <lpage>18</lpage>
        <pub-id pub-id-type="doi">10.1021/ci0498719</pub-id>
        <pub-id pub-id-type="pmid">15446830</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Durant</surname>
            <given-names>JL</given-names>
          </name>
          <name>
            <surname>Leland</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>Henry</surname>
            <given-names>DR</given-names>
          </name>
          <name>
            <surname>Nourse</surname>
            <given-names>JG</given-names>
          </name>
        </person-group>
        <article-title>Reoptimization of mdl keys for use in drug discovery</article-title>
        <source>J Chem Inf Comput Sci</source>
        <year>2002</year>
        <volume>42</volume>
        <issue>6</issue>
        <fpage>1273</fpage>
        <lpage>80</lpage>
        <pub-id pub-id-type="doi">10.1021/ci010132r</pub-id>
        <pub-id pub-id-type="pmid">12444722</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Wang S, Song T, Zhang S, Jiang M, Wei Z, Li Z. Molecular substructure tree generative model for de novo drug design. Brief Bioinform. 2022;23(2):bbab592.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Cui</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Graph neural networks: A review of methods and applications</article-title>
        <source>AI Open</source>
        <year>2020</year>
        <volume>1</volume>
        <fpage>57</fpage>
        <lpage>81</lpage>
        <pub-id pub-id-type="doi">10.1016/j.aiopen.2021.01.001</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tsubaki</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Tomii</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Sese</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Compound–protein interaction prediction with end-to-end learning of neural networks for graphs and sequences</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>35</volume>
        <issue>2</issue>
        <fpage>309</fpage>
        <lpage>18</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty535</pub-id>
        <pub-id pub-id-type="pmid">29982330</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nguyen</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Quinn</surname>
            <given-names>TP</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>TD</given-names>
          </name>
          <name>
            <surname>Venkatesh</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Graphdta: Predicting drug–target binding affinity with graph neural networks</article-title>
        <source>Bioinformatics</source>
        <year>2021</year>
        <volume>37</volume>
        <issue>8</issue>
        <fpage>1140</fpage>
        <lpage>7</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa921</pub-id>
        <pub-id pub-id-type="pmid">33119053</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Mcn-cpi: multiscale convolutional network for compound–protein interaction prediction</article-title>
        <source>Biomolecules</source>
        <year>2021</year>
        <volume>11</volume>
        <issue>8</issue>
        <fpage>1119</fpage>
        <pub-id pub-id-type="doi">10.3390/biom11081119</pub-id>
        <pub-id pub-id-type="pmid">34439785</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Feng Q, Dueva E, Cherkasov A, Ester M. Padme: A deep learning-based framework for drug-target interaction prediction. arXiv preprint arXiv:1807.09741. 2018.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, Tunyasuvunakool K, Bates R, žídek A, Potapenko A, et al.Highly accurate protein structure prediction with alphafold. Nature. 2021:1–11.</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Drug–target affinity prediction using graph neural network and contact maps</article-title>
        <source>RSC Adv</source>
        <year>2020</year>
        <volume>10</volume>
        <issue>35</issue>
        <fpage>20701</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1039/D0RA02297G</pub-id>
        <pub-id pub-id-type="pmid">35517730</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Rao R, Meier J, Sercu T, Ovchinnikov S, Rives A. Transformer protein language models are unsupervised structure learners. In: International Conference on Learning Representations: 2020.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Davis</surname>
            <given-names>MI</given-names>
          </name>
          <name>
            <surname>Hunt</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Herrgard</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ciceri</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Wodicka</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Pallares</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Hocker</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Treiber</surname>
            <given-names>DK</given-names>
          </name>
          <name>
            <surname>Zarrinkar</surname>
            <given-names>PP</given-names>
          </name>
        </person-group>
        <article-title>Comprehensive analysis of kinase inhibitor selectivity</article-title>
        <source>Nat Biotechnol</source>
        <year>2011</year>
        <volume>29</volume>
        <issue>11</issue>
        <fpage>1046</fpage>
        <lpage>51</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt.1990</pub-id>
        <pub-id pub-id-type="pmid">22037378</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Szwajda</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Shakyawar</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hintsanen</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Wennerberg</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Aittokallio</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Making sense of large-scale kinase inhibitor bioactivity data sets: a comparative and integrative analysis</article-title>
        <source>J Chem Inf Model</source>
        <year>2014</year>
        <volume>54</volume>
        <issue>3</issue>
        <fpage>735</fpage>
        <lpage>43</lpage>
        <pub-id pub-id-type="doi">10.1021/ci400709d</pub-id>
        <pub-id pub-id-type="pmid">24521231</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wishart</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Feunang</surname>
            <given-names>YD</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Lo</surname>
            <given-names>EJ</given-names>
          </name>
          <name>
            <surname>Marcu</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Grant</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Sajed</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Sayeeda</surname>
            <given-names>Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Drugbank 5.0: a major update to the drugbank database for 2018</article-title>
        <source>Nucleic Acids Res</source>
        <year>2018</year>
        <volume>46</volume>
        <issue>D1</issue>
        <fpage>1074</fpage>
        <lpage>82</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkx1037</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Günther</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kuhn</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Dunkel</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Campillos</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Senger</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Petsalaki</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Ahmed</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Urdiales</surname>
            <given-names>EG</given-names>
          </name>
          <name>
            <surname>Gewiess</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Jensen</surname>
            <given-names>LJ</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Supertarget and matador: resources for exploring drug-target relationships</article-title>
        <source>Nucleic Acids Res</source>
        <year>2007</year>
        <volume>36</volume>
        <issue>suppl_1</issue>
        <fpage>919</fpage>
        <lpage>22</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkm862</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mysinger</surname>
            <given-names>MM</given-names>
          </name>
          <name>
            <surname>Carchia</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Irwin</surname>
            <given-names>JJ</given-names>
          </name>
          <name>
            <surname>Shoichet</surname>
            <given-names>BK</given-names>
          </name>
        </person-group>
        <article-title>Directory of useful decoys, enhanced (dud-e): better ligands and decoys for better benchmarking</article-title>
        <source>J Med Chem</source>
        <year>2012</year>
        <volume>55</volume>
        <issue>14</issue>
        <fpage>6582</fpage>
        <lpage>94</lpage>
        <pub-id pub-id-type="doi">10.1021/jm300687e</pub-id>
        <pub-id pub-id-type="pmid">22716043</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Michel</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Menéndez Hurtado</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Elofsson</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Pconsc4: fast, accurate and hassle-free contact predictions</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>35</volume>
        <issue>15</issue>
        <fpage>2677</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty1036</pub-id>
        <pub-id pub-id-type="pmid">30590407</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Kipf TN, Welling M. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907. 2016.</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Veličković P, Cucurull G, Casanova A, Romero A, Lio P, Bengio Y. Graph attention networks. arXiv preprint arXiv:1710.10903. 2017.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Lide DR. Handbook of chemistry and physics. 72nd edition. Boca Raton: CRC Press; 1991-1992. volume 72.</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sereda</surname>
            <given-names>TJ</given-names>
          </name>
          <name>
            <surname>Mant</surname>
            <given-names>CT</given-names>
          </name>
          <name>
            <surname>Sönnichsen</surname>
            <given-names>FD</given-names>
          </name>
          <name>
            <surname>Hodges</surname>
            <given-names>RS</given-names>
          </name>
        </person-group>
        <article-title>Reversed-phase chromatography of synthetic amphipathic <italic>α</italic>-helical peptides as a model for ligand/receptor interactions effect of changing hydrophobic environment on the relative hydrophilicity/hydrophobicity of amino acid side-chains</article-title>
        <source>J Chromatogr A</source>
        <year>1994</year>
        <volume>676</volume>
        <issue>1</issue>
        <fpage>139</fpage>
        <lpage>53</lpage>
        <pub-id pub-id-type="doi">10.1016/0021-9673(94)00371-8</pub-id>
        <pub-id pub-id-type="pmid">7921171</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Landrum G, et al. RDKit: A software suite for cheminformatics, computational chemistry, and predictive modeling. 2013.</mixed-citation>
    </ref>
  </ref-list>
</back>
