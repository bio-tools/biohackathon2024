<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neurosci</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neurosci</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neurosci.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroscience</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1662-4548</issn>
    <issn pub-type="epub">1662-453X</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9201625</article-id>
    <article-id pub-id-type="doi">10.3389/fnins.2022.876065</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>O-Net: A Novel Framework With Deep Fusion of CNN and Transformer for Simultaneous Segmentation and Classification</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Tao</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1571033/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lan</surname>
          <given-names>Junlin</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Han</surname>
          <given-names>Zixin</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1605975/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hu</surname>
          <given-names>Ziwei</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1599752/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Huang</surname>
          <given-names>Yuxiu</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Deng</surname>
          <given-names>Yanglin</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Hejun</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Jianchao</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1753603/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chen</surname>
          <given-names>Musheng</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1793780/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jiang</surname>
          <given-names>Haiyan</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lee</surname>
          <given-names>Ren-Guey</given-names>
        </name>
        <xref rid="aff5" ref-type="aff">
          <sup>5</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1718550/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gao</surname>
          <given-names>Qinquan</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff6" ref-type="aff">
          <sup>6</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1698082/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Du</surname>
          <given-names>Ming</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Tong</surname>
          <given-names>Tong</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff6" ref-type="aff">
          <sup>6</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/556226/overview"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Chen</surname>
          <given-names>Gang</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <xref rid="aff7" ref-type="aff">
          <sup>7</sup>
        </xref>
        <xref rid="c002" ref-type="corresp">
          <sup>*</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>College of Physics and Information Engineering, Fuzhou University</institution>, <addr-line>Fuzhou</addr-line>, <country>China</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Fujian Key Lab of Medical Instrumentation and Pharmaceutical Technology, Fuzhou University</institution>, <addr-line>Fuzhou</addr-line>, <country>China</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Department of Pathology, Fujian Cancer Hospital, Fujian Medical University</institution>, <addr-line>Fuzhou</addr-line>, <country>China</country></aff>
    <aff id="aff4"><sup>4</sup><institution>College of Electrical Engineering and Automation, Fuzhou University</institution>, <addr-line>Fuzhou</addr-line>, <country>China</country></aff>
    <aff id="aff5"><sup>5</sup><institution>Department of Electronic Engineering, National Taipei University of Technology</institution>, <addr-line>Taipei</addr-line>, <country>Taiwan</country></aff>
    <aff id="aff6"><sup>6</sup><institution>Imperial Vision Technology</institution>, <addr-line>Fuzhou</addr-line>, <country>China</country></aff>
    <aff id="aff7"><sup>7</sup><institution>Fujian Provincial Key Laboratory of Translational Cancer Medicin</institution>, <addr-line>Fuzhou</addr-line>, <country>China</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Zhengwang Wu, University of North Carolina at Chapel Hill, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Yi Wang, Northwestern Polytechnical University, China; Anubha Gupta, Indraprastha Institute of Information Technology Delhi, India</p>
      </fn>
      <corresp id="c001">*Correspondence: Tong Tong <email>ttraveltong@gmail.com</email></corresp>
      <corresp id="c002">Gang Chen <email>naichengang@126.com</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Original Research Article, a section of the journal Frontiers in Neuroscience</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>02</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>16</volume>
    <elocation-id>876065</elocation-id>
    <history>
      <date date-type="received">
        <day>15</day>
        <month>2</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>05</day>
        <month>5</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2022 Wang, Lan, Han, Hu, Huang, Deng, Zhang, Wang, Chen, Jiang, Lee, Gao, Du, Tong and Chen.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Wang, Lan, Han, Hu, Huang, Deng, Zhang, Wang, Chen, Jiang, Lee, Gao, Du, Tong and Chen</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>The application of deep learning in the medical field has continuously made huge breakthroughs in recent years. Based on convolutional neural network (CNN), the U-Net framework has become the benchmark of the medical image segmentation task. However, this framework cannot fully learn global information and remote semantic information. The transformer structure has been demonstrated to capture global information relatively better than the U-Net, but the ability to learn local information is not as good as CNN. Therefore, we propose a novel network referred to as the O-Net, which combines the advantages of CNN and transformer to fully use both the global and the local information for improving medical image segmentation and classification. In the encoder part of our proposed O-Net framework, we combine the CNN and the Swin Transformer to acquire both global and local contextual features. In the decoder part, the results of the Swin Transformer and the CNN blocks are fused to get the final results. We have evaluated the proposed network on the synapse multi-organ CT dataset and the ISIC 2017 challenge dataset for the segmentation task. The classification network is simultaneously trained by using the encoder weights of the segmentation network. The experimental results show that our proposed O-Net achieves superior segmentation performance than state-of-the-art approaches, and the segmentation results are beneficial for improving the accuracy of the classification task. The codes and models of this study are available at <ext-link xlink:href="https://github.com/ortonwang/O-Net" ext-link-type="uri">https://github.com/ortonwang/O-Net</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>CNN</kwd>
      <kwd>transformer</kwd>
      <kwd>medical image segmentation</kwd>
      <kwd>deep learning</kwd>
      <kwd>classification</kwd>
    </kwd-group>
    <counts>
      <fig-count count="7"/>
      <table-count count="6"/>
      <equation-count count="14"/>
      <ref-count count="55"/>
      <page-count count="13"/>
      <word-count count="7892"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>Image enhancement has been extensively performed on medical images based on morphology, such as clustering (Vasuda and Satheesh, <xref rid="B43" ref-type="bibr">1713</xref>), edge detection (Patil and Deore, <xref rid="B32" ref-type="bibr">2013</xref>), and threshold segmentation (Wang et al., <xref rid="B47" ref-type="bibr">2015</xref>) to assist doctors in diagnosis in the early days. With the development of artificial intelligence, deep learning technology has been widely used in medical image processing and analysis in recent years, and the accuracy of segmentation and classification on medical images is of great significance to the diagnosis of diseases today. In clinical practice, accurate image segmentation can provide clinicians with quantitative information, which can help clinicians make diagnostic decisions more precisely and efficiently (Liang et al., <xref rid="B26" ref-type="bibr">2020</xref>). In addition, the additional information provided by computing methods is subjective and can avoid the objective bias by humans.</p>
    <p>Nowadays, Convolutional Neural Network (CNN), especially Full Convolutional Network (FCN) is an effective segmentation method (Wang et al., <xref rid="B46" ref-type="bibr">2021b</xref>) and it has been widely used in dense classification tasks such as semantic segmentation (Ji et al., <xref rid="B22" ref-type="bibr">2020</xref>). Among different CNN networks, U-Net (Ronneberger et al., <xref rid="B34" ref-type="bibr">2015</xref>) is a deep learning network with encoder and decoder structures, which has been widely used in medical image segmentation. In recent years, it has been widely used in medical image segmentation tasks due to its strong generalization. U-Net and its variants UNet++ (Zhou et al., <xref rid="B54" ref-type="bibr">2018</xref>), UNet 3+ (Huang et al., <xref rid="B21" ref-type="bibr">2020</xref>), CE-Net (Gu et al., <xref rid="B15" ref-type="bibr">2019</xref>) have shown excellent performance in tasks, such as lesion segmentation, heart segmentation, and other organ segmentation. Based on the strong ability of learning and discriminating features, Res-UNet (Xiao et al., <xref rid="B49" ref-type="bibr">2018</xref>) improves the performance of the network by introducing a residual network into the encoder part of U-Net. EfficientNet (Tan and Le, <xref rid="B41" ref-type="bibr">2019</xref>) proposed a new scaling method that uniformly all dimensions of the depth, width, and resolution of the network through simple but efficient composite coefficients, which not only reduces a certain amount of calculation, but also improves the segmentation performance. Many experimental results have shown that the use of EfficientNet as an encoder can often further improve the performance of the network without increasing the amount of calculation.</p>
    <p>However, these networks are faced with the common problem of CNN: it is difficult for CNN-based methods to learn the global and remote semantic information interaction (Chen et al., <xref rid="B2" ref-type="bibr">2021</xref>) clearly. This is due to the fact that CNN extracts features with a convolutional process. Some studies tried to use image feature pyramid (Lin et al., <xref rid="B28" ref-type="bibr">2017</xref>), atrous convolution layers (Chen et al., <xref rid="B3" ref-type="bibr">2017</xref>, <xref rid="B4" ref-type="bibr">2018</xref>; Gu et al., <xref rid="B15" ref-type="bibr">2019</xref>), and self-attention mechanisms (Wang et al., <xref rid="B48" ref-type="bibr">2018</xref>; Schlemper et al., <xref rid="B36" ref-type="bibr">2019</xref>) to solve this problem. However, the global and remote semantic information is not fully learnt using these strategies. Inspired by the great success of transformer (Vaswani et al., <xref rid="B44" ref-type="bibr">2017</xref>) in the field of natural language processing (NLP), researchers have tried to introduce transformer to make up for the shortcomings of CNN in global and remote information interaction. A transformer is an attention-based model and self-attention mechanism (SA) is a key component of transformer. It can model the correlation of all input tags which makes room for the transformer to deal with long-range dependencies. In Dosovitskiy et al. (<xref rid="B10" ref-type="bibr">2020</xref>), vision transformer (ViT) was applied to perform image recognition tasks and achieved relatively good results. After that, a novel framework called Swin Transformer (Liu et al., <xref rid="B29" ref-type="bibr">2021</xref>) was proposed and significantly improved the performance of ViT in different tasks, such as image classification (Liu et al., <xref rid="B29" ref-type="bibr">2021</xref>), object detection (Xu et al., <xref rid="B52" ref-type="bibr">2021</xref>), and semantic segmentation (Xie et al., <xref rid="B50" ref-type="bibr">2021</xref>). Based on the Swin Transformer, Cao et al. (<xref rid="B1" ref-type="bibr">2021</xref>) proposed Swin-Unet, which combined the U-Net structure and Swin Transformer for medical image segmentation, the encoding part and the decoding part in Swin-Unet were both performed using Swin Transformer. With the proposal of these methods, the accuracy of segmentation tasks is further improved. However, the input in transformer is formed as one-dimensional sequence. The transformer networks focus on learning the global contextual information, but may lose some local details. Therefore, it is beneficial to combine the global information learnt by transformer and the local information by CNN to enrich the learnt features.</p>
    <p>Based on the advantages of CNN and transformer, we propose an O-Net framework to combine the CNN and the transformer to learn both global and local contextual features. We combine the CNN and Swin Transformer as encoder first and send them into a CNN-based decoder and a Swin Transformer-based decoder, respectively. The results of two decoders are fused to get the final result. This network combines the advantages of CNN and transformer and may improve the performance of medical image segmentation. Our experimental results have shown that the performance of the network can be significantly improved by combining CNN and transformer. In addition, a classification task is simultaneously performed based on the O-Net. Experiments show that the segmentation results are beneficial for improving the accuracy of the classification task. Experiments on the synapse multi-organ segmentation dataset and the ISIC2017 skin lesion challenge dataset have demonstrated the superiority of our method compared to other state-of-the-art segmentation methods. In addition, based on the segmentation network, the performance of the classification network has also been greatly improved.</p>
  </sec>
  <sec id="s2">
    <title>2. Related Works</title>
    <p><bold>CNN-based methods:</bold> CNN is a kind of feedforward neural network that includes convolution calculations and has a deep structure. It is one of the representative algorithms of deep learning. Lenet[18] first defined the CNN network structure in 1998, and it was not until the publication of AlexNet (Krizhevsky et al., <xref rid="B23" ref-type="bibr">2012</xref>) in 2012 that CNN has gradually become mainstream. Since then, lots of efficient and deep convolutional neural networks have been proposed. For example, VGG (Simonyan and Zisserman, <xref rid="B37" ref-type="bibr">2014</xref>), ResNet (He et al., <xref rid="B16" ref-type="bibr">2016</xref>), DenseNet (Huang et al., <xref rid="B20" ref-type="bibr">2017</xref>), GoogleNet (Szegedy et al., <xref rid="B39" ref-type="bibr">2015</xref>), HRNet (Sun et al., <xref rid="B38" ref-type="bibr">2019</xref>), Inception v3 (Szegedy et al., <xref rid="B40" ref-type="bibr">2016</xref>), and EfficientNet (Tan and Le, <xref rid="B41" ref-type="bibr">2019</xref>). These networks perform well in various applications. In addition to these network innovations, new convolutional layers such as deformable convolution (Dai et al., <xref rid="B7" ref-type="bibr">2017</xref>; Zhu et al., <xref rid="B55" ref-type="bibr">2019</xref>) and depth-wise convolution (Xie et al., <xref rid="B51" ref-type="bibr">2017</xref>) were proposed for different tasks. With the development of CNN, U-Net was proposed and widely used in segmentation tasks because of its simple structure, good effects, and strong generalization. After that, various U-shape network based U-Net have been proposed such as U-SegNet (Kumar et al., <xref rid="B24" ref-type="bibr">2018</xref>), Res-UNet (Xiao et al., <xref rid="B49" ref-type="bibr">2018</xref>), Dense-UNet (Li et al., <xref rid="B25" ref-type="bibr">2018</xref>), U-Net++ (Zhou et al., <xref rid="B54" ref-type="bibr">2018</xref>), U-2-Net (Qin et al., <xref rid="B33" ref-type="bibr">2020</xref>), and UNet3+ (Huang et al., <xref rid="B21" ref-type="bibr">2020</xref>) CE-Net (Gu et al., <xref rid="B15" ref-type="bibr">2019</xref>). Gehlot et al. (<xref rid="B14" ref-type="bibr">2020</xref>) proposed an Encoder-Decoder based CNN with Nested-Feature Concatenation (EDNFC-Net) for automatic segmentation. Some networks introduce novel structures in the encoder part while others in the decoder part. Because of the strong generalization of the network, the U-shaped architecture network has also been extended to 3D medical image segmentation, such as 3D-UNet (Çiçek et al., <xref rid="B5" ref-type="bibr">2016</xref>) and V-Net (Milletari et al., <xref rid="B30" ref-type="bibr">2016</xref>). Moreover, Gehlot et al. proposed AION (Gehlot and Gupta, <xref rid="B13" ref-type="bibr">2021</xref>), an architecture with two coupled networks and classification heads which is applicable for stain normalization, classification, and segmentation tasks.</p>
    <p><bold>Transformers:</bold> Transformer was first proposed for machine translation and achieved the best performance in many NLP tasks. To combine computer vision (CV) and natural language processing (NLP) domain knowledge, researchers developed Vision Transformer (ViT) (Dosovitskiy et al., <xref rid="B10" ref-type="bibr">2020</xref>) by directly applying transformers with global self-focus to full-size images. The ViT model achieved both high efficiency and accuracy in image recognition tasks. Based on ViT, Chen et al. (<xref rid="B2" ref-type="bibr">2021</xref>) proposed the first transformer-based medical image segmentation framework TransUNet which further improved the accuracy of image segmentation tasks. However, ViT needs to be pre-trained on its large datasets to achieve good performance. To solve this problem, some training schemes were designed in Deit (Touvron et al., <xref rid="B42" ref-type="bibr">2021</xref>) so that the algorithm can perform well on smaller data sets. To further improve the accuracy, a new vision transformer called Swin Transformer (Liu et al., <xref rid="B29" ref-type="bibr">2021</xref>) was proposed, it is a hierarchical transformer whose representation is computed with Shifted windows. This hierarchical architecture has the flexibility of modeling at various scales and has linear computational complexity relative to the image size. These features make it compatible with many vision tasks, including image classification and semantic segmentation. Based on Swin Transformer, Cao et al. (<xref rid="B1" ref-type="bibr">2021</xref>) proposed a pure transformer U-shaped encoder-decoder network named Swin-Unet for medical image segmentation, which has relatively good performance in some datasets.</p>
    <p><bold>Self-attention/transformer combined with CNN:</bold> In recent years, researchers have tried to improve the performance of the network through the self-attention mechanism (Wang et al., <xref rid="B48" ref-type="bibr">2018</xref>) to overcome the shortcomings of CNN learning global semantic information. In Schlemper et al. (<xref rid="B36" ref-type="bibr">2019</xref>), the skip-connections with additive attention gate were integrated with U-shaped architecture to improve medical image segmentation. But this is still the method based on CNN after all and it has not completely solved the limitation of learning global information. Several studies have been carried out to combine CNN and transformer. TransUNet (Chen et al., <xref rid="B2" ref-type="bibr">2021</xref>) was proposed by combining the advantages of transformer and CNN. The transformer encodes image patches from a CNN feature map as the input sequence for extracting global contexts. A mixed transformer module (MTM) (Wang et al., <xref rid="B45" ref-type="bibr">2021a</xref>) was proposed for simultaneous inter- and intra- affinities learning. TransFuse (Zhang et al., <xref rid="B53" ref-type="bibr">2021</xref>) combines transformers and CNNs in a parallel style to capture both global dependency and low-level spatial details efficiently in a much shallower manner for medical image segmentations. Liang et al. (<xref rid="B27" ref-type="bibr">2022</xref>) proposed transconver with a parallel module named transformer-convolution inception which extracts local and global information <italic>via</italic> convolution blocks and transformer blocks, respectively. TransMed (Dai et al., <xref rid="B8" ref-type="bibr">2021</xref>) was proposed for multi-modal medical image classification which combines the advantages of CNN and transformer to extract low-level features of images efficiently and establish long-range dependencies between modalities. These algorithms improve the global attention of the model based on their complementarity by directly combining CNN and transformer.</p>
  </sec>
  <sec id="s3">
    <title>3. The Proposed Method</title>
    <sec>
      <title>3.1. Overall Architecture Design</title>
      <p>A schematic view of the proposed O-Net is presented in <xref rid="F1" ref-type="fig">Figure 1</xref>. O-Net is composed of two parts: an encoder module and a decoder module. The basic units of O-Net include the Swin Transformer block, EfficientNet block, and CNN Decoder block. During the segmentation task, the encoder module extracts the features of the input image to obtain the high-dimensional and low-dimensional features, which are then decoded back to the full spatial resolution by the decoder module. After extracting the features in the encoder part, the segmentation network provided an interface to integrate a classification network for simultaneously performing the classification task. Each module is described in detail below.</p>
      <fig position="float" id="F1">
        <label>Figure 1</label>
        <caption>
          <p>The architecture of our proposed O-Net.</p>
        </caption>
        <graphic xlink:href="fnins-16-876065-g0001" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.2. Swin Transformer Block</title>
      <p>Different from the transformer, Swin Transformer is built based on shifted windows rather than the standard multi-head self attention (MSA) module. Two consecutive Swin Transformer blocks are presented in <xref rid="F2" ref-type="fig">Figure 2</xref>. Each Swin Transformer block consists of residual connection and 2-layer MLP with Gaussian Error Linear Units (GELU) non-linearity, LayerNorm (LN) layer, and multi-head self attention module. The shifted window-based multi-head self attention (SW-MSA) module and the window-based multi-head self attention (W-MSA) module are applied in the two successive transformer blocks, respectively. Based on such a window partitioning approach, successive Swin Transformer blocks can be formulated as follows:</p>
      <disp-formula id="E1">
        <label>(1)</label>
        <mml:math id="M1" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>ẑ</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>l</mml:mi>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mi>W</mml:mi>
                <mml:mo>-</mml:mo>
                <mml:mi>M</mml:mi>
                <mml:mi>S</mml:mi>
                <mml:mi>A</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>L</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mi>z</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>l</mml:mi>
                            <mml:mo>-</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>+</mml:mo>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>z</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>l</mml:mi>
                    <mml:mo>-</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>,</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <disp-formula id="E2">
        <label>(2)</label>
        <mml:math id="M2" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>z</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>l</mml:mi>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mi>M</mml:mi>
                <mml:mi>L</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>L</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mi>ẑ</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>l</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>+</mml:mo>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>ẑ</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>l</mml:mi>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>,</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <disp-formula id="E3">
        <label>(3)</label>
        <mml:math id="M3" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>z</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>l</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mi>S</mml:mi>
                <mml:mi>W</mml:mi>
                <mml:mo>-</mml:mo>
                <mml:mi>M</mml:mi>
                <mml:mi>S</mml:mi>
                <mml:mi>A</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>L</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mi>z</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>l</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>+</mml:mo>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>z</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>l</mml:mi>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>,</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <disp-formula id="E4">
        <label>(4)</label>
        <mml:math id="M4" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>z</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>l</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mi>M</mml:mi>
                <mml:mi>L</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>L</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mi>z</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>l</mml:mi>
                            <mml:mo>+</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>+</mml:mo>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>z</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>l</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>,</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>Where <italic>z</italic><sup><italic>l</italic></sup> and ẑ<sup><italic>l</italic></sup> represent the output features of the (S)W-MSA module and the MLP module of the <italic>l</italic><sup><italic>th</italic></sup> block, respectively. Similar to the previous works (Hu et al., <xref rid="B18" ref-type="bibr">2018</xref>, <xref rid="B19" ref-type="bibr">2019</xref>), self-attention is computed as follows:</p>
      <disp-formula id="E5">
        <label>(5)</label>
        <mml:math id="M5" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>A</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>Q</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>K</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>V</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mi>S</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>f</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mi>M</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mi>Q</mml:mi>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mi>K</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>T</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msqrt>
                          <mml:mrow>
                            <mml:mi>d</mml:mi>
                          </mml:mrow>
                        </mml:msqrt>
                      </mml:mrow>
                    </mml:mfrac>
                    <mml:mo>+</mml:mo>
                    <mml:mi>B</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mi>V</mml:mi>
                <mml:mo>,</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where <italic>Q, K, V</italic>∈ℝ<sup><italic>M</italic></sup><sup>2</sup>×<italic>d</italic> denote the query, key, and value matrices. M<sup>2</sup> represents the number of patches in a window, and d is the query dimension. Since the relative position along each axis is within the range[−M+1, M−1], the values in B are taken from the bias matrix <inline-formula><mml:math id="M6" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:mn>2</mml:mn><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>.</p>
      <fig position="float" id="F2">
        <label>Figure 2</label>
        <caption>
          <p>Two successive Swin Transformer block.</p>
        </caption>
        <graphic xlink:href="fnins-16-876065-g0002" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.3. EfficientNet Block</title>
      <p>EfficientNet block (Tan and Le, <xref rid="B41" ref-type="bibr">2019</xref>) was proposed based on a neural structure search. This block uses composite coefficients to uniformly scale the depth, width, and resolution of the network. A schematic view of the EfficientNet block is presented in <xref rid="F3" ref-type="fig">Figure 3</xref>. Each EfficientNet block is composed of MBConvBlocks (Sandler et al., <xref rid="B35" ref-type="bibr">2018</xref>) which consists of convolution, batch normalization, and Swish activation layers. The network achieves better performance with the same parameters by uniformly scaling the network width, depth, or resolution in a fixed proportion. We employ the EfficientNet block as the encoder part of CNN to extract features efficiently and effectively.</p>
      <fig position="float" id="F3">
        <label>Figure 3</label>
        <caption>
          <p>The architecture of EfficientNet Block.</p>
        </caption>
        <graphic xlink:href="fnins-16-876065-g0003" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.4. Encoder Module</title>
      <p>In the encoder part, we combine EfficientNet and Swin Transformer. For the Swin Transformer Encoder, it is composed of Swin Transformer Block and patch merging layer. Images are separated into non-overlapping patches with a patch size of 4 × 4 to transform the inputs into sequence embeddings, then concatenated together by the patch merging layer. The feature resolution will be down-sampled by 2 × after such processing, and the feature dimension of each patch becomes to 4 × 4 × 3 = 48. Furthermore, a linear embedding layer is applied to project feature dimension into an arbitrary dimension (represented as C). The transformed patch tokens pass through several Swin Transformer blocks and patch merging layers to generate the hierarchical feature representations.</p>
      <p>For the EfficientNet encoder, the input image is convoluted and down-sampled first. Feature extraction is carried out through the EfficientNet block which uniformly scales the depth, width, and resolution of the network through composite coefficients. We can achieve relatively efficient feature extraction with only a small amount of computation using this module. Since the feature dimensions of two encoders are different, it is required to normalize the dimension before fusing them. The features extracted by the Swin Transformer are set to C × H × W using a linear projection. After that, the features are fused with the features extracted by the EfficientNet block <italic>via</italic> skip-connections. Similarly, when using the Swin Transformer decoder, we project the features extracted by the EfficientNet block through the linear embedding layer and fuse them with the features extracted by the Swin Transformer encoder.</p>
    </sec>
    <sec>
      <title>3.5. Decoder Module</title>
      <p>The decoder module is adopted to restore the high-level semantic features extracted from the encoder module. The decoder part consists of the Swin Transformer decoder block and the CNN decoder block. A schematic view of the decoder modules is presented in <xref rid="F4" ref-type="fig">Figure 4</xref>. The Swin decoder block is composed of a patch expanding layer and a Swin Transformer block. The features extracted by the encoder are multi-scale fused through skip-connections. The patch expanding layer reshapes feature maps of adjacent dimensions into large feature maps with 2 × up-sampling of resolution. In the end, the last patch expanding layer is used to perform 4 × up-sampling to restore the resolution of the feature maps to the input resolution (W × H), and a linear projection layer is applied on these up-sampled features to output the pixel-level segmentation predictions.</p>
      <fig position="float" id="F4">
        <label>Figure 4</label>
        <caption>
          <p>The architecture of decoder module.</p>
        </caption>
        <graphic xlink:href="fnins-16-876065-g0004" position="float"/>
      </fig>
      <p>The CNN decoder block is composed of a 2 × upsampling operator, two 3 × 3 convolution layers, and a batch normalization layer with a Rectified Linear Units(ReLU) layer. Simple upsampling and convolution are two common operations of the decoder in the CNN decoder blocks. After the 2 × upsampling operator, the features were fused with those from encoders through skip-connection. After the two convolution processes, the features are input for the next decoder. At the end of the decoder, a convolution layer is applied to output the pixel-level segmentation predictions. Finally, the outputs of the two decoders are fused to obtain the final segmentation result.</p>
    </sec>
    <sec>
      <title>3.6. Classification Method</title>
      <p>The encoder part of the segmentation network and the classification network share the same structure. When encoders perform the classification task, the role of encoders is to extract contextual features and locate the target region like the segmentation task. The primary task of classification aims to accurately locate the target area, and the purpose of the segmentation network is to realize it. Therefore, after the training of the segmentation network, we use the learned weights of the encoder in the network as the initial parameters of the classification network. After that, we utilize the features of the lowest dimension in the encoder through the average pooling layer and a fully connected layer (FC) to perform the classification task.</p>
    </sec>
  </sec>
  <sec id="s4">
    <title>4. Experiments</title>
    <sec>
      <title>4.1. Datasets</title>
      <p><bold>Synapse multi-organ segmentation dataset (synapse):</bold> The dataset includes 30 abdominal CT scans from MICCAI 2015 Multi-Atlas Abdomen Labeling Challenge. Each CT volume consists of 85−198 slices of 512 × 512 pixels and there are 3,779 axial abdominal clinical CT images in total. Following Chen et al. (<xref rid="B2" ref-type="bibr">2021</xref>) and Liu et al. (<xref rid="B29" ref-type="bibr">2021</xref>), 18 samples were used as the training set and 12 samples as the testing set. The annotation of each image includes 8 abdominal organs (aorta, gallbladder, spleen, left kidney, right kidney, liver, pancreas, spleen, and stomach). The dice metric and the average Hausdorff Distance (HD) are used to evaluate our method on this dataset. The dice metric evaluates the degree of pixel overlap between the ground truth and prediction results and it is calculated as follows:</p>
      <disp-formula id="E6">
        <label>(6)</label>
        <mml:math id="M7" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>D</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>c</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mn>2</mml:mn>
                    <mml:mo>×</mml:mo>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mn>2</mml:mn>
                    <mml:mo>×</mml:mo>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>F</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where TP, FP, and FN refer to the number of true positives, false positives, and false negatives, respectively, besides, TN means true negatives. The HD calculates the maximum distance between the contours of the ground truth and predicted results, which can be formulated as follows:</p>
      <disp-formula id="E7">
        <label>(7)</label>
        <mml:math id="M8" overflow="scroll">
          <mml:mi>H</mml:mi>
          <mml:mo stretchy="false">(</mml:mo>
          <mml:mi>A</mml:mi>
          <mml:mo>,</mml:mo>
          <mml:mi>B</mml:mi>
          <mml:mo stretchy="false">)</mml:mo>
          <mml:mtext> </mml:mtext>
          <mml:mo>=</mml:mo>
          <mml:mtext> </mml:mtext>
          <mml:mi>m</mml:mi>
          <mml:mi>a</mml:mi>
          <mml:mi>x</mml:mi>
          <mml:mo stretchy="false">(</mml:mo>
          <mml:mi>h</mml:mi>
          <mml:mo stretchy="false">(</mml:mo>
          <mml:mi>A</mml:mi>
          <mml:mo>,</mml:mo>
          <mml:mi>B</mml:mi>
          <mml:mo stretchy="false">)</mml:mo>
          <mml:mo>,</mml:mo>
          <mml:mi>h</mml:mi>
          <mml:mo stretchy="false">(</mml:mo>
          <mml:mi>B</mml:mi>
          <mml:mo>,</mml:mo>
          <mml:mi>A</mml:mi>
          <mml:mo stretchy="false">)</mml:mo>
        </mml:math>
      </disp-formula>
      <disp-formula id="E8">
        <label>(8)</label>
        <mml:math id="M9" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>h</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>A</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>B</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munder class="msub">
                    <mml:mrow>
                      <mml:mo class="qopname">max</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>a</mml:mi>
                      <mml:mo>∈</mml:mo>
                      <mml:mi>A</mml:mi>
                    </mml:mrow>
                  </mml:munder>
                </mml:mstyle>
                <mml:mrow>
                  <mml:mo>{</mml:mo>
                  <mml:mrow>
                    <mml:mstyle displaystyle="true">
                      <mml:munder class="msub">
                        <mml:mrow>
                          <mml:mo class="qopname">min</mml:mo>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>b</mml:mi>
                          <mml:mo>∈</mml:mo>
                          <mml:mi>B</mml:mi>
                        </mml:mrow>
                      </mml:munder>
                    </mml:mstyle>
                    <mml:mrow>
                      <mml:mo>{</mml:mo>
                      <mml:mrow>
                        <mml:mo>∥</mml:mo>
                        <mml:mi>a</mml:mi>
                        <mml:mo>-</mml:mo>
                        <mml:mi>b</mml:mi>
                        <mml:mo>∥</mml:mo>
                      </mml:mrow>
                      <mml:mo>}</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mo>}</mml:mo>
                </mml:mrow>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <disp-formula id="E9">
        <label>(9)</label>
        <mml:math id="M10" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>h</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>B</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>A</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munder class="msub">
                    <mml:mrow>
                      <mml:mo class="qopname">max</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>b</mml:mi>
                      <mml:mo>∈</mml:mo>
                      <mml:mi>B</mml:mi>
                    </mml:mrow>
                  </mml:munder>
                </mml:mstyle>
                <mml:mrow>
                  <mml:mo>{</mml:mo>
                  <mml:mrow>
                    <mml:mstyle displaystyle="true">
                      <mml:munder class="msub">
                        <mml:mrow>
                          <mml:mo class="qopname">min</mml:mo>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>a</mml:mi>
                          <mml:mo>∈</mml:mo>
                          <mml:mi>A</mml:mi>
                        </mml:mrow>
                      </mml:munder>
                    </mml:mstyle>
                    <mml:mrow>
                      <mml:mo>{</mml:mo>
                      <mml:mrow>
                        <mml:mo>∥</mml:mo>
                        <mml:mi>b</mml:mi>
                        <mml:mo>-</mml:mo>
                        <mml:mi>a</mml:mi>
                        <mml:mo>∥</mml:mo>
                      </mml:mrow>
                      <mml:mo>}</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mo>}</mml:mo>
                </mml:mrow>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where A and B denote the contours of the ground truth and predicted results, respectively, and h(A,B) denotes the unidirectional HD from A to B.</p>
      <p><bold>ISIC2017 skin lesion challenge dataset (ISIC2017):</bold> The 2017 International Skin Imaging Collaboration (ISIC) skin lesion segmentation challenge dataset (Codella et al., <xref rid="B6" ref-type="bibr">2018</xref>) includes 2,000 training images, 150 validation images, and 600 test dermoscopic images. Each image is paired with an expert manual tracing of skin lesion boundaries for the segmentation task and the lesion gold standard diagnosis (i.e., nevus, melanoma, and seborrheic keratosis) for the classification task. The size of the images in the dataset varies from 453 × 679 to 4499 × 6748 pixels. We used Dice, Mean Intersection over Union (IoU), Precision (Pre), Recall, F1-score, and Pixel Accuracy (PA) as the metrics to evaluate the accuracy of the segmentation work. In addition, we used Accuracy (AC), F1-score, precision (Pre), and specificity (SP) as the metrics to evaluate the classification task. These metric are calculated as follows:</p>
      <disp-formula id="E10">
        <label>(10)</label>
        <mml:math id="M11" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>I</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>U</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>F</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <disp-formula id="E11">
        <label>(11)</label>
        <mml:math id="M12" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>P</mml:mi>
                <mml:mi>r</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <disp-formula id="E12">
        <label>(12)</label>
        <mml:math id="M13" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>P</mml:mi>
                <mml:mi>A</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>T</mml:mi>
                    <mml:mi>N</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>T</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>F</mml:mi>
                    <mml:mi>N</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <disp-formula id="E13">
        <label>(13)</label>
        <mml:math id="M14" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>A</mml:mi>
                <mml:mi>C</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>T</mml:mi>
                    <mml:mi>F</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>T</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>F</mml:mi>
                    <mml:mi>N</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <disp-formula id="E14">
        <label>(14)</label>
        <mml:math id="M15" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>F</mml:mi>
                <mml:mn>1</mml:mn>
                <mml:mo>-</mml:mo>
                <mml:mi>s</mml:mi>
                <mml:mi>c</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>r</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mn>2</mml:mn>
                    <mml:mo>×</mml:mo>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mn>2</mml:mn>
                    <mml:mo>×</mml:mo>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mi>F</mml:mi>
                    <mml:mi>N</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
    </sec>
    <sec>
      <title>4.2. Implementation Details</title>
      <p>Our method was implemented based on the Pytorch Deep Learning framework using python. For all training cases, flips and rotations were used as data augmentation to improve the generalization ability of the model. We trained our model on an Nvidia RTX 3090 GPU with 24GB memory. The input image size was set to 224 × 224 on the synapse dataset and 512 × 512 on the ISIC2017 dataset. The patch on the size was set to 4 in both tasks. All encoders and Swin Transformer blocks in the model were pretrained on ImageNet (Deng et al., <xref rid="B9" ref-type="bibr">2009</xref>). During the training process of the synapse dataset, the batch size was set to 24 and the popular SGD optimizer with momentum of 0.9 and weight decay of 1e-4 and a learning rate of 1e-4 is used for the backpropagation of the model. During the process of ISIC2017 dataset, the models were optimized by AdamW with a learning rate of 1e-4 and a batch size of 8.</p>
    </sec>
    <sec>
      <title>4.3. Experiment Results on the Synapse Dataset</title>
      <p>The comparison of the proposed O-Net with previous state-of-the-art methods on the synapse multi-organ CT dataset is presented in <xref rid="T1" ref-type="table">Table 1</xref>. Experimental results demonstrate that our algorithm achieves the best performance with a segmentation accuracy of 80.61% (Dice↑) and 21.04 (HD↓) performance. We can see from the results that the CNN-based method performs worse on edge predictions than the transformer method from the metric of HD. This also indicates that our algorithm not only performs better in terms of segmentation, but also has a good performance in edge prediction. For organs with high segmentation difficulty such as Pancreas and Gallbladder, our method obtains the best and the third results, respectively, which also reflects the strong generalization of our algorithm. The specific segmentation results of different algorithms on this dataset are presented in <xref rid="F5" ref-type="fig">Figure 5</xref>. In this work, we demonstrate that the in-depth combination of CNN and Swin Transformer can learn both the global and the local contextual features, thereby obtaining better segmentation results.</p>
      <table-wrap position="float" id="T1">
        <label>Table 1</label>
        <caption>
          <p>Experimental results of different methods on the synapse multi-organ CT dataset.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Method</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Dice↑</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>HD↓</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Aorta</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Gallbladder</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Kidney(L)</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Kidney(R)</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Liver</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Pancreas</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Spleen</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Stomach</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">V-Net Milletari et al. (<xref rid="B30" ref-type="bibr">2016</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">68.81</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.34</td>
              <td valign="top" align="center" rowspan="1" colspan="1">51.87</td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.10</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>80.75</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.84</td>
              <td valign="top" align="center" rowspan="1" colspan="1">40.05</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.56</td>
              <td valign="top" align="center" rowspan="1" colspan="1">56.98</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DARR Fu et al. (<xref rid="B12" ref-type="bibr">2020</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">69.77</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">74.74</td>
              <td valign="top" align="center" rowspan="1" colspan="1">53.77</td>
              <td valign="top" align="center" rowspan="1" colspan="1">72.31</td>
              <td valign="top" align="center" rowspan="1" colspan="1">73.24</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.08</td>
              <td valign="top" align="center" rowspan="1" colspan="1">54.18</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.90</td>
              <td valign="top" align="center" rowspan="1" colspan="1">45.96</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">R50 ViT Chen et al. (<xref rid="B2" ref-type="bibr">2021</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">71.29</td>
              <td valign="top" align="center" rowspan="1" colspan="1">32.87</td>
              <td valign="top" align="center" rowspan="1" colspan="1">73.73</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.13</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.80</td>
              <td valign="top" align="center" rowspan="1" colspan="1">72.20</td>
              <td valign="top" align="center" rowspan="1" colspan="1">91.51</td>
              <td valign="top" align="center" rowspan="1" colspan="1">45.99</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.99</td>
              <td valign="top" align="center" rowspan="1" colspan="1">73.95</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">U-SegNet Kumar et al. (<xref rid="B24" ref-type="bibr">2018</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">72.61</td>
              <td valign="top" align="center" rowspan="1" colspan="1">43.94</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85.69</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64.33</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.12</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.41</td>
              <td valign="top" align="center" rowspan="1" colspan="1">91.72</td>
              <td valign="top" align="center" rowspan="1" colspan="1">50.59</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.07</td>
              <td valign="top" align="center" rowspan="1" colspan="1">62.96</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">R50 U-Net Chen et al. (<xref rid="B2" ref-type="bibr">2021</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">74.68</td>
              <td valign="top" align="center" rowspan="1" colspan="1">36.87</td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.74</td>
              <td valign="top" align="center" rowspan="1" colspan="1">63.66</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.60</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.19</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93.74</td>
              <td valign="top" align="center" rowspan="1" colspan="1">56.90</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85.87</td>
              <td valign="top" align="center" rowspan="1" colspan="1">74.16</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">AION Gehlot and Gupta (<xref rid="B13" ref-type="bibr">2021</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.54</td>
              <td valign="top" align="center" rowspan="1" colspan="1">32.27</td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.59</td>
              <td valign="top" align="center" rowspan="1" colspan="1">58.74</td>
              <td valign="top" align="center" rowspan="1" colspan="1">82.47</td>
              <td valign="top" align="center" rowspan="1" colspan="1">73.45</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93.47</td>
              <td valign="top" align="center" rowspan="1" colspan="1">49.44</td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.52</td>
              <td valign="top" align="center" rowspan="1" colspan="1">71.61</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">R50 Att-UNet Chen et al. (<xref rid="B2" ref-type="bibr">2021</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.57</td>
              <td valign="top" align="center" rowspan="1" colspan="1">36.97</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.92</td>
              <td valign="top" align="center" rowspan="1" colspan="1">63.91</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.20</td>
              <td valign="top" align="center" rowspan="1" colspan="1">72.71</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93.56</td>
              <td valign="top" align="center" rowspan="1" colspan="1">49.37</td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.19</td>
              <td valign="top" align="center" rowspan="1" colspan="1">74.95</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">U-Net Ronneberger et al. (<xref rid="B34" ref-type="bibr">2015</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">76.85</td>
              <td valign="top" align="center" rowspan="1" colspan="1">39.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.07</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>69.72</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.77</td>
              <td valign="top" align="center" rowspan="1" colspan="1">68.60</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93.43</td>
              <td valign="top" align="center" rowspan="1" colspan="1">53.98</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86.67</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.58</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">EDNFC-Net Gehlot et al. (<xref rid="B14" ref-type="bibr">2020</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.21</td>
              <td valign="top" align="center" rowspan="1" colspan="1">35.07</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86.08</td>
              <td valign="top" align="center" rowspan="1" colspan="1">62.47</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.31</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.27</td>
              <td valign="top" align="center" rowspan="1" colspan="1">92.61</td>
              <td valign="top" align="center" rowspan="1" colspan="1">57.31</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85.36</td>
              <td valign="top" align="center" rowspan="1" colspan="1">71.24</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">TransUNet Chen et al. (<xref rid="B2" ref-type="bibr">2021</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.48</td>
              <td valign="top" align="center" rowspan="1" colspan="1">31.69</td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.23</td>
              <td valign="top" align="center" rowspan="1" colspan="1">63.13</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.87</td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.02</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.08</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.86</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85.08</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.62</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Att-UNet Oktay et al. (<xref rid="B31" ref-type="bibr">2018</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.77</td>
              <td valign="top" align="center" rowspan="1" colspan="1">36.02</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>89.55</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">68.88</td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.98</td>
              <td valign="top" align="center" rowspan="1" colspan="1">71.11</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93.57</td>
              <td valign="top" align="center" rowspan="1" colspan="1">58.04</td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.30</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.75</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">TransFuse Zhang et al. (<xref rid="B53" ref-type="bibr">2021</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.95</td>
              <td valign="top" align="center" rowspan="1" colspan="1">26.59</td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.09</td>
              <td valign="top" align="center" rowspan="1" colspan="1">61.64</td>
              <td valign="top" align="center" rowspan="1" colspan="1">82.20</td>
              <td valign="top" align="center" rowspan="1" colspan="1">76.91</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.19</td>
              <td valign="top" align="center" rowspan="1" colspan="1">59.01</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.86</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.73</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Swin-Unet Cao et al. (<xref rid="B1" ref-type="bibr">2021</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.13</td>
              <td valign="top" align="center" rowspan="1" colspan="1">21.55</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85.47</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.53</td>
              <td valign="top" align="center" rowspan="1" colspan="1">83.28</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.61</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.29</td>
              <td valign="top" align="center" rowspan="1" colspan="1">56.58</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>90.66</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">76.60</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">O-Net</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>80.61</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>21.04</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">88.36</td>
              <td valign="top" align="center" rowspan="1" colspan="1">67.45</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>84.44</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.13</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>95.24</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>61.52</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">90.03</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>80.74</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>The symbol ↑ means the higher value, the better</italic>.</p>
          <p>
            <italic>The symbol ↓ means the lower value, the better.</italic>
          </p>
          <p><italic>Bold font to highlight the optimal values</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <fig position="float" id="F5">
        <label>Figure 5</label>
        <caption>
          <p>Conparision of different methods on the Synapse multi-organ dataset by visualization. From left to right: <bold>(A)</bold> Ground Truth, <bold>(B)</bold> O-Net, <bold>(C)</bold> SwinUNet, <bold>(D)</bold> TransUNet, and <bold>(E)</bold> R50 AttUNet.</p>
        </caption>
        <graphic xlink:href="fnins-16-876065-g0005" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>4.4. Experiment Results on the ISIC2017 Dataset</title>
      <p>We further evaluated the proposed method for medical image segmentation and classification using the ISIC2017 dataset. The results of segmentation and classification are presented in <xref rid="T2" ref-type="table">Tables 2</xref>, <xref rid="T3" ref-type="table">3</xref>. From <xref rid="T2" ref-type="table">Table 2</xref>, we can see that in the segmentation task, the combination of the CNN and the Swin Transformer can achieve better performance than that of single CNN or that of only the Swin Transformer. This indicates the effectiveness of the combination of these two structures. The O-Net has achieved the best performance in the six metrics which reflects the superiority of our method. The Receiver Operating Characteristic (ROC) curves of the classification methods are shown in <xref rid="F6" ref-type="fig">Figure 6</xref>. The Area Under Curve (AUC) value for O-Net is 0.9264 which is the best performance among compared methods. Based on the data from <xref rid="T3" ref-type="table">Table 3</xref> and the ROC curves of the classification task, we can see that O-Net has also achieved excellent performance in the classification task. The specific segmentation results of different algorithms on this dataset are presented in <xref rid="F7" ref-type="fig">Figure 7</xref>. The experimental results of classification tasks on this dataset indicate that combining CNN and Swin Transformer for classification tasks can improve the accuracy of the classification tasks. The performance can be further improved by initializing the classification network with the parameters from the encoder part of the segmentation network.</p>
      <table-wrap position="float" id="T2">
        <label>Table 2</label>
        <caption>
          <p>Segmentation results of different methods on the ISIC2017 dataset.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Method</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Dice</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>mIoU</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Pre</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>recall</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>F1-score</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>PA</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">U-Net Ronneberger et al. (<xref rid="B34" ref-type="bibr">2015</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85.22</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">91.17</td>
              <td valign="top" align="center" rowspan="1" colspan="1">73.98</td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.80</td>
              <td valign="top" align="center" rowspan="1" colspan="1">91.19</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">R50-U-Net Xiao et al. (<xref rid="B49" ref-type="bibr">2018</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.48</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.86</td>
              <td valign="top" align="center" rowspan="1" colspan="1">92.99</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.19</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.70</td>
              <td valign="top" align="center" rowspan="1" colspan="1">92.19</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">U-SegNet Kumar et al. (<xref rid="B24" ref-type="bibr">2018</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.87</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.22</td>
              <td valign="top" align="center" rowspan="1" colspan="1">90.50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.13</td>
              <td valign="top" align="center" rowspan="1" colspan="1">82.49</td>
              <td valign="top" align="center" rowspan="1" colspan="1">92.33</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">ENDFC-Net Gehlot et al. (<xref rid="B14" ref-type="bibr">2020</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">88.00</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.43</td>
              <td valign="top" align="center" rowspan="1" colspan="1">90.26</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>82.10</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">82.80</td>
              <td valign="top" align="center" rowspan="1" colspan="1">92.29</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">M-Net Fu et al. (<xref rid="B11" ref-type="bibr">2018</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">88.33</td>
              <td valign="top" align="center" rowspan="1" colspan="1">82.25</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.46</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.04</td>
              <td valign="top" align="center" rowspan="1" colspan="1">83.38</td>
              <td valign="top" align="center" rowspan="1" colspan="1">92.67</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">AION Gehlot and Gupta (<xref rid="B13" ref-type="bibr">2021</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">88.84</td>
              <td valign="top" align="center" rowspan="1" colspan="1">82.56</td>
              <td valign="top" align="center" rowspan="1" colspan="1">92.26</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.95</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.02</td>
              <td valign="top" align="center" rowspan="1" colspan="1">92.88</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">CE-Net Gu et al. (<xref rid="B15" ref-type="bibr">2019</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.64</td>
              <td valign="top" align="center" rowspan="1" colspan="1">83.56</td>
              <td valign="top" align="center" rowspan="1" colspan="1">95.40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.47</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.99</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93.67</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Swin-Unet Cao et al. (<xref rid="B1" ref-type="bibr">2021</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">88.77</td>
              <td valign="top" align="center" rowspan="1" colspan="1">82.69</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.64</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.16</td>
              <td valign="top" align="center" rowspan="1" colspan="1">83.51</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.04</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">TransFuse Zhang et al. (<xref rid="B53" ref-type="bibr">2021</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.63</td>
              <td valign="top" align="center" rowspan="1" colspan="1">83.78</td>
              <td valign="top" align="center" rowspan="1" colspan="1">95.56</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.35</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.75</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93.73</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">TransUNet Chen et al. (<xref rid="B2" ref-type="bibr">2021</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.99</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.21</td>
              <td valign="top" align="center" rowspan="1" colspan="1">95.59</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.21</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85.42</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93.97</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>O-Net</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>90.30</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>84.52</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>95.65</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.72</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>85.89</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>94.09</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>Bold font to highlight the optimal values</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <table-wrap position="float" id="T3">
        <label>Table 3</label>
        <caption>
          <p>Classification accuracy of different methods on the ISIC2017 dataset.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Average</bold>
              </th>
              <th valign="top" align="center" colspan="4" rowspan="1">
                <bold>Nevus classification</bold>
              </th>
              <th rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Method</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AC</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AC</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>F1-score</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Pre</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>SP</bold>
              </th>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Swin Transformer Liu et al. (<xref rid="B29" ref-type="bibr">2021</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.22</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.18</td>
              <td valign="top" align="center" rowspan="1" colspan="1">62.16</td>
              <td valign="top" align="center" rowspan="1" colspan="1">91.76</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">AION Gehlot and Gupta (<xref rid="B13" ref-type="bibr">2021</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.55</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85.33</td>
              <td valign="top" align="center" rowspan="1" colspan="1">76.01</td>
              <td valign="top" align="center" rowspan="1" colspan="1">50.74</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86.86</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">TransMed Dai et al. (<xref rid="B8" ref-type="bibr">2021</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.11</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.19</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.10</td>
              <td valign="top" align="center" rowspan="1" colspan="1">61.90</td>
              <td valign="top" align="center" rowspan="1" colspan="1">92.16</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MobileNetV3 Howard et al. (<xref rid="B17" ref-type="bibr">2019</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.89</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.33</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.53</td>
              <td valign="top" align="center" rowspan="1" colspan="1">60.83</td>
              <td valign="top" align="center" rowspan="1" colspan="1">90.78</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">EfficientNet-B3 Tan and Le (<xref rid="B41" ref-type="bibr">2019</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85.22</td>
              <td valign="top" align="center" rowspan="1" colspan="1">90.67</td>
              <td valign="top" align="center" rowspan="1" colspan="1">82.64</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.67</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93.33</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Inception v4 Szegedy et al. (<xref rid="B40" ref-type="bibr">2016</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85.33</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.16</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.45</td>
              <td valign="top" align="center" rowspan="1" colspan="1">60.16</td>
              <td valign="top" align="center" rowspan="1" colspan="1">90.39</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">ResNet50 He et al. (<xref rid="B16" ref-type="bibr">2016</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85.44</td>
              <td valign="top" align="center" rowspan="1" colspan="1">91.00</td>
              <td valign="top" align="center" rowspan="1" colspan="1">82.97</td>
              <td valign="top" align="center" rowspan="1" colspan="1">68.37</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93.92</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DenseNet201 Huang et al. (<xref rid="B20" ref-type="bibr">2017</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86.56</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>92.00</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>85.36</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">69.81</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93.73</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">O-Net</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>87.22</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">91.67</td>
              <td valign="top" align="center" rowspan="1" colspan="1">83.51</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>72.73</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>95.29</bold>
              </td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>Average</bold>
              </td>
              <td valign="top" align="center" colspan="4" rowspan="1">
                <bold>Melanoma classification</bold>
              </td>
              <td valign="top" align="center" colspan="4" rowspan="1">
                <bold>Keratosis classification</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>Method</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>AC</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>AC</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>F1-score</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>Pre</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>SP</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>AC</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>F1-score</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>Pre</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>SP</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Swin Transformer Liu et al. (<xref rid="B29" ref-type="bibr">2021</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.22</td>
              <td valign="top" align="center" rowspan="1" colspan="1">73.00</td>
              <td valign="top" align="center" rowspan="1" colspan="1">71.63</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.07</td>
              <td valign="top" align="center" rowspan="1" colspan="1">73.91</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.17</td>
              <td valign="top" align="center" rowspan="1" colspan="1">68.45</td>
              <td valign="top" align="center" rowspan="1" colspan="1">45.33</td>
              <td valign="top" align="center" rowspan="1" colspan="1">83.02</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">AION Gehlot and Gupta (<xref rid="B13" ref-type="bibr">2021</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.55</td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.33</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.69</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85.40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">74.40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">82.00</td>
              <td valign="top" align="center" rowspan="1" colspan="1">69.73</td>
              <td valign="top" align="center" rowspan="1" colspan="1">54.46</td>
              <td valign="top" align="center" rowspan="1" colspan="1">90.48</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">TransMed Dai et al. (<xref rid="B8" ref-type="bibr">2021</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.11</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.17</td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.13</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.72</td>
              <td valign="top" align="center" rowspan="1" colspan="1">71.50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.00</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.83</td>
              <td valign="top" align="center" rowspan="1" colspan="1">59.63</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.56</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MobileNetV3 Howard et al. (<xref rid="B17" ref-type="bibr">2019</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.89</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.78</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86.51</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.36</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.83</td>
              <td valign="top" align="center" rowspan="1" colspan="1">74.59</td>
              <td valign="top" align="center" rowspan="1" colspan="1">62.75</td>
              <td valign="top" align="center" rowspan="1" colspan="1">92.13</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">EfficientNet-B3 Tan and Le (<xref rid="B41" ref-type="bibr">2019</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85.22</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.82</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86.70</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.85</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>75.71</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">59.84</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.86</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Inception v4 Szegedy et al. (<xref rid="B40" ref-type="bibr">2016</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85.33</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.83</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.05</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86.39</td>
              <td valign="top" align="center" rowspan="1" colspan="1">74.88</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86.00</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.94</td>
              <td valign="top" align="center" rowspan="1" colspan="1">67.37</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93.58</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">ResNet50 He et al. (<xref rid="B16" ref-type="bibr">2016</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85.44</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.99</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>87.90</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>78.26</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">83.83</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.28</td>
              <td valign="top" align="center" rowspan="1" colspan="1">57.69</td>
              <td valign="top" align="center" rowspan="1" colspan="1">88.61</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DenseNet201 Huang et al. (<xref rid="B20" ref-type="bibr">2017</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86.56</td>
              <td valign="top" align="center" rowspan="1" colspan="1">83.50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.55</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86.57</td>
              <td valign="top" align="center" rowspan="1" colspan="1">73.91</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.17</td>
              <td valign="top" align="center" rowspan="1" colspan="1">72.48</td>
              <td valign="top" align="center" rowspan="1" colspan="1">61.96</td>
              <td valign="top" align="center" rowspan="1" colspan="1">92.75</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">O-Net</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>87.22</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>84.17</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>81.58</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.49</td>
              <td valign="top" align="center" rowspan="1" colspan="1">67.63</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>85.83</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">74.19</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>70.00</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>95.03</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="TN1">
            <p><italic>Bold font to highlight the optimal values</italic>.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <fig position="float" id="F6">
        <label>Figure 6</label>
        <caption>
          <p>Receiver Operating Characteristic curves of the different methods for classification task on the ISIC2017 dataset.</p>
        </caption>
        <graphic xlink:href="fnins-16-876065-g0006" position="float"/>
      </fig>
      <fig position="float" id="F7">
        <label>Figure 7</label>
        <caption>
          <p>Comparison of different methods on the ISIC2017 dataset by visualization. <bold>(A)</bold> Image, <bold>(B)</bold> Ground Truth, <bold>(C)</bold> O-Net, <bold>(D)</bold> TransUNet, <bold>(E)</bold> Swin-UNet, <bold>(F)</bold> CE-Net, <bold>(G)</bold> R50 AttUNet, and <bold>(H)</bold> UNet.</p>
        </caption>
        <graphic xlink:href="fnins-16-876065-g0007" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>4.5. Ablation Study</title>
      <p>The results of the ablation studies are shown in <xref rid="T4" ref-type="table">Tables 4</xref>, <xref rid="T5" ref-type="table">5</xref>. We will compare and analyze the effects of different factors on the segmentation performance in the following sections.</p>
      <table-wrap position="float" id="T4">
        <label>Table 4</label>
        <caption>
          <p>Ablation study on the encoder of CNN method.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Encoder</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Params</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Dice↑</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>HD↓</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Aorta</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Gallbladder</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Kidney(L)</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Kidney(R)</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Liver</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Pancreas</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Spleen</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Stomach</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MobileNetV3 Howard et al. (<xref rid="B17" ref-type="bibr">2019</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>5.48</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">76.66</td>
              <td valign="top" align="center" rowspan="1" colspan="1">26.27</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86.12</td>
              <td valign="top" align="center" rowspan="1" colspan="1">62.25</td>
              <td valign="top" align="center" rowspan="1" colspan="1">82.07</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>90.65</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.06</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.72</td>
              <td valign="top" align="center" rowspan="1" colspan="1">88.62</td>
              <td valign="top" align="center" rowspan="1" colspan="1">73.77</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DenseNet201 Huang et al. (<xref rid="B20" ref-type="bibr">2017</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">20.01</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.91</td>
              <td valign="top" align="center" rowspan="1" colspan="1">20.45</td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.52</td>
              <td valign="top" align="center" rowspan="1" colspan="1">65.52</td>
              <td valign="top" align="center" rowspan="1" colspan="1">82.61</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.20</td>
              <td valign="top" align="center" rowspan="1" colspan="1">95.05</td>
              <td valign="top" align="center" rowspan="1" colspan="1">57.42</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86.40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.65</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Resnet50 He et al. (<xref rid="B16" ref-type="bibr">2016</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">25.55</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.16</td>
              <td valign="top" align="center" rowspan="1" colspan="1">23.01</td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.71</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.86</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.73</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.22</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.18</td>
              <td valign="top" align="center" rowspan="1" colspan="1">58.86</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>90.42</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.31</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Inception v3 Szegedy et al. (<xref rid="B40" ref-type="bibr">2016</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">23.83</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.36</td>
              <td valign="top" align="center" rowspan="1" colspan="1">22.78</td>
              <td valign="top" align="center" rowspan="1" colspan="1">88.09</td>
              <td valign="top" align="center" rowspan="1" colspan="1">63.76</td>
              <td valign="top" align="center" rowspan="1" colspan="1">82.19</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.25</td>
              <td valign="top" align="center" rowspan="1" colspan="1">95.16</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>65.17</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.12</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>82.13</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">EfficientNet-b3 Tan and Le (<xref rid="B41" ref-type="bibr">2019</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">12.23</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>80.61</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>21.04</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>88.36</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>67.45</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>84.44</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.13</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>95.24</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">61.52</td>
              <td valign="top" align="center" rowspan="1" colspan="1">90.03</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.74</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>The symbol ↑ means the higher value, the better</italic>.</p>
          <p>
            <italic>The symbol ↓ means the lower value, the better.</italic>
          </p>
          <p><italic>Bold font to highlight the optimal values</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <table-wrap position="float" id="T5">
        <label>Table 5</label>
        <caption>
          <p>Ablation study on the combination of CNN method and Swin Transformer method.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" colspan="2" rowspan="1">
                <bold>Encoder</bold>
              </th>
              <th valign="top" align="center" colspan="2" rowspan="1">
                <bold>Decoder</bold>
              </th>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Efficient</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Swin</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>CNN</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Swin transformer</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Dice↑</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>HD↓</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Aorta</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Gallbladder</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Kidney(L)</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Kidney(R)</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Liver</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Pancreas</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Spleen</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Stomach</bold>
              </th>
            </tr>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>net-block</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>transformer</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>decoder</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>decoder</bold>
              </th>
              <th rowspan="1" colspan="1"/>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">✓</td>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">✓</td>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">78.86</td>
              <td valign="top" align="center" rowspan="1" colspan="1">28.86</td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.72</td>
              <td valign="top" align="center" rowspan="1" colspan="1">62.19</td>
              <td valign="top" align="center" rowspan="1" colspan="1">83.11</td>
              <td valign="top" align="center" rowspan="1" colspan="1">76.67</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.49</td>
              <td valign="top" align="center" rowspan="1" colspan="1">56.61</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.48</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.58</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">✓</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">✓</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.93</td>
              <td valign="top" align="center" rowspan="1" colspan="1">26.88</td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.90</td>
              <td valign="top" align="center" rowspan="1" colspan="1">68.09</td>
              <td valign="top" align="center" rowspan="1" colspan="1">83.89</td>
              <td valign="top" align="center" rowspan="1" colspan="1">76.05</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.42</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>62.95</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.32</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.86</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">✓</td>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">✓</td>
              <td valign="top" align="center" rowspan="1" colspan="1">✓</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.34</td>
              <td valign="top" align="center" rowspan="1" colspan="1">22.53</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>88.67</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">67.38</td>
              <td valign="top" align="center" rowspan="1" colspan="1">83.95</td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.01</td>
              <td valign="top" align="center" rowspan="1" colspan="1">95.12</td>
              <td valign="top" align="center" rowspan="1" colspan="1">60.06</td>
              <td valign="top" align="center" rowspan="1" colspan="1">88.76</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>81.77</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">✓</td>
              <td valign="top" align="center" rowspan="1" colspan="1">✓</td>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">77.55</td>
              <td valign="top" align="center" rowspan="1" colspan="1">31.03</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86.14</td>
              <td valign="top" align="center" rowspan="1" colspan="1">63.49</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.59</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.82</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93.68</td>
              <td valign="top" align="center" rowspan="1" colspan="1">54.61</td>
              <td valign="top" align="center" rowspan="1" colspan="1">90.19</td>
              <td valign="top" align="center" rowspan="1" colspan="1">74.87</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">✓</td>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">✓</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.13</td>
              <td valign="top" align="center" rowspan="1" colspan="1">21.55</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85.47</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.53</td>
              <td valign="top" align="center" rowspan="1" colspan="1">83.28</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.61</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.29</td>
              <td valign="top" align="center" rowspan="1" colspan="1">56.58</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>90.66</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">76.60</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">✓</td>
              <td valign="top" align="center" rowspan="1" colspan="1">✓</td>
              <td valign="top" align="center" rowspan="1" colspan="1">✓</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.38</td>
              <td valign="top" align="center" rowspan="1" colspan="1">22.34</td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.60</td>
              <td valign="top" align="center" rowspan="1" colspan="1">62.53</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.86</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>80.54</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.42</td>
              <td valign="top" align="center" rowspan="1" colspan="1">58.75</td>
              <td valign="top" align="center" rowspan="1" colspan="1">90.64</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.66</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">✓</td>
              <td valign="top" align="center" rowspan="1" colspan="1">✓</td>
              <td valign="top" align="center" rowspan="1" colspan="1">✓</td>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">79.47</td>
              <td valign="top" align="center" rowspan="1" colspan="1">29.19</td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.71</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.21</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.64</td>
              <td valign="top" align="center" rowspan="1" colspan="1">74.69</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.65</td>
              <td valign="top" align="center" rowspan="1" colspan="1">61.61</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.19</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.02</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">✓</td>
              <td valign="top" align="center" rowspan="1" colspan="1">✓</td>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">✓</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.41</td>
              <td valign="top" align="center" rowspan="1" colspan="1">27.33</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86.74</td>
              <td valign="top" align="center" rowspan="1" colspan="1">71.19</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.32</td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.29</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.30</td>
              <td valign="top" align="center" rowspan="1" colspan="1">60.63</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.64</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">✓</td>
              <td valign="top" align="center" rowspan="1" colspan="1">✓</td>
              <td valign="top" align="center" rowspan="1" colspan="1">✓</td>
              <td valign="top" align="center" rowspan="1" colspan="1">✓</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>80.61</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>21.04</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">88.36</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>67.45</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>84.44</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.13</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>95.24</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">61.52</td>
              <td valign="top" align="center" rowspan="1" colspan="1">90.03</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.74</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>The symbol ↑ means the higher value, the better</italic>.</p>
          <p><italic>The symbol ↓ means the lower value, the better</italic>.</p>
          <p><italic>Bold font to highlight the optimal values</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <p><bold>Effect of encoder:</bold> The experimental results in <xref rid="T4" ref-type="table">Table 4</xref> show that the best results are achieved by using the EfficientNet block as the encoder, while the number of parameters is not large. The parameter quantity of the MobileNet is smaller than that of the EfficientNet, but its accuracy is far too poor than the others. The accuracy of Inception v3 is similar to ours, but the amount of calculation is much larger than that of EfficientNet. Therefore, we use EfficientNet as a CNN-based encoder.</p>
      <p><bold>Effect of combination:</bold> The segmentation network consists of encoder and decoder. How to combine the CNN based method and the Swin Transformer based method is a point worth exploring. <xref rid="T5" ref-type="table">Table 5</xref> shows the effects of adopting different models for encoder and decoder. It can be seen from the results that the best performance is achieved by combining them in both the encoder and decoder parts. As can be seen from the results, better segmentation performance is achieved when CNN is used in the encoder part and Swin Transformer is used in the decoder part.</p>
      <p><bold>Effect of learning rate and batch size:</bold> To explore the best learning rate and batch size in the training process of the algorithm, we carried out a series of experiments. The experimental results are shown in <xref rid="T6" ref-type="table">Table 6</xref>. It can be seen from the top half of the chart that the best Dice was obtained when the learning rate was set to 1e-2. Although the best HD was obtained when the learning rate was set to 1e-1, its Dice was lower, therefore, we chose the learning rate of 5e-2. We can also draw from the bottom half of the chart that the best dice was obtained when the batch size was set to 24. Although the HD is lower when batch size was set to 8 and 6, the Dice of the Gallbladder is far too low, which is not conducive to the overall segmentation, therefore, the batch size of 24 would be more appropriate.</p>
      <table-wrap position="float" id="T6">
        <label>Table 6</label>
        <caption>
          <p>Ablation study on learning rate and batch size.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Learn rate</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Dice↑</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>HD↓</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Aorta</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Gallbladder</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Kidney(L)</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Kidney(R)</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Liver</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Pancreas</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Spleen</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Stomach</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">1e-1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.21</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>20.06</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">86.56</td>
              <td valign="top" align="center" rowspan="1" colspan="1">63.48</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>84.61</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>77.14</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.32</td>
              <td valign="top" align="center" rowspan="1" colspan="1">56.99</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>91.90</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.69</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">5e-2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>80.61</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">21.04</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>88.36</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">67.45</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.44</td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.13</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>95.24</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>61.52</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">90.03</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.74</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">1e-2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.07</td>
              <td valign="top" align="center" rowspan="1" colspan="1">20.14</td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.64</td>
              <td valign="top" align="center" rowspan="1" colspan="1">67.74</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.95</td>
              <td valign="top" align="center" rowspan="1" colspan="1">74.69</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.71</td>
              <td valign="top" align="center" rowspan="1" colspan="1">58.33</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.44</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.03</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">5e-3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.76</td>
              <td valign="top" align="center" rowspan="1" colspan="1">23.07</td>
              <td valign="top" align="center" rowspan="1" colspan="1">88.18</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>68.51</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">83.60</td>
              <td valign="top" align="center" rowspan="1" colspan="1">76.92</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.42</td>
              <td valign="top" align="center" rowspan="1" colspan="1">58.84</td>
              <td valign="top" align="center" rowspan="1" colspan="1">88.59</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.03</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">1e-3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">76.57</td>
              <td valign="top" align="center" rowspan="1" colspan="1">30.37</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85.28</td>
              <td valign="top" align="center" rowspan="1" colspan="1">62.96</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.61</td>
              <td valign="top" align="center" rowspan="1" colspan="1">74.51</td>
              <td valign="top" align="center" rowspan="1" colspan="1">92.96</td>
              <td valign="top" align="center" rowspan="1" colspan="1">54.13</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86.37</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>84.70</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>Batch size</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>Dice</bold>↑</td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>HD</bold>↓</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>Aorta</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>Gallbladder</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>Kidney(L)</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>Kidney(R)</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>Liver</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>Pancreas</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>Spleen</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>Stomach</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.81</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>15.67</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">88.12</td>
              <td valign="top" align="center" rowspan="1" colspan="1">44.45</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>84.59</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>80.24</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.73</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>67.40</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.97</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.00</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">16</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.36</td>
              <td valign="top" align="center" rowspan="1" colspan="1">18.25</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>88.69</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">38.12</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.57</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.46</td>
              <td valign="top" align="center" rowspan="1" colspan="1">95.16</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.20</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>91.44</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>83.27</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">24</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>80.61</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">21.04</td>
              <td valign="top" align="center" rowspan="1" colspan="1">88.36</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>67.45</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.44</td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.13</td>
              <td valign="top" align="center" rowspan="1" colspan="1">95.24</td>
              <td valign="top" align="center" rowspan="1" colspan="1">61.52</td>
              <td valign="top" align="center" rowspan="1" colspan="1">90.03</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.74</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">32</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.35</td>
              <td valign="top" align="center" rowspan="1" colspan="1">27.93</td>
              <td valign="top" align="center" rowspan="1" colspan="1">88.32</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.70</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.94</td>
              <td valign="top" align="center" rowspan="1" colspan="1">76.19</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>95.31</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">64.06</td>
              <td valign="top" align="center" rowspan="1" colspan="1">88.94</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.27</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>The symbol ↑ means the higher value, the better</italic>.</p>
          <p>
            <italic>The symbol ↓ means the lower value, the better.</italic>
          </p>
          <p><italic>Bold font to highlight the optimal values</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="s5">
    <title>5. Conclusion</title>
    <p>We introduce a novel method based on the combination of CNN and Swin Transformer for medical image segmentation and classification. To make full use of the global and the local information to improve medical image segmentation and classification, we propose O-Net, which combines the advantages of these two structures for improving both the segmentation and the classification performance. We combine CNN and transformer in both encoder and decoder parts of the network. In addition, we have shown that the proposed segmentation network is beneficial for the classification task. Experimental results have demonstrated that the proposed O-Net achieves competitive performance and good generalization ability in both the segmentation and the classification tasks.</p>
  </sec>
  <sec sec-type="data-availability" id="s6">
    <title>Data Availability Statement</title>
    <p>The original contributions presented in the study are included in the article/supplementary material, further inquiries can be directed to the corresponding authors.</p>
  </sec>
  <sec id="s7">
    <title>Author Contributions</title>
    <p>TW, JL, ZHa, ZHu, YH, YD, QG, MD, TT, and GC: concept and design. TW, JL, HZ, JW, MC, and TT: acquisition of data. TW, JL, ZHa, ZHu, QG, and TT: model design. TW, JL, ZHa, ZHu, YH, YD, and TT: data analysis. TW, JL, ZHa, ZHu, YH, YD, TT, and GC: manuscript drafting. TW, JL, ZHa, ZHu, YH, YD, HZ, JW, MC, HJ, R-GL, QG, MD, TT, and GC: approval. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec sec-type="funding-information" id="s8">
    <title>Funding</title>
    <p>This work was supported in part by the National Natural Science Foundation of China under Grant Nos. 61901120 and 62171133, the Science and Technology Program of Fujian Province of China under Grant No. 2019YZ016006, and Health and Family Planning Research Talent Training Program of Fujian Province under Grant No. 2020GGB009.</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of Interest</title>
    <p>QG and TT were employed by Imperial Vision Technology. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s9">
    <title>Publisher's Note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>H.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Jiang</surname><given-names>D.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Tian</surname><given-names>Q.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>Swin-unet: unet-like pure transformer for medical image segmentation</article-title>. <source>arXiv [Preprint] arXiv:</source>2105.05537. <pub-id pub-id-type="doi">10.48550/arXiv.2105.05537</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Lu</surname><given-names>Y.</given-names></name><name><surname>Yu</surname><given-names>Q.</given-names></name><name><surname>Luo</surname><given-names>X.</given-names></name><name><surname>Adeli</surname><given-names>E.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>Transunet: Transformers make strong encoders for medical image segmentation</article-title>. <source>arXiv [Preprint] arXiv:</source>2102.04306. <pub-id pub-id-type="doi">10.48550/arXiv.2102.04306</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L.-C.</given-names></name><name><surname>Papandreou</surname><given-names>G.</given-names></name><name><surname>Kokkinos</surname><given-names>I.</given-names></name><name><surname>Murphy</surname><given-names>K.</given-names></name><name><surname>Yuille</surname><given-names>A. L.</given-names></name></person-group> (<year>2017</year>). <article-title>Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>. <volume>40</volume>, <fpage>834</fpage>–<lpage>848</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id><?supplied-pmid 28463186?><pub-id pub-id-type="pmid">28463186</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L.-C.</given-names></name><name><surname>Zhu</surname><given-names>Y.</given-names></name><name><surname>Papandreou</surname><given-names>G.</given-names></name><name><surname>Schroff</surname><given-names>F.</given-names></name><name><surname>Adam</surname><given-names>H.</given-names></name></person-group> (<year>2018</year>). <article-title>“Encoder-decoder with atrous separable convolution for semantic image segmentation,”</article-title> in <source>Proceedings of the European Conference on Computer Vision (ECCV)</source> (<publisher-loc>Munich</publisher-loc>), <fpage>801</fpage>–<lpage>818</lpage>. <pub-id pub-id-type="doi">10.1145/3065386</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Çiçek</surname><given-names>Ö.</given-names></name><name><surname>Abdulkadir</surname><given-names>A.</given-names></name><name><surname>Lienkamp</surname><given-names>S. S.</given-names></name><name><surname>Brox</surname><given-names>T.</given-names></name><name><surname>Ronneberger</surname><given-names>O.</given-names></name></person-group> (<year>2016</year>). <article-title>“3d u-net: learning dense volumetric segmentation from sparse annotation,”</article-title> in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source> (<publisher-loc>Istanbul</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>424</fpage>–<lpage>432</lpage>.</mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Codella</surname><given-names>N. C.</given-names></name><name><surname>Gutman</surname><given-names>D.</given-names></name><name><surname>Celebi</surname><given-names>M. E.</given-names></name><name><surname>Helba</surname><given-names>B.</given-names></name><name><surname>Marchetti</surname><given-names>M. A.</given-names></name><name><surname>Dusza</surname><given-names>S. W.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>“Skin lesion analysis toward melanoma detection: a challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic),”</article-title> in <source>2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</source> (<publisher-loc>Washington, DC</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>168</fpage>–<lpage>172</lpage>.</mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dai</surname><given-names>J.</given-names></name><name><surname>Qi</surname><given-names>H.</given-names></name><name><surname>Xiong</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Zhang</surname><given-names>G.</given-names></name><name><surname>Hu</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>“Deformable convolutional networks,”</article-title> in <source>Proceedings of the IEEE International Conference on Computer Vision</source> (<publisher-loc>Venice</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>764</fpage>–<lpage>773</lpage>.</mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dai</surname><given-names>Y.</given-names></name><name><surname>Gao</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>F.</given-names></name></person-group> (<year>2021</year>). <article-title>Transmed: Transformers advance multi-modal medical image classification</article-title>. <source>Diagnostics</source>
<volume>11</volume>, <fpage>1384</fpage>. <pub-id pub-id-type="doi">10.3390/diagnostics11081384</pub-id><?supplied-pmid 34441318?><pub-id pub-id-type="pmid">34441318</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J.</given-names></name><name><surname>Dong</surname><given-names>W.</given-names></name><name><surname>Socher</surname><given-names>R.</given-names></name><name><surname>Li</surname><given-names>L.-J.</given-names></name><name><surname>Li</surname><given-names>K.</given-names></name><name><surname>Fei-Fei</surname><given-names>L.</given-names></name></person-group> (<year>2009</year>). <article-title>“Imagenet: A large-scale hierarchical image database,”</article-title> in <source>2009 IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Miami, FL</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>248</fpage>–<lpage>255</lpage></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dosovitskiy</surname><given-names>A.</given-names></name><name><surname>Beyer</surname><given-names>L.</given-names></name><name><surname>Kolesnikov</surname><given-names>A.</given-names></name><name><surname>Weissenborn</surname><given-names>D.</given-names></name><name><surname>Zhai</surname><given-names>X.</given-names></name><name><surname>Unterthiner</surname><given-names>T.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>An image is worth 16x16 words: transformers for image recognition at scale</article-title>. <source>arXiv [Preprint] arXiv:</source>2010.11929. <pub-id pub-id-type="doi">10.48550/arXiv.2010.11929</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>H.</given-names></name><name><surname>Cheng</surname><given-names>J.</given-names></name><name><surname>Xu</surname><given-names>Y.</given-names></name><name><surname>Wong</surname><given-names>D. W. K.</given-names></name><name><surname>Liu</surname><given-names>J.</given-names></name><name><surname>Cao</surname><given-names>X.</given-names></name></person-group> (<year>2018</year>). <article-title>Joint optic disc and cup segmentation based on multi-label deep network and polar transformation</article-title>. <source>IEEE Trans. Med. Imaging</source>
<volume>37</volume>, <fpage>1597</fpage>–<lpage>1605</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2018.2791488</pub-id><?supplied-pmid 29969410?><pub-id pub-id-type="pmid">29969410</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>S.</given-names></name><name><surname>Lu</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Zhou</surname><given-names>Y.</given-names></name><name><surname>Shen</surname><given-names>W.</given-names></name><name><surname>Fishman</surname><given-names>E.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>“Domain adaptive relational reasoning for 3d multi-organ segmentation,”</article-title> in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source> (<publisher-loc>Lima</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>656</fpage>–<lpage>666</lpage>.</mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gehlot</surname><given-names>S.</given-names></name><name><surname>Gupta</surname><given-names>A.</given-names></name></person-group> (<year>2021</year>). <article-title>“Self-supervision based dual-transformation learning for stain normalization, classification andsegmentation,”</article-title> in <source>International Workshop on Machine Learning in Medical Imaging</source> (<publisher-loc>Strasbourg</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>477</fpage>–<lpage>486</lpage>.</mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gehlot</surname><given-names>S.</given-names></name><name><surname>Gupta</surname><given-names>A.</given-names></name><name><surname>Gupta</surname><given-names>R.</given-names></name></person-group> (<year>2020</year>). <article-title>“Ednfc-net: Convolutional neural network with nested feature concatenation for nuclei-instance segmentation,”</article-title> in <source>ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source> (<publisher-loc>Barcelona</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>1389</fpage>–<lpage>1393</lpage>.</mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Z.</given-names></name><name><surname>Cheng</surname><given-names>J.</given-names></name><name><surname>Fu</surname><given-names>H.</given-names></name><name><surname>Zhou</surname><given-names>K.</given-names></name><name><surname>Hao</surname><given-names>H.</given-names></name><name><surname>Zhao</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Ce-net: Context encoder network for 2d medical image segmentation</article-title>. <source>IEEE Trans. Med. Imaging</source><volume>38</volume>, <fpage>2281</fpage>–<lpage>2292</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2019.2903562</pub-id><?supplied-pmid 30843824?><pub-id pub-id-type="pmid">30843824</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). <article-title>“Deep residual learning for image recognition,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Las Vegas, NV</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>770</fpage>–<lpage>778</lpage>.<?supplied-pmid 32166560?></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>A.</given-names></name><name><surname>Sandler</surname><given-names>M.</given-names></name><name><surname>Chu</surname><given-names>G.</given-names></name><name><surname>Chen</surname><given-names>L.-C.</given-names></name><name><surname>Chen</surname><given-names>B.</given-names></name><name><surname>Tan</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>“Searching for mobilenetv3,”</article-title> in <source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source> (<publisher-loc>Seoul</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>1314</fpage>–<lpage>1324</lpage>.</mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>H.</given-names></name><name><surname>Gu</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Dai</surname><given-names>J.</given-names></name><name><surname>Wei</surname><given-names>Y.</given-names></name></person-group> (<year>2018</year>). <article-title>“Relation networks for object detection,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Salt Lake City, UT</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>3588</fpage>–<lpage>3597</lpage>.</mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>H.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Xie</surname><given-names>Z.</given-names></name><name><surname>Lin</surname><given-names>S.</given-names></name></person-group> (<year>2019</year>). <article-title>“Local relation networks for image recognition,”</article-title> in <source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source>, (Seoul) <fpage>3464</fpage>–<lpage>3473</lpage>.</mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>G.</given-names></name><name><surname>Liu</surname><given-names>Z.</given-names></name><name><surname>Van Der Maaten</surname><given-names>L.</given-names></name><name><surname>Weinberger</surname><given-names>K. Q.</given-names></name></person-group> (<year>2017</year>). <article-title>“Densely connected convolutional networks,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Honolulu, HI</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>4700</fpage>–<lpage>4708</lpage>.</mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>H.</given-names></name><name><surname>Lin</surname><given-names>L.</given-names></name><name><surname>Tong</surname><given-names>R.</given-names></name><name><surname>Hu</surname><given-names>H.</given-names></name><name><surname>Zhang</surname><given-names>Q.</given-names></name><name><surname>Iwamoto</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>“Unet 3+: a full-scale connected unet for medical image segmentation,”</article-title> in <source>ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source> (<publisher-loc>Barcelona</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>1055</fpage>–<lpage>1059</lpage>.</mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ji</surname><given-names>J.</given-names></name><name><surname>Lu</surname><given-names>X.</given-names></name><name><surname>Luo</surname><given-names>M.</given-names></name><name><surname>Yin</surname><given-names>M.</given-names></name><name><surname>Miao</surname><given-names>Q.</given-names></name><name><surname>Liu</surname><given-names>X.</given-names></name></person-group> (<year>2020</year>). <article-title>Parallel fully convolutional network for semantic segmentation</article-title>. <source>IEEE Access</source>
<volume>9</volume>, <fpage>673</fpage>–<lpage>682</lpage>. <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3042254</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Hinton</surname><given-names>G. E.</given-names></name></person-group> (<year>2012</year>). <article-title>Imagenet classification with deep convolutional neural networks</article-title>. <source>Adv. Neural Inf. Process. Syst</source>. <volume>25</volume>, <fpage>1097</fpage>–<lpage>1105</lpage>.</mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>P.</given-names></name><name><surname>Nagar</surname><given-names>P.</given-names></name><name><surname>Arora</surname><given-names>C.</given-names></name><name><surname>Gupta</surname><given-names>A.</given-names></name></person-group> (<year>2018</year>). <article-title>“U-segnet: fully convolutional neural network based automated brain tissue segmentation tool,”</article-title> in <source>2018 25th IEEE International Conference on Image Processing (ICIP)</source> (<publisher-loc>Athens</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>3503</fpage>–<lpage>3507</lpage>.</mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Chen</surname><given-names>H.</given-names></name><name><surname>Qi</surname><given-names>X.</given-names></name><name><surname>Dou</surname><given-names>Q.</given-names></name><name><surname>Fu</surname><given-names>C.-W.</given-names></name><name><surname>Heng</surname><given-names>P.-A.</given-names></name></person-group> (<year>2018</year>). <article-title>H-denseunet: hybrid densely connected unet for liver and tumor segmentation from ct volumes</article-title>. <source>IEEE Trans. Med. Imaging</source>
<volume>37</volume>, <fpage>2663</fpage>–<lpage>2674</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2018.2845918</pub-id><?supplied-pmid 29994201?><pub-id pub-id-type="pmid">29994201</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>D.</given-names></name><name><surname>Qiu</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Yin</surname><given-names>X.</given-names></name><name><surname>Xing</surname><given-names>J.</given-names></name><name><surname>Yang</surname><given-names>Z.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Coronary angiography video segmentation method for assisting cardiovascular disease interventional treatment</article-title>. <source>BMC Med. Imaging</source><volume>20</volume>, <fpage>1</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1186/s12880-020-00460-9</pub-id><?supplied-pmid 32546137?><pub-id pub-id-type="pmid">32546137</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>J.</given-names></name><name><surname>Yang</surname><given-names>C.</given-names></name><name><surname>Zeng</surname><given-names>M.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name></person-group> (<year>2022</year>). <article-title>Transconver: transformer and convolution parallel network for developing automatic brain tumor segmentation in mri images</article-title>. <source>Quant. Imaging Med. Surg</source>. <volume>12</volume>, <fpage>2397</fpage>. <pub-id pub-id-type="doi">10.21037/qims-21-919</pub-id><?supplied-pmid 35371952?><pub-id pub-id-type="pmid">35371952</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>T.-Y.</given-names></name><name><surname>Dollár</surname><given-names>P.</given-names></name><name><surname>Girshick</surname><given-names>R.</given-names></name><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Hariharan</surname><given-names>B.</given-names></name><name><surname>Belongie</surname><given-names>S.</given-names></name></person-group> (<year>2017</year>). <article-title>“Feature pyramid networks for object detection,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Honolulu, HI</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>2117</fpage>–<lpage>2125</lpage>.</mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Z.</given-names></name><name><surname>Lin</surname><given-names>Y.</given-names></name><name><surname>Cao</surname><given-names>Y.</given-names></name><name><surname>Hu</surname><given-names>H.</given-names></name><name><surname>Wei</surname><given-names>Y.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>Swin transformer: Hierarchical vision transformer using shifted windows</article-title>. <source>arXiv [Preprint] arXiv:</source>2103.14030. <pub-id pub-id-type="doi">10.1109/ICCV48922.2021.00986</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Milletari</surname><given-names>F.</given-names></name><name><surname>Navab</surname><given-names>N.</given-names></name><name><surname>Ahmadi</surname><given-names>S.-A.</given-names></name></person-group> (<year>2016</year>). <article-title>“V-net: fully convolutional neural networks for volumetric medical image segmentation,”</article-title> in <source>2016 Fourth International Conference on 3D Vision (3DV)</source> (<publisher-loc>Stanford, CA</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>565</fpage>–<lpage>571</lpage>.</mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Oktay</surname><given-names>O.</given-names></name><name><surname>Schlemper</surname><given-names>J.</given-names></name><name><surname>Folgoc</surname><given-names>L. L.</given-names></name><name><surname>Lee</surname><given-names>M.</given-names></name><name><surname>Heinrich</surname><given-names>M.</given-names></name><name><surname>Misawa</surname><given-names>K.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Attention u-net: learning where to look for the pancreas</article-title>. <source>arXiv [Preprint] arXiv:</source>1804.03999. <pub-id pub-id-type="doi">10.48550/arXiv.1804.03999</pub-id><?supplied-pmid 35474556?></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patil</surname><given-names>D. D.</given-names></name><name><surname>Deore</surname><given-names>S. G.</given-names></name></person-group> (<year>2013</year>). <article-title>Medical image segmentation: a review</article-title>. <source>Int. J. Comput. Sci. Mobile Comput</source>. <volume>2</volume>, <fpage>22</fpage>–<lpage>27</lpage>.</mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qin</surname><given-names>X.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Huang</surname><given-names>C.</given-names></name><name><surname>Dehghan</surname><given-names>M.</given-names></name><name><surname>Zaiane</surname><given-names>O. R.</given-names></name><name><surname>Jagersand</surname><given-names>M.</given-names></name></person-group> (<year>2020</year>). <article-title>U2-net: Going deeper with nested u-structure for salient object detection</article-title>. <source>Pattern Recognit</source>. <volume>106</volume>, <fpage>107404</fpage>. <pub-id pub-id-type="doi">10.1016/j.patcog.2020.107404</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O.</given-names></name><name><surname>Fischer</surname><given-names>P.</given-names></name><name><surname>Brox</surname><given-names>T.</given-names></name></person-group> (<year>2015</year>). <article-title>“U-net: convolutional networks for biomedical image segmentation,”</article-title> in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source> (<publisher-loc>Munich</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>234</fpage>–<lpage>241</lpage>.</mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sandler</surname><given-names>M.</given-names></name><name><surname>Howard</surname><given-names>A.</given-names></name><name><surname>Zhu</surname><given-names>M.</given-names></name><name><surname>Zhmoginov</surname><given-names>A.</given-names></name><name><surname>Chen</surname><given-names>L.-C.</given-names></name></person-group> (<year>2018</year>). <article-title>“Mobilenetv2: inverted residuals and linear bottlenecks,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Salt Lake City, UT</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>4510</fpage>–<lpage>4520</lpage>.</mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schlemper</surname><given-names>J.</given-names></name><name><surname>Oktay</surname><given-names>O.</given-names></name><name><surname>Schaap</surname><given-names>M.</given-names></name><name><surname>Heinrich</surname><given-names>M.</given-names></name><name><surname>Kainz</surname><given-names>B.</given-names></name><name><surname>Glocker</surname><given-names>B.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Attention gated networks: Learning to leverage salient regions in medical images</article-title>. <source>Med. Image Anal</source>. <volume>53</volume>, <fpage>197</fpage>–<lpage>207</lpage>. <pub-id pub-id-type="doi">10.1016/j.media.2019.01.012</pub-id><?supplied-pmid 30802813?><pub-id pub-id-type="pmid">30802813</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K.</given-names></name><name><surname>Zisserman</surname><given-names>A.</given-names></name></person-group> (<year>2014</year>). <article-title>Very deep convolutional networks for large-scale image recognition</article-title>. <source>arXiv [Preprint] arXiv:</source>1409.1556. <pub-id pub-id-type="doi">10.48550/arXiv.1409.1556</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>K.</given-names></name><name><surname>Xiao</surname><given-names>B.</given-names></name><name><surname>Liu</surname><given-names>D.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name></person-group> (<year>2019</year>). <article-title>“Deep high-resolution representation learning for human pose estimation,”</article-title> in <source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Long Beach, CA</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>5693</fpage>–<lpage>5703</lpage>.</mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C.</given-names></name><name><surname>Liu</surname><given-names>W.</given-names></name><name><surname>Jia</surname><given-names>Y.</given-names></name><name><surname>Sermanet</surname><given-names>P.</given-names></name><name><surname>Reed</surname><given-names>S.</given-names></name><name><surname>Anguelov</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>“Going deeper with convolutions,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Boston, MA</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C.</given-names></name><name><surname>Vanhoucke</surname><given-names>V.</given-names></name><name><surname>Ioffe</surname><given-names>S.</given-names></name><name><surname>Shlens</surname><given-names>J.</given-names></name><name><surname>Wojna</surname><given-names>Z.</given-names></name></person-group> (<year>2016</year>). <article-title>“Rethinking the inception architecture for computer vision,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Las Vegas, NV</publisher-loc>,: <publisher-name>IEEE</publisher-name>), <fpage>2818</fpage>–<lpage>2826</lpage>.</mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>M.</given-names></name><name><surname>Le</surname><given-names>Q.</given-names></name></person-group> (<year>2019</year>). <article-title>“Efficientnet: rethinking model scaling for convolutional neural networks,”</article-title> in <source>International Conference on Machine Learning</source> (<publisher-loc>Taiyuan</publisher-loc>: <publisher-name>PMLR</publisher-name>), <fpage>6105</fpage>–<lpage>6114</lpage>.</mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Touvron</surname><given-names>H.</given-names></name><name><surname>Cord</surname><given-names>M.</given-names></name><name><surname>Douze</surname><given-names>M.</given-names></name><name><surname>Massa</surname><given-names>F.</given-names></name><name><surname>Sablayrolles</surname><given-names>A.</given-names></name><name><surname>Jégou</surname><given-names>H.</given-names></name></person-group> (<year>2021</year>). <article-title>“Training data-efficient image transformers &amp;distillation through attention,”</article-title> in <source>International Conference on Machine Learning</source> (<publisher-loc>PMLR</publisher-loc>), <fpage>10347</fpage>–<lpage>10357</lpage>.</mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vasuda</surname><given-names>P.</given-names></name><name><surname>Satheesh</surname><given-names>S.</given-names></name></person-group> (<year>1713</year>). <article-title>Improved fuzzy c-means algorithm for mr brain image segmentation</article-title>. <source>Int. J. Comput. Sci. Eng</source>. <volume>2</volume>, <fpage>2010</fpage>.</mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A.</given-names></name><name><surname>Shazeer</surname><given-names>N.</given-names></name><name><surname>Parmar</surname><given-names>N.</given-names></name><name><surname>Uszkoreit</surname><given-names>J.</given-names></name><name><surname>Jones</surname><given-names>L.</given-names></name><name><surname>Gomez</surname><given-names>A. N.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>“Attention is all you need,”</article-title> in <source>Advances in Neural Information Processing Systems</source>, (Long Beach, CA), <fpage>5998</fpage>–<lpage>6008</lpage>.</mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>H.</given-names></name><name><surname>Xie</surname><given-names>S.</given-names></name><name><surname>Lin</surname><given-names>L.</given-names></name><name><surname>Iwamoto</surname><given-names>Y.</given-names></name><name><surname>Han</surname><given-names>X.-H.</given-names></name><name><surname>Chen</surname><given-names>Y.-W.</given-names></name><etal/></person-group>. (<year>2021a</year>). <article-title>Mixed transformer u-net for medical image segmentation</article-title>. <source>arXiv [Preprint] arXiv:</source>2111.04734. <pub-id pub-id-type="doi">10.1109/ICASSP43922.2022.9746172</pub-id><?supplied-pmid 27295638?></mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>R.</given-names></name><name><surname>Cao</surname><given-names>S.</given-names></name><name><surname>Ma</surname><given-names>K.</given-names></name><name><surname>Zheng</surname><given-names>Y.</given-names></name><name><surname>Meng</surname><given-names>D.</given-names></name></person-group> (<year>2021b</year>). <article-title>Pairwise learning for medical image segmentation</article-title>. <source>Med. Image Anal</source>. <volume>67</volume>, <fpage>101876</fpage>. <pub-id pub-id-type="doi">10.1016/j.media.2020.101876</pub-id><?supplied-pmid 33197863?><pub-id pub-id-type="pmid">33197863</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>R.</given-names></name><name><surname>Zhou</surname><given-names>Y.</given-names></name><name><surname>Zhao</surname><given-names>C.</given-names></name><name><surname>Wu</surname><given-names>H.</given-names></name></person-group> (<year>2015</year>). <article-title>A hybrid flower pollination algorithm based modified randomized location for multi-threshold medical image segmentation</article-title>. <source>Biomed. Mater. Eng</source>. <volume>26</volume>, <fpage>S1345</fpage>–<lpage>S1351</lpage>. <pub-id pub-id-type="doi">10.3233/BME-151432</pub-id><?supplied-pmid 26405895?><pub-id pub-id-type="pmid">26405895</pub-id></mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Girshick</surname><given-names>R.</given-names></name><name><surname>Gupta</surname><given-names>A.</given-names></name><name><surname>He</surname><given-names>K.</given-names></name></person-group> (<year>2018</year>). <article-title>“Non-local neural networks,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Salt Lake City, UT</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>7794</fpage>–<lpage>7803</lpage>.</mixed-citation>
    </ref>
    <ref id="B49">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>X.</given-names></name><name><surname>Lian</surname><given-names>S.</given-names></name><name><surname>Luo</surname><given-names>Z.</given-names></name><name><surname>Li</surname><given-names>S.</given-names></name></person-group> (<year>2018</year>). <article-title>“Weighted res-unet for high-quality retina vessel segmentation,”</article-title> in <source>2018 9th International Conference on Information Technology in Medicine and Education (ITME)</source> (<publisher-loc>Hangzhou</publisher-loc>,: <publisher-name>IEEE</publisher-name>), <fpage>327</fpage>–<lpage>331</lpage>.</mixed-citation>
    </ref>
    <ref id="B50">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>E.</given-names></name><name><surname>Wang</surname><given-names>W.</given-names></name><name><surname>Yu</surname><given-names>Z.</given-names></name><name><surname>Anandkumar</surname><given-names>A.</given-names></name><name><surname>Alvarez</surname><given-names>J. M.</given-names></name><name><surname>Luo</surname><given-names>P.</given-names></name></person-group> (<year>2021</year>). <article-title>“Segformer: simple and efficient design for semantic segmentation with transformers,”</article-title> in <source>Advances in Neural Information Processing Systems</source>. <fpage>34</fpage>. <pub-id pub-id-type="doi">10.48550/arXiv.2105.15203</pub-id></mixed-citation>
    </ref>
    <ref id="B51">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>S.</given-names></name><name><surname>Girshick</surname><given-names>R.</given-names></name><name><surname>Dollár</surname><given-names>P.</given-names></name><name><surname>Tu</surname><given-names>Z.</given-names></name><name><surname>He</surname><given-names>K.</given-names></name></person-group> (<year>2017</year>). <article-title>“Aggregated residual transformations for deep neural networks,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Honolulu, HI</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>1492</fpage>–<lpage>1500</lpage>.<?supplied-pmid 31141794?></mixed-citation>
    </ref>
    <ref id="B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>X.</given-names></name><name><surname>Feng</surname><given-names>Z.</given-names></name><name><surname>Cao</surname><given-names>C.</given-names></name><name><surname>Li</surname><given-names>M.</given-names></name><name><surname>Wu</surname><given-names>J.</given-names></name><name><surname>Wu</surname><given-names>Z.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>An improved swin transformer-based model for remote sensing object detection and instance segmentation</article-title>. <source>Remote Sens</source>. <volume>13</volume>, <fpage>4779</fpage>. <pub-id pub-id-type="doi">10.3390/rs13234779</pub-id></mixed-citation>
    </ref>
    <ref id="B53">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>H.</given-names></name><name><surname>Hu</surname><given-names>Q.</given-names></name></person-group> (<year>2021</year>). <article-title>“Transfuse: fusing transformers and cnns for medical image segmentation,”</article-title> in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source> (<publisher-loc>Strasbourg</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>14</fpage>–<lpage>24</lpage>.</mixed-citation>
    </ref>
    <ref id="B54">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Z.</given-names></name><name><surname>Siddiquee</surname><given-names>M. M. R.</given-names></name><name><surname>Tajbakhsh</surname><given-names>N.</given-names></name><name><surname>Liang</surname><given-names>J.</given-names></name></person-group> (<year>2018</year>). <article-title>“Unet++: a nested u-net architecture for medical image segmentation,”</article-title> in <source>Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</source> (<publisher-loc>Granada</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>3</fpage>–<lpage>11</lpage>.<?supplied-pmid 32613207?></mixed-citation>
    </ref>
    <ref id="B55">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>X.</given-names></name><name><surname>Hu</surname><given-names>H.</given-names></name><name><surname>Lin</surname><given-names>S.</given-names></name><name><surname>Dai</surname><given-names>J.</given-names></name></person-group> (<year>2019</year>). <article-title>“Deformable convnets v2: more deformable, better results,”</article-title> in <source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Long Beach, CA</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>9308</fpage>–<lpage>9316</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
