<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7355246</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btaa477</article-id>
    <article-id pub-id-type="publisher-id">btaa477</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Bioinformatics of Microbes and Microbiomes</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>IDMIL: an alignment-free Interpretable Deep Multiple Instance Learning (MIL) for predicting disease from whole-metagenomic data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Rahman</surname>
          <given-names>Mohammad Arifur</given-names>
        </name>
        <xref ref-type="corresp" rid="btaa477-cor1"/>
        <xref ref-type="aff" rid="btaa477-aff1"/>
        <!--<email>mrahma23@gmu.edu</email>-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Rangwala</surname>
          <given-names>Huzefa</given-names>
        </name>
        <xref ref-type="aff" rid="btaa477-aff1"/>
      </contrib>
    </contrib-group>
    <aff id="btaa477-aff1"><institution>Department of Computer Science, George Mason University</institution>, Fairfax, VA 22030, <country country="US">USA</country></aff>
    <author-notes>
      <corresp id="btaa477-cor1">To whom correspondence should be addressed. E-mail: <email>mrahma23@gmu.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2020-07-13">
      <day>13</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>36</volume>
    <issue>Suppl 1</issue>
    <issue-title>ISMB 2020 Proceedings</issue-title>
    <fpage>i39</fpage>
    <lpage>i47</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btaa477.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>The human body hosts more microbial organisms than human cells. Analysis of this microbial diversity provides key insight into the role played by these microorganisms on human health. Metagenomics is the collective DNA sequencing of coexisting microbial organisms in an environmental sample or a host. This has several applications in precision medicine, agriculture, environmental science and forensics. State-of-the-art predictive models for phenotype predictions from metagenomic data rely on alignments, assembly, extensive pruning, taxonomic profiling and reference sequence databases. These processes are time consuming and they do not consider novel microbial sequences when aligned with the reference genome, limiting the potential of whole metagenomics. We formulate the problem of predicting human disease from whole-metagenomic data using Multiple Instance Learning (MIL), a popular supervised learning paradigm. Our proposed alignment-free approach provides higher accuracy in prediction by harnessing the capability of deep convolutional neural network (CNN) within a MIL framework and provides interpretability via neural attention mechanism.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>The MIL formulation combined with the hierarchical feature extraction capability of deep-CNN provides significantly better predictive performance compared to popular existing approaches. The attention mechanism allows for the identification of groups of sequences that are likely to be correlated to diseases providing the much-needed interpretation. Our proposed approach does not rely on alignment, assembly and reference sequence databases; making it fast and scalable for large-scale metagenomic data. We evaluate our method on well-known large-scale metagenomic studies and show that our proposed approach outperforms comparative state-of-the-art methods for disease prediction.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p><ext-link ext-link-type="uri" xlink:href="https://github.com/mrahma23/IDMIL">https://github.com/mrahma23/IDMIL</ext-link>.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Science Foundation</institution>
            <institution-id institution-id-type="DOI">10.13039/100000001</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>NSF</institution>
            <institution-id institution-id-type="DOI">10.13039/100000001</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>1252318</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="9"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction and background</title>
    <p>The human body hosts one of the densest and diverse microbial environments in the world. Trillions of microbial cells in the human body are collectively referred to as the <italic>human microbiome</italic> (<xref rid="btaa477-B6" ref-type="bibr">Backhed, 2005</xref>; <xref rid="btaa477-B48" ref-type="bibr">Turnbaugh <italic>et al.</italic>, 2007</xref>). Metagenomics is the sequencing of the collective DNA of microbial organisms coexisting as communities in an environmental sample or a host (<xref rid="btaa477-B17" ref-type="bibr">Hugenholtz and Tyson, 2008</xref>). Metagenomics has enabled the investigation of the human microbiome and provided key insights into the roles played by microbes in a host. Typical Metagenome Wide Association Study (MWAS) produces millions of DNA sequence fragments from healthy and unhealthy cohorts. This genomic information can be utilized to estimate microbial diversity and predict diseases leading to the design of novel therapeutics and diagnostics (<xref rid="btaa477-B8" ref-type="bibr">Chiu <italic>et al.</italic>, 2019</xref>). However, sequencing technologies do not deliver the complete genome of an organism (millions in length), but a large number of short contiguous subsequences called <italic>reads</italic> in random order. Sequence reads from different microbes are mixed with a high amount of repetitions (<xref rid="btaa477-B17" ref-type="bibr">Hugenholtz and Tyson, 2008</xref>) resulting in large datasets that range from gigabytes (GB) to terabytes (TB). These factors impose serious challenges when developing machine learning algorithms to predict disease from whole-metagenomic data.</p>
    <p>This article focuses on predicting clinical phenotypes, i.e. diseases from whole-metagenomic data. Existing approaches for disease prediction utilize microbial profiling (<xref rid="btaa477-B25" ref-type="bibr">McIntyre <italic>et al.</italic>, 2017</xref>) combined with conventional supervised learning methods for predictive analysis. Microbial profiling involves searching of the input metagenomic sequences against the known microbial genome using computationally expensive alignments. The current knowledge about microbes is largely achieved in the unnatural conditions of growing them in artificial media in pure culture without ecological context (<xref rid="btaa477-B37" ref-type="bibr">Quince <italic>et al.</italic>, 2017</xref>). Moreover, it is estimated that less than 2% of the bacteria can be cultured in the laboratory (<xref rid="btaa477-B15" ref-type="bibr">Handelsman, 2004</xref>; <xref rid="btaa477-B50" ref-type="bibr">Wade, 2002</xref>). When microbial profiling is used before disease prediction, the sequences that match with known microbial genome contribute to the feature creation process. The sequences without sufficient match which may represent partial genome of potentially novel microbes, are ignored and do not partake in feature creation. A metagenomic sample with millions of sequence reads is represented with a single vector which is then used to train classification models. The predictive models also include errors and biases from prior alignments and microbial profiling processes.</p>
    <p>In our proposed approach, we aim to avoid the use of sequence assembly and microbial profiling before classifying diseases from metagenomic samples. To scale with millions of repetitive sequence reads and achieve efficiency, we embed the DNA sequences in a fixed-length vector representation and perform clustering on these embeddings avoiding the computationally expensive alignment-based clustering. For the prediction, we leverage the Multiple Instance Learning (MIL) framework, where a single sample (known as a <italic>bag</italic>) may include many data <italic>instances</italic>. MIL imposes two restrictions on the generic classifiers: (i) the data will be represented with the bag-instance relations and (ii) instances need to be labeled along with the bags. Each of the instances in the bag has its features, collectively representing the bag. As a result, the classifier is exposed to more variations among the samples which help to classify accurately. The second restriction allows for easy interpretations. These utilities make MIL a good candidate for various predictive analyses in metagenomics. We represent the metagenomic sample of a person as a bag and the cluster prototypes within the sample as instances. Besides bag-level classification, the MIL approach allows for inferring which instances, i.e. groups of DNA sequences are likely to be associated with the phenotypic labels.</p>
    <p>Once formulated, the classification in MIL can be performed in different ways, i.e. modification of the maximum margin formulation of support vector machine (SVM), deep learning, etc. We utilize deep learning to solve the MIL formulation because: (i) it extracts relevant latent features from the earlier layers in a step-by-step manner—a unique capability of deep learning compared to other machine learning approaches and (ii) deep-learning approaches are highly nonlinear making them suitable for learning complex relations among the latent features. Specifically, we use a novel attention-based deep convolutional neural network (CNN). Deep CNN models (<xref rid="btaa477-B20" ref-type="bibr">Krizhevsky <italic>et al.</italic>, 2012</xref>; Simonyan and Zisserman, 2004) extract latent features from input in a step-by-step manner using multiple layers of convolution operations. The neural attention (<xref rid="btaa477-B49" ref-type="bibr">Vaswani <italic>et al.</italic>, 2017</xref>) learns which features to focus on as part of the deep-learning model. We use the positional attention values to infer which groups of sequences in the unhealthy cohort may correlate to a disease. We refer to our approach as Interpretable Deep Multiple Instance Learning (IDMIL). We apply IDMIL on five large-scale metagenomic datasets. We show that IDMIL outperforms existing state-of-the-art approaches in predictive performance. Via qualitative case studies, we show the interpretation capability provided by IDMIL.</p>
    <sec>
      <title>1.1 Metagenomics and disease prediction</title>
      <p><xref rid="btaa477-B35" ref-type="bibr">Qin <italic>et al.</italic> (2012)</xref> performed statistical analysis on microbial diversity among the samples from type-2 diabetes (T2D) patients and healthy controls for marker identification. <xref rid="btaa477-B43" ref-type="bibr">Saulnier <italic>et al.</italic> (2011)</xref> identified significant differences between the gut microbiomes of healthy people and people who suffer from irritable bowel syndrome. Other studies discovered the correlations among colorectal cancer (<xref rid="btaa477-B51" ref-type="bibr">Zeller <italic>et al.</italic>, 2014</xref>), inflammatory bowel disease (IBD) (<xref rid="btaa477-B34" ref-type="bibr">Qin <italic>et al.</italic>, 2010</xref>) and obesity (<xref rid="btaa477-B24" ref-type="bibr">Le Chatelier <italic>et al.</italic>, 2013</xref>); and the variations in microbial abundance. These approaches require extensive preprocessing of samples and microbial profiling.</p>
      <p>MetAML (<xref rid="btaa477-B31" ref-type="bibr">Pasolli <italic>et al.</italic>, 2016</xref>) uses machine-learning methods for disease classification. It first identifies marker genes and species-level abundance using the MetaPhlAn2 (<xref rid="btaa477-B47" ref-type="bibr">Truong <italic>et al.</italic>, 2015</xref>) method. MetaPhlAn2 (<xref rid="btaa477-B47" ref-type="bibr">Truong <italic>et al.</italic>, 2015</xref>) is used for the quantitative taxonomic profiling of the microbial communities in metagenomic samples. MetAML uses these features to train a random forest or SVM classifier to predict clinical phenotypes. <xref rid="btaa477-B40" ref-type="bibr">Rahman <italic>et al.</italic> (2017)</xref> used clustering to identify cluster centroids and then used the minimum distance from a sample to the centroids as features for classifiers.</p>
    </sec>
    <sec>
      <title>1.2 Deep learning and metagenomics</title>
      <p>Deep learning has achieved unprecedented success in wide-ranging domains (<xref rid="btaa477-B14" ref-type="bibr">Gu <italic>et al.</italic>, 2018</xref>). Deep learning has been used in metagenomic studies for different prediction tasks. <xref rid="btaa477-B12" ref-type="bibr">Fioravanti <italic>et al.</italic> (2018)</xref> proposed Ph-CNN which takes as input the operational taxonomic unit (OTU) abundance distribution and the OTU distance matrix, and outputs the class of each sample using deep CNN. DeepARG (<xref rid="btaa477-B4" ref-type="bibr">Arango-Argoty <italic>et al.</italic>, 2018</xref>) uses a deep neural network on the dissimilarity matrix created using all known categories of antibiotic resistance genes to predict their presence in metagenomic samples. RegMIL (<xref rid="btaa477-B39" ref-type="bibr">Rahman and Rangwala, 2018</xref>) uses Canopy-based sequence clustering (<xref rid="btaa477-B40" ref-type="bibr">Rahman <italic>et al.</italic>, 2017</xref>) to score the reads based on the cluster memberships. It then uses a deep neural network-based regression to score sequences in test samples and classify samples based on sequence score distributions.</p>
      <p>Dna2vec (<xref rid="btaa477-B28" ref-type="bibr">Ng, 2017</xref>) uses similar technique as word-embedding (<xref rid="btaa477-B23" ref-type="bibr">Le and Mikolov, 2014</xref>) in natural language processing to embed DNA sequences. This approach uses random lengths for DNA sub-sequences (kmers) which increases entropy and directly affects the reproducibility of the sequence representations. PLG–ABD (<xref rid="btaa477-B29" ref-type="bibr">Nguyen <italic>et al.</italic>, 2017</xref>) uses MetaPhlAn2 (<xref rid="btaa477-B47" ref-type="bibr">Truong <italic>et al.</italic>, 2015</xref>) method to estimate microbial abundance and then sorts species-level abundances based on biological taxonomy to represent the metagenomic samples as images. It then uses deep CNN to classify healthy and patient samples. It suffers from the same limitations as MetAML that discards a large number of DNA sequences during the profiling process. Moreover, the possibility of interpretation is lost in the latent space after convolution.</p>
    </sec>
    <sec>
      <title>1.3 Multiple Instance Learning</title>
      <p>In the original formation of MIL (<xref rid="btaa477-B11" ref-type="bibr">Dietterich <italic>et al.</italic>, 1997</xref>), a bag is classified as <italic>positive</italic> if one or more instances within it are positive, whereas a negative bag contains only negative instances. Different formulations of the MIL problem have been developed over the years (<xref rid="btaa477-B2" ref-type="bibr">Amores, 2013</xref>). MISVM (<xref rid="btaa477-B3" ref-type="bibr">Andrews <italic>et al.</italic>, 2003</xref>) and sbMIL (<xref rid="btaa477-B7" ref-type="bibr">Bunescu and Mooney, 2007</xref>) are two of the popular MIL algorithms which follow the standard assumption. These methods use local information-based comparisons between individual instances and treat bag labels as aggregations of instance labels. <xref rid="btaa477-B19" ref-type="bibr">Kotzias <italic>et al.</italic> (2015)</xref> proposed a MIL formulation titled group-instance cost function (GICF) where negative bags contain some positive instances and developed a general cost function for determining individual instance labels from group labels. In contrast with the standard assumption, this is referred to as the <italic>collective</italic> assumption.</p>
      <p>Neural attention has been used to create bag-level representation from instances (<xref rid="btaa477-B18" ref-type="bibr">Ilse, 2018</xref>) and then classify the bags. This approach uses a deep CNN for latent feature extraction and applies neural attention to the fully connected layer when all latent features are already extracted. As a result, mapping with the original feature space is lost, and the attention values are not interpretable, especially for nonimage input data, i.e. DNA sequences. We refer to this approach as attention-based deep MIL (AttMIL) and use it as one of the state-of-the-art models for comparative analysis. <xref rid="btaa477-B22" ref-type="bibr">LaPierre <italic>et al.</italic> (2016)</xref> proposed a phenotype prediction from metagenomic data that utilizes instance space. This approach used the distances in instance space to represent bags which were then classified by the SVM classifier.</p>
    </sec>
  </sec>
  <sec>
    <title>2 Problem formulation</title>
    <p>Given a metagenomic sample of a person consisting of sequence reads, our objective is to classify the person as either healthy or unhealthy. We represent groups of similar DNA sequences in a metagenomic sample as instances and use the instances to represent a sample as a bag in MIL. Formally, the <italic>j</italic>th sample is represented as a bag <inline-formula id="IE1"><mml:math id="IM1"><mml:mrow><mml:msub><mml:mi mathvariant="script">B</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with a set of <italic>c</italic> instances <inline-formula id="IE2"><mml:math id="IM2"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>. Here, an instance <inline-formula id="IE3"><mml:math id="IM3"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents a cluster of similar DNA sequences in the sample <italic>j</italic>. We associate with each bag <inline-formula id="IE4"><mml:math id="IM4"><mml:mrow><mml:msub><mml:mi mathvariant="script">B</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with a class label <inline-formula id="IE5"><mml:math id="IM5"><mml:mrow><mml:msub><mml:mi mathvariant="script">Y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> to indicate if the <italic>j</italic>th person is healthy (<inline-formula id="IE6"><mml:math id="IM6"><mml:mrow><mml:msub><mml:mi mathvariant="script">Y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) or unhealthy (<inline-formula id="IE7"><mml:math id="IM7"><mml:mrow><mml:msub><mml:mi mathvariant="script">Y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>). For a total of <italic>m</italic> samples, the problem of predicting disease from metagenomic data now can be formulated as learning a function <inline-formula id="IE8"><mml:math id="IM8"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="script">B</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>→</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> where <inline-formula id="IE9"><mml:math id="IM9"><mml:mrow><mml:msub><mml:mi mathvariant="script">B</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="script">B</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">B</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">B</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> are the collections of bags representing the samples taken from the healthy and unhealthy cohorts and <inline-formula id="IE10"><mml:math id="IM10"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="script">Y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">Y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">Y</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> are the bag labels such that a bag <inline-formula id="IE11"><mml:math id="IM11"><mml:mrow><mml:msub><mml:mi mathvariant="script">B</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> has the label <inline-formula id="IE12"><mml:math id="IM12"><mml:mrow><mml:msub><mml:mi mathvariant="script">Y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. We also want to learn the contributions of the DNA sequences to the disease state within a bag.</p>
  </sec>
  <sec>
    <title>3 Materials and methods </title>
    <p><xref ref-type="fig" rid="btaa477-F1">Figure 1</xref> shows an overview of our proposed IDMIL. We first embed the DNA subsequences (kmers) into a fixed dimension and then use power-mean statistics to represent a sequence as a vector from its corresponding kmers. These sequence representations are clustered to create instances. We then represent the metagenomic samples as bags composed of these instances and classify the bags.
</p>
    <fig id="btaa477-F1" orientation="portrait" position="float">
      <label>Fig. 1.</label>
      <caption>
        <p>Overview of our proposed IDMIL. We first embed the kmers in a fixed dimension. The kmer embeddings are used to represent the sequence reads. Clustering is performed to create instances of MIL which form the bags in MIL, as healthy or unhealthy</p>
      </caption>
      <graphic xlink:href="btaa477f1"/>
    </fig>
    <sec>
      <title>3.1 DNA subsequence (kmer) embedding</title>
      <p>We first embed the <italic>k</italic>-length contiguous DNA subsequences (known as kmer) in a fixed-length vector space. For this purpose, we use an approach similar to continuous bag-of-word-based representation (<xref rid="btaa477-B27" ref-type="bibr">Mikolov <italic>et al.</italic>, 2013</xref>). For a read <italic>S</italic> with a sequence of <italic>t</italic> kmers, <inline-formula id="IE13"><mml:math id="IM13"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mtext>kmer</mml:mtext></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mtext>kmer</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mtext>kmer</mml:mtext></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, we define <inline-formula id="IE14"><mml:math id="IM14"><mml:mrow><mml:msub><mml:mrow><mml:mtext>Pref</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mtext>kmer</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mo>α</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mtext>kmer</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mo>α</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mtext>kmer</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> as the set of kmers immediately preceding kmer<sub><italic>i</italic></sub> in <italic>S</italic> and <inline-formula id="IE15"><mml:math id="IM15"><mml:mrow><mml:msub><mml:mrow><mml:mtext>Suf</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mtext>kmer</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mtext>kmer</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mtext>kmer</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mo>α</mml:mo></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> as the set of kmers immediately following <italic>kmer<sub>i</sub></italic> in <italic>S</italic> where <inline-formula id="IE16"><mml:math id="IM16"><mml:mrow><mml:mo>α</mml:mo><mml:mo>∼</mml:mo><mml:mtext>Uniform</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>8</mml:mn><mml:mi mathvariant="normal">k</mml:mi><mml:mo>,</mml:mo><mml:mn>16</mml:mn><mml:mi mathvariant="normal">k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is a randomly chosen <italic>context window</italic> such that <inline-formula id="IE17"><mml:math id="IM17"><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mtext>Pref</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mo>≤</mml:mo><mml:mo>α</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE18"><mml:math id="IM18"><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mtext>Suf</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mo>≤</mml:mo><mml:mo>α</mml:mo></mml:mrow></mml:math></inline-formula>. We define <inline-formula id="IE19"><mml:math id="IM19"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mtext>Pref</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>∪</mml:mo><mml:msub><mml:mrow><mml:mtext>Suf</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> as the <italic>context</italic> set of <italic>kmer<sub>i</sub></italic> w.r.t. <italic>S<sub>kmers</sub></italic> such that <inline-formula id="IE20"><mml:math id="IM20"><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mo>≤</mml:mo><mml:mn>2</mml:mn><mml:mo>α</mml:mo></mml:mrow></mml:math></inline-formula>. We learn a <italic>d</italic>-dimensional embedding vector for each <italic>kmer<sub>i</sub></italic> by maximizing its probability of appearing in a DNA sequence given its context. Hence, the probability of <italic>kmer<sub>i</sub></italic> given its context set <italic>Q<sub>i</sub></italic>:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mi mathvariant="script">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mtext>kmer</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>U</mml:mi><mml:mi>i</mml:mi><mml:mi mathvariant="normal">⊤</mml:mi></mml:msubsup><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mn>4</mml:mn><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:munderover><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>U</mml:mi><mml:mi>j</mml:mi><mml:mi mathvariant="normal">⊤</mml:mi></mml:msubsup><mml:msub><mml:mi>V</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE21"><mml:math id="IM21"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE22"><mml:math id="IM22"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> are the <italic>output</italic> vector and the context-based <italic>input</italic> vector of <italic>kmer<sub>i</sub></italic>, respectively. <italic>V<sub>i</sub></italic> is computed as the average of all the input embedding vectors of kmers in the context set <italic>Q<sub>i</sub></italic>. We use a shallow two-layer neural network to train the kmer embeddings that maximize the probability of kmers given the suffix and prefix. The error in kmer predictions from their context is used to update the network parameters via a backpropagation algorithm using the Adaptive Moment Estimation (Adam) optimizer (<xref rid="btaa477-B5" ref-type="bibr">Ba <italic>et al.</italic>, 2016</xref>). This context-based kmer embedding helps to reduce noise inherent in the metagenomic data. Two kmers with similar suffix and prefix will be closer to each other in the embedding space. Subtle noise within similar kmers with similar suffix and prefix will not affect the kmer embeddings drastically. Inspired by the natural language processing, we perform term frequency–inverse document frequency (TF–IDF) (<xref rid="btaa477-B38" ref-type="bibr">Rajaraman and Ullman, 2011</xref>) based kmer pruning before starting the kmer embedding process for better efficiency. The TF–IDF score of <italic>kmer<sub>i</sub></italic> for any sequence <italic>S<sub>j</sub></italic> is represented as:
<disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mtext>log</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mfrac><mml:mi>D</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE23"><mml:math id="IM23"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the score of <italic>kmer<sub>i</sub></italic> in a sequence <italic>S<sub>j</sub></italic>, <inline-formula id="IE24"><mml:math id="IM24"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the frequency of <italic>kmer<sub>i</sub></italic> in sequence <italic>S<sub>j</sub></italic>, <italic>D</italic> is the total number of sequences in the sample and <italic>df<sub>i</sub></italic> is the total number of sequences in the sample containing <italic>kmer<sub>i</sub></italic>. Based on this score, we take the top 70% of the kmers from each sequence and prune the rest. The set of the remaining kmers from all DNA sequences creates our kmer vocabulary. The TF-term <inline-formula id="IE25"><mml:math id="IM25"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> ensures that infrequent kmers which are likely to be inherent noise, receive low scores. The IDF-term <inline-formula id="IE26"><mml:math id="IM26"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>/</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> ensures highly frequent kmers appearing in most of the sequences, receive low scores. Such kmers are also likely to be system-generated noise and do not provide any discriminatory information. This way TF–IDF effectively reduces the total number of kmers and noise. We also utilize binary tree-based hierarchical softmax with Huffman encoding to estimate the final softmax (<xref ref-type="disp-formula" rid="E1">Eq. 1</xref>) (<xref rid="btaa477-B27" ref-type="bibr">Mikolov <italic>et al.</italic>, 2013</xref>). This reduces the complexity of softmax computations from <italic>O</italic>(<italic>N</italic>) to <italic>O</italic>(log<italic>N</italic>) where <italic>N</italic> is the size of the pruned kmer vocabulary.</p>
    </sec>
    <sec>
      <title>3.2 DNA sequence representation</title>
      <p>We use the kmer embeddings (Section 3.1) to represent the raw sequence reads in the whole-metagenomic sample. In natural language processing, sentences and documents are often embedded in fixed-dimensional vector space using Sentence2vec and Doc2vec, respectively (<xref rid="btaa477-B23" ref-type="bibr">Le and Mikolov, 2014</xref>). If we consider each DNA sequence as a sentence and kmers as words then Sentence2vec requires learning the weights for millions of DNA sequence making the process computationally infeasible. We can consider the metagenomic samples as documents and embed the whole sample using the kmer embeddings with an approach similar to doc2vec (<xref rid="btaa477-B23" ref-type="bibr">Le and Mikolov, 2014</xref>). But this approach will not help to interpret the predictions. State-of-the-art encoder–decoder-based sentence embedding approaches, for example recurrent neural network (<xref rid="btaa477-B26" ref-type="bibr">Mikolov <italic>et al.</italic>, 2010</xref>), gated recurrent unit (<xref rid="btaa477-B9" ref-type="bibr">Chung <italic>et al.</italic>, 2014</xref>) and long–short-term memory networks (<xref rid="btaa477-B30" ref-type="bibr">Palangi <italic>et al.</italic>, 2016</xref>) are capable of encoding sequence ordering. But they operate on a few hundreds of time-steps and suitable for text data with only a few thousands of sentences, unlike the whole-metagenomic data with millions of sequences per sample.</p>
      <p>We use an approach similar to <italic>concatenated power-mean of word-embeddings</italic> (<xref rid="btaa477-B42" ref-type="bibr">Ruckle <italic>et al.</italic>, 2018</xref>) to represent sequence reads using kmer embeddings. Power-mean does not require learning additional weights, making it a suitable choice for whole-metagenomic data. The power-mean (<xref rid="btaa477-B16" ref-type="bibr">Hardy <italic>et al.</italic>, 1952</xref>) is a generalization of the averaging. For a sequence <italic>S</italic> with total <italic>t</italic> kmers, <inline-formula id="IE27"><mml:math id="IM27"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mtext>kmer</mml:mtext></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mtext>kmer</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mtext>kmer</mml:mtext></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula id="IE28"><mml:math id="IM28"><mml:mrow><mml:msub><mml:mrow><mml:mtext>kmer</mml:mtext></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mtext>kmer</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mtext>kmer</mml:mtext></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are embedded using <italic>d</italic>-dimensional vectors <inline-formula id="IE29"><mml:math id="IM29"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, respectively (Section 3.1); the element-wise power-mean is defined as:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:mo>∀</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:msub><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>p</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:msub><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>p</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>p</mml:mi></mml:msubsup></mml:mrow><mml:mi>t</mml:mi></mml:mfrac></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>p</mml:mi></mml:mfrac></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>∈</mml:mo><mml:mo>ℝ</mml:mo><mml:mo>∪</mml:mo><mml:mo>{</mml:mo><mml:mo>±</mml:mo><mml:mo>∞</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>Different values of power-mean (<italic>p</italic>) provide interesting statistics, for example minimum (<italic>p</italic>=−<italic>∞</italic>), harmonic mean (<italic>p</italic>=−1), geometric mean (<italic>p </italic>=<italic> </italic>0), arithmetic mean (<italic>p </italic>=<italic> </italic>1), maximum (<italic>p</italic>=+<italic>∞</italic>) and others. We make a matrix <inline-formula id="IE30"><mml:math id="IM30"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> from all the kmer embeddings in a sequence <italic>S</italic> with <italic>t</italic> kmers where each <inline-formula id="IE31"><mml:math id="IM31"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE32"><mml:math id="IM32"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Let <inline-formula id="IE33"><mml:math id="IM33"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> be the vector whose <italic>d</italic> components are the element-wise power-means of <inline-formula id="IE34"><mml:math id="IM34"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <italic>p</italic> is the power-mean value. To get a summary statistics from the kmer embeddings, we calculate total <italic>n</italic> power-means and represent <italic>S</italic> as a vector:
<disp-formula id="E4"><label>(4)</label><mml:math id="M4"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="normal">p</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⊕</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⊕</mml:mo><mml:mo>…</mml:mo><mml:mo>⊕</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="normal">p</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>where ⊕ stands for concatenation and <inline-formula id="IE35"><mml:math id="IM35"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are total <italic>n</italic> different power-mean values. Concatenation is effective here because it produces a more precise summary and reduces the uncertainty of representations compared to averaging the kmer embeddings which may result in similar representations of different DNA sequences due to their similarities in averages. Among the choices of power-mean values minimum, maximum and positive odd numbers are found to be effective in text processing (<xref rid="btaa477-B42" ref-type="bibr">Ruckle <italic>et al.</italic>, 2018</xref>). The minimum and maximum considers the range of kmer embedding values. The rationale for positive numbers is that, negative power-mean are discontinuous and undefined when input is zero. Odd power-mean values are preferable because they preserve the input sign. A higher positive value of <italic>p</italic> tends to <inline-formula id="IE36"><mml:math id="IM36"><mml:mrow><mml:mo>+</mml:mo><mml:mo>∞</mml:mo></mml:mrow></mml:math></inline-formula> and can significantly increase the data dimension. Hence, we have used all <inline-formula id="IE37"><mml:math id="IM37"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mo>±</mml:mo><mml:mo>∞</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> and concatenate them for sequence representations.</p>
    </sec>
    <sec>
      <title>3.3 Instance representation</title>
      <p>After representing the DNA sequences with a fixed dimension, we perform clustering with the Mini-batch KMeans algorithm (<xref rid="btaa477-B44" ref-type="bibr">Sculley, 2010</xref>) within each metagenomic sample. Metagenomics produces repetitive DNA sequence fragments from different parts of the microbial whole-genome. By clustering the embedded sequences and using the cluster prototypes, we remove redundancies. The Mini-batch KMeans is a variant of the KMeans algorithm which uses mini-batches to reduce the computation time, while still attempting to optimize the same objective function of KMeans. Say the <italic>j</italic>th metagenomic sample is represented by a set of sequence representations <italic>B<sub>j</sub></italic> (Section 3.2). The objective function of the Mini-batch KMeans:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5"><mml:mrow><mml:mtext>min</mml:mtext><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow/></mml:munderover><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:msup><mml:mo>|</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>where, <inline-formula id="IE38"><mml:math id="IM38"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> is a set of <italic>c</italic> cluster centers in the <italic>j</italic>th sample, each <inline-formula id="IE39"><mml:math id="IM39"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a sequence representation (Section 3.2) in the <italic>j</italic>th sample and <inline-formula id="IE40"><mml:math id="IM40"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> returns the nearest cluster center of <italic>x</italic> using Euclidean distance. Each mini-batch updates the clusters using a convex combination of the values of the cluster centers and the data by applying a learning rate that decreases with the number of iterations. As a result, Mini-batch KMeans converges faster than naive KMeans and does not suffer increased computational cost for large-scale metagenomic data. The resultant cluster representatives become the instances in our proposed MIL approach.</p>
    </sec>
    <sec>
      <title>3.4 Bag representation</title>
      <p>We order the instances in a bag based on their Euclidean distances from a reference data point. We define the reference data point <italic>h</italic><sub>ref</sub> as the average of all the cluster centers (instances) in the training set. Instances in a bag are sorted in ascending order based on their corresponding distances from <italic>h</italic><sub>ref</sub>. For the <italic>j</italic>th sample with total <italic>c</italic> ordered instances <inline-formula id="IE41"><mml:math id="IM41"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, this implies:
<disp-formula id="E6"><label>(6)</label><mml:math id="M6"><mml:mrow><mml:mo>∀</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≺</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⇒</mml:mo><mml:mtext>dist</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mtext>ref</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mtext>dist</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mtext>ref</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE42"><mml:math id="IM42"><mml:mrow><mml:mtext>dist</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> returns the Euclidean distance between two vectors. Finally, we represent the <italic>j</italic>th bag with ordered instances as a matrix <inline-formula id="IE43"><mml:math id="IM43"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> where <inline-formula id="IE44"><mml:math id="IM44"><mml:mrow><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula> is the instance dimension (Section 3.3). Here, <inline-formula id="IE45"><mml:math id="IM45"><mml:mrow><mml:mi>d</mml:mi><mml:mo>′</mml:mo><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mo>.</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> for kmer-embedding size of <italic>d</italic> and total <italic>n</italic> power-mean values. The ordering serves two purposes: (i) it combines similar instances within a locality which is required by the CNN during the prediction phase and (ii) the positional information of instances can be used for interpreting prediction results via the attention mechanism.</p>
    </sec>
    <sec>
      <title>3.5 Classification model</title>
      <p><xref ref-type="fig" rid="btaa477-F2">Figure 2</xref> shows an overview of our proposed prediction model. We use a deep CNN on the bag representation. Our motivation for using a deep CNN model: (i) CNNs are extremely effective in extracting the latent features hierarchically from the input, (ii) they learn complex, nonlinear relationship among these latent features which can be leveraged for a complex classification task such as ours where we differentiate samples at molecular level, (iii) CNNs provide an advantage over feed-forward networks by considering locality of features, i.e. our proposed instance ordering within the bags and (iv) a carefully designed CNN model can provide interpretation of the predictions in addition to the high accuracy. Our model is similar to AlexNet (<xref rid="btaa477-B20" ref-type="bibr">Krizhevsky <italic>et al.</italic>, 2012</xref>), a popular deep CNN-based model while keeping the number of layers and learning weights minimal to make the model scalable for whole-metagenomic data and avoid overfitting. We use multiple hidden layers with rectified linear unit (ReLU) as the nonlinear activation function. We use the negative log-likelihood (NLL) as the loss function for our binary classification tasks (healthy versus unhealthy):
<disp-formula id="E7"><label>(7)</label><mml:math id="M7"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="script">Y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="script">Y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="script">Y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="script">Y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE46"><mml:math id="IM46"><mml:mrow><mml:msub><mml:mi mathvariant="script">Y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> is the true class label of any bag <inline-formula id="IE47"><mml:math id="IM47"><mml:mrow><mml:msub><mml:mi mathvariant="script">B</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE48"><mml:math id="IM48"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="script">Y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the model’s predicted probability of the class being <inline-formula id="IE49"><mml:math id="IM49"><mml:mrow><mml:msub><mml:mi mathvariant="script">Y</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Because we use the NLL-loss, the last layer is equipped with the log-softmax activation function which produces the log probability vector (<xref rid="btaa477-B13" ref-type="bibr">Goodfellow <italic>et al.</italic>, 2016</xref>). We adopt the dropout regularization at hidden layers (<xref rid="btaa477-B46" ref-type="bibr">Srivastava, 2014</xref>) which helps the model to avoid overfitting by ignoring randomly selected hidden units during the training. The objective of the classification model is to minimize the loss. The error in prediction is used to update the network parameters via a backpropagation algorithm using the Adaptive Moment Estimation (Adam) optimizer (<xref rid="btaa477-B5" ref-type="bibr">Ba <italic>et al.</italic>, 2016</xref>). We start our model with only one hidden layer. The number of hidden layers is then increased by one until the increment does not result in achieving a higher area under curve of the receiver operating characteristic (AUC-ROC) value on a validation set.
</p>
      <fig id="btaa477-F2" orientation="portrait" position="float">
        <label>Fig. 2.</label>
        <caption>
          <p>Overview of the deep CNN-based classification model. The figure shows operations and shapes of transformed data. The operations follow from the top-left to the bottom-right. After applying attention, the bags are reshaped into a 3D tensor where each embedding dimension becomes a channel. A series of convolution layers is applied before classification. <inline-formula id="IE50"><mml:math id="IM50"><mml:mrow><mml:msub><mml:mrow><mml:mtext>FC</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the <italic>i</italic>th fully connected layer</p>
        </caption>
        <graphic xlink:href="btaa477f2"/>
      </fig>
    </sec>
    <sec>
      <title>3.6 Attention mechanism</title>
      <p>Attention in deep learning involves learning a vector of importance-weights of elements in the data, i.e. group of pixels in an image, word or sentence in a text corpus (<xref rid="btaa477-B49" ref-type="bibr">Vaswani <italic>et al.</italic>, 2017</xref>). In the attention mechanism, we estimate how strongly an element is correlated with (<italic>attends to</italic>) other elements for the classification. Attentions weights are usually learned as part of the deep-learning model of the actual classification task. The same back-propagation algorithm that trains the classification model also trains the attention values. Softmax is used for the final calculation of the attention weights. As a result, these weights are within the range <inline-formula id="IE51"><mml:math id="IM51"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> and sum to 1. The attention values reduce noise by <italic>dampening</italic> or <italic>highlighting</italic> data instances such that it reduces classification errors. It also helps to interpret the prediction results. We start with the bag representation of dimension <inline-formula id="IE52"><mml:math id="IM52"><mml:mrow><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula> where <italic>c</italic> is the number of instances and <inline-formula id="IE53"><mml:math id="IM53"><mml:mrow><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula> is the instance dimension. Here, <inline-formula id="IE54"><mml:math id="IM54"><mml:mrow><mml:mi>d</mml:mi><mml:mo>′</mml:mo><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mo>.</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> for <italic>d</italic>-dimensional kmer embedding and <italic>n</italic> power-mean values. We apply attention <inline-formula id="IE55"><mml:math id="IM55"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to instance positions as follows:
<disp-formula id="E8"><label>(8)</label><mml:math id="M8"><mml:mrow><mml:mo>∀</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo>{</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi mathvariant="normal">⊤</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mtext>tanh</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:msubsup><mml:mi>r</mml:mi><mml:mi>j</mml:mi><mml:mi mathvariant="normal">⊤</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>⊙</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>U</mml:mi><mml:msubsup><mml:mi>r</mml:mi><mml:mi>j</mml:mi><mml:mi mathvariant="normal">⊤</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:munderover><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo></mml:mrow><mml:mo>{</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi mathvariant="normal">⊤</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mtext>tanh</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi mathvariant="normal">⊤</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>⊙</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>U</mml:mi><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi mathvariant="normal">⊤</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE56"><mml:math id="IM56"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mo>ℝ</mml:mo></mml:mrow></mml:math></inline-formula> is the attention value for the <italic>j</italic>th row of a bag, <inline-formula id="IE57"><mml:math id="IM57"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the <italic>j</italic>th instance of a bag, <inline-formula id="IE58"><mml:math id="IM58"><mml:mrow><mml:mi>W</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi>V</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE59"><mml:math id="IM59"><mml:mrow><mml:mi>U</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> are the learning weights of the hidden layers for the attention vector with <italic>l</italic> units, <inline-formula id="IE60"><mml:math id="IM60"><mml:mrow><mml:mtext>tan</mml:mtext><mml:mo> </mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the element-wise hyperbolic tangent function, <inline-formula id="IE61"><mml:math id="IM61"><mml:mrow><mml:mo>σ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the element-wise sigmoid function and <inline-formula id="IE62"><mml:math id="IM62"><mml:mo>⊙</mml:mo></mml:math></inline-formula> is the element-wise multiplication. For attention calculations, we use the tangent hyperbolic nonlinearity which includes both negative and positive values and ensures proper gradient flow. However, tanh(<italic>x</italic>) becomes linear for <inline-formula id="IE63"><mml:math id="IM63"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> limiting the effectiveness the deep learning model. To solve this issue, we use the gating mechanism (<xref rid="btaa477-B10" ref-type="bibr">Dauphin <italic>et al.</italic>, 2017</xref>) which combines hyperbolic tangent and sigmoid nonlinearity for effective gradient propagations. This way we achieve an attention values for each of the <italic>c</italic> rows (instances) of the bag matrix.</p>
      <p>We label the disease and healthy states with class labels 1 and 0, respectively. Higher attention value for an instance position infers that the instances at that position in the unhealthy samples can be associated with the disease. This allows us to map the original sequences to the clusters representing these highly <italic>attended</italic> instances. This way we can interpret the result of the prediction by inferring association between groups of similar sequences and a disease. Alignments and microbial profiling can be performed only on these selected sets of sequences reducing much of the computational overhead. Applying attention to hidden layers will not help to interpret because mapping between the latent features and the original feature space is not straightforward (<italic>blackbox</italic>), especially for nonimage data where hidden layers are unlikely to be human-understandable when visualized. We first multiply the bags with positional attention values before proceeding with the rest of the proposed model.</p>
    </sec>
    <sec>
      <title>3.7 Convolutional neural network configuration</title>
      <p>Color image data are represented in a CNN model using 3D tensors with shape <inline-formula id="IE64"><mml:math id="IM64"><mml:mrow><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula> where <italic>w</italic>, <italic>h</italic> and <italic>c</italic> represent the width, height and the color channels (one for gray-scale and three for color), respectively. Following this notation, we reshape a bag <inline-formula id="IE65"><mml:math id="IM65"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> into a 3D tensor <inline-formula id="IE66"><mml:math id="IM66"><mml:mrow><mml:msubsup><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> where <inline-formula id="IE67"><mml:math id="IM67"><mml:mrow><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>, the instance-dimension becomes the number of channels. The convolution operation involves calculating weighted averages of the locales in the matrix by sliding a convolutional <italic>kernel</italic> over the matrix. The weights are learned by the neural network using the back-propagation algorithm. When applying convolution on multichannel data, each convolutional <italic>filter</italic> is composed of the same number of kernels as the number of input channels. Each kernel performs convolution on their respective channels. These channel-wise convolution outputs are then combined by an element-wise addition. After adding a bias term, the filter outputs a single channel from all the input channels.</p>
      <p>We apply <italic>one-dimensional</italic> convolutions with filters of size 9. In IDMIL, the instances within a bag are ordered based on their corresponding distances from a reference point which is the average of all instances in the training set (Section 3.4). The effectiveness of this ordering is subjected to the choice of reference point. When we apply a one-dimensional convolutional filter of size 9, the convolution will take total nine instances for each of the weighted average calculations. The filters will see a similar set of instances when slid on the tensor which reduces the effect of variations in the instance ordering. We start with only 2<sup>4</sup> convolutional filters per hidden layer and increase this number as a power of two until the increase does not provide higher AUC-ROC values on a validation set. We apply a one-dimensional average pooling of size and stride of two for sampling the learned features. The last two layers apply convolution with smaller filter sizes (5 and 3) which allows the model to focus more within the smaller locality of more relevant features than earlier layers. Finally, fully connected layers perform the predictions upon collapsing the hidden units.</p>
    </sec>
    <sec>
      <title>3.8 Data augmentation</title>
      <p>Data augmentations are used to introduce variations to the prediction model which helps the model to avoid overfitting and generalize. In image classification, data augmentations involve duplicating the images with a combination of affine transformations. Additional neural networks have been utilized to learn the augmentation process (<xref rid="btaa477-B33" ref-type="bibr">Perez and Wang, 2017</xref>) which involves learning more weights and additional computations. Most of the MWASs contain a few hundred samples each with millions of DNA sequences. In our proposed approach, we utilize data augmentation without introducing any additional learning weights to make the prediction model generalized. Once the bag is formed with ordered instances, we randomly shuffle the positions of the instances within a small window of size <inline-formula id="IE68"><mml:math id="IM68"><mml:mrow><mml:mi>z</mml:mi><mml:mo>∼</mml:mo><mml:mtext>Uniform</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. This introduces perturbation within a small locality of the bag when <italic>z </italic>&gt;<italic> </italic>1. The label of this new bag remains the same as the original one.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Experimental setup</title>
    <sec>
      <title>4.1 Datasets</title>
      <p><xref rid="btaa477-T1" ref-type="table">Table 1</xref> shows the key statistics regarding the metagenomic datasets used in this article with the number of unhealthy and healthy samples and the average number of DNA sequences (in millions) in each sample. These datasets are the metagenomic studies of liver cirrhosis (<xref rid="btaa477-B36" ref-type="bibr">Qin <italic>et al.</italic>, 2014</xref>), colorectal cancer (<xref rid="btaa477-B51" ref-type="bibr">Zeller <italic>et al.</italic>, 2014</xref>), IBD (<xref rid="btaa477-B34" ref-type="bibr">Qin <italic>et al.</italic>, 2010</xref>), obesity (<xref rid="btaa477-B24" ref-type="bibr">Le Chatelier <italic>et al.</italic>, 2013</xref>) and T2D (<xref rid="btaa477-B35" ref-type="bibr">Qin <italic>et al.</italic>, 2012</xref>). The minimum number of average DNA sequences in a sample is 40.2 million in the T2D dataset. This shows how large these datasets can be. We removed class imbalances by combining random oversampling and our proposed data augmentations (Section 3.8) within the training set of each 10-fold cross-validation experiment.
</p>
      <table-wrap id="btaa477-T1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>Dataset statistics</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Datasets</th>
              <th rowspan="1" colspan="1">Total samples</th>
              <th rowspan="1" colspan="1">Unhealthy cases</th>
              <th rowspan="1" colspan="1">Healthy controls</th>
              <th rowspan="1" colspan="1">Avg. sequences per-sample (std) (in millions)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Liver cirrhosis</td>
              <td rowspan="1" colspan="1">232</td>
              <td rowspan="1" colspan="1">118</td>
              <td rowspan="1" colspan="1">114</td>
              <td rowspan="1" colspan="1">51.6M (30.9M)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Colorectal cancer</td>
              <td rowspan="1" colspan="1">121</td>
              <td rowspan="1" colspan="1">48</td>
              <td rowspan="1" colspan="1">73</td>
              <td rowspan="1" colspan="1">60.0M (25.5M)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Inflammatory bowel disease (IBD)</td>
              <td rowspan="1" colspan="1">110</td>
              <td rowspan="1" colspan="1">25</td>
              <td rowspan="1" colspan="1">85</td>
              <td rowspan="1" colspan="1">45.2M (18.4M)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Obesity</td>
              <td rowspan="1" colspan="1">253</td>
              <td rowspan="1" colspan="1">164</td>
              <td rowspan="1" colspan="1">89</td>
              <td rowspan="1" colspan="1">68.2M (23.2M)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Type-2 diabetes (T2D)</td>
              <td rowspan="1" colspan="1">344</td>
              <td rowspan="1" colspan="1">170</td>
              <td rowspan="1" colspan="1">174</td>
              <td rowspan="1" colspan="1">40.2M (11.8M)</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>4.2 Data preprocessing</title>
      <p>We preprocessed the FASTQ files using fastp (<ext-link ext-link-type="uri" xlink:href="https://github.com/OpenGene/fastp">https://github.com/OpenGene/fastp</ext-link>) an efficient, opensource FASTQ processing tool. The adapter sequences are removed using the fastp tool. Quality scores are checked from both 5′ to 3′ and 3′ to 5′ ends. fastp then checks how many of the total bases from both ends have a quality score lower than a predefined quality-threshold (20 in our case). If 30% (threshold parameter) of the bases are low quality, then the read is removed. fastp then merges paired-end reads based on the overlapping information. It tries to find an overlap between the forward-read to the reverse complement of the reverse-read. This is performed for each pair in parallel. If the length of the overlapping region is lower than 30 (threshold parameter) and the number of mismatches within the overlap is higher than 20% (threshold parameter) of the overlapping length, then fastp removes the read. Otherwise, a merged sequence is created from each paired-end read. This process creates a single FASTA file with the merged sequences.</p>
    </sec>
    <sec>
      <title>4.3 Evaluation metrics</title>
      <p>We evaluate the success of our MIL-based approach in several ways. The simplest measure is accuracy which measures the percentage of samples that are classified correctly. We also use the AUC-ROC, which is the plot of false-positive rate versus true-positive rate. We repeated each experiment 10 times and calculated the margin of error for the mean with 95% confidence interval. The margin of error can be calculated as:
<disp-formula id="E9"><label>(9)</label><mml:math id="M9"><mml:mrow><mml:mtext>ME</mml:mtext><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo>α</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>σ</mml:mo><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE69"><mml:math id="IM69"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo>α</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the significance level, <inline-formula id="IE70"><mml:math id="IM70"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo>α</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is critical value of the <italic>t</italic> distribution with <italic>n</italic> – 1 degrees of freedom for an area of <inline-formula id="IE71"><mml:math id="IM71"><mml:mrow><mml:mo>α</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> for the upper tail, <italic>σ</italic><sub>s</sub> is the sample standard deviation and <italic>n</italic> is the sample size.</p>
    </sec>
    <sec>
      <title>4.4 Software and hardware</title>
      <p>We implemented the kmer embedding and the deep CNN model using PyTorch (<ext-link ext-link-type="uri" xlink:href="https://pytorch.org/">https://pytorch.org/</ext-link>) a popular Python-based open-source deep-learning framework. For the Mini-batch KMeans, we used Scikit-learn (<xref rid="btaa477-B32" ref-type="bibr">Pedregosa <italic>et al.</italic>, 2011</xref>). We used the ARGO computing cluster configured with dual Intel Xeon 28 core CPUs, 1.5 TB RAM and four Nvidia v100 GPUs each with 32GB VRAM available at George Mason University (<ext-link ext-link-type="uri" xlink:href="http://wiki.orc.gmu.edu/index.php/About_ARGO">http://wiki.orc.gmu.edu/index.php/About_ARGO</ext-link>).</p>
    </sec>
  </sec>
  <sec>
    <title>5 Discussion</title>
    <sec>
      <title>5.1 Comparative analysis</title>
      <p><xref rid="btaa477-T2" ref-type="table">Table 2</xref> shows the comparison of mean accuracies and mean AUC-ROC values with margin of errors from different approaches for 10-fold cross validations on the metagenomic datasets used in this article. We compare our proposed approach, IDMIL with non-MIL-based approaches such as MetAML (<xref rid="btaa477-B31" ref-type="bibr">Pasolli <italic>et al.</italic>, 2016</xref>) and PLG–ABD (<xref rid="btaa477-B29" ref-type="bibr">Nguyen <italic>et al.</italic>, 2017</xref>). We also compare IDMIL with some of the popular MIL approaches such as miSVM (<xref rid="btaa477-B3" ref-type="bibr">Andrews <italic>et al.</italic>, 2003</xref>), MISVM (<xref rid="btaa477-B3" ref-type="bibr">Andrews <italic>et al.</italic>, 2003</xref>), sbMIL (<xref rid="btaa477-B7" ref-type="bibr">Bunescu and Mooney, 2007</xref>), GICF (<xref rid="btaa477-B19" ref-type="bibr">Kotzias <italic>et al.</italic>, 2015</xref>) and AttMIL (<xref rid="btaa477-B18" ref-type="bibr">Ilse, 2018</xref>). All of these approaches are briefly discussed in Section 1. Among these approaches miSVM, MISVM, sbMIL and GICF were developed to handle numeric data and most of them rely on costly kernel computation in the instance space. We used the publicly available open-source (<ext-link ext-link-type="uri" xlink:href="https://github.com/garydoranjr/misvm">https://github.com/garydoranjr/misvm</ext-link>) implementations of these models. For the AttMIL, we used the author provided source code (<ext-link ext-link-type="uri" xlink:href="https://github.com/AMLab-Amsterdam/AttentionDeepMIL">https://github.com/AMLab-Amsterdam/AttentionDeepMIL</ext-link>). To run these MIL algorithms in our problem setup, we used our proposed instance and bag representations (Section 3.4) as input to these MIL approaches. We observe that our proposed, IDMIL significantly outperforms all the baseline approaches used in this article.
</p>
      <table-wrap id="btaa477-T2" orientation="portrait" position="float">
        <label>Table 2.</label>
        <caption>
          <p>Comparison of mean performances (10-fold cross validation) on different datasets with the margin of errors for 10 repeated trials. </p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="2" colspan="1">Methods</th>
              <th colspan="2" rowspan="1">Cirrhosis<hr/></th>
              <th colspan="2" rowspan="1">Colorectal<hr/></th>
              <th colspan="2" rowspan="1">IBD<hr/></th>
              <th colspan="2" rowspan="1">Obesity<hr/></th>
              <th colspan="2" rowspan="1">T2D<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Accuracy</th>
              <th rowspan="1" colspan="1">AUC-ROC</th>
              <th rowspan="1" colspan="1">Accuracy</th>
              <th rowspan="1" colspan="1">AUC-ROC</th>
              <th rowspan="1" colspan="1">Accuracy</th>
              <th rowspan="1" colspan="1">AUC-ROC</th>
              <th rowspan="1" colspan="1">Accuracy</th>
              <th rowspan="1" colspan="1">AUC-ROC</th>
              <th rowspan="1" colspan="1">Accuracy</th>
              <th rowspan="1" colspan="1">AUC-ROC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="2" colspan="1">MetAML (<xref rid="btaa477-B31" ref-type="bibr">Pasolli <italic>et al.</italic>, 2016</xref>)</td>
              <td rowspan="1" colspan="1">0.877</td>
              <td rowspan="1" colspan="1">0.945</td>
              <td rowspan="1" colspan="1">0.805</td>
              <td rowspan="1" colspan="1">0.873</td>
              <td rowspan="1" colspan="1">0.809</td>
              <td rowspan="1" colspan="1">0.809</td>
              <td rowspan="1" colspan="1">0.644</td>
              <td rowspan="1" colspan="1">0.655</td>
              <td rowspan="1" colspan="1">0.664</td>
              <td rowspan="1" colspan="1">0.744</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">(0.042)</td>
              <td rowspan="1" colspan="1">(0.029)</td>
              <td rowspan="1" colspan="1">(0.061)</td>
              <td rowspan="1" colspan="1">(0.053)</td>
              <td rowspan="1" colspan="1">(0.042)</td>
              <td rowspan="1" colspan="1">(0.044)</td>
              <td rowspan="1" colspan="1">(0.036)</td>
              <td rowspan="1" colspan="1">(0.071)</td>
              <td rowspan="1" colspan="1">(0.054)</td>
              <td rowspan="1" colspan="1">(0.048)</td>
            </tr>
            <tr>
              <td rowspan="2" colspan="1">PLG–ABD (<xref rid="btaa477-B29" ref-type="bibr">Nguyen <italic>et al.</italic>, 2017</xref>)</td>
              <td rowspan="1" colspan="1">0.891</td>
              <td rowspan="1" colspan="1">0.914</td>
              <td rowspan="1" colspan="1">0.742</td>
              <td rowspan="1" colspan="1">0.815</td>
              <td rowspan="1" colspan="1">0.836</td>
              <td rowspan="1" colspan="1">0.847</td>
              <td rowspan="1" colspan="1">0.660</td>
              <td rowspan="1" colspan="1">0.675</td>
              <td rowspan="1" colspan="1">0.626</td>
              <td rowspan="1" colspan="1">0.691</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">(0.031)</td>
              <td rowspan="1" colspan="1">(0.026)</td>
              <td rowspan="1" colspan="1">(0.049)</td>
              <td rowspan="1" colspan="1">(0.042)</td>
              <td rowspan="1" colspan="1">(0.030)</td>
              <td rowspan="1" colspan="1">(0.016)</td>
              <td rowspan="1" colspan="1">(0.032)</td>
              <td rowspan="1" colspan="1">(0.042)</td>
              <td rowspan="1" colspan="1">(0.048)</td>
              <td rowspan="1" colspan="1">(0.039)</td>
            </tr>
            <tr>
              <td rowspan="2" colspan="1">miSVM (<xref rid="btaa477-B3" ref-type="bibr">Andrews <italic>et al.</italic>, 2003</xref>)</td>
              <td rowspan="1" colspan="1">0.772</td>
              <td rowspan="1" colspan="1">0.815</td>
              <td rowspan="1" colspan="1">0.692</td>
              <td rowspan="1" colspan="1">0.743</td>
              <td rowspan="1" colspan="1">0.742</td>
              <td rowspan="1" colspan="1">0.759</td>
              <td rowspan="1" colspan="1">0.594</td>
              <td rowspan="1" colspan="1">0.617</td>
              <td rowspan="1" colspan="1">0.584</td>
              <td rowspan="1" colspan="1">0.611</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">(0.038)</td>
              <td rowspan="1" colspan="1">(0.041)</td>
              <td rowspan="1" colspan="1">(0.036)</td>
              <td rowspan="1" colspan="1">(0.042)</td>
              <td rowspan="1" colspan="1">(0.029)</td>
              <td rowspan="1" colspan="1">(0.044)</td>
              <td rowspan="1" colspan="1">(0.027)</td>
              <td rowspan="1" colspan="1">(0.041)</td>
              <td rowspan="1" colspan="1">(0.042)</td>
              <td rowspan="1" colspan="1">(0.041)</td>
            </tr>
            <tr>
              <td rowspan="2" colspan="1">MISVM (<xref rid="btaa477-B3" ref-type="bibr">Andrews <italic>et al.</italic>, 2003</xref>)</td>
              <td rowspan="1" colspan="1">0.796</td>
              <td rowspan="1" colspan="1">0.826</td>
              <td rowspan="1" colspan="1">0.664</td>
              <td rowspan="1" colspan="1">0.728</td>
              <td rowspan="1" colspan="1">0.739</td>
              <td rowspan="1" colspan="1">0.748</td>
              <td rowspan="1" colspan="1">0.576</td>
              <td rowspan="1" colspan="1">0.592</td>
              <td rowspan="1" colspan="1">0.592</td>
              <td rowspan="1" colspan="1">0.628</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">(0.022)</td>
              <td rowspan="1" colspan="1">(0.019)</td>
              <td rowspan="1" colspan="1">(0.038)</td>
              <td rowspan="1" colspan="1">(0.051)</td>
              <td rowspan="1" colspan="1">(0.036)</td>
              <td rowspan="1" colspan="1">(0.051)</td>
              <td rowspan="1" colspan="1">(0.026)</td>
              <td rowspan="1" colspan="1">(0.037)</td>
              <td rowspan="1" colspan="1">(0.035)</td>
              <td rowspan="1" colspan="1">(0.038)</td>
            </tr>
            <tr>
              <td rowspan="2" colspan="1">sbMIL (<xref rid="btaa477-B7" ref-type="bibr">Bunescu and Mooney, 2007</xref>)</td>
              <td rowspan="1" colspan="1">0.782</td>
              <td rowspan="1" colspan="1">0.818</td>
              <td rowspan="1" colspan="1">0.714</td>
              <td rowspan="1" colspan="1">0.753</td>
              <td rowspan="1" colspan="1">0.752</td>
              <td rowspan="1" colspan="1">0.763</td>
              <td rowspan="1" colspan="1">0.602</td>
              <td rowspan="1" colspan="1">0.618</td>
              <td rowspan="1" colspan="1">0.597</td>
              <td rowspan="1" colspan="1">0.612</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">(0.042)</td>
              <td rowspan="1" colspan="1">(0.029)</td>
              <td rowspan="1" colspan="1">(0.035)</td>
              <td rowspan="1" colspan="1">(0.048)</td>
              <td rowspan="1" colspan="1">(0.021)</td>
              <td rowspan="1" colspan="1">(0.032)</td>
              <td rowspan="1" colspan="1">(0.038)</td>
              <td rowspan="1" colspan="1">(0.026)</td>
              <td rowspan="1" colspan="1">(0.016)</td>
              <td rowspan="1" colspan="1">(0.017)</td>
            </tr>
            <tr>
              <td rowspan="2" colspan="1">GICF (<xref rid="btaa477-B19" ref-type="bibr">Kotzias <italic>et al.</italic>, 2015</xref>)</td>
              <td rowspan="1" colspan="1">0.812</td>
              <td rowspan="1" colspan="1">0.847</td>
              <td rowspan="1" colspan="1">0.738</td>
              <td rowspan="1" colspan="1">0.785</td>
              <td rowspan="1" colspan="1">0.772</td>
              <td rowspan="1" colspan="1">0.792</td>
              <td rowspan="1" colspan="1">0.624</td>
              <td rowspan="1" colspan="1">0.648</td>
              <td rowspan="1" colspan="1">0.622</td>
              <td rowspan="1" colspan="1">0.684</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">(0.029)</td>
              <td rowspan="1" colspan="1">(0.032)</td>
              <td rowspan="1" colspan="1">(0.029)</td>
              <td rowspan="1" colspan="1">(0.037)</td>
              <td rowspan="1" colspan="1">(0.028)</td>
              <td rowspan="1" colspan="1">(0.035)</td>
              <td rowspan="1" colspan="1">(0.038)</td>
              <td rowspan="1" colspan="1">(0.024)</td>
              <td rowspan="1" colspan="1">(0.021)</td>
              <td rowspan="1" colspan="1">(0.033)</td>
            </tr>
            <tr>
              <td rowspan="2" colspan="1">AttMIL (<xref rid="btaa477-B18" ref-type="bibr">Ilse, 2018</xref>)</td>
              <td rowspan="1" colspan="1">0.864</td>
              <td rowspan="1" colspan="1">0.881</td>
              <td rowspan="1" colspan="1">0.792</td>
              <td rowspan="1" colspan="1">0.826</td>
              <td rowspan="1" colspan="1">0.813</td>
              <td rowspan="1" colspan="1">0.847</td>
              <td rowspan="1" colspan="1">0.688</td>
              <td rowspan="1" colspan="1">0.724</td>
              <td rowspan="1" colspan="1">0.724</td>
              <td rowspan="1" colspan="1">0.759</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">(0.018)</td>
              <td rowspan="1" colspan="1">(0.024)</td>
              <td rowspan="1" colspan="1">(0.021)</td>
              <td rowspan="1" colspan="1">(0.033)</td>
              <td rowspan="1" colspan="1">(0.025)</td>
              <td rowspan="1" colspan="1">(0.031)</td>
              <td rowspan="1" colspan="1">(0.026)</td>
              <td rowspan="1" colspan="1">(0.019)</td>
              <td rowspan="1" colspan="1">(0.029)</td>
              <td rowspan="1" colspan="1">(0.037)</td>
            </tr>
            <tr>
              <td rowspan="2" colspan="1">
                <bold>IDMIL</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.917</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.951</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.845</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.895</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.867</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.882</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.767</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.793</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.782</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.816</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>(0.027)</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>(0.021)</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>(0.042)</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>(0.035)</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>(0.024)</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>(0.024)</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>(0.047)</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>(0.028)</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>(041)</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>(0.036)</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic>Note</italic>: Bold texts indicate comparatively better performances.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p><italic>Note</italic>: Bold texts indicate comparatively better performances.</p>
      <p>When predicting liver cirrhosis disease IDMIL has achieved 91.7% accuracy and 95.1% AUC-ROC which is better than the non-MIL-based approaches, i.e. MetAML (87.7% accuracy, 94.5% AUC-ROC), PLG–ABD (89.1% accuracy, 91.4% AUC-ROC) and other nondeep-learning-based MIL approaches such as GICF (81.2% accuracy, 84.7% AUC-ROC) as well as deep-learning-based MIL approach AttMIL (86.4% accuracy, 88.1% AUC-ROC). We see that other MIL-based approaches provide competitive predictive performance compared to the non-MIL approaches. This shows the effectiveness of our proposed instance and bag generation mechanism as well as the effectiveness of the MIL paradigm in predicting disease from large-scale DNA sequence data.</p>
      <p>In this article, we only focus on the DNA sequence data for disease predictions. IDMIL performs well with only the DNA sequence data compared to other approaches. However, factors other than the microbial composition can affect health conditions. For obesity and T2D datasets, all approaches showed lower predictive performances compared to the other datasets. Possible reasons may include more subtle shifts in microbial diversity from a healthy state to obesity or T2D compared to other diseases. External factors such as medication, living style and diet (<xref rid="btaa477-B31" ref-type="bibr">Pasolli <italic>et al.</italic>, 2016</xref>) may play key role in identifying these diseases. Therefore, the type of disease is an important factor impacting the predictive performance. IDMIL’s better performance can be attributed to:
</p>
      <list list-type="bullet">
        <list-item>
          <p><italic>Noise reduction:</italic> The TF–IDF-based pruning removes kmers that are likely to be noise. The kmer-embedding considers the suffix and prefix reducing the effects of few nucleotide changes among the kmers. Instances are represented as cluster centroids which reduces dependency on a single sequence. The initial attention mechanism only prioritizes instances that help to classify the sample accurately. As a result, the effects of nondiscriminative instances are reduced.</p>
        </list-item>
        <list-item>
          <p><italic>Data utilization:</italic> IDMIL takes the raw sequences as the input. It does not restrict itself by using reference genome sequences or any knowledge as <italic>a priori</italic>. Therefore, IDMIL can generalize over different datasets as well as various sequencing technologies. Unlike MetAML (<xref rid="btaa477-B31" ref-type="bibr">Pasolli <italic>et al.</italic>, 2016</xref>) and PLG–ABD (<xref rid="btaa477-B29" ref-type="bibr">Nguyen <italic>et al.</italic>, 2017</xref>), IDMIL avoids microbial profiling prior to prediction and takes full advantage of the vast amount of genomic information inherent in the whole-metagenomic data.</p>
        </list-item>
        <list-item>
          <p><italic>Hierarchical feature extraction:</italic> The CNN models are proven to be effective for hierarchically generating the latent features. Unlike other approaches, deep CNN-based IDMIL learns complex nonlinear relations among these latent features.</p>
        </list-item>
        <list-item>
          <p><italic>Data augmentation:</italic> The data augmentation process and a minimal number of learning weights ensure that the model avoids overfitting and generalizes easily.</p>
        </list-item>
      </list>
    </sec>
    <sec>
      <title>5.2 Parameters and sensitivity analysis</title>
      <p>We used a grid search on a validation set for model selection. We train the model with 0.0001 as the learning rate, 500 as the maximum number of iterations and 5 as the batch size. The size of kmer vocabulary increases exponentially with the length of the kmers (<inline-formula id="IE72"><mml:math id="IM72"><mml:mrow><mml:msup><mml:mn>4</mml:mn><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> for <italic>k</italic>-length kmers) which directly affects the runtime of the kmer-embedding process (Section 3.1). <xref ref-type="fig" rid="btaa477-F3">Figure 3a</xref> shows the effect of various kmer-lengths on average AUC-ROC values with the margin-of-errors from ten repeated trials. We observe that when <italic>k’</italic>s value is in the range <inline-formula id="IE73"><mml:math id="IM73"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>8</mml:mn><mml:mo>–</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, we get the steepest ascent in the average AUC-ROC values. We also observe that the margin-of-error reduces with increasing value of <italic>k</italic>. This is because the model better approximates the sequence reads with long subsequences. The value of <italic>k </italic>=<italic> </italic>10 is suggestive from our empirical evaluation for high accuracy and scalability.
</p>
      <fig id="btaa477-F3" orientation="portrait" position="float">
        <label>Fig. 3.</label>
        <caption>
          <p>Sensitivity of AUC-ROC with respect to (<bold>a</bold>) kmer-lengths, (<bold>b</bold>) kmer embedding dimensions and (<bold>c</bold>) number of clusters in Mini-batch KMeans. Each experiment is performed a total 10 times and the average AUC-ROC values are reported with the margin of errors. Cutoff values used in this article are shown using vertical dashed lines with the corresponding values</p>
        </caption>
        <graphic xlink:href="btaa477f3"/>
      </fig>
      <p><xref ref-type="fig" rid="btaa477-F3">Figure 3b</xref> shows the effect of various kmer embedding dimensions (increased as the power of 2) on AUC-ROC values. We notice a sharp increase in AUC-ROC values as we increase the embedding size from 8 to 64. Further increase in the embedding dimension does not provide any dramatic improvement in predictive performance but increases the computational overhead. We observe lower values of AUC-ROC (<xref ref-type="fig" rid="btaa477-F3">Fig. 3c</xref>) when the number of clusters is small. This is because the clustering algorithm forcefully combines dissimilar sequences in the same cluster when the number of clusters is small and the bags contain inadequate numbers of instances. However, selecting a large number of clusters result in near-empty clusters and higher computational runtime. Overall, an embedding dimension of <inline-formula id="IE74"><mml:math id="IM74"><mml:mrow><mml:mn>64</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mi mathvariant="normal">6</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and a total of <inline-formula id="IE75"><mml:math id="IM75"><mml:mrow><mml:mn>16</mml:mn><mml:mo> </mml:mo><mml:mn>384</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mn>14</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> clusters per sample provided the best results for all the datasets used in this article.</p>
    </sec>
    <sec>
      <title>5.3 Interpreting instance attentions</title>
      <p>The attention mechanism enables us to assign a weight <inline-formula id="IE76"><mml:math id="IM76"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> to each instance position <italic>i</italic> in the bags. A higher value of <italic>a<sub>i</sub></italic> implies that the instances in position <italic>i</italic> of the bags contribute more to the class label <inline-formula id="IE77"><mml:math id="IM77"><mml:mrow><mml:mi mathvariant="script">Y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> (unhealthy) than the class label <inline-formula id="IE78"><mml:math id="IM78"><mml:mrow><mml:mi mathvariant="script">Y</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (healthy), whereas lower <italic>a<sub>i</sub></italic> implies the opposite. Our experiments show that <italic>highly attended</italic> sequence groups come from some of the well-known pathogens. After training the model, we take the instances from the unhealthy bags of the training set where the instance position receives attention higher than 0.5. Each instance maps to a group of similar DNA sequences. We use the basic local alignment search tool (<xref rid="btaa477-B1" ref-type="bibr">Altschul <italic>et al.</italic>, 1990</xref>) to identify the species represented by those clusters. An identified species receives the attention of the cluster it belongs to. If a species is identified in multiple clusters then we take the average of the attention weights.</p>
      <p>We show two use cases using the instance-level interpretation for liver cirrhosis and colorectal cancer disease. <xref ref-type="fig" rid="btaa477-F4">Figure 4a</xref> shows some of the identified species with high attention weights for liver cirrhosis diseases. <italic>Veillonella dispar, Klebsiella pneumoniae</italic> and <italic>Streptococcus anginosus</italic> are some of the known pathogens for liver cirrhosis (<xref rid="btaa477-B31" ref-type="bibr">Pasolli <italic>et al.</italic>, 2016</xref>; <xref rid="btaa477-B36" ref-type="bibr">Qin <italic>et al.</italic>, 2014</xref>). <italic>Fusobacterium nucleatum, Peptostreptococcus stomatis, Gemella morbillorum</italic> and others (<xref ref-type="fig" rid="btaa477-F4">Fig. 4b</xref>) are found to be associated with colorectal cancer disease (<xref rid="btaa477-B21" ref-type="bibr">Kwong <italic>et al.</italic>, 2018</xref>; <xref rid="btaa477-B51" ref-type="bibr">Zeller <italic>et al.</italic>, 2014</xref>). IDMIL’s user can utilize the attention-based ranking by pruning the <italic>lowly attended</italic> sequences and use any assembler and profiling tool on the remaining <italic>highly attended</italic> sequences. The pruning reduces much of the computational burden from subsequent analysis. Our proposed approach does not restrict how microbes will be quantified in a cohort. It uses the disease classifications to infer which of the sequences can be ignored before proceeding to the microbial quantification. This allows for the discovery of novel microbes, better data utilization and easy generalization. This shows that IDMIL is also interpretable, and it has clinical significance.
</p>
      <fig id="btaa477-F4" orientation="portrait" position="float">
        <label>Fig. 4.</label>
        <caption>
          <p>Association between species and (<bold>a</bold>) liver cirrhosis disease and (<bold>b</bold>) colorectal cancer disease. Sequences from the unhealthy cohort with high attention values were used</p>
        </caption>
        <graphic xlink:href="btaa477f4"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>6 Conclusion</title>
    <p>We demonstrate a MIL-based disease prediction method from large-scale metagenomic data harnessing the hierarchical latent feature extraction capability of deep CNN. For this purpose, we propose an efficient, scalable and unsupervised bag-instance representation of the whole-metagenomic data. Our proposed approach does not require sequence assembly and microbial profiling before the disease classification. The data representations are parallel, and the prediction phase involves a minimal number of learning weights which helps this approach scale with metagenomic data. IDMIL is capable of fully utilizing the enormous amount of sequence reads in the whole-metagenomic data while providing high accuracy and efficiency. IDMIL can infer associations between DNA sequences and diseases using the attention mechanism which can lead to efficient microbial profiling and finding associations between microbes and diseases. Our proposed model is highly parallel in nature and easy to replicate in any distributed system. More generally, we have shown the effectiveness of MIL methods within metagenomics. The proposed approach has shown both strong results and significant potential for further improvements.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by National Science Foundation (NSF) Career Award 1252318 to Huzefa Rangwala. </p>
    <p><italic>Conflict of Interest</italic>: none declared. </p>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btaa477-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Altschul</surname><given-names>S.F.</given-names></name></person-group><etal>et al</etal> (<year>1990</year>) 
<article-title>Basic local alignment search tool</article-title>. <source>J. Mol. Biol</source>., <volume>215</volume>, <fpage>403</fpage>–<lpage>410</lpage>.<pub-id pub-id-type="pmid">2231712</pub-id></mixed-citation>
    </ref>
    <ref id="btaa477-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Amores</surname><given-names>J.</given-names></name></person-group> (<year>2013</year>) 
<article-title>Multiple instance classification: review, taxonomy and comparative study</article-title>. <source>Artif. Intell</source>., <volume>201</volume>, <fpage>81</fpage>–<lpage>105</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa477-B3">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Andrews</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2003</year>) Support vector machines for multiple-instance learning. In: <italic>Advances in Neural Information Processing Systems</italic>, pp. <fpage>577</fpage>–<lpage>584</lpage>. MIT Press. Cambridge, MA.</mixed-citation>
    </ref>
    <ref id="btaa477-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Arango-Argoty</surname><given-names>G.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>DeepARG: a deep learning approach for predicting antibiotic resistance genes from metagenomic data</article-title>. <source>Microbiome</source>, <volume>6</volume>, <fpage>23</fpage>.<pub-id pub-id-type="pmid">29391044</pub-id></mixed-citation>
    </ref>
    <ref id="btaa477-B5">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Ba</surname><given-names>J.L.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) Layer normalization. arXiv preprint arXiv:1607.06450. Cornell University. Ithaca, New York, US.</mixed-citation>
    </ref>
    <ref id="btaa477-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Backhed</surname><given-names>F.</given-names></name></person-group> (<year>2005</year>) 
<article-title>Host-bacterial mutualism in the human intestine</article-title>. <source>Science</source>, <volume>307</volume>, <fpage>1915</fpage>–<lpage>1920</lpage>.<pub-id pub-id-type="pmid">15790844</pub-id></mixed-citation>
    </ref>
    <ref id="btaa477-B7">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Bunescu</surname><given-names>R.C.</given-names></name>, <name name-style="western"><surname>Mooney</surname><given-names>R.J.</given-names></name></person-group> (<year>2007</year>) Multiple instance learning for sparse positive bags. In: <italic>Proceedings of the 24th International Conference on Machine Learning</italic>, pp. <fpage>105</fpage>–<lpage>112</lpage>. Association for Computing Machinery (ACM). New York, US.</mixed-citation>
    </ref>
    <ref id="btaa477-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chiu</surname><given-names>C.Y.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>Clinical metagenomics</article-title>. <source>Nat. Rev. Genet</source>., <volume>20</volume>, <fpage>341</fpage>–<lpage>355</lpage>.<pub-id pub-id-type="pmid">30918369</pub-id></mixed-citation>
    </ref>
    <ref id="btaa477-B9">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Chung</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555. Cornell University. Ithaca, New York, US.</mixed-citation>
    </ref>
    <ref id="btaa477-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dauphin</surname><given-names>Y.N.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) Language modeling with gated convolutional networks. In Proceedings of the 34th International Conference on Machine Learning, Vol. <volume>70</volume>, pp. <fpage>933</fpage>–<lpage>941</lpage>. JMLR.org. Sydney, Australia.</mixed-citation>
    </ref>
    <ref id="btaa477-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dietterich</surname><given-names>T.G.</given-names></name></person-group><etal>et al</etal> (<year>1997</year>) 
<article-title>Solving the multiple instance problem with axis-parallel rectangles</article-title>. <source>Artif. Intell</source>., <volume>89</volume>, <fpage>31</fpage>–<lpage>71</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa477-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fioravanti</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Phylogenetic convolutional neural networks in metagenomics</article-title>. <source>BMC Bioinformatics</source>, <volume>19</volume>, <fpage>49</fpage>.<pub-id pub-id-type="pmid">29536822</pub-id></mixed-citation>
    </ref>
    <ref id="btaa477-B13">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Goodfellow</surname><given-names>I.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) <source>Deep learning</source>. 
<publisher-name>MIT Press</publisher-name>. Cambridge, MA, US.</mixed-citation>
    </ref>
    <ref id="btaa477-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gu</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Recent advances in convolutional neural networks</article-title>. <source>Pattern Recogn</source>., <volume>77</volume>, <fpage>354</fpage>–<lpage>377</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa477-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Handelsman</surname><given-names>J.</given-names></name></person-group> (<year>2004</year>) 
<article-title>Metagenomics: application of genomics to uncultured microorganisms</article-title>. <source>Microbiol. Mol. Biol. Rev</source>., <volume>68</volume>, <fpage>669</fpage>–<lpage>685</lpage>.<pub-id pub-id-type="pmid">15590779</pub-id></mixed-citation>
    </ref>
    <ref id="btaa477-B16">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Hardy</surname><given-names>G.H.</given-names></name></person-group><etal>et al</etal> (<year>1952</year>) <source>Inequalities</source>. 
<publisher-name>Cambridge University Press</publisher-name>. Cambridge, United Kingdom.</mixed-citation>
    </ref>
    <ref id="btaa477-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hugenholtz</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Tyson</surname><given-names>G.W.</given-names></name></person-group> (<year>2008</year>) 
<article-title>Microbiology: metagenomics</article-title>. <source>Nature</source>, <volume>455</volume>, <fpage>481</fpage>–<lpage>483</lpage>.<pub-id pub-id-type="pmid">18818648</pub-id></mixed-citation>
    </ref>
    <ref id="btaa477-B18">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Ilse</surname><given-names>M.</given-names></name></person-group> (<year>2018</year>) Attention-based deep multiple instance learning. arXiv preprint arXiv:1802.04712. Cornell University. Ithaca, New York, US.</mixed-citation>
    </ref>
    <ref id="btaa477-B19">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Kotzias</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) From group to individual labels using deep features. In <italic>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</italic>, pp. <fpage>597</fpage>–<lpage>606</lpage>. Association for Computing Machinery (ACM). New York, US.</mixed-citation>
    </ref>
    <ref id="btaa477-B20">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Krizhevsky</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) Imagenet classification with deep convolutional neural networks. In: <italic>Advances in Neural Information Processing Systems</italic>, pp. <fpage>1097</fpage>–<lpage>1105</lpage>. Curran Associates, Inc. NY.</mixed-citation>
    </ref>
    <ref id="btaa477-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kwong</surname><given-names>T.N.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Association between bacteremia from specific microbes and subsequent diagnosis of colorectal cancer</article-title>. <source>Gastroenterology</source>, <volume>155</volume>, <fpage>383</fpage>–<lpage>390</lpage>.<pub-id pub-id-type="pmid">29729257</pub-id></mixed-citation>
    </ref>
    <ref id="btaa477-B22">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>LaPierre</surname><given-names>N.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) CAMIL: Clustering and Assembly with Multiple Instance Learning for phenotype prediction. In <italic>2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</italic>, pp. <fpage>33</fpage>–<lpage>40</lpage>. IEEE.</mixed-citation>
    </ref>
    <ref id="btaa477-B23">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Le</surname><given-names>Q.</given-names></name>, <name name-style="western"><surname>Mikolov</surname><given-names>T.</given-names></name></person-group> (<year>2014</year>) Distributed representations of sentences and documents. In: <italic>International Conference on Machine Learning</italic>, pp. <fpage>1188</fpage>–<lpage>1196</lpage>. JMLR Workshop and Conference Proceedings. Volume 32.</mixed-citation>
    </ref>
    <ref id="btaa477-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Le Chatelier</surname><given-names>E.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Richness of human gut microbiome correlates with metabolic markers</article-title>. <source>Nature</source>, <volume>500</volume>, <fpage>541</fpage>–<lpage>546</lpage>.<pub-id pub-id-type="pmid">23985870</pub-id></mixed-citation>
    </ref>
    <ref id="btaa477-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>McIntyre</surname><given-names>A.B.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Comprehensive benchmarking and ensemble approaches for metagenomic classifiers</article-title>. <source>Genome Biol</source>., <volume>18.1</volume>, <fpage>182</fpage>.<pub-id pub-id-type="pmid">28934964</pub-id></mixed-citation>
    </ref>
    <ref id="btaa477-B26">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Mikolov</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) Recurrent neural network based language model. In: <italic>Eleventh Annual Conference of the International Speech Communication Association</italic>. International Speech Communication Association (ISCA).</mixed-citation>
    </ref>
    <ref id="btaa477-B27">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Mikolov</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. Cornell University. Ithaca, New York, US.</mixed-citation>
    </ref>
    <ref id="btaa477-B28">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Ng</surname><given-names>P.</given-names></name></person-group> (<year>2017</year>) dna2vec: Consistent vector representations of variable-length k-mers. arXiv preprint arXiv:1701.06279. Cornell University. Ithaca, New York, US.</mixed-citation>
    </ref>
    <ref id="btaa477-B29">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>T.H.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) Deep learning for metagenomic data: using 2d embeddings and convolutional neural networks. arXiv preprint arXiv:1712.00244. Cornell University. Ithaca, New York, US.</mixed-citation>
    </ref>
    <ref id="btaa477-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Palangi</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Deep sentence embedding using long short-term memory networks: analysis and application to information retrieval</article-title>. <source>IEEE/ACM Trans. Audio Speech Lang. Process. (TASLP)</source>, <volume>24</volume>, <fpage>694</fpage>–<lpage>707</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa477-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pasolli</surname><given-names>E.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Machine learning meta-analysis of large metagenomic datasets: tools and biological insights</article-title>. <source>PLoS Comput. Biol</source>., <volume>12</volume>, <fpage>e1004977</fpage>.<pub-id pub-id-type="pmid">27400279</pub-id></mixed-citation>
    </ref>
    <ref id="btaa477-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pedregosa</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Scikit-learn: machine learning in Python</article-title>. <source>J. Mach. Learn. Res</source>., <volume>12</volume>, <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa477-B33">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Perez</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name></person-group> (<year>2017</year>) The effectiveness of data augmentation in image classification using deep learning. arXiv preprint arXiv:1712.04621. Cornell University. Ithaca, New York, US.</mixed-citation>
    </ref>
    <ref id="btaa477-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qin</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>A human gut microbial gene catalogue established by metagenomic sequencing</article-title>. <source>Nature</source>, <volume>464</volume>, <fpage>59</fpage>–<lpage>65</lpage>.<pub-id pub-id-type="pmid">20203603</pub-id></mixed-citation>
    </ref>
    <ref id="btaa477-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qin</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>A metagenome-wide association study of gut microbiota in type 2 diabetes</article-title>. <source>Nature</source>, <volume>490</volume>, <fpage>55</fpage>–<lpage>60</lpage>.<pub-id pub-id-type="pmid">23023125</pub-id></mixed-citation>
    </ref>
    <ref id="btaa477-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qin</surname><given-names>N.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Alterations of the human gut microbiome in liver cirrhosis</article-title>. <source>Nature</source>, <volume>513</volume>, <fpage>59</fpage>–<lpage>64</lpage>.<pub-id pub-id-type="pmid">25079328</pub-id></mixed-citation>
    </ref>
    <ref id="btaa477-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Quince</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Shotgun metagenomics, from sampling to analysis</article-title>. <source>Nat. Biotechnol</source>., <volume>35</volume>, <fpage>833</fpage>–<lpage>844</lpage>.<pub-id pub-id-type="pmid">28898207</pub-id></mixed-citation>
    </ref>
    <ref id="btaa477-B38">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Rajaraman</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Ullman</surname><given-names>J.D.</given-names></name></person-group> (<year>2011</year>) <source>Mining of massive datasets</source>. 
<publisher-name>Cambridge University Press</publisher-name>. Cambridge, United Kingdom.</mixed-citation>
    </ref>
    <ref id="btaa477-B39">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Rahman</surname><given-names>M.A.</given-names></name>, <name name-style="western"><surname>Rangwala</surname><given-names>H.</given-names></name></person-group> (<year>2018</year>) RegMIL: Phenotype classification from metagenomic data. In <italic>Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics</italic>, pp. 145–154. Association for Computing Machinery (ACM). New York, US.</mixed-citation>
    </ref>
    <ref id="btaa477-B40">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Rahman</surname><given-names>M.A.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) Phenotype prediction from metagenomic data using Clustering and Assembly with Multiple Instance Learning (CAMIL). In: <italic>IEEE/ACM Transactions on Computational Biology and Bioinformatics</italic>. Institute of Electrical and Electronics Engineers (IEEE). New Jersey, US.</mixed-citation>
    </ref>
    <ref id="btaa477-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rahman</surname><given-names>M.A.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Metagenome sequence clustering with hash-based canopies</article-title>. <source>J. Bioinf. Comput. Biol</source>., <volume>15</volume>, <fpage>1740006. World Scientific</fpage>.</mixed-citation>
    </ref>
    <ref id="btaa477-B42">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Ruckle</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) Concatenated power mean word embeddings as universal cross-lingual sentence representations. arXiv preprint arXiv:1803.01400. Cornell University. Ithaca, New York, US.</mixed-citation>
    </ref>
    <ref id="btaa477-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Saulnier</surname><given-names>D.M.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Gastrointestinal microbiome signatures of pediatric patients with irritable bowel syndrome</article-title>. <source>Gastroenterology</source>, <volume>141</volume>, <fpage>1782</fpage>–<lpage>1791</lpage>.<pub-id pub-id-type="pmid">21741921</pub-id></mixed-citation>
    </ref>
    <ref id="btaa477-B44">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Sculley</surname><given-names>D.</given-names></name></person-group> (<year>2010</year>) Web-scale k-means clustering. In <italic>Proceedings of the 19th International Conference on World Wide Web</italic> ACM, pp. <fpage>1177</fpage>–<lpage>1178</lpage>. Association for Computing Machinery (ACM). New York, US.</mixed-citation>
    </ref>
    <ref id="btaa477-B45">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Simonyan</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Zisserman</surname><given-names>A.</given-names></name></person-group> (<year>2014</year>) Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556. Cornell University. Ithaca, New York, US.</mixed-citation>
    </ref>
    <ref id="btaa477-B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Srivastava</surname><given-names>N.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>. <source>J. Mach. Learn. Res</source>., <volume>15</volume>, <fpage>1929</fpage>–<lpage>1958</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa477-B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Truong</surname><given-names>D.T.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>MetaPhlAn2 for enhanced metagenomic taxonomic profiling</article-title>. <source>Nat. Methods</source>, <volume>12</volume>, <fpage>902</fpage>–<lpage>903</lpage>.<pub-id pub-id-type="pmid">26418763</pub-id></mixed-citation>
    </ref>
    <ref id="btaa477-B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Turnbaugh</surname><given-names>P.J.</given-names></name></person-group><etal>et al</etal> (<year>2007</year>) 
<article-title>The human microbiome project</article-title>. <source>Nature</source>, <volume>449</volume>, <fpage>804</fpage>–<lpage>810</lpage>.<pub-id pub-id-type="pmid">17943116</pub-id></mixed-citation>
    </ref>
    <ref id="btaa477-B49">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Vaswani</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) Attention is all you need. In: <italic>Advances in Neural Information Processing Systems</italic>, pp. <fpage>5998</fpage>–<lpage>6008</lpage>. Curran Associates, Inc.</mixed-citation>
    </ref>
    <ref id="btaa477-B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wade</surname><given-names>W.</given-names></name></person-group> (<year>2002</year>) 
<article-title>Unculturable bacteria—the uncharacterized organisms that cause oral infections</article-title>. <source>J. R. Soc. Med</source>., <volume>95</volume>, <fpage>81</fpage>–<lpage>83</lpage>.<pub-id pub-id-type="pmid">11823550</pub-id></mixed-citation>
    </ref>
    <ref id="btaa477-B51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zeller</surname><given-names>G.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Potential of fecal microbiota for early-stage detection of colorectal cancer</article-title>. <source>Mol. Syst. Biol</source>., <volume>10</volume>, <fpage>766</fpage>.<pub-id pub-id-type="pmid">25432777</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
