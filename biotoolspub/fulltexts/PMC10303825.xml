<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName archivearticle.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neuroinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neuroinform</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neuroinform.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1662-5196</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10303825</article-id>
    <article-id pub-id-type="doi">10.3389/fninf.2023.1174049</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Methods</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SNAP: a structure-based neuron morphology reconstruction automatic pruning pipeline</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Ding</surname>
          <given-names>Liya</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1432119/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhao</surname>
          <given-names>Xuan</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Guo</surname>
          <given-names>Shuxia</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/2326380/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Yufeng</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/2274454/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Lijuan</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Yimin</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/649792/overview"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Peng</surname>
          <given-names>Hanchuan</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="c002" ref-type="corresp">
          <sup>*</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Institute for Brain and Intelligence, Southeast University</institution>, <addr-line>Nanjing</addr-line>, <country>China</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Guangdong Institute of Intelligence Science and Technology</institution>, <addr-line>Zhuhai</addr-line>, <country>China</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Anan Li, Huazhong University of Science and Technology, China</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Qing Huang, Wuhan Institute of Technology, China; Ye Li, University of Michigan, United States</p>
      </fn>
      <corresp id="c001">*Correspondence: Liya Ding <email>dinglyosu@seu.edu.cn</email></corresp>
      <corresp id="c002">Hanchuan Peng <email>h@braintell.org</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>14</day>
      <month>6</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>17</volume>
    <elocation-id>1174049</elocation-id>
    <history>
      <date date-type="received">
        <day>25</day>
        <month>2</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>22</day>
        <month>5</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2023 Ding, Zhao, Guo, Liu, Liu, Wang and Peng.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Ding, Zhao, Guo, Liu, Liu, Wang and Peng</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <sec>
        <title>Background</title>
        <p>Neuron morphology analysis is an essential component of neuron cell-type definition. Morphology reconstruction represents a bottleneck in high-throughput morphology analysis workflow, and erroneous extra reconstruction owing to noise and entanglements in dense neuron regions restricts the usability of automated reconstruction results. We propose SNAP, a structure-based neuron morphology reconstruction pruning pipeline, to improve the usability of results by reducing erroneous extra reconstruction and splitting entangled neurons.</p>
      </sec>
      <sec>
        <title>Methods</title>
        <p>For the four different types of erroneous extra segments in reconstruction (caused by noise in the background, entanglement with dendrites of close-by neurons, entanglement with axons of other neurons, and entanglement within the same neuron), SNAP incorporates specific statistical structure information into rules for erroneous extra segment detection and achieves pruning and multiple dendrite splitting.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>Experimental results show that this pipeline accomplishes pruning with satisfactory precision and recall. It also demonstrates good multiple neuron-splitting performance. As an effective tool for post-processing reconstruction, SNAP can facilitate neuron morphology analysis.</p>
      </sec>
    </abstract>
    <kwd-group>
      <kwd>neuron morphology reconstruction</kwd>
      <kwd>bioinformatics</kwd>
      <kwd>image processing</kwd>
      <kwd>post-processing</kwd>
      <kwd>dendrite tracing</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">
          <institution-wrap>
            <institution>National Natural Science Foundation of China-Guangdong Joint Fund</institution>
            <institution-id institution-id-type="doi">10.13039/501100014857</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group>
        <funding-source id="cn002">
          <institution-wrap>
            <institution>Special Project for Research and Development in Key areas of Guangdong Province</institution>
            <institution-id institution-id-type="doi">10.13039/501100015956</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group>
        <funding-source id="cn003">
          <institution-wrap>
            <institution>Fundamental Research Funds for the Central Universities</institution>
            <institution-id institution-id-type="doi">10.13039/501100012226</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group>
        <funding-source id="cn004">
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="doi">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <funding-statement>This work was funded by NSFC-Guangdong Joint Fund-U20A6005, Key-Area Research and Development Program of Guangdong Province (2018B030331001), Fundamental Research Funds for the Central Universities (2242022R10089) to LD, NNSFC Grant 32071367, and NSF Shanghai Grant 20ZR1420100 to YW.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="9"/>
      <table-count count="1"/>
      <equation-count count="4"/>
      <ref-count count="36"/>
      <page-count count="13"/>
      <word-count count="7826"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>Characterization of neuron cell type is an international research frontier in neuron science (Zeng and Sanes, <xref rid="B33" ref-type="bibr">2017</xref>). Neuron morphology is considered to be a critical component of neuron cell type identification (Ascoli et al., <xref rid="B1" ref-type="bibr">2008</xref>). In recent years, there has been considerable development of techniques, including sparse, robust, and consistent fluorescent labeling of a wide range of neuronal types (Peng et al., <xref rid="B23" ref-type="bibr">2021</xref>) and fluorescence micro-optical sectioning tomography (fMOST; Gong et al., <xref rid="B6" ref-type="bibr">2016</xref>). With these techniques, reconstruction of single-neuron morphology from optical microscopy images has become possible and now has an essential role in neuron science. Researchers have developed various manual, semi-automated, and automated neuron reconstruction tools for digital reconstruction of neuron morphology (Meijering, <xref rid="B18" ref-type="bibr">2010</xref>). Research institutions have also held competitions and established worldwide projects, such as the DIADEM competition (Liu, <xref rid="B15" ref-type="bibr">2011</xref>) and BigNeuron (Peng et al., <xref rid="B21" ref-type="bibr">2015</xref>; Manubens-Gil et al., <xref rid="B17" ref-type="bibr">2023</xref>). A large number of automated neuron reconstruction algorithms exist. For example, the 3D Visualization-Assisted Analysis software suite Vaa3D (Peng et al., <xref rid="B20" ref-type="bibr">2014</xref>) has more than 32 plugins, including ENT (Wang et al., <xref rid="B27" ref-type="bibr">2017</xref>), APP (Peng et al., <xref rid="B22" ref-type="bibr">2011</xref>), APP2 (Xiao and Peng, <xref rid="B30" ref-type="bibr">2013</xref>), NeuTube (Zhao et al., <xref rid="B34" ref-type="bibr">2011</xref>), MOST (Wu et al., <xref rid="B29" ref-type="bibr">2014</xref>), and ST (Chen et al., <xref rid="B3" ref-type="bibr">2015</xref>).</p>
    <p>Nevertheless, neuron morphology reconstruction remains an unsolved problem (Li S. et al., <xref rid="B13" ref-type="bibr">2019</xref>). The wide variety of brain images in terms of background noise, complicated branching patterns, and clutter of neuron fibers presents challenges for automated neuron reconstruction. Existing automated reconstruction algorithms are generally effective only for a few specific data sets. Owing to the complexity of the images and the limitations of automated reconstruction algorithms, these algorithms are unsuitable for whole-brain images. Moreover, for data sets with a low signal-to-noise ratio and dense neuron distribution with neuron fiber entanglement, the existing reconstruction algorithms do not show satisfactory performance. Pre-processing algorithms, including multi-scale enhancement (Zhou et al., <xref rid="B36" ref-type="bibr">2015</xref>), CaNE (Liang et al., <xref rid="B14" ref-type="bibr">2017</xref>), and filtering-based enhancement (Guo et al., <xref rid="B8" ref-type="bibr">2022</xref>) aim to enhance images by reducing background noise and improving image contrast. Deep learning–based approaches have been investigated for neuron tracing. Among them, weakly supervised learning (Huang et al., <xref rid="B10" ref-type="bibr">2020</xref>) and false negative mining (Liu et al., <xref rid="B16" ref-type="bibr">2022</xref>) are proposed to rescue and connect the weak and broken neurites in the segmentation step for reconstruction; subgraph connection (SGC) method (Huang et al., <xref rid="B9" ref-type="bibr">2022</xref>) starts from prediction map obtained by CNN to link the broken reconstruction; crossover structure separation (CSS) method (Guo et al., <xref rid="B7" ref-type="bibr">2021</xref>) is proposed to detect the crossover structures and generate deformed separated neuronal fibers in the images to eliminate entanglements in reconstruction. However, even with these pre-processing and advanced deep learning–based approaches, the results of automated reconstruction still contain complex errors and cannot be used directly in analysis. To obtain gold-standard morphology reconstruction, researchers need to curate reconstruction results with manual reconstruction platforms such as Vaa3D (Peng et al., <xref rid="B20" ref-type="bibr">2014</xref>), TeraVR (Wang et al., <xref rid="B28" ref-type="bibr">2019</xref>), or FNT (Gao et al., <xref rid="B5" ref-type="bibr">2022</xref>); however, such manual annotation is labor-intensive and time-consuming, limiting the throughput of the morphology reconstruction workflow.</p>
    <p>In morphology reconstruction systems, therefore, the manual annotation time should be reduced to achieve high throughput, which means the errors resulting from automated reconstruction must be reduced. We closely studied the errors in reconstruction results from several automated algorithms, including ENT (Wang et al., <xref rid="B27" ref-type="bibr">2017</xref>), APP2 (Xiao and Peng, <xref rid="B30" ref-type="bibr">2013</xref>), and ST (Chen et al., <xref rid="B3" ref-type="bibr">2015</xref>). Based on observations of a vast number of samples (see <xref rid="SM1" ref-type="supplementary-material">Supplementary material</xref>, Section 1), we identified several types of error: missed reconstruction and erroneous extra reconstruction due to entanglement, noise, or other artifacts. Note that by the term “entanglement” in this paper, we mean neuron fibers very close to each other in optical microscopy images that are difficult to distinguish, resulting in “crossing” structures in reconstruction. These intertwined reconstructions within the same neuron or from different neurons constitute significant challenges for automated reconstruction. <xref rid="F1" ref-type="fig">Figures 1A</xref>–<xref rid="F1" ref-type="fig">D</xref> show examples from various situations of automated neuron reconstruction results with errors. With manual annotation for error type on the error sample set (see <xref rid="SM1" ref-type="supplementary-material">Supplementary material</xref>, Section 1), we found the majority (around 63.53%) were erroneous extra reconstructions (false positive), whereas a reasonable number (around 24.16%) were due to missed reconstruction (false negative), and the rest (around 12.31%) were combined errors. On the other hand, we carried out a survey for the annotation personnel on their opinion on which of the two tasks, annotating automated reconstruction results with some extra segments or reconstruction with some missing segments, would be more time-consuming or tiring. Ninety percentage of the group believed the process of eliminating extra reconstruction segments is more time-consuming or laborious than adding missing segments. Reducing erroneous extra reconstruction segments could expedite the process of manual annotation. Therefore, it is a promising approach to prune automated reconstruction results.</p>
    <fig position="float" id="F1">
      <label>Figure 1</label>
      <caption>
        <p>Examples of automated reconstruction results and SNAP pipeline processes. <bold>(A–D)</bold> Display MIP images of neurons overlaid with automated reconstruction results in red. The green arrows point to places of erroneous reconstruction. Four situations of erroneous extra reconstruction segments are shown: <bold>(A)</bold> noisy segments in the APP2 results; <bold>(B)</bold> dense neuron fibers with entanglement with other neurons in the APP2 results; <bold>(C)</bold> entanglement with passing neuron fibers in the ST results; <bold>(D)</bold> entanglement with the neuron itself in the ENT results. <bold>(E)</bold> The workflow of the SNAP pipeline illustrated with an example. In each step, the corresponding type of wrong segments are pruned away. Step 1 prunes noisy segments; step 2 prunes entanglements with other dendrites; step 3 prunes crossings involving passing fibers of other neurons or fibers of the neuron of interest itself. Note that blue segments are the result of each step, while red ones are the pruned–way ones in each step.</p>
      </caption>
      <graphic xlink:href="fninf-17-1174049-g0001" position="float"/>
    </fig>
    <p>In the literature, there are several papers describing post-processing of automated reconstruction results using various methods, e.g., G-Cut (Li R. et al., <xref rid="B12" ref-type="bibr">2019</xref>), ray-shooting based repairer (Yu et al., <xref rid="B32" ref-type="bibr">2021</xref>), and solemnization algorithm (Jiang et al., <xref rid="B11" ref-type="bibr">2020</xref>). However, only some of these studies focused on the pruning of results. In the challenging scenario of group neuron reconstruction in densely labeled regions with entanglement of dendrites from multiple neuron cells, the main errors are erroneous extra reconstructions due to crossings, as mentioned above. Solutions to this problem include G-Cut (Li R. et al., <xref rid="B12" ref-type="bibr">2019</xref>), NeuroGPS-Tree (Quan et al., <xref rid="B25" ref-type="bibr">2016</xref>; Zhou et al., <xref rid="B35" ref-type="bibr">2021</xref>), and TREES Toolbox (Cuntz et al., <xref rid="B4" ref-type="bibr">2010</xref>), which separate densely intertwined neurons. G-Cut determines which neuron a node belongs to by judging the angle between the local segment and the line connecting the soma and the node. NeuroGPS-Tree identifies spurious links (“bridges”) between the reconstructions of two neurons in an iterative manner and separates the neurons by removing certain ends of bridges. TREES Toolbox employs competitive branch order in neuron splitting. However, most of these software tools do not handle other errors, such as entanglement errors within the same neuron and errors involving other axons passing by, which are essential tasks in pruning.</p>
    <p>This paper proposes SNAP, a structure-based neuron reconstruction automated pruning pipeline. It aims to prune away errors in the reconstruction results while keeping correct reconstructions, thereby speeding up further curation. It also separates the entangled reconstructions of multiple neurons as this is part of the pruning problem. We focus particularly on dendrite reconstruction as this is the basic component of neuron reconstruction. The dendrite corresponds to the near-soma region, which serves as the first image block of UltraTracer (Peng et al., <xref rid="B24" ref-type="bibr">2017</xref>) for complete neuron morphology reconstruction. When post-processing in this first block reduces errors, fewer wrong reconstructions will be made when UltraTracer adaptively explores and traces neighboring subareas, which will improve the overall reconstruction performance. When developing SNAP, we thoroughly studied dendrite structure and identified models for the four main categories of errors we needed to prune. SNAP has three main steps, and the pipeline is illustrated in <xref rid="F1" ref-type="fig">Figure 1E</xref>. The algorithms are described in Section 2. The performance of our proposed SNAP pipeline is validated (in Section 3) by applying it to automated reconstruction results and comparing the pruned results with those of gold-standard manual annotation. We demonstrate that a great proportion of erroneous extra reconstruction segments are removed, and thus the reconstruction quality is improved substantially.</p>
  </sec>
  <sec sec-type="methods" id="s2">
    <title>2. Methods</title>
    <p>The digital neuron morphology reconstruction results can be organized into a tree-like set of nodes with parent–child relationships (O'Halloran, <xref rid="B19" ref-type="bibr">2020</xref>) and are usually stored in standardized SWC files (Cannon et al., <xref rid="B2" ref-type="bibr">1998</xref>). In SNAP, the reconstructions are first converted into a segment-based tree data structure, as shown in <xref rid="F2" ref-type="fig">Figure 2</xref>. We denote the segment set as {<italic>S</italic><sub><italic>i</italic></sub>}, <italic>i</italic> = 1, 2, ..., <italic>N</italic>, where <italic>N</italic> is the total number of segments. The parent and child relationships of nodes in SWC format are converted into the parent and child relationships of the segments. The nodes, including the soma point, bifurcation points, and endpoints, have a facilitating role, and we denote the corresponding node set as {<italic>B</italic><sub><italic>j</italic></sub>}, <italic>j</italic> = 1, 2, ...<italic>M</italic>, where <italic>M</italic> is the total number of nodes. If a segment is the furthest segment from the soma, without any child segments, we call it a leaf segment. The level of the segment, <inline-formula><mml:math id="M1" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, is calculated as the number of segments <italic>S</italic><sub><italic>i</italic></sub> that must be passed through to reach a leaf segment. For example, the leaf segment's level is 0, its parent segment's level is 1, and so on. Note that the segments are oriented, in the direction of reconstruction outwards from the soma. SNAP aims to identify the erroneous extra reconstruction segments in the segment set of {<italic>S</italic><sub><italic>i</italic></sub>}.</p>
    <fig position="float" id="F2">
      <label>Figure 2</label>
      <caption>
        <p>SNAP reconstruction data structure: illustration of converted reconstruction format of directed segments and node-based tree structure.</p>
      </caption>
      <graphic xlink:href="fninf-17-1174049-g0002" position="float"/>
    </fig>
    <p>Note also that SNAP targets single-neuron reconstruction, so if there is more than one connected structure in the reconstruction results, the tree associated with the soma understudy will be kept and worked on, whereas the other parts (including some broken reconstruction fragments) will be discarded.</p>
    <p>Four major categories of erroneous extra reconstruction are identified in the statistical analysis mentioned above. <italic>C</italic><sub>1</sub> are segments caused by noise in the background. The other three categories are segments caused by entanglement with dendrites of other neurons (<italic>C</italic><sub>2</sub>), axons of other neurons (<italic>C</italic><sub>3</sub>), or the same neuron (<italic>C</italic><sub>4</sub>). The pipeline is designed to deal with all four categories. It starts with the relatively easier category <italic>C</italic><sub>1</sub> to simplify the situation and then moves on to the harder cases. Thus, the pipeline deals first with the <italic>C</italic><sub>1</sub> type in Step 1, then with <italic>C</italic><sub>2</sub> in Step 2, and finally with <italic>C</italic><sub>3</sub> and <italic>C</italic><sub>4</sub> in Step 3, as shown in <xref rid="F1" ref-type="fig">Figure 1E</xref>.</p>
    <sec>
      <title>2.1. Step 1: removal of noisy segments (<italic>C</italic><sub>1</sub>)</title>
      <p>Segments in <italic>C</italic><sub>1</sub> are usually caused by noise in the background, including noise due to microscopy imaging, signals from irrelevant particles, or the halo of a strong signal. In general, these noisy segments are leaf segments and are relatively short. A key observation is that the linearity of these segments is weak, whereas the linearity of true neuron fiber segments is strong. Using a set of gold-standard manual annotations, statistical analysis of the length of leaf segments <inline-formula><mml:math id="M2" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is performed, as shown in <xref rid="F3" ref-type="fig">Figure 3A</xref>. One percentile of the population is set as a reasonable threshold (<italic>T</italic><sup><italic>Len</italic></sup>) to identify such short segments. Furthermore, the linearity feature of each segment is calculated. The “anisotropy” values of each node in the segments, λ<sub>1</sub>, λ<sub>2</sub>, andλ<sub>3</sub> (λ<sub>1</sub> &gt; λ<sub>2</sub> &gt; λ<sub>3</sub>), are the eigenvalues of the node; hence, the linearity feature is calculated as <inline-formula><mml:math id="M3" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mtext>λ</mml:mtext></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mrow><mml:mtext>λ</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, where <italic>N</italic><sub><italic>i</italic></sub> is the number of the node in <italic>S</italic><sub><italic>i</italic></sub>. Based on the histogram of <inline-formula><mml:math id="M4" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> of the training data set, a valid segment usually has a <italic>S</italic><sup><italic>Lin</italic></sup> value greater than <italic>T</italic><sup><italic>Lin</italic></sup> (as in <xref rid="F3" ref-type="fig">Figure 3B</xref>), which is one percentile of the population. In applications, leaf segments are removed using rules based on <italic>T</italic><sup><italic>Len</italic></sup> and <italic>T</italic><sup><italic>Lin</italic></sup>. This process is repeated until no further leaf segments can be removed. <xref rid="F3" ref-type="fig">Figure 3C</xref> shows an example of Step 1.</p>
      <fig position="float" id="F3">
        <label>Figure 3</label>
        <caption>
          <p>SNAP step 1. <bold>(A)</bold> Histogram of leaf segment lengths from the training set. One percentile of the population is taken as <italic>T</italic><sup><italic>Len</italic></sup> = 6.6 μ<italic>m</italic>. <bold>(B)</bold> Histogram of leaf segment linearity from the training set. One percentile of the population is taken as <italic>T</italic><sup><italic>Lin</italic></sup> = 1.4. <bold>(C)</bold> Example result of step 1. From left to right: MIP of a neuron image overlaid with APP2 results shown in red, MIP overlaid with SNAP results shown in green, zoomed-in image of the small region overlaid with SNAP results shown in green, with pruned segments shown in red.</p>
        </caption>
        <graphic xlink:href="fninf-17-1174049-g0003" position="float"/>
      </fig>
      <p>Removing <italic>C</italic><sub>1</sub> is a simple procedure and does not involve much of the neuron structure. Part of the purpose of Step 1 is to avoid artifacts caused by these short and noisy segments from persisting into Steps 2 and 3. Steps 2 and 3, which deal with the remaining wrong segments <italic>C</italic>2, <italic>C</italic>3, and <italic>C</italic>4, are much more closely related to the dendrite structure and represent the main contribution of our proposed pipeline.</p>
    </sec>
    <sec>
      <title>2.2. Step 2: separation of entangled dendrites (<italic>C</italic><sub>2</sub>)</title>
      <p>For the pruning of segments involving nearby neurons, which usually have their dendrites entangled with the dendrites of the current neuron, we need to define locations to separate the reconstruction into multiple neurons.</p>
      <p>Without loss of generality, we assume a pair of neurons with soma <italic>A</italic> and soma <italic>A</italic>′ that need to be separated. The path linking <italic>A</italic> and <italic>A</italic>′ has bifurcation point set <italic>B</italic><sub><italic>i</italic></sub>, for <italic>i</italic> = 1, 2, ...<italic>N</italic><sub><italic>B</italic></sub>, where <italic>N</italic><sub><italic>B</italic></sub> is the total number of the bifurcation points on this path (<xref rid="F4" ref-type="fig">Figure 4A</xref>). Each bifurcation point is a candidate separation site, and we need to identify the bifurcation point that best separates the path. After the separation, there are two reconstructions on the path: <italic>R</italic><sub><italic>A</italic></sub> for the neuron with soma <italic>A</italic>, and <inline-formula><mml:math id="M5" overflow="scroll"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:math></inline-formula> for the neuron with soma <italic>A</italic>′ (<xref rid="F4" ref-type="fig">Figure 4B</xref>). Using this bifurcation point to separate the path should be beneficial to the reconstruction of both neurons. The goal of Step 2 is to maximize the sum of the likelihood of branching patterns in <italic>R</italic><sub><italic>A</italic></sub> and <inline-formula><mml:math id="M6" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>. Here, weight <italic>W</italic><sub><italic>X</italic><sub><italic>B</italic></sub><sub><italic>i</italic></sub></sub> is introduced to reflect the likelihood of <italic>B</italic><sub><italic>i</italic></sub> belonging to the neuron with soma <italic>X</italic> (where <italic>X</italic> is either <italic>A</italic> or <italic>A</italic>′). The summation of weights for each bifurcation is used to reflect the joint likelihood. Thus, the best bifurcation point for separation, <italic>B</italic><sub><italic>s</italic></sub>, can be identified by:</p>
      <disp-formula id="E1">
        <label>(1)</label>
        <mml:math id="M7" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mstyle displaystyle="true">
                  <mml:munder class="msub">
                    <mml:mrow>
                      <mml:mo class="qopname">arg max</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>s</mml:mi>
                    </mml:mrow>
                  </mml:munder>
                </mml:mstyle>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>W</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>s</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munderover accentunder="false" accent="false">
                    <mml:mrow>
                      <mml:mo>∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>s</mml:mi>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>W</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>A</mml:mi>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>B</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>+</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munderover accentunder="false" accent="false">
                    <mml:mrow>
                      <mml:mo>∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>j</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mi>s</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>N</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>B</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>W</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mi>A</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>′</mml:mi>
                      </mml:mrow>
                    </mml:msup>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>B</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>j</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>.</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>Using statistical analysis of the branching pattern in training data sets, the weight is defined based on the angle of main path segments and child segments and on the distance between bifurcation points and the soma location (details in <xref rid="SM1" ref-type="supplementary-material">Supplementary material</xref>, Section 2). Putting this weight into the argmax target <italic>W</italic><sub><italic>s</italic></sub> above, the best separation point <italic>B</italic><sub><italic>s</italic></sub> can be identified. Then, the two neurons are separated at this point into two reconstructions, <italic>R</italic><sub><italic>A</italic></sub> and <inline-formula><mml:math id="M8" overflow="scroll"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:math></inline-formula>.</p>
      <fig position="float" id="F4">
        <label>Figure 4</label>
        <caption>
          <p>SNAP step 2 for separating neurons. <bold>(A, B)</bold> Illustrate the separating process, where somata, bifurcation points, and segments are represented by triangles, dots, and lines, respectively. <bold>(A)</bold> The original path linking the two somata, A and A'. The blue segments construct the main path, and the yellow segments are the child segments. The blue arrow points to the identified best dividing bifurcation point. <bold>(B)</bold> The resultant divided parts. <bold>(C)</bold> An example of separation. From left to right: MIP of the original image block overlaid with APP2 results shown in red, overlaid with SNAP results shown in green, and a zoomed-in view of the small region around the entanglement.</p>
        </caption>
        <graphic xlink:href="fninf-17-1174049-g0004" position="float"/>
      </fig>
      <p>This process is applied to all paths linking neuron pairs; thus, they can all be separated. When applying pruning to the neuron of interest, reconstructions are separated, and the resulting reconstructions belonging to other neurons are removed.</p>
      <p>Note that in the process above, the soma locations are known. In addition, most of the abnormally long paths also have the problem of entanglement with other neurons, even when no somata of other neurons are close by. In such cases, a patch is added that uses the endpoints of those paths as “fake” soma locations for purposes of the separation. <xref rid="F4" ref-type="fig">Figure 4C</xref> shows an example of separation.</p>
    </sec>
    <sec>
      <title>2.3. Step 3: pruning for “crossings” (<italic>C</italic><sub>3</sub> and <italic>C</italic><sub>4</sub>)</title>
      <p>Finally, <italic>C</italic><sub>3</sub> and <italic>C</italic><sub>4</sub> are pruned. In both these categories, the “wrong” segments are caused by local entanglement, involving either passing fibers of other neurons (<italic>C</italic><sub>3</sub>) or fibers of the neuron of interest itself (<italic>C</italic><sub>4</sub>). Crossings due to entanglements are commonly found in automated reconstruction results and contribute to the majority of wrong reconstructions that are troublesome to manually correct. The removal of these two types is important and a key target of SNAP.</p>
      <p>All branching structures in the reconstruction are checked. Based on the bifurcation number in the local neighborhood of “crossings”, there are two main types of structures: (1) one bifurcation without nearby bifurcations; and (2) more than one bifurcation nearby.</p>
      <p>One bifurcation structure can be modeled as <bold>Y</bold> or <bold>T</bold>, as in <xref rid="F5" ref-type="fig">Figures 5A</xref>, <xref rid="F5" ref-type="fig">B</xref>. For <bold>Y</bold>, the two segments that are best aligned are termed <italic>S</italic><sub>1</sub> and <italic>S</italic><sub>2</sub>, and the other segment as <italic>S</italic><sub>3</sub>, and <italic>S</italic><sub>2</sub> is assigned to the segment with a smaller angle with <italic>S</italic><sub>3</sub>. Different situations of parent–child segment relationships are examined. When the parent segment is <italic>S</italic><sub>1</sub>, we have a typical bifurcation; otherwise, the child segments could represent an error involving other dendrites or axons and thus a wrong segment due to “crossing.” When the parent segment is <italic>S</italic><sub>3</sub>, <italic>S</italic><sub>1</sub>, and <italic>S</italic><sub>2</sub> are considered wrong and will be removed. When the parent segment is <italic>S</italic><sub>2</sub>, then <italic>S</italic><sub>3</sub> is suspicious; the determination of <italic>S</italic><sub>3</sub> will be solved in the degenerated <bold>X</bold> case as described later. When the angle <inline-formula><mml:math id="M9" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="M10" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:math></inline-formula> are both close to 90 degrees, the <bold>Y</bold> type becomes a <bold>T</bold> type, which is processed in a similar way to the <bold>Y</bold> type.</p>
      <fig position="float" id="F5">
        <label>Figure 5</label>
        <caption>
          <p>SNAP step 3 models and examples. <bold>(A)</bold>
<bold>Y</bold> models. <bold>(B)</bold>
<bold>T</bold> models. <bold>(C)</bold>
<bold>X</bold> model. <bold>(D)</bold>
<bold>H</bold> model. <bold>(E)</bold> Histogram of <italic>β</italic><sub><italic>GT</italic></sub> in the training data set. <bold>(F)</bold> Reconstruction for suspicious <bold>Y</bold> models as degenerated <bold>X</bold> models. <bold>(G)</bold> An example of a pruned <bold>X</bold> structure. <bold>(H)</bold> An example of a pruned <bold>H</bold> structure. Red arrows for wrong segments, orange arrows for suspicious segments, yellow dots for bifurcation points, and a blue triangle for the reconstruction direction.</p>
        </caption>
        <graphic xlink:href="fninf-17-1174049-g0005" position="float"/>
      </fig>
      <p>The multiple bifurcation types are generally double or more <bold>Y</bold> with bifurcation points very close to each other. We define the confusing types with two bifurcation points as models of <bold>X</bold> and <bold>H</bold> as in <xref rid="F5" ref-type="fig">Figures 5C</xref>, <xref rid="F5" ref-type="fig">D</xref>, based on whether the short segment <italic>S</italic><sub>5</sub> linking the two bifurcations is correct or not, where it is correct in <bold>X</bold> and wrong in <bold>H</bold>. The child–child segment angle <italic>β</italic> plays a major part in <bold>H</bold> and <bold>X</bold> pruning. The angle threshold <italic>T</italic><sup><italic>β</italic></sup> is defined as 99% of the child–child segment angle population in the training data set, as in <xref rid="F5" ref-type="fig">Figure 5E</xref>, to define outliers. The <bold>H</bold> model is prioritized for pruning. We identify the pair of child segments that are both leaf segments in this structure (<italic>S</italic><sub>3</sub> and <italic>S</italic><sub>4</sub> as in <xref rid="F5" ref-type="fig">Figure 5D</xref>). If the angle <italic>β</italic> between them is larger than <italic>T</italic><sup><italic>β</italic></sup>, these segments and their parent segment are pruned away. This process continues recursively until no further <bold>H</bold> can be identified. Then, for <bold>X</bold> models, we identify the segment linking the two bifurcations; the angles between its child segments and “brother” segments (e.g., <italic>S</italic><sub>3</sub> and <italic>S</italic><sub>4</sub> as in <xref rid="F5" ref-type="fig">Figure 5C</xref>) are all calculated, and the two segments with the maximum angle, if larger than <italic>T</italic><sup><italic>β</italic></sup>, are pruned away. More details of the <bold>XH</bold> model-based method are described in <xref rid="SM1" ref-type="supplementary-material">Supplementary material</xref>, Section 3.</p>
      <p>In real data, there are many <bold>X</bold> and <bold>H</bold> structures with missing segments. As above, a suspicious <bold>Y</bold> or <bold>T</bold> model can be such a <bold>X</bold> or <bold>H</bold> model with missing segments. When the <bold>Y</bold> cases are considered suspicious, they are treated as degenerated cases of <bold>X</bold>. The pipeline has a local “re-tracing” process to help determine the removal. For a suspicious segment, the node with a distance of <italic>Len</italic><sub><italic>R</italic></sub> from the bifurcation point is used as the starting point, and the rest of the segment is masked out from the image. FastMarching is run to see whether reconstruction grows out to the two other segments (<xref rid="F5" ref-type="fig">Figure 5F</xref>). If so, the segment is considered correct; if not, we believe it can be attributed to the “crossing” that this segment belongs to and hence this segment is pruned away. <xref rid="F5" ref-type="fig">Figures 5G</xref>, <xref rid="F5" ref-type="fig">H</xref> show examples pruned X and H structures.</p>
      <p>Note that when the models have even more missing segments, there will be no bifurcation points left, and the segments become single segments. Therefore, single segments need to be checked if they are degenerated cases of <bold>YTXH</bold> structures. “Inflection” points are identified and pseudo-<bold>X</bold> structures are pruned as described in <xref rid="SM1" ref-type="supplementary-material">Supplementary material</xref>, Section 4.</p>
    </sec>
  </sec>
  <sec id="s3">
    <title>3. Data set and results</title>
    <sec>
      <title>3.1. Data set</title>
      <p>This study was based on three-dimensional images of single neurons acquired from 28 mouse brains with two-photon fluorescence imaging system fMOST (Gong et al., <xref rid="B6" ref-type="bibr">2016</xref>). In this fMOST dataset, the whole-brain image at the second-highest resolution level (with pixel resolution around 0.6 μ<italic>m</italic> × 0.6 μ<italic>m</italic> × 1 μ<italic>m</italic> in the <italic>x</italic>-<italic>y</italic>-<italic>z</italic> axes) was cropped into image blocks of fixed size (512px × 512px × 256px in the <italic>x</italic>, <italic>y</italic>, and <italic>z</italic> dimensions), each covering the dendritic region of a neuron with the cell body (soma) in the block center. We obtained gold-standard manual annotations from SEU-Allen Joint Center and identified the corresponding dendrite reconstruction results in the cropped images. Six hundred of them were randomly selected as the training data set for the statistical analysis throughout this work. Another 1,000 neurons constituted the testing data set, independent of the training data set. SNAP can be applied to reconstruction results from many different algorithms, e.g., ENT, ST, MST, etc. In our experiments here, the original automated neuron reconstruction results were obtained using the Vaa3D-APP2 platform with adaptive intensity threshold and default parameters for the algorithm. We opted for APP2 since it produces high-quality results on the data set we used.</p>
    </sec>
    <sec>
      <title>3.2. Qualitative evaluation</title>
      <p>SNAP was applied to the reconstruction results for the 1,000 images in fMOST testing data set. The pruning results were satisfactory. Visual examples are shown in <xref rid="F6" ref-type="fig">Figure 6</xref>. The pruning of <italic>C</italic>1 performed effectively, as exemplified by <xref rid="F6" ref-type="fig">Figure 6A</xref>, which includes zoom-in inset regions highlighting the removal of noisy segments. Multiple-neuron entanglements <italic>C</italic>2 were successfully resolved as in <xref rid="F6" ref-type="fig">Figures 6B</xref>–<xref rid="F6" ref-type="fig">F</xref>, where the reconstruction for single target neurons is separated out from the entangled multi-neuron reconstructions. The pruning of <italic>C</italic>3 &amp; <italic>C</italic>4 entanglement segments was also effective as in <xref rid="F6" ref-type="fig">Figures 6G</xref>, <xref rid="F6" ref-type="fig">H</xref>, where local and passing fiber entanglements were pruned away.</p>
      <fig position="float" id="F6">
        <label>Figure 6</label>
        <caption>
          <p>Example SNAP results on fMOST data set. The columns from left to right are as follows: MIP of original image block; overlaid with APP2 results shown in red; overlaid with SNAP results shown in green; overlaid with BP shown in yellow; overlaid with GT shown in magenta. Red arrows point to locations of pruned segments. Examples show pruning results of <bold>(A)</bold> noisy segments (with inset of zoom-in region in light blue boundary boxes); <bold>(B–F)</bold> entanglements with close-by dendrites; <bold>(G)</bold> local entanglement; and <bold>(H)</bold> passing fiber entanglement.</p>
        </caption>
        <graphic xlink:href="fninf-17-1174049-g0006" position="float"/>
      </fig>
      <p>In order to test SNAP's capability to automatically prune reconstruction obtained by a variety of algorithms from images other than the fMOST dataset above, we checked into BigNeuron (Manubens-Gil et al., <xref rid="B17" ref-type="bibr">2023</xref>), which contains various neuron images with benchmarking reconstruction. SNAP with default parameters was applied to the high-rank automated reconstructions of mouse neuron images. Two examples are shown in <xref rid="F7" ref-type="fig">Figure 7</xref>. In the first example as in <xref rid="F7" ref-type="fig">Figure 7A</xref>, the input automated reconstruction was obtained with 3D Tubular Models (Santamaría-Pang et al., <xref rid="B26" ref-type="bibr">2015</xref>), and pruning of wrong crossings within the same neuron was successful (see the zoom-in regions in <xref rid="F7" ref-type="fig">Figures 7B</xref>, <xref rid="F7" ref-type="fig">C</xref>). In the second example, as in <xref rid="F7" ref-type="fig">Figure 7D</xref>, the input automated reconstruction result was obtained with NeuroGPS-Tree and pruning of entanglements with passing fibers was effective (see the zoom-in regions in <xref rid="F7" ref-type="fig">Figures 7E</xref>, <xref rid="F7" ref-type="fig">F</xref>).</p>
      <fig position="float" id="F7">
        <label>Figure 7</label>
        <caption>
          <p>Example SNAP results on BigNeuron data set. The columns from left to right are as follows: MIP of original image block; overlaid with APP2 results shown in red; overlaid with SNAP results shown in green; overlaid with GT shown in magenta. <bold>(A–C)</bold> and <bold>(D–F)</bold> are two sets of examples, where <bold>(A, D)</bold> are full image; <bold>(B, C)</bold> are zoomed-in regions as in the light-blue and orange bounding boxes overlaid on <bold>(A)</bold>, displaying pruning of wrong crossings within the same neuron; <bold>(E, F)</bold> are zoomed-in regions as in the yellow and green bounding boxes overlaid on <bold>(B)</bold> (with slightly different viewing angle), displaying pruning of entanglements with passing fibers.</p>
        </caption>
        <graphic xlink:href="fninf-17-1174049-g0007" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.3. Quantitative performance evaluation</title>
      <p>To demonstrate the performance of SNAP, we provide a quantitative evaluation. As gold-standard manual annotation results were available, we could compare the output to this “ground truth” (GT) to determine the accuracy. However, since we start with the automated reconstruction results, and the algorithm prunes but does not add any missing segments, a direct comparison is not an appropriate choice. Hence, the “best possible pruned” result (BP) is calculated by removing all the segments from APP2 results that are not present in the GT based on their distance to GT segments (see <xref rid="SM1" ref-type="supplementary-material">Supplementary material</xref>, Section 5 for the BP calculation). As BP keeps some short segments due to noise that are very close to ground truth segments, we evaluate step 1 separately and then evaluate steps 2 and 3 (without the involvement of the short segments in step 1).</p>
      <p>In the first experiment, the performance of step 1 (pruning C1 segments) was checked. One hundred dendrites were randomly chosen from the testing data. The original reconstruction and pruned results were presented to human annotators, who were asked to label the correctly pruned segments and also the wrongly pruned ones. The results showed that out of the 48, 793 segments, <inline-formula><mml:math id="M11" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>400</mml:mn></mml:math></inline-formula> segments were pruned away, of which <inline-formula><mml:math id="M12" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>233</mml:mn></mml:math></inline-formula> were true noisy segments, and <inline-formula><mml:math id="M13" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>165</mml:mn></mml:math></inline-formula> were correct segments that were mistakenly removed. In the analog to a detection problem (where a true positive corresponds to correctly pruned segments), SNAP step 1 was quantitatively evaluated as follows.</p>
      <disp-formula id="E2">
        <mml:math id="M14" overflow="scroll">
          <mml:mtable columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mtext class="textrm" mathvariant="normal">Precision of step 1: </mml:mtext>
                <mml:mi>P</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>V</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>S</mml:mi>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>+</mml:mo>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>F</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mo>=</mml:mo>
                <mml:mn>98.4</mml:mn>
                <mml:mi>%</mml:mi>
                <mml:mo>.</mml:mo>
              </mml:mtd>
            </mml:mtr>
            <mml:mtr>
              <mml:mtd>
                <mml:mtext class="textrm" mathvariant="normal">False discovery rate of step 1: </mml:mtext>
                <mml:mi>F</mml:mi>
                <mml:mi>D</mml:mi>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>R</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>S</mml:mi>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>F</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>+</mml:mo>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>F</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mo>=</mml:mo>
                <mml:mn>1.6</mml:mn>
                <mml:mi>%</mml:mi>
                <mml:mo>.</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>Note that we did not ask annotators to determine false negatives, owing to the heavy manual labor cost of this task. Hence, no sensitivity or miss rate is given here. However, such segments would go through later steps and possibly be included in the evaluation of steps 2 and 3.</p>
      <p>Having validated step 1, we used the current pruned results to calculate the BP results. Examples of the BP results and also the GT are shown in <xref rid="F6" ref-type="fig">Figure 6</xref>. BP was not identical to GT, since BP results are the biggest matching subset of APP2 results. Note that although there has been research involving further post-processing to rescue missing segments, this is beyond the scope of this paper.</p>
      <p>In the second experiment, steps 2 and 3 were evaluated together. The SNAP results are compared with the “ground truth” given by BP. For the 1,000 neurons in the testing data set, there were <inline-formula><mml:math id="M15" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>398</mml:mn><mml:mo>,</mml:mo><mml:mn>846</mml:mn></mml:math></inline-formula> segments, and <inline-formula><mml:math id="M16" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>248</mml:mn><mml:mo>,</mml:mo><mml:mn>378</mml:mn></mml:math></inline-formula> segments were removed, of which <inline-formula><mml:math id="M17" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>238</mml:mn><mml:mo>,</mml:mo><mml:mn>537</mml:mn></mml:math></inline-formula> were true wrong segments and <inline-formula><mml:math id="M18" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>9</mml:mn><mml:mo>,</mml:mo><mml:mn>841</mml:mn></mml:math></inline-formula> were mistakenly removed correct segments. There were also <inline-formula><mml:math id="M19" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>58</mml:mn><mml:mo>,</mml:mo><mml:mn>132</mml:mn></mml:math></inline-formula> segments that should have been pruned but were not. In the analog to a detection problem, SNAP steps 2 and 3 were quantitatively evaluated as follows.</p>
      <disp-formula id="E3">
        <mml:math id="M20" overflow="scroll">
          <mml:mtable columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mtext class="textrm" mathvariant="normal">Precision of steps 2 and 3: </mml:mtext>
                <mml:mi>P</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>V</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>S</mml:mi>
                    <mml:mn>23</mml:mn>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>23</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>23</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>+</mml:mo>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>F</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>23</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mo>=</mml:mo>
                <mml:mn>96.0</mml:mn>
                <mml:mi>%</mml:mi>
                <mml:mo>.</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <disp-formula id="E4">
        <mml:math id="M21" overflow="scroll">
          <mml:mtable columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mtext class="textrm" mathvariant="normal">Sensitivity of steps 2 and 3: </mml:mtext>
                <mml:mi>T</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>R</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>S</mml:mi>
                    <mml:mn>23</mml:mn>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>23</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>23</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>+</mml:mo>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>F</mml:mi>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>23</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mo>=</mml:mo>
                <mml:mn>80.4</mml:mn>
                <mml:mi>%</mml:mi>
                <mml:mo>.</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>To reflect the differences in length among segments, we further evaluated SNAP steps 2 and 3 using segment length. Altogether, <inline-formula><mml:math id="M22" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>36273284</mml:mn><mml:mo>.</mml:mo><mml:mn>5</mml:mn></mml:math></inline-formula> pixels; we removed <inline-formula><mml:math id="M23" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>22429289</mml:mn><mml:mo>.</mml:mo><mml:mn>5</mml:mn></mml:math></inline-formula> pixels, where <inline-formula><mml:math id="M24" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1262893</mml:mn><mml:mo>.</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> pixels were true positives, <inline-formula><mml:math id="M25" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1166393</mml:mn><mml:mo>.</mml:mo><mml:mn>8</mml:mn></mml:math></inline-formula> pixels were false positives, and <inline-formula><mml:math id="M26" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>4393361</mml:mn><mml:mo>.</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> pixels were false negatives. The evaluation above could be re-done as <inline-formula><mml:math id="M27" overflow="scroll"><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>94</mml:mn><mml:mo>.</mml:mo><mml:mn>8</mml:mn><mml:mi>%</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M28" overflow="scroll"><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>82</mml:mn><mml:mo>.</mml:mo><mml:mn>9</mml:mn><mml:mi>%</mml:mi></mml:math></inline-formula>.</p>
    </sec>
    <sec>
      <title>3.4. Comparisons with other approaches</title>
      <p>To fully evaluate the proposed algorithm, we compared the performance of SNAP with that of other approaches. G-Cut (Li R. et al., <xref rid="B12" ref-type="bibr">2019</xref>), NeuroGPS-Tree (Quan et al., <xref rid="B25" ref-type="bibr">2016</xref>), and TREES toolbox (Cuntz et al., <xref rid="B4" ref-type="bibr">2010</xref>) are post-processing algorithms that can deal with the dissembling of multiple neuron entanglement by “separating” the neuron reconstruction results. From a single-cell perspective, these methods also prune away wrong segments that do not belong to the cell of interest. Hence we evaluate the pruning performance of these software tools and compare them. To ensure a fair comparison of pruning performance, we would like to rule out effects from different automated reconstruction methods. So the same input reconstruction should be provided to them. Here APP2 reconstruction results were used as the base reconstruction results for all of these tools.</p>
      <p>Of the 1,000 testing neurons, 598 involved multiple-neuron involved. The four tools were applied to all these samples with given soma locations and used to quantitatively evaluate each result for the neuron of interest. Specifically, when there were several dendrites close to the neuron of interest, the result was the separated and processed reconstruction of this neuron, disregarding the results for other neurons; this evaluation was done for 454 neurons (samples not included are: ones with multiple neuron, but APP2 results don't involve entanglements with multiple neurons; ones with no pruning happened thus precision is not defined). Three commonly used metrics, precision, sensitivity, and F1-score, were calculated for each neuron. For this specific separation problem, we adopted Miss-Extra-Score (MES; Xie et al., <xref rid="B31" ref-type="bibr">2011</xref>) as used in the evaluation of G-Cut (Li R. et al., <xref rid="B12" ref-type="bibr">2019</xref>), as MES provides a global view for neuron reconstruction based on accuracy and undesired components. MES was originally defined as <inline-formula><mml:math id="M29" overflow="scroll"><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:math></inline-formula>, where <italic>S</italic><sub><italic>G</italic></sub> is the total length of all segments in the GT trace, and <italic>S</italic><sub><italic>miss</italic></sub> and <italic>S</italic><sub><italic>extra</italic></sub> are the total lengths of missing and extra segments in the automated trace, respectively (compared with the GT). In our pruning setting, MES was reformulated as <inline-formula><mml:math id="M30" overflow="scroll"><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula>. Both segment-based and length-based metrics are presented.</p>
      <p><xref rid="F8" ref-type="fig">Figures 8A</xref>–<xref rid="F8" ref-type="fig">E</xref> show several visual examples of results. All four algorithms performed reasonably well in separating the target neuron from entangled reconstructions. Some detailed differences are: (1) SNAP and NeuroGPS-Tree are both capable of removing entanglement segments of close-by neurons even when their soma locations are not within the image region. G-Cut and TREES Toolbox rely on the clear definition of all nearby soma locations(as in <xref rid="F8" ref-type="fig">Figures 8B</xref>, <xref rid="F8" ref-type="fig">C</xref> with yellow arrows pointing to the correct removal of these segments in SNAP and NeuroGPS-Tree and red arrows pointing to unsuccessful removal in G-Cut and TREES Toolbox). (2) Similar to (1), SNAP and NeuroGPS-Tree could be on the strict side in pruning(see in <xref rid="F8" ref-type="fig">Figures 8C</xref>–<xref rid="F8" ref-type="fig">E</xref> with orange arrows pointing to over-pruning). (3) In some cases, SNAP, G-cut, and TREES Toolbox have difficulty removing segments in conjunction region of two neurons (see in <xref rid="F8" ref-type="fig">Figure 8D</xref> with blue arrows pointing to under-pruning).</p>
      <fig position="float" id="F8">
        <label>Figure 8</label>
        <caption>
          <p>Performance comparison. <bold>(A–E)</bold> Visual examples. From left to right: the original image (displayed as MIP, same for the other sub-figures) overlaid with input APP2 results (red), SNAP (green), G-Cut (blue), TREES Toolbox (orange), NeuroGPS-Tree (purple), and BP results (yellow). The arrows point to locations of some differences between the algorithms: yellow arrows for correct removal, red arrows for unsuccessful removal, orange arrows for over-pruning, and blue arrows for under-pruning. <bold>(F)</bold> The four quantitative metrics based on segments for the four algorithms are shown with box plots. <bold>(G)</bold> Length-based metrics. In <bold>(F, G)</bold>, “N-GPS-T” is used as the abbreviation for “NeuroGPS Tree.”</p>
        </caption>
        <graphic xlink:href="fninf-17-1174049-g0008" position="float"/>
      </fig>
      <p><xref rid="F8" ref-type="fig">Figures 8F</xref>, <xref rid="F8" ref-type="fig">G</xref> show box plots of precision, sensitivity, F1-score, and MES for SNAP, G-Cut, NeuroGPS-Tree and TREES Toolbox. We can see G-Cut has best precision, and SNAP has the best sensitivity, F1-score, and MES scores. Since SNAP and G-Cut perform relatively comparable, we further counted how often SNAP or G-Cut algorithms performed better than the other for each neuron, and how often they performed equally well, based on these four metrics (<xref rid="T1" ref-type="table">Table 1</xref>). Overall, SNAP had relatively lower precision but better sensitivity, F1-score, and MES; hence, in general, SNAP outperformed the rest of the algorithms.</p>
      <table-wrap position="float" id="T1">
        <label>Table 1</label>
        <caption>
          <p>Tables of performance comparison results for SNAP and G-Cut.</p>
        </caption>
        <table frame="box" rules="all">
          <thead>
            <tr style="background-color:919498;color:ffffff">
              <th valign="top" align="left" colspan="4" rowspan="1">
                <bold>Segment-based</bold>
              </th>
            </tr>
            <tr style="background-color:#919497;color:#ffffff">
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>Metric</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>SNAP is better (%)</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>SNAP = G-Cut (%)</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>G-Cut is better (%)</bold>
              </td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Precision</td>
              <td valign="top" align="center" rowspan="1" colspan="1">33</td>
              <td valign="top" align="center" rowspan="1" colspan="1">19</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>48</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Sensitivity</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>60</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">39</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">F1-score</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>55</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">44</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MES</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>47</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">11</td>
              <td valign="top" align="center" rowspan="1" colspan="1">42</td>
            </tr>
            <tr style="background-color:#919497;color:#ffffff">
              <td valign="top" align="left" colspan="4" rowspan="1">
                <bold>Length-based</bold>
              </td>
            </tr>
            <tr style="background-color:#919497;color:#ffffff">
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>Metric</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>SNAP is better (%)</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>SNAP = G-Cut (%)</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>G-Cut is better (%)</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Precision</td>
              <td valign="top" align="center" rowspan="1" colspan="1">38</td>
              <td valign="top" align="center" rowspan="1" colspan="1">8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>54</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Sensitivity</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>61</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">38</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">F1-score</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>54</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">46</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MES</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>45</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">11</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>45</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>Both segment-based metrics (top) and length-based metrics (bottom) are presented. The bold numbers indicate the superior algorithm in the comparison.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>With the ability of separating target neuron from entangled reconstruction, SNAP natural achieved multiple neuron separation functionality in dense and entangled neuron reconstruction problems by pruning w.r.t. each of the neuron. One example was shown for its multiple neuron separation performance and compare it with that of G-Cut, NeuroGPS-Tree, and TREES toolbox. An image block with nine neurons (mostly dendrite portions) was reconstructed with APP2. As shown in <xref rid="F9" ref-type="fig">Figure 9</xref>, all nine neurons were entangled as one reconstruction. We applied the four algorithms in separating the nine neurons with soma locations given. <xref rid="F9" ref-type="fig">Figures 9B</xref>–<xref rid="F9" ref-type="fig">E</xref> show the SNAP results, the G-Cut results, the NeuroGPS-Tree results, and the TREES Toolbox. We can see that all algorithms could separate the neurons reasonably well. There are some differences within these results, and similar to examples in <xref rid="F8" ref-type="fig">Figure 8</xref> there are some over pruning and under pruning involved along the separation. <xref rid="F9" ref-type="fig">Figure 9F</xref> provides the Best Possible pruned results from APP2 results with manual annotation of the two neurons visible in this field of view (the rest of the neurons don't have manual annotations). From the visual comparison, we can see SNAP achieved good separation and pruning for this group of neurons.</p>
      <fig position="float" id="F9">
        <label>Figure 9</label>
        <caption>
          <p>A multiple neurons separating and pruning example. MIPs are displayed and overlaid with reconstruction. Different neurons are shown in different colors that are consistent across the results of all algorithms and manual annotation. <bold>(A)</bold> APP2 results as input. <bold>(B)</bold> SNAP results. <bold>(C)</bold> G-Cut results. <bold>(D)</bold> NeuroGPS-Tree results. <bold>(E)</bold> TREES Toolbox results. <bold>(F)</bold> Best Possible results of the two neurons with manual annotation. The red arrows point to locations with entanglements between neurons which some algorithms can remove and some cannot. Orange arrows point to locations of over-pruning.</p>
        </caption>
        <graphic xlink:href="fninf-17-1174049-g0009" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec id="s4">
    <title>4. Software availability</title>
    <p>This study was conducted with the support of the Vaa3D platform (v.3.601). The released binary and the source code for the Vaa3D platform are available through the GitHub release page of vaa3d.org (<ext-link xlink:href="https://github.com/Vaa3D" ext-link-type="uri">https://github.com/Vaa3D</ext-link>). The software implementation of the method presented here was developed in C++ and built as a plugin in the Vaa3D framework (Peng et al., <xref rid="B20" ref-type="bibr">2014</xref>) with Qt-4.7.2 installed. SNAP implementation was tested using both CentOS and Windows operating systems. It is available for download at <ext-link xlink:href="https://github.com/Vaa3D/vaa3d_tools/tree/master/hackathon/XuanZhao/SNAP" ext-link-type="uri">https://github.com/Vaa3D/vaa3d_tools/tree/master/hackathon/XuanZhao/SNAP</ext-link>. Guidance for use of the plugin is included in the README.txt file.</p>
  </sec>
  <sec id="s5">
    <title>5. Conclusion and discussion</title>
    <p>In this paper, we present SNAP, a structure-based neuron morphology reconstruction automated pruning pipeline. It incorporates statistical analysis and structure modeling into rules for removing erroneous extra segments, thereby improving neuron reconstruction workflow throughput. Experimental results, especially for quantitative evaluation with high precision and recall, demonstrate the effectiveness of SNAP. SNAP also achieved neuron separation in entangled neuron problems.</p>
    <p>Note that the methods in SNAP depend on statistical priors and use empirical values as thresholds. Here, it is important to point out that the prior knowledge drawn from careful study of gold-standard manual annotation data is on the different types of errors and structural models, which are independent of the choice of the automated reconstruction algorithm. SNAP can be applied to the results of any automated reconstruction algorithm.</p>
    <p>As SNAP reduces the number of wrong segments, manual curation can be speeded up. The results obtained with SNAP could serve as an improved basis for further post-processing algorithms, e.g., repair algorithms to make up the missing branches. SNAP could also be applied to manual annotation as a QC tool to identify segments that are possibly wrong. Hence, it is a powerful tool facilitating high-through neuron morphology reconstruction.</p>
  </sec>
  <sec sec-type="data-availability" id="s6">
    <title>Data availability statement</title>
    <p>The original contributions presented in the study are included in the article/<xref rid="SM1" ref-type="supplementary-material">Supplementary material</xref>, further inquiries can be directed to the corresponding authors.</p>
  </sec>
  <sec sec-type="author-contributions" id="s7">
    <title>Author contributions</title>
    <p>LD conceptualized the project, developed the algorithm with help from the team, and wrote the manuscript. XZ assisted with algorithm development and implemented the software. YL contributed to algorithm development. LL led the annotation of the gold-standard data. SG and YW contributed to algorithm development and manuscript writing. HP supervised the project. All authors contributed to the article and approved the submitted version.</p>
  </sec>
</body>
<back>
  <ack>
    <p>Thanks to SEU-Allen Joint Center annotation team for the gold standard annotation of whole-brain neuron morphology. Thanks to the Allen Institute for Brain Science for sparsely labeled mouse brains and Huazhong University of Science and Technology for fMOST images. Thanks for their practice in open science principles by sharing the data online. We thank Zhi Zhou, Yuanyuan Song, Lulu Yin, Shichen Zhang, Jintao Pan, Yanting Liu, Guodong Hong, Jia Yuan, Yanjun Duan, Yaping Wang, Qiang Ouyang, Zijun Zhao, Wan Wan, Peng Wang, Ping He, Lingsheng Kong, Feng Xiong, and other members in SEU-Allen Joint Center annotation team for their effort in gold-standard morphology data production. We thank Xin Chen, Bingjie Shao, Mengyu Wang, Haoyu Zeng, Gaoyu Wang, Yiwei Li, Nan Mo, Xiaoqin Gu, Xiaoxuan Jiang, and Hairuo Zhang in the annotation team for taking the annotation survey. We thank Zhangcan Ding, Ping He, Mengya Chen, and Peng Wang for their statistical analysis of error types. We thank Jie Xue for her help in performance evaluation.</p>
  </ack>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s9">
    <title>Publisher's note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
  <sec sec-type="supplementary-material" id="s10">
    <title>Supplementary material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link xlink:href="https://www.frontiersin.org/articles/10.3389/fninf.2023.1174049/full#supplementary-material" ext-link-type="uri">https://www.frontiersin.org/articles/10.3389/fninf.2023.1174049/full#supplementary-material</ext-link></p>
    <supplementary-material id="SM1" position="float" content-type="local-data">
      <media xlink:href="Data_Sheet_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ascoli</surname><given-names>G. A.</given-names></name><name><surname>Alonso-Nanclares</surname><given-names>L.</given-names></name><name><surname>Anderson</surname><given-names>S. A.</given-names></name><name><surname>Barrionuevo</surname><given-names>G.</given-names></name><name><surname>Benavides-Piccione</surname><given-names>R.</given-names></name><name><surname>Burkhalter</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>2008</year>). <article-title>Petilla terminology: nomenclature of features of gabaergic interneurons of the cerebral cortex</article-title>. <source>Nat. Rev. Neurosci.</source><volume>9</volume>, <fpage>557</fpage>–<lpage>568</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2402</pub-id><?supplied-pmid 18568015?><pub-id pub-id-type="pmid">18568015</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cannon</surname><given-names>R.</given-names></name><name><surname>Turner</surname><given-names>D.</given-names></name><name><surname>Pyapali</surname><given-names>G.</given-names></name><name><surname>Wheal</surname><given-names>H.</given-names></name></person-group> (<year>1998</year>). <article-title>An on-line archive of reconstructed hippocampal neurons</article-title>. <source>J. Neurosci. Methods</source>
<volume>84</volume>, <fpage>49</fpage>–<lpage>54</lpage>. <pub-id pub-id-type="doi">10.1016/s0165-0270(98)00091-0</pub-id><?supplied-pmid 9821633?><pub-id pub-id-type="pmid">9821633</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>H.</given-names></name><name><surname>Xiao</surname><given-names>H.</given-names></name><name><surname>Liu</surname><given-names>T.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name></person-group> (<year>2015</year>). <article-title>Smarttracing: self-learning-based neuron reconstruction</article-title>. <source>Brain Informatics</source>
<volume>2</volume>, <fpage>135</fpage>–<lpage>144</lpage>. <pub-id pub-id-type="doi">10.1007/s40708-015-0018-y</pub-id><?supplied-pmid 27747506?><pub-id pub-id-type="pmid">27747506</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cuntz</surname><given-names>H.</given-names></name><name><surname>Forstner</surname><given-names>F.</given-names></name><name><surname>Borst</surname><given-names>A.</given-names></name><name><surname>Häusser</surname><given-names>M.</given-names></name></person-group> (<year>2010</year>). <article-title>One rule to grow them all: a general theory of neuronal branching and its practical application</article-title>. <source>PLoS Comput. Biol.</source>
<volume>6</volume>, <fpage>1000877</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1000877</pub-id><?supplied-pmid 20700495?><pub-id pub-id-type="pmid">20700495</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>L.</given-names></name><name><surname>Liu</surname><given-names>S.</given-names></name><name><surname>Gou</surname><given-names>L.</given-names></name><name><surname>Hu</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Deng</surname><given-names>L.</given-names></name><etal/></person-group>. (<year>2022</year>). <article-title>Single-neuron projectome of mouse prefrontal cortex</article-title>. <source>Nat. Neurosci.</source><volume>25</volume>, <fpage>515</fpage>–<lpage>529</lpage>.<pub-id pub-id-type="pmid">35361973</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gong</surname><given-names>H.</given-names></name><name><surname>Xu</surname><given-names>D.</given-names></name><name><surname>Yuan</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Guo</surname><given-names>C.</given-names></name><name><surname>Peng</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>High-throughput dual-colour precision imaging for brain-wide connectome with cytoarchitectonic landmarks at the cellular level</article-title>. <source>Nat. Commun.</source><volume>7</volume>, <fpage>12142</fpage><pub-id pub-id-type="doi">10.1038/ncomms12142</pub-id><?supplied-pmid 27374071?><pub-id pub-id-type="pmid">27374071</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>C.</given-names></name><name><surname>Liu</surname><given-names>M.</given-names></name><name><surname>Guan</surname><given-names>T.</given-names></name><name><surname>Chen</surname><given-names>W.</given-names></name><name><surname>Wen</surname><given-names>H.</given-names></name><name><surname>Zeng</surname><given-names>T.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>Crossover structure separation with application to neuron tracing in volumetric images</article-title>. <source>IEEE Trans. Instrument. Measure.</source><volume>70</volume>, <fpage>1</fpage>–<lpage>13</lpage>. <pub-id pub-id-type="doi">10.1109/TIM.2021.3072119</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>S.</given-names></name><name><surname>Zhao</surname><given-names>X.</given-names></name><name><surname>Jiang</surname><given-names>S.</given-names></name><name><surname>Ding</surname><given-names>L.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name></person-group> (<year>2022</year>). <article-title>Image enhancement to leverage the 3d morphological reconstruction of single-cell neurons</article-title>. <source>Bioinformatics</source>
<volume>38</volume>, <fpage>503</fpage>–<lpage>512</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btab638</pub-id><?supplied-pmid 34515755?><pub-id pub-id-type="pmid">34515755</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Q.</given-names></name><name><surname>Cao</surname><given-names>T.</given-names></name><name><surname>Zeng</surname><given-names>S.</given-names></name><name><surname>Li</surname><given-names>A.</given-names></name><name><surname>Quan</surname><given-names>T.</given-names></name></person-group> (<year>2022</year>). <article-title>Minimizing probability graph connectivity cost for discontinuous filamentary structures tracing in neuron image</article-title>. <source>IEEE J. Biomed. Health Inform.</source>
<volume>26</volume>, <fpage>3092</fpage>–<lpage>3103</lpage>. <pub-id pub-id-type="doi">10.1109/JBHI.2022.3147512</pub-id><?supplied-pmid 35104232?><pub-id pub-id-type="pmid">35104232</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Q.</given-names></name><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>S.</given-names></name><name><surname>Xu</surname><given-names>C.</given-names></name><name><surname>Cao</surname><given-names>T.</given-names></name><name><surname>Xu</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Weakly supervised learning of 3d deep network for neuron reconstruction</article-title>. <source>Front. Neuroanat.</source><volume>14</volume>, <fpage>38</fpage>. <pub-id pub-id-type="doi">10.3389/fnana.2020.00038</pub-id><?supplied-pmid 32848636?><pub-id pub-id-type="pmid">32848636</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>S.</given-names></name><name><surname>Pan</surname><given-names>Z.</given-names></name><name><surname>Feng</surname><given-names>Z.</given-names></name><name><surname>Guan</surname><given-names>Y.</given-names></name><name><surname>Ren</surname><given-names>M.</given-names></name><name><surname>Ding</surname><given-names>Z.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Skeleton optimization of neuronal morphology based on three-dimensional shape restrictions</article-title>. <source>BMC Bioinformatics</source><volume>21</volume>, <fpage>395</fpage>. <pub-id pub-id-type="doi">10.1186/s12859-020-03714-z</pub-id><?supplied-pmid 32887543?><pub-id pub-id-type="pmid">32887543</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>R.</given-names></name><name><surname>Zhu</surname><given-names>M.</given-names></name><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Bienkowski</surname><given-names>M. S.</given-names></name><name><surname>Foster</surname><given-names>N. N.</given-names></name><name><surname>Xu</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Precise segmentation of densely interweaving neuron clusters using g-cut</article-title>. <source>Nat. Commun.</source><volume>10</volume>:<fpage>1549</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-09515-0</pub-id><?supplied-pmid 30948706?><pub-id pub-id-type="pmid">30948706</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>S.</given-names></name><name><surname>Quan</surname><given-names>T.</given-names></name><name><surname>Zhou</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>A.</given-names></name><name><surname>Fu</surname><given-names>L.</given-names></name><name><surname>Gong</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Research progress of neuron morphological reconstruction tools</article-title>. <source>Prog. Biochem. Biophys.</source><volume>46</volume>, <fpage>266</fpage>–<lpage>275</lpage>.</mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>H.</given-names></name><name><surname>Acton</surname><given-names>S. T.</given-names></name><name><surname>Weller</surname><given-names>D. S.</given-names></name></person-group> (<year>2017</year>). <article-title>“Content-aware neuron image enhancement,”</article-title> in <source>IEEE International Conference on Image Processing (ICIP)</source> (<publisher-loc>Beijing</publisher-loc>).<?supplied-pmid 30716037?></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y.</given-names></name></person-group> (<year>2011</year>). <article-title>The diadem and beyond</article-title>. <source>Neuroinformatics</source>
<volume>9</volume>, <fpage>99</fpage>–<lpage>102</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-011-9102-5</pub-id><?supplied-pmid 21431331?><pub-id pub-id-type="pmid">21431331</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Zhong</surname><given-names>Y.</given-names></name><name><surname>Zhao</surname><given-names>X.</given-names></name><name><surname>Liu</surname><given-names>L.</given-names></name><name><surname>Ding</surname><given-names>L.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name></person-group> (<year>2022</year>). <article-title>Tracing weak neuron fibers</article-title>. <source>Bioinformatics</source> 39, btac816. <pub-id pub-id-type="doi">10.1093/bioinformatics/btac816</pub-id><?supplied-pmid 36571479?><pub-id pub-id-type="pmid">36571479</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manubens-Gil</surname><given-names>L.</given-names></name><name><surname>Zhou</surname><given-names>Z.</given-names></name><name><surname>Chen</surname><given-names>H.</given-names></name><name><surname>Ramanathan</surname><given-names>A.</given-names></name><name><surname>Liu</surname><given-names>X.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2023</year>). <article-title>Bigneuron: a resource to benchmark and predict performance of algorithms for automated tracing of neurons in light microscopy datasets</article-title>. <source>Nat. Methods</source>. <pub-id pub-id-type="doi">10.1038/s41592-023-01848-5.</pub-id> [Epub ahead of print].<?supplied-pmid 37069271?><pub-id pub-id-type="pmid">37069271</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meijering</surname><given-names>E.</given-names></name></person-group> (<year>2010</year>). <article-title>Neuron tracing in perspective</article-title>. <source>Cytomet. A</source>
<volume>77</volume>, <fpage>693</fpage>–<lpage>704</lpage>. <pub-id pub-id-type="doi">10.1002/cyto.a.20895</pub-id><?supplied-pmid 20583273?><pub-id pub-id-type="pmid">20583273</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Halloran</surname><given-names>D. M.</given-names></name></person-group> (<year>2020</year>). <article-title>Module for swc neuron morphology file validation and correction enabled for high throughput batch processing</article-title>. <source>PLoS ONE</source>
<volume>15</volume>, <fpage>e0228091</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0228091</pub-id><?supplied-pmid 31971963?><pub-id pub-id-type="pmid">31971963</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>H.</given-names></name><name><surname>Bria</surname><given-names>A.</given-names></name><name><surname>Zhou</surname><given-names>Z.</given-names></name><name><surname>Iannello</surname><given-names>G.</given-names></name><name><surname>Long</surname><given-names>F.</given-names></name></person-group> (<year>2014</year>). <article-title>Extensible visualization and analysis for multidimensional images using vaa3d</article-title>. <source>Nat. Protoc.</source>
<volume>9</volume>, <fpage>193</fpage>–<lpage>208</lpage>. <pub-id pub-id-type="doi">10.1038/nprot.2014.011</pub-id><?supplied-pmid 24385149?><pub-id pub-id-type="pmid">24385149</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>H.</given-names></name><name><surname>Hawrylycz</surname><given-names>M.</given-names></name><name><surname>Roskams</surname><given-names>J.</given-names></name><name><surname>Hill</surname><given-names>S.</given-names></name><name><surname>Spruston</surname><given-names>N.</given-names></name><name><surname>Meijering</surname><given-names>E.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>Bigneuron: large-scale 3d neuron reconstruction from optical microscopy images</article-title>. <source>Neuron</source><volume>87</volume>, <fpage>252</fpage>–<lpage>256</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2015.06.036</pub-id><?supplied-pmid 26182412?><pub-id pub-id-type="pmid">26182412</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>H.</given-names></name><name><surname>Long</surname><given-names>F.</given-names></name><name><surname>Myers</surname><given-names>G. J. B.</given-names></name></person-group> (<year>2011</year>). <article-title>Automatic 3D neuron tracing using all-path pruning</article-title>. <source>Bioinformatics</source>
<volume>27</volume>, <fpage>i239</fpage>–<lpage>i247</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btr237</pub-id><?supplied-pmid 21685076?><pub-id pub-id-type="pmid">21685076</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>H.</given-names></name><name><surname>Xie</surname><given-names>P.</given-names></name><name><surname>Liu</surname><given-names>L.</given-names></name><name><surname>Kuang</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Qu</surname><given-names>L.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>Morphological diversity of single neurons in molecularly defined cell types</article-title>. <source>Nature</source><volume>598</volume>, <fpage>174</fpage>–<lpage>181</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-021-03941-1</pub-id><?supplied-pmid 34616072?><pub-id pub-id-type="pmid">34616072</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>H.</given-names></name><name><surname>Zhou</surname><given-names>Z.</given-names></name><name><surname>Meijering</surname><given-names>E.</given-names></name><name><surname>Zhao</surname><given-names>T.</given-names></name><name><surname>Ascoli</surname><given-names>G.</given-names></name><name><surname>Hawrylycz</surname><given-names>M.</given-names></name></person-group> (<year>2017</year>). <article-title>Automatic tracing of ultra-volumes of neuronal imagesnature methods</article-title>. <source>Nat. Methods</source>
<volume>14</volume>, <fpage>332</fpage>–<lpage>333</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.4233</pub-id><?supplied-pmid 28362437?><pub-id pub-id-type="pmid">28362437</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quan</surname><given-names>T.</given-names></name><name><surname>Zhou</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>S.</given-names></name><name><surname>Li</surname><given-names>A.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>Neurogps-tree: automatic reconstruction of large-scale neuronal populations with dense neurites</article-title>. <source>Nat. Methods</source><volume>13</volume>, <fpage>51</fpage>–<lpage>54</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.3662</pub-id><?supplied-pmid 26595210?><pub-id pub-id-type="pmid">26595210</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Santamaría-Pang</surname><given-names>A.</given-names></name><name><surname>Hernandez-Herrera</surname><given-names>P.</given-names></name><name><surname>Papadakis</surname><given-names>M.</given-names></name><name><surname>Saggau</surname><given-names>P.</given-names></name><name><surname>Kakadiaris</surname><given-names>I. A.</given-names></name></person-group> (<year>2015</year>). <article-title>Automatic morphological reconstruction of neurons from multiphoton and confocal microscopy images using 3D tubular models</article-title>. <source>Neuroinformatics</source>
<volume>13</volume>, <fpage>297</fpage>–<lpage>320</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-014-9253-2</pub-id><?supplied-pmid 25631538?><pub-id pub-id-type="pmid">25631538</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C.</given-names></name><name><surname>Lee</surname><given-names>Y.</given-names></name><name><surname>Pradana</surname><given-names>H.</given-names></name><name><surname>Zhou</surname><given-names>Z.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name></person-group> (<year>2017</year>). <article-title>Ensemble neuron tracer for 3d neuron reconstruction</article-title>. <source>Neuroinformatics</source>
<volume>15</volume>, <fpage>185</fpage>–<lpage>198</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-017-9325-1</pub-id><?supplied-pmid 28185058?><pub-id pub-id-type="pmid">28185058</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>Q.</given-names></name><name><surname>Liu</surname><given-names>L.</given-names></name></person-group> (<year>2019</year>). <article-title>TeraVR empowers precise reconstruction of complete 3-d neuronal morphology in the whole brain</article-title>. <source>Nat. Commun.</source>
<volume>10</volume>, <fpage>3474</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-11443-y</pub-id><?supplied-pmid 31375678?><pub-id pub-id-type="pmid">31375678</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>J.</given-names></name><name><surname>He</surname><given-names>Y.</given-names></name><name><surname>Yang</surname><given-names>Z.</given-names></name><name><surname>Guo</surname><given-names>C.</given-names></name><name><surname>Luo</surname><given-names>Q.</given-names></name><name><surname>Zhou</surname><given-names>W.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>3D brainCV: simultaneous visualization and analysis of cells and capillaries in a whole mouse brain with one-micron voxel resolution</article-title>. <source>NeuroImage</source><volume>87</volume>, <fpage>199</fpage>–<lpage>208</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.036</pub-id><?supplied-pmid 24185025?><pub-id pub-id-type="pmid">24185025</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>H.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name></person-group> (<year>2013</year>). <article-title>App2: automatic tracing of 3D neuron morphology based on hierarchical pruning of gray-weighted image distance-trees</article-title>. <source>Bioinformatics</source>
<volume>29</volume>, <fpage>1448</fpage>–<lpage>1454</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btt170</pub-id><?supplied-pmid 23603332?><pub-id pub-id-type="pmid">23603332</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>J.</given-names></name><name><surname>Zhao</surname><given-names>T.</given-names></name><name><surname>Lee</surname><given-names>T.</given-names></name><name><surname>Myers</surname><given-names>E.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name></person-group> (<year>2011</year>). <article-title>Anisotropic path searching for automatic neuron reconstruction</article-title>. <source>Med. Image Anal.</source>
<volume>15</volume>, <fpage>680</fpage>–<lpage>689</lpage>. <pub-id pub-id-type="doi">10.1016/j.media.2011.05.013</pub-id><?supplied-pmid 21669547?><pub-id pub-id-type="pmid">21669547</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>F.</given-names></name><name><surname>Liu</surname><given-names>M.</given-names></name><name><surname>Chen</surname><given-names>W.</given-names></name><name><surname>Zeng</surname><given-names>T.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name></person-group> (<year>2021</year>). <article-title>Automatic repair of 3d neuron reconstruction based on topological feature points and a most-based repairer</article-title>. <source>IEEE Trans. Instrument. Measure.</source>
<volume>70</volume>, <fpage>1</fpage>–<lpage>13</lpage>. <pub-id pub-id-type="doi">10.1109/TIM.2020.3033057</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>H.</given-names></name><name><surname>Sanes</surname><given-names>J. R.</given-names></name></person-group> (<year>2017</year>). <article-title>Neuronal cell-type classification: challenges, opportunities and the path forward</article-title>. <source>Nat. Rev. Neurosci.</source>
<volume>18</volume>, <fpage>530</fpage>–<lpage>546</lpage>. <pub-id pub-id-type="doi">10.1038/nrn.2017.85</pub-id><?supplied-pmid 28775344?><pub-id pub-id-type="pmid">28775344</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>T.</given-names></name><name><surname>Xie</surname><given-names>J.</given-names></name><name><surname>Amat</surname><given-names>F.</given-names></name><name><surname>Clack</surname><given-names>N.</given-names></name><name><surname>Ahammad</surname><given-names>P.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2011</year>). <article-title>Automated reconstruction of neuronal morphology based on local geometrical and global structural models</article-title>. <source>Neuroinformatics</source><volume>9</volume>, <fpage>247</fpage>–<lpage>261</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-011-9120-3</pub-id><?supplied-pmid 21547564?><pub-id pub-id-type="pmid">21547564</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>S.</given-names></name><name><surname>Li</surname><given-names>A.</given-names></name><name><surname>Huang</surname><given-names>Q.</given-names></name><name><surname>Xiong</surname><given-names>F.</given-names></name><name><surname>Li</surname><given-names>N.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>GTree: an open-source tool for dense reconstruction of brain-wide neuronal population</article-title>. <source>Neuroinformatics</source><volume>19</volume>, <fpage>305</fpage>–<lpage>317</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-020-09484-6</pub-id><?supplied-pmid 32844332?><pub-id pub-id-type="pmid">32844332</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Z.</given-names></name><name><surname>Sorensen</surname><given-names>S.</given-names></name><name><surname>Zeng</surname><given-names>H.</given-names></name><name><surname>Hawrylycz</surname><given-names>M.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name></person-group> (<year>2015</year>). <article-title>Adaptive image enhancement for tracing 3d morphologies of neurons and brain vasculatures</article-title>. <source>Neuroinformatics</source>
<volume>13</volume>, <fpage>153</fpage>–<lpage>166</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-014-9249-y</pub-id><?supplied-pmid 25310965?><pub-id pub-id-type="pmid">25310965</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName archivearticle.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neuroinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neuroinform</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neuroinform.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1662-5196</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10303825</article-id>
    <article-id pub-id-type="doi">10.3389/fninf.2023.1174049</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Methods</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SNAP: a structure-based neuron morphology reconstruction automatic pruning pipeline</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Ding</surname>
          <given-names>Liya</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1432119/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhao</surname>
          <given-names>Xuan</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Guo</surname>
          <given-names>Shuxia</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/2326380/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Yufeng</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/2274454/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Lijuan</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Yimin</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/649792/overview"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Peng</surname>
          <given-names>Hanchuan</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="c002" ref-type="corresp">
          <sup>*</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Institute for Brain and Intelligence, Southeast University</institution>, <addr-line>Nanjing</addr-line>, <country>China</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Guangdong Institute of Intelligence Science and Technology</institution>, <addr-line>Zhuhai</addr-line>, <country>China</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Anan Li, Huazhong University of Science and Technology, China</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Qing Huang, Wuhan Institute of Technology, China; Ye Li, University of Michigan, United States</p>
      </fn>
      <corresp id="c001">*Correspondence: Liya Ding <email>dinglyosu@seu.edu.cn</email></corresp>
      <corresp id="c002">Hanchuan Peng <email>h@braintell.org</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>14</day>
      <month>6</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>17</volume>
    <elocation-id>1174049</elocation-id>
    <history>
      <date date-type="received">
        <day>25</day>
        <month>2</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>22</day>
        <month>5</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2023 Ding, Zhao, Guo, Liu, Liu, Wang and Peng.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Ding, Zhao, Guo, Liu, Liu, Wang and Peng</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <sec>
        <title>Background</title>
        <p>Neuron morphology analysis is an essential component of neuron cell-type definition. Morphology reconstruction represents a bottleneck in high-throughput morphology analysis workflow, and erroneous extra reconstruction owing to noise and entanglements in dense neuron regions restricts the usability of automated reconstruction results. We propose SNAP, a structure-based neuron morphology reconstruction pruning pipeline, to improve the usability of results by reducing erroneous extra reconstruction and splitting entangled neurons.</p>
      </sec>
      <sec>
        <title>Methods</title>
        <p>For the four different types of erroneous extra segments in reconstruction (caused by noise in the background, entanglement with dendrites of close-by neurons, entanglement with axons of other neurons, and entanglement within the same neuron), SNAP incorporates specific statistical structure information into rules for erroneous extra segment detection and achieves pruning and multiple dendrite splitting.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>Experimental results show that this pipeline accomplishes pruning with satisfactory precision and recall. It also demonstrates good multiple neuron-splitting performance. As an effective tool for post-processing reconstruction, SNAP can facilitate neuron morphology analysis.</p>
      </sec>
    </abstract>
    <kwd-group>
      <kwd>neuron morphology reconstruction</kwd>
      <kwd>bioinformatics</kwd>
      <kwd>image processing</kwd>
      <kwd>post-processing</kwd>
      <kwd>dendrite tracing</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">
          <institution-wrap>
            <institution>National Natural Science Foundation of China-Guangdong Joint Fund</institution>
            <institution-id institution-id-type="doi">10.13039/501100014857</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group>
        <funding-source id="cn002">
          <institution-wrap>
            <institution>Special Project for Research and Development in Key areas of Guangdong Province</institution>
            <institution-id institution-id-type="doi">10.13039/501100015956</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group>
        <funding-source id="cn003">
          <institution-wrap>
            <institution>Fundamental Research Funds for the Central Universities</institution>
            <institution-id institution-id-type="doi">10.13039/501100012226</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group>
        <funding-source id="cn004">
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="doi">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <funding-statement>This work was funded by NSFC-Guangdong Joint Fund-U20A6005, Key-Area Research and Development Program of Guangdong Province (2018B030331001), Fundamental Research Funds for the Central Universities (2242022R10089) to LD, NNSFC Grant 32071367, and NSF Shanghai Grant 20ZR1420100 to YW.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="9"/>
      <table-count count="1"/>
      <equation-count count="4"/>
      <ref-count count="36"/>
      <page-count count="13"/>
      <word-count count="7826"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>Characterization of neuron cell type is an international research frontier in neuron science (Zeng and Sanes, <xref rid="B33" ref-type="bibr">2017</xref>). Neuron morphology is considered to be a critical component of neuron cell type identification (Ascoli et al., <xref rid="B1" ref-type="bibr">2008</xref>). In recent years, there has been considerable development of techniques, including sparse, robust, and consistent fluorescent labeling of a wide range of neuronal types (Peng et al., <xref rid="B23" ref-type="bibr">2021</xref>) and fluorescence micro-optical sectioning tomography (fMOST; Gong et al., <xref rid="B6" ref-type="bibr">2016</xref>). With these techniques, reconstruction of single-neuron morphology from optical microscopy images has become possible and now has an essential role in neuron science. Researchers have developed various manual, semi-automated, and automated neuron reconstruction tools for digital reconstruction of neuron morphology (Meijering, <xref rid="B18" ref-type="bibr">2010</xref>). Research institutions have also held competitions and established worldwide projects, such as the DIADEM competition (Liu, <xref rid="B15" ref-type="bibr">2011</xref>) and BigNeuron (Peng et al., <xref rid="B21" ref-type="bibr">2015</xref>; Manubens-Gil et al., <xref rid="B17" ref-type="bibr">2023</xref>). A large number of automated neuron reconstruction algorithms exist. For example, the 3D Visualization-Assisted Analysis software suite Vaa3D (Peng et al., <xref rid="B20" ref-type="bibr">2014</xref>) has more than 32 plugins, including ENT (Wang et al., <xref rid="B27" ref-type="bibr">2017</xref>), APP (Peng et al., <xref rid="B22" ref-type="bibr">2011</xref>), APP2 (Xiao and Peng, <xref rid="B30" ref-type="bibr">2013</xref>), NeuTube (Zhao et al., <xref rid="B34" ref-type="bibr">2011</xref>), MOST (Wu et al., <xref rid="B29" ref-type="bibr">2014</xref>), and ST (Chen et al., <xref rid="B3" ref-type="bibr">2015</xref>).</p>
    <p>Nevertheless, neuron morphology reconstruction remains an unsolved problem (Li S. et al., <xref rid="B13" ref-type="bibr">2019</xref>). The wide variety of brain images in terms of background noise, complicated branching patterns, and clutter of neuron fibers presents challenges for automated neuron reconstruction. Existing automated reconstruction algorithms are generally effective only for a few specific data sets. Owing to the complexity of the images and the limitations of automated reconstruction algorithms, these algorithms are unsuitable for whole-brain images. Moreover, for data sets with a low signal-to-noise ratio and dense neuron distribution with neuron fiber entanglement, the existing reconstruction algorithms do not show satisfactory performance. Pre-processing algorithms, including multi-scale enhancement (Zhou et al., <xref rid="B36" ref-type="bibr">2015</xref>), CaNE (Liang et al., <xref rid="B14" ref-type="bibr">2017</xref>), and filtering-based enhancement (Guo et al., <xref rid="B8" ref-type="bibr">2022</xref>) aim to enhance images by reducing background noise and improving image contrast. Deep learning–based approaches have been investigated for neuron tracing. Among them, weakly supervised learning (Huang et al., <xref rid="B10" ref-type="bibr">2020</xref>) and false negative mining (Liu et al., <xref rid="B16" ref-type="bibr">2022</xref>) are proposed to rescue and connect the weak and broken neurites in the segmentation step for reconstruction; subgraph connection (SGC) method (Huang et al., <xref rid="B9" ref-type="bibr">2022</xref>) starts from prediction map obtained by CNN to link the broken reconstruction; crossover structure separation (CSS) method (Guo et al., <xref rid="B7" ref-type="bibr">2021</xref>) is proposed to detect the crossover structures and generate deformed separated neuronal fibers in the images to eliminate entanglements in reconstruction. However, even with these pre-processing and advanced deep learning–based approaches, the results of automated reconstruction still contain complex errors and cannot be used directly in analysis. To obtain gold-standard morphology reconstruction, researchers need to curate reconstruction results with manual reconstruction platforms such as Vaa3D (Peng et al., <xref rid="B20" ref-type="bibr">2014</xref>), TeraVR (Wang et al., <xref rid="B28" ref-type="bibr">2019</xref>), or FNT (Gao et al., <xref rid="B5" ref-type="bibr">2022</xref>); however, such manual annotation is labor-intensive and time-consuming, limiting the throughput of the morphology reconstruction workflow.</p>
    <p>In morphology reconstruction systems, therefore, the manual annotation time should be reduced to achieve high throughput, which means the errors resulting from automated reconstruction must be reduced. We closely studied the errors in reconstruction results from several automated algorithms, including ENT (Wang et al., <xref rid="B27" ref-type="bibr">2017</xref>), APP2 (Xiao and Peng, <xref rid="B30" ref-type="bibr">2013</xref>), and ST (Chen et al., <xref rid="B3" ref-type="bibr">2015</xref>). Based on observations of a vast number of samples (see <xref rid="SM1" ref-type="supplementary-material">Supplementary material</xref>, Section 1), we identified several types of error: missed reconstruction and erroneous extra reconstruction due to entanglement, noise, or other artifacts. Note that by the term “entanglement” in this paper, we mean neuron fibers very close to each other in optical microscopy images that are difficult to distinguish, resulting in “crossing” structures in reconstruction. These intertwined reconstructions within the same neuron or from different neurons constitute significant challenges for automated reconstruction. <xref rid="F1" ref-type="fig">Figures 1A</xref>–<xref rid="F1" ref-type="fig">D</xref> show examples from various situations of automated neuron reconstruction results with errors. With manual annotation for error type on the error sample set (see <xref rid="SM1" ref-type="supplementary-material">Supplementary material</xref>, Section 1), we found the majority (around 63.53%) were erroneous extra reconstructions (false positive), whereas a reasonable number (around 24.16%) were due to missed reconstruction (false negative), and the rest (around 12.31%) were combined errors. On the other hand, we carried out a survey for the annotation personnel on their opinion on which of the two tasks, annotating automated reconstruction results with some extra segments or reconstruction with some missing segments, would be more time-consuming or tiring. Ninety percentage of the group believed the process of eliminating extra reconstruction segments is more time-consuming or laborious than adding missing segments. Reducing erroneous extra reconstruction segments could expedite the process of manual annotation. Therefore, it is a promising approach to prune automated reconstruction results.</p>
    <fig position="float" id="F1">
      <label>Figure 1</label>
      <caption>
        <p>Examples of automated reconstruction results and SNAP pipeline processes. <bold>(A–D)</bold> Display MIP images of neurons overlaid with automated reconstruction results in red. The green arrows point to places of erroneous reconstruction. Four situations of erroneous extra reconstruction segments are shown: <bold>(A)</bold> noisy segments in the APP2 results; <bold>(B)</bold> dense neuron fibers with entanglement with other neurons in the APP2 results; <bold>(C)</bold> entanglement with passing neuron fibers in the ST results; <bold>(D)</bold> entanglement with the neuron itself in the ENT results. <bold>(E)</bold> The workflow of the SNAP pipeline illustrated with an example. In each step, the corresponding type of wrong segments are pruned away. Step 1 prunes noisy segments; step 2 prunes entanglements with other dendrites; step 3 prunes crossings involving passing fibers of other neurons or fibers of the neuron of interest itself. Note that blue segments are the result of each step, while red ones are the pruned–way ones in each step.</p>
      </caption>
      <graphic xlink:href="fninf-17-1174049-g0001" position="float"/>
    </fig>
    <p>In the literature, there are several papers describing post-processing of automated reconstruction results using various methods, e.g., G-Cut (Li R. et al., <xref rid="B12" ref-type="bibr">2019</xref>), ray-shooting based repairer (Yu et al., <xref rid="B32" ref-type="bibr">2021</xref>), and solemnization algorithm (Jiang et al., <xref rid="B11" ref-type="bibr">2020</xref>). However, only some of these studies focused on the pruning of results. In the challenging scenario of group neuron reconstruction in densely labeled regions with entanglement of dendrites from multiple neuron cells, the main errors are erroneous extra reconstructions due to crossings, as mentioned above. Solutions to this problem include G-Cut (Li R. et al., <xref rid="B12" ref-type="bibr">2019</xref>), NeuroGPS-Tree (Quan et al., <xref rid="B25" ref-type="bibr">2016</xref>; Zhou et al., <xref rid="B35" ref-type="bibr">2021</xref>), and TREES Toolbox (Cuntz et al., <xref rid="B4" ref-type="bibr">2010</xref>), which separate densely intertwined neurons. G-Cut determines which neuron a node belongs to by judging the angle between the local segment and the line connecting the soma and the node. NeuroGPS-Tree identifies spurious links (“bridges”) between the reconstructions of two neurons in an iterative manner and separates the neurons by removing certain ends of bridges. TREES Toolbox employs competitive branch order in neuron splitting. However, most of these software tools do not handle other errors, such as entanglement errors within the same neuron and errors involving other axons passing by, which are essential tasks in pruning.</p>
    <p>This paper proposes SNAP, a structure-based neuron reconstruction automated pruning pipeline. It aims to prune away errors in the reconstruction results while keeping correct reconstructions, thereby speeding up further curation. It also separates the entangled reconstructions of multiple neurons as this is part of the pruning problem. We focus particularly on dendrite reconstruction as this is the basic component of neuron reconstruction. The dendrite corresponds to the near-soma region, which serves as the first image block of UltraTracer (Peng et al., <xref rid="B24" ref-type="bibr">2017</xref>) for complete neuron morphology reconstruction. When post-processing in this first block reduces errors, fewer wrong reconstructions will be made when UltraTracer adaptively explores and traces neighboring subareas, which will improve the overall reconstruction performance. When developing SNAP, we thoroughly studied dendrite structure and identified models for the four main categories of errors we needed to prune. SNAP has three main steps, and the pipeline is illustrated in <xref rid="F1" ref-type="fig">Figure 1E</xref>. The algorithms are described in Section 2. The performance of our proposed SNAP pipeline is validated (in Section 3) by applying it to automated reconstruction results and comparing the pruned results with those of gold-standard manual annotation. We demonstrate that a great proportion of erroneous extra reconstruction segments are removed, and thus the reconstruction quality is improved substantially.</p>
  </sec>
  <sec sec-type="methods" id="s2">
    <title>2. Methods</title>
    <p>The digital neuron morphology reconstruction results can be organized into a tree-like set of nodes with parent–child relationships (O'Halloran, <xref rid="B19" ref-type="bibr">2020</xref>) and are usually stored in standardized SWC files (Cannon et al., <xref rid="B2" ref-type="bibr">1998</xref>). In SNAP, the reconstructions are first converted into a segment-based tree data structure, as shown in <xref rid="F2" ref-type="fig">Figure 2</xref>. We denote the segment set as {<italic>S</italic><sub><italic>i</italic></sub>}, <italic>i</italic> = 1, 2, ..., <italic>N</italic>, where <italic>N</italic> is the total number of segments. The parent and child relationships of nodes in SWC format are converted into the parent and child relationships of the segments. The nodes, including the soma point, bifurcation points, and endpoints, have a facilitating role, and we denote the corresponding node set as {<italic>B</italic><sub><italic>j</italic></sub>}, <italic>j</italic> = 1, 2, ...<italic>M</italic>, where <italic>M</italic> is the total number of nodes. If a segment is the furthest segment from the soma, without any child segments, we call it a leaf segment. The level of the segment, <inline-formula><mml:math id="M1" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, is calculated as the number of segments <italic>S</italic><sub><italic>i</italic></sub> that must be passed through to reach a leaf segment. For example, the leaf segment's level is 0, its parent segment's level is 1, and so on. Note that the segments are oriented, in the direction of reconstruction outwards from the soma. SNAP aims to identify the erroneous extra reconstruction segments in the segment set of {<italic>S</italic><sub><italic>i</italic></sub>}.</p>
    <fig position="float" id="F2">
      <label>Figure 2</label>
      <caption>
        <p>SNAP reconstruction data structure: illustration of converted reconstruction format of directed segments and node-based tree structure.</p>
      </caption>
      <graphic xlink:href="fninf-17-1174049-g0002" position="float"/>
    </fig>
    <p>Note also that SNAP targets single-neuron reconstruction, so if there is more than one connected structure in the reconstruction results, the tree associated with the soma understudy will be kept and worked on, whereas the other parts (including some broken reconstruction fragments) will be discarded.</p>
    <p>Four major categories of erroneous extra reconstruction are identified in the statistical analysis mentioned above. <italic>C</italic><sub>1</sub> are segments caused by noise in the background. The other three categories are segments caused by entanglement with dendrites of other neurons (<italic>C</italic><sub>2</sub>), axons of other neurons (<italic>C</italic><sub>3</sub>), or the same neuron (<italic>C</italic><sub>4</sub>). The pipeline is designed to deal with all four categories. It starts with the relatively easier category <italic>C</italic><sub>1</sub> to simplify the situation and then moves on to the harder cases. Thus, the pipeline deals first with the <italic>C</italic><sub>1</sub> type in Step 1, then with <italic>C</italic><sub>2</sub> in Step 2, and finally with <italic>C</italic><sub>3</sub> and <italic>C</italic><sub>4</sub> in Step 3, as shown in <xref rid="F1" ref-type="fig">Figure 1E</xref>.</p>
    <sec>
      <title>2.1. Step 1: removal of noisy segments (<italic>C</italic><sub>1</sub>)</title>
      <p>Segments in <italic>C</italic><sub>1</sub> are usually caused by noise in the background, including noise due to microscopy imaging, signals from irrelevant particles, or the halo of a strong signal. In general, these noisy segments are leaf segments and are relatively short. A key observation is that the linearity of these segments is weak, whereas the linearity of true neuron fiber segments is strong. Using a set of gold-standard manual annotations, statistical analysis of the length of leaf segments <inline-formula><mml:math id="M2" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is performed, as shown in <xref rid="F3" ref-type="fig">Figure 3A</xref>. One percentile of the population is set as a reasonable threshold (<italic>T</italic><sup><italic>Len</italic></sup>) to identify such short segments. Furthermore, the linearity feature of each segment is calculated. The “anisotropy” values of each node in the segments, λ<sub>1</sub>, λ<sub>2</sub>, andλ<sub>3</sub> (λ<sub>1</sub> &gt; λ<sub>2</sub> &gt; λ<sub>3</sub>), are the eigenvalues of the node; hence, the linearity feature is calculated as <inline-formula><mml:math id="M3" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mtext>λ</mml:mtext></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mrow><mml:mtext>λ</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, where <italic>N</italic><sub><italic>i</italic></sub> is the number of the node in <italic>S</italic><sub><italic>i</italic></sub>. Based on the histogram of <inline-formula><mml:math id="M4" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> of the training data set, a valid segment usually has a <italic>S</italic><sup><italic>Lin</italic></sup> value greater than <italic>T</italic><sup><italic>Lin</italic></sup> (as in <xref rid="F3" ref-type="fig">Figure 3B</xref>), which is one percentile of the population. In applications, leaf segments are removed using rules based on <italic>T</italic><sup><italic>Len</italic></sup> and <italic>T</italic><sup><italic>Lin</italic></sup>. This process is repeated until no further leaf segments can be removed. <xref rid="F3" ref-type="fig">Figure 3C</xref> shows an example of Step 1.</p>
      <fig position="float" id="F3">
        <label>Figure 3</label>
        <caption>
          <p>SNAP step 1. <bold>(A)</bold> Histogram of leaf segment lengths from the training set. One percentile of the population is taken as <italic>T</italic><sup><italic>Len</italic></sup> = 6.6 μ<italic>m</italic>. <bold>(B)</bold> Histogram of leaf segment linearity from the training set. One percentile of the population is taken as <italic>T</italic><sup><italic>Lin</italic></sup> = 1.4. <bold>(C)</bold> Example result of step 1. From left to right: MIP of a neuron image overlaid with APP2 results shown in red, MIP overlaid with SNAP results shown in green, zoomed-in image of the small region overlaid with SNAP results shown in green, with pruned segments shown in red.</p>
        </caption>
        <graphic xlink:href="fninf-17-1174049-g0003" position="float"/>
      </fig>
      <p>Removing <italic>C</italic><sub>1</sub> is a simple procedure and does not involve much of the neuron structure. Part of the purpose of Step 1 is to avoid artifacts caused by these short and noisy segments from persisting into Steps 2 and 3. Steps 2 and 3, which deal with the remaining wrong segments <italic>C</italic>2, <italic>C</italic>3, and <italic>C</italic>4, are much more closely related to the dendrite structure and represent the main contribution of our proposed pipeline.</p>
    </sec>
    <sec>
      <title>2.2. Step 2: separation of entangled dendrites (<italic>C</italic><sub>2</sub>)</title>
      <p>For the pruning of segments involving nearby neurons, which usually have their dendrites entangled with the dendrites of the current neuron, we need to define locations to separate the reconstruction into multiple neurons.</p>
      <p>Without loss of generality, we assume a pair of neurons with soma <italic>A</italic> and soma <italic>A</italic>′ that need to be separated. The path linking <italic>A</italic> and <italic>A</italic>′ has bifurcation point set <italic>B</italic><sub><italic>i</italic></sub>, for <italic>i</italic> = 1, 2, ...<italic>N</italic><sub><italic>B</italic></sub>, where <italic>N</italic><sub><italic>B</italic></sub> is the total number of the bifurcation points on this path (<xref rid="F4" ref-type="fig">Figure 4A</xref>). Each bifurcation point is a candidate separation site, and we need to identify the bifurcation point that best separates the path. After the separation, there are two reconstructions on the path: <italic>R</italic><sub><italic>A</italic></sub> for the neuron with soma <italic>A</italic>, and <inline-formula><mml:math id="M5" overflow="scroll"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:math></inline-formula> for the neuron with soma <italic>A</italic>′ (<xref rid="F4" ref-type="fig">Figure 4B</xref>). Using this bifurcation point to separate the path should be beneficial to the reconstruction of both neurons. The goal of Step 2 is to maximize the sum of the likelihood of branching patterns in <italic>R</italic><sub><italic>A</italic></sub> and <inline-formula><mml:math id="M6" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>. Here, weight <italic>W</italic><sub><italic>X</italic><sub><italic>B</italic></sub><sub><italic>i</italic></sub></sub> is introduced to reflect the likelihood of <italic>B</italic><sub><italic>i</italic></sub> belonging to the neuron with soma <italic>X</italic> (where <italic>X</italic> is either <italic>A</italic> or <italic>A</italic>′). The summation of weights for each bifurcation is used to reflect the joint likelihood. Thus, the best bifurcation point for separation, <italic>B</italic><sub><italic>s</italic></sub>, can be identified by:</p>
      <disp-formula id="E1">
        <label>(1)</label>
        <mml:math id="M7" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mstyle displaystyle="true">
                  <mml:munder class="msub">
                    <mml:mrow>
                      <mml:mo class="qopname">arg max</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>s</mml:mi>
                    </mml:mrow>
                  </mml:munder>
                </mml:mstyle>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>W</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>s</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munderover accentunder="false" accent="false">
                    <mml:mrow>
                      <mml:mo>∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>s</mml:mi>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>W</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>A</mml:mi>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>B</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>+</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munderover accentunder="false" accent="false">
                    <mml:mrow>
                      <mml:mo>∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>j</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mi>s</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>N</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>B</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>W</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mi>A</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>′</mml:mi>
                      </mml:mrow>
                    </mml:msup>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>B</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>j</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>.</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>Using statistical analysis of the branching pattern in training data sets, the weight is defined based on the angle of main path segments and child segments and on the distance between bifurcation points and the soma location (details in <xref rid="SM1" ref-type="supplementary-material">Supplementary material</xref>, Section 2). Putting this weight into the argmax target <italic>W</italic><sub><italic>s</italic></sub> above, the best separation point <italic>B</italic><sub><italic>s</italic></sub> can be identified. Then, the two neurons are separated at this point into two reconstructions, <italic>R</italic><sub><italic>A</italic></sub> and <inline-formula><mml:math id="M8" overflow="scroll"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:math></inline-formula>.</p>
      <fig position="float" id="F4">
        <label>Figure 4</label>
        <caption>
          <p>SNAP step 2 for separating neurons. <bold>(A, B)</bold> Illustrate the separating process, where somata, bifurcation points, and segments are represented by triangles, dots, and lines, respectively. <bold>(A)</bold> The original path linking the two somata, A and A'. The blue segments construct the main path, and the yellow segments are the child segments. The blue arrow points to the identified best dividing bifurcation point. <bold>(B)</bold> The resultant divided parts. <bold>(C)</bold> An example of separation. From left to right: MIP of the original image block overlaid with APP2 results shown in red, overlaid with SNAP results shown in green, and a zoomed-in view of the small region around the entanglement.</p>
        </caption>
        <graphic xlink:href="fninf-17-1174049-g0004" position="float"/>
      </fig>
      <p>This process is applied to all paths linking neuron pairs; thus, they can all be separated. When applying pruning to the neuron of interest, reconstructions are separated, and the resulting reconstructions belonging to other neurons are removed.</p>
      <p>Note that in the process above, the soma locations are known. In addition, most of the abnormally long paths also have the problem of entanglement with other neurons, even when no somata of other neurons are close by. In such cases, a patch is added that uses the endpoints of those paths as “fake” soma locations for purposes of the separation. <xref rid="F4" ref-type="fig">Figure 4C</xref> shows an example of separation.</p>
    </sec>
    <sec>
      <title>2.3. Step 3: pruning for “crossings” (<italic>C</italic><sub>3</sub> and <italic>C</italic><sub>4</sub>)</title>
      <p>Finally, <italic>C</italic><sub>3</sub> and <italic>C</italic><sub>4</sub> are pruned. In both these categories, the “wrong” segments are caused by local entanglement, involving either passing fibers of other neurons (<italic>C</italic><sub>3</sub>) or fibers of the neuron of interest itself (<italic>C</italic><sub>4</sub>). Crossings due to entanglements are commonly found in automated reconstruction results and contribute to the majority of wrong reconstructions that are troublesome to manually correct. The removal of these two types is important and a key target of SNAP.</p>
      <p>All branching structures in the reconstruction are checked. Based on the bifurcation number in the local neighborhood of “crossings”, there are two main types of structures: (1) one bifurcation without nearby bifurcations; and (2) more than one bifurcation nearby.</p>
      <p>One bifurcation structure can be modeled as <bold>Y</bold> or <bold>T</bold>, as in <xref rid="F5" ref-type="fig">Figures 5A</xref>, <xref rid="F5" ref-type="fig">B</xref>. For <bold>Y</bold>, the two segments that are best aligned are termed <italic>S</italic><sub>1</sub> and <italic>S</italic><sub>2</sub>, and the other segment as <italic>S</italic><sub>3</sub>, and <italic>S</italic><sub>2</sub> is assigned to the segment with a smaller angle with <italic>S</italic><sub>3</sub>. Different situations of parent–child segment relationships are examined. When the parent segment is <italic>S</italic><sub>1</sub>, we have a typical bifurcation; otherwise, the child segments could represent an error involving other dendrites or axons and thus a wrong segment due to “crossing.” When the parent segment is <italic>S</italic><sub>3</sub>, <italic>S</italic><sub>1</sub>, and <italic>S</italic><sub>2</sub> are considered wrong and will be removed. When the parent segment is <italic>S</italic><sub>2</sub>, then <italic>S</italic><sub>3</sub> is suspicious; the determination of <italic>S</italic><sub>3</sub> will be solved in the degenerated <bold>X</bold> case as described later. When the angle <inline-formula><mml:math id="M9" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="M10" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:math></inline-formula> are both close to 90 degrees, the <bold>Y</bold> type becomes a <bold>T</bold> type, which is processed in a similar way to the <bold>Y</bold> type.</p>
      <fig position="float" id="F5">
        <label>Figure 5</label>
        <caption>
          <p>SNAP step 3 models and examples. <bold>(A)</bold>
<bold>Y</bold> models. <bold>(B)</bold>
<bold>T</bold> models. <bold>(C)</bold>
<bold>X</bold> model. <bold>(D)</bold>
<bold>H</bold> model. <bold>(E)</bold> Histogram of <italic>β</italic><sub><italic>GT</italic></sub> in the training data set. <bold>(F)</bold> Reconstruction for suspicious <bold>Y</bold> models as degenerated <bold>X</bold> models. <bold>(G)</bold> An example of a pruned <bold>X</bold> structure. <bold>(H)</bold> An example of a pruned <bold>H</bold> structure. Red arrows for wrong segments, orange arrows for suspicious segments, yellow dots for bifurcation points, and a blue triangle for the reconstruction direction.</p>
        </caption>
        <graphic xlink:href="fninf-17-1174049-g0005" position="float"/>
      </fig>
      <p>The multiple bifurcation types are generally double or more <bold>Y</bold> with bifurcation points very close to each other. We define the confusing types with two bifurcation points as models of <bold>X</bold> and <bold>H</bold> as in <xref rid="F5" ref-type="fig">Figures 5C</xref>, <xref rid="F5" ref-type="fig">D</xref>, based on whether the short segment <italic>S</italic><sub>5</sub> linking the two bifurcations is correct or not, where it is correct in <bold>X</bold> and wrong in <bold>H</bold>. The child–child segment angle <italic>β</italic> plays a major part in <bold>H</bold> and <bold>X</bold> pruning. The angle threshold <italic>T</italic><sup><italic>β</italic></sup> is defined as 99% of the child–child segment angle population in the training data set, as in <xref rid="F5" ref-type="fig">Figure 5E</xref>, to define outliers. The <bold>H</bold> model is prioritized for pruning. We identify the pair of child segments that are both leaf segments in this structure (<italic>S</italic><sub>3</sub> and <italic>S</italic><sub>4</sub> as in <xref rid="F5" ref-type="fig">Figure 5D</xref>). If the angle <italic>β</italic> between them is larger than <italic>T</italic><sup><italic>β</italic></sup>, these segments and their parent segment are pruned away. This process continues recursively until no further <bold>H</bold> can be identified. Then, for <bold>X</bold> models, we identify the segment linking the two bifurcations; the angles between its child segments and “brother” segments (e.g., <italic>S</italic><sub>3</sub> and <italic>S</italic><sub>4</sub> as in <xref rid="F5" ref-type="fig">Figure 5C</xref>) are all calculated, and the two segments with the maximum angle, if larger than <italic>T</italic><sup><italic>β</italic></sup>, are pruned away. More details of the <bold>XH</bold> model-based method are described in <xref rid="SM1" ref-type="supplementary-material">Supplementary material</xref>, Section 3.</p>
      <p>In real data, there are many <bold>X</bold> and <bold>H</bold> structures with missing segments. As above, a suspicious <bold>Y</bold> or <bold>T</bold> model can be such a <bold>X</bold> or <bold>H</bold> model with missing segments. When the <bold>Y</bold> cases are considered suspicious, they are treated as degenerated cases of <bold>X</bold>. The pipeline has a local “re-tracing” process to help determine the removal. For a suspicious segment, the node with a distance of <italic>Len</italic><sub><italic>R</italic></sub> from the bifurcation point is used as the starting point, and the rest of the segment is masked out from the image. FastMarching is run to see whether reconstruction grows out to the two other segments (<xref rid="F5" ref-type="fig">Figure 5F</xref>). If so, the segment is considered correct; if not, we believe it can be attributed to the “crossing” that this segment belongs to and hence this segment is pruned away. <xref rid="F5" ref-type="fig">Figures 5G</xref>, <xref rid="F5" ref-type="fig">H</xref> show examples pruned X and H structures.</p>
      <p>Note that when the models have even more missing segments, there will be no bifurcation points left, and the segments become single segments. Therefore, single segments need to be checked if they are degenerated cases of <bold>YTXH</bold> structures. “Inflection” points are identified and pseudo-<bold>X</bold> structures are pruned as described in <xref rid="SM1" ref-type="supplementary-material">Supplementary material</xref>, Section 4.</p>
    </sec>
  </sec>
  <sec id="s3">
    <title>3. Data set and results</title>
    <sec>
      <title>3.1. Data set</title>
      <p>This study was based on three-dimensional images of single neurons acquired from 28 mouse brains with two-photon fluorescence imaging system fMOST (Gong et al., <xref rid="B6" ref-type="bibr">2016</xref>). In this fMOST dataset, the whole-brain image at the second-highest resolution level (with pixel resolution around 0.6 μ<italic>m</italic> × 0.6 μ<italic>m</italic> × 1 μ<italic>m</italic> in the <italic>x</italic>-<italic>y</italic>-<italic>z</italic> axes) was cropped into image blocks of fixed size (512px × 512px × 256px in the <italic>x</italic>, <italic>y</italic>, and <italic>z</italic> dimensions), each covering the dendritic region of a neuron with the cell body (soma) in the block center. We obtained gold-standard manual annotations from SEU-Allen Joint Center and identified the corresponding dendrite reconstruction results in the cropped images. Six hundred of them were randomly selected as the training data set for the statistical analysis throughout this work. Another 1,000 neurons constituted the testing data set, independent of the training data set. SNAP can be applied to reconstruction results from many different algorithms, e.g., ENT, ST, MST, etc. In our experiments here, the original automated neuron reconstruction results were obtained using the Vaa3D-APP2 platform with adaptive intensity threshold and default parameters for the algorithm. We opted for APP2 since it produces high-quality results on the data set we used.</p>
    </sec>
    <sec>
      <title>3.2. Qualitative evaluation</title>
      <p>SNAP was applied to the reconstruction results for the 1,000 images in fMOST testing data set. The pruning results were satisfactory. Visual examples are shown in <xref rid="F6" ref-type="fig">Figure 6</xref>. The pruning of <italic>C</italic>1 performed effectively, as exemplified by <xref rid="F6" ref-type="fig">Figure 6A</xref>, which includes zoom-in inset regions highlighting the removal of noisy segments. Multiple-neuron entanglements <italic>C</italic>2 were successfully resolved as in <xref rid="F6" ref-type="fig">Figures 6B</xref>–<xref rid="F6" ref-type="fig">F</xref>, where the reconstruction for single target neurons is separated out from the entangled multi-neuron reconstructions. The pruning of <italic>C</italic>3 &amp; <italic>C</italic>4 entanglement segments was also effective as in <xref rid="F6" ref-type="fig">Figures 6G</xref>, <xref rid="F6" ref-type="fig">H</xref>, where local and passing fiber entanglements were pruned away.</p>
      <fig position="float" id="F6">
        <label>Figure 6</label>
        <caption>
          <p>Example SNAP results on fMOST data set. The columns from left to right are as follows: MIP of original image block; overlaid with APP2 results shown in red; overlaid with SNAP results shown in green; overlaid with BP shown in yellow; overlaid with GT shown in magenta. Red arrows point to locations of pruned segments. Examples show pruning results of <bold>(A)</bold> noisy segments (with inset of zoom-in region in light blue boundary boxes); <bold>(B–F)</bold> entanglements with close-by dendrites; <bold>(G)</bold> local entanglement; and <bold>(H)</bold> passing fiber entanglement.</p>
        </caption>
        <graphic xlink:href="fninf-17-1174049-g0006" position="float"/>
      </fig>
      <p>In order to test SNAP's capability to automatically prune reconstruction obtained by a variety of algorithms from images other than the fMOST dataset above, we checked into BigNeuron (Manubens-Gil et al., <xref rid="B17" ref-type="bibr">2023</xref>), which contains various neuron images with benchmarking reconstruction. SNAP with default parameters was applied to the high-rank automated reconstructions of mouse neuron images. Two examples are shown in <xref rid="F7" ref-type="fig">Figure 7</xref>. In the first example as in <xref rid="F7" ref-type="fig">Figure 7A</xref>, the input automated reconstruction was obtained with 3D Tubular Models (Santamaría-Pang et al., <xref rid="B26" ref-type="bibr">2015</xref>), and pruning of wrong crossings within the same neuron was successful (see the zoom-in regions in <xref rid="F7" ref-type="fig">Figures 7B</xref>, <xref rid="F7" ref-type="fig">C</xref>). In the second example, as in <xref rid="F7" ref-type="fig">Figure 7D</xref>, the input automated reconstruction result was obtained with NeuroGPS-Tree and pruning of entanglements with passing fibers was effective (see the zoom-in regions in <xref rid="F7" ref-type="fig">Figures 7E</xref>, <xref rid="F7" ref-type="fig">F</xref>).</p>
      <fig position="float" id="F7">
        <label>Figure 7</label>
        <caption>
          <p>Example SNAP results on BigNeuron data set. The columns from left to right are as follows: MIP of original image block; overlaid with APP2 results shown in red; overlaid with SNAP results shown in green; overlaid with GT shown in magenta. <bold>(A–C)</bold> and <bold>(D–F)</bold> are two sets of examples, where <bold>(A, D)</bold> are full image; <bold>(B, C)</bold> are zoomed-in regions as in the light-blue and orange bounding boxes overlaid on <bold>(A)</bold>, displaying pruning of wrong crossings within the same neuron; <bold>(E, F)</bold> are zoomed-in regions as in the yellow and green bounding boxes overlaid on <bold>(B)</bold> (with slightly different viewing angle), displaying pruning of entanglements with passing fibers.</p>
        </caption>
        <graphic xlink:href="fninf-17-1174049-g0007" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.3. Quantitative performance evaluation</title>
      <p>To demonstrate the performance of SNAP, we provide a quantitative evaluation. As gold-standard manual annotation results were available, we could compare the output to this “ground truth” (GT) to determine the accuracy. However, since we start with the automated reconstruction results, and the algorithm prunes but does not add any missing segments, a direct comparison is not an appropriate choice. Hence, the “best possible pruned” result (BP) is calculated by removing all the segments from APP2 results that are not present in the GT based on their distance to GT segments (see <xref rid="SM1" ref-type="supplementary-material">Supplementary material</xref>, Section 5 for the BP calculation). As BP keeps some short segments due to noise that are very close to ground truth segments, we evaluate step 1 separately and then evaluate steps 2 and 3 (without the involvement of the short segments in step 1).</p>
      <p>In the first experiment, the performance of step 1 (pruning C1 segments) was checked. One hundred dendrites were randomly chosen from the testing data. The original reconstruction and pruned results were presented to human annotators, who were asked to label the correctly pruned segments and also the wrongly pruned ones. The results showed that out of the 48, 793 segments, <inline-formula><mml:math id="M11" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>400</mml:mn></mml:math></inline-formula> segments were pruned away, of which <inline-formula><mml:math id="M12" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>233</mml:mn></mml:math></inline-formula> were true noisy segments, and <inline-formula><mml:math id="M13" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>165</mml:mn></mml:math></inline-formula> were correct segments that were mistakenly removed. In the analog to a detection problem (where a true positive corresponds to correctly pruned segments), SNAP step 1 was quantitatively evaluated as follows.</p>
      <disp-formula id="E2">
        <mml:math id="M14" overflow="scroll">
          <mml:mtable columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mtext class="textrm" mathvariant="normal">Precision of step 1: </mml:mtext>
                <mml:mi>P</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>V</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>S</mml:mi>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>+</mml:mo>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>F</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mo>=</mml:mo>
                <mml:mn>98.4</mml:mn>
                <mml:mi>%</mml:mi>
                <mml:mo>.</mml:mo>
              </mml:mtd>
            </mml:mtr>
            <mml:mtr>
              <mml:mtd>
                <mml:mtext class="textrm" mathvariant="normal">False discovery rate of step 1: </mml:mtext>
                <mml:mi>F</mml:mi>
                <mml:mi>D</mml:mi>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>R</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>S</mml:mi>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>F</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>+</mml:mo>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>F</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mo>=</mml:mo>
                <mml:mn>1.6</mml:mn>
                <mml:mi>%</mml:mi>
                <mml:mo>.</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>Note that we did not ask annotators to determine false negatives, owing to the heavy manual labor cost of this task. Hence, no sensitivity or miss rate is given here. However, such segments would go through later steps and possibly be included in the evaluation of steps 2 and 3.</p>
      <p>Having validated step 1, we used the current pruned results to calculate the BP results. Examples of the BP results and also the GT are shown in <xref rid="F6" ref-type="fig">Figure 6</xref>. BP was not identical to GT, since BP results are the biggest matching subset of APP2 results. Note that although there has been research involving further post-processing to rescue missing segments, this is beyond the scope of this paper.</p>
      <p>In the second experiment, steps 2 and 3 were evaluated together. The SNAP results are compared with the “ground truth” given by BP. For the 1,000 neurons in the testing data set, there were <inline-formula><mml:math id="M15" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>398</mml:mn><mml:mo>,</mml:mo><mml:mn>846</mml:mn></mml:math></inline-formula> segments, and <inline-formula><mml:math id="M16" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>248</mml:mn><mml:mo>,</mml:mo><mml:mn>378</mml:mn></mml:math></inline-formula> segments were removed, of which <inline-formula><mml:math id="M17" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>238</mml:mn><mml:mo>,</mml:mo><mml:mn>537</mml:mn></mml:math></inline-formula> were true wrong segments and <inline-formula><mml:math id="M18" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>9</mml:mn><mml:mo>,</mml:mo><mml:mn>841</mml:mn></mml:math></inline-formula> were mistakenly removed correct segments. There were also <inline-formula><mml:math id="M19" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>58</mml:mn><mml:mo>,</mml:mo><mml:mn>132</mml:mn></mml:math></inline-formula> segments that should have been pruned but were not. In the analog to a detection problem, SNAP steps 2 and 3 were quantitatively evaluated as follows.</p>
      <disp-formula id="E3">
        <mml:math id="M20" overflow="scroll">
          <mml:mtable columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mtext class="textrm" mathvariant="normal">Precision of steps 2 and 3: </mml:mtext>
                <mml:mi>P</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>V</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>S</mml:mi>
                    <mml:mn>23</mml:mn>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>23</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>23</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>+</mml:mo>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>F</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>23</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mo>=</mml:mo>
                <mml:mn>96.0</mml:mn>
                <mml:mi>%</mml:mi>
                <mml:mo>.</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <disp-formula id="E4">
        <mml:math id="M21" overflow="scroll">
          <mml:mtable columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mtext class="textrm" mathvariant="normal">Sensitivity of steps 2 and 3: </mml:mtext>
                <mml:mi>T</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>R</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>S</mml:mi>
                    <mml:mn>23</mml:mn>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>23</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>23</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>+</mml:mo>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>F</mml:mi>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mn>23</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mo>=</mml:mo>
                <mml:mn>80.4</mml:mn>
                <mml:mi>%</mml:mi>
                <mml:mo>.</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>To reflect the differences in length among segments, we further evaluated SNAP steps 2 and 3 using segment length. Altogether, <inline-formula><mml:math id="M22" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>36273284</mml:mn><mml:mo>.</mml:mo><mml:mn>5</mml:mn></mml:math></inline-formula> pixels; we removed <inline-formula><mml:math id="M23" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>22429289</mml:mn><mml:mo>.</mml:mo><mml:mn>5</mml:mn></mml:math></inline-formula> pixels, where <inline-formula><mml:math id="M24" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1262893</mml:mn><mml:mo>.</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> pixels were true positives, <inline-formula><mml:math id="M25" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1166393</mml:mn><mml:mo>.</mml:mo><mml:mn>8</mml:mn></mml:math></inline-formula> pixels were false positives, and <inline-formula><mml:math id="M26" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>4393361</mml:mn><mml:mo>.</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> pixels were false negatives. The evaluation above could be re-done as <inline-formula><mml:math id="M27" overflow="scroll"><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>94</mml:mn><mml:mo>.</mml:mo><mml:mn>8</mml:mn><mml:mi>%</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M28" overflow="scroll"><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mn>23</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>82</mml:mn><mml:mo>.</mml:mo><mml:mn>9</mml:mn><mml:mi>%</mml:mi></mml:math></inline-formula>.</p>
    </sec>
    <sec>
      <title>3.4. Comparisons with other approaches</title>
      <p>To fully evaluate the proposed algorithm, we compared the performance of SNAP with that of other approaches. G-Cut (Li R. et al., <xref rid="B12" ref-type="bibr">2019</xref>), NeuroGPS-Tree (Quan et al., <xref rid="B25" ref-type="bibr">2016</xref>), and TREES toolbox (Cuntz et al., <xref rid="B4" ref-type="bibr">2010</xref>) are post-processing algorithms that can deal with the dissembling of multiple neuron entanglement by “separating” the neuron reconstruction results. From a single-cell perspective, these methods also prune away wrong segments that do not belong to the cell of interest. Hence we evaluate the pruning performance of these software tools and compare them. To ensure a fair comparison of pruning performance, we would like to rule out effects from different automated reconstruction methods. So the same input reconstruction should be provided to them. Here APP2 reconstruction results were used as the base reconstruction results for all of these tools.</p>
      <p>Of the 1,000 testing neurons, 598 involved multiple-neuron involved. The four tools were applied to all these samples with given soma locations and used to quantitatively evaluate each result for the neuron of interest. Specifically, when there were several dendrites close to the neuron of interest, the result was the separated and processed reconstruction of this neuron, disregarding the results for other neurons; this evaluation was done for 454 neurons (samples not included are: ones with multiple neuron, but APP2 results don't involve entanglements with multiple neurons; ones with no pruning happened thus precision is not defined). Three commonly used metrics, precision, sensitivity, and F1-score, were calculated for each neuron. For this specific separation problem, we adopted Miss-Extra-Score (MES; Xie et al., <xref rid="B31" ref-type="bibr">2011</xref>) as used in the evaluation of G-Cut (Li R. et al., <xref rid="B12" ref-type="bibr">2019</xref>), as MES provides a global view for neuron reconstruction based on accuracy and undesired components. MES was originally defined as <inline-formula><mml:math id="M29" overflow="scroll"><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:math></inline-formula>, where <italic>S</italic><sub><italic>G</italic></sub> is the total length of all segments in the GT trace, and <italic>S</italic><sub><italic>miss</italic></sub> and <italic>S</italic><sub><italic>extra</italic></sub> are the total lengths of missing and extra segments in the automated trace, respectively (compared with the GT). In our pruning setting, MES was reformulated as <inline-formula><mml:math id="M30" overflow="scroll"><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula>. Both segment-based and length-based metrics are presented.</p>
      <p><xref rid="F8" ref-type="fig">Figures 8A</xref>–<xref rid="F8" ref-type="fig">E</xref> show several visual examples of results. All four algorithms performed reasonably well in separating the target neuron from entangled reconstructions. Some detailed differences are: (1) SNAP and NeuroGPS-Tree are both capable of removing entanglement segments of close-by neurons even when their soma locations are not within the image region. G-Cut and TREES Toolbox rely on the clear definition of all nearby soma locations(as in <xref rid="F8" ref-type="fig">Figures 8B</xref>, <xref rid="F8" ref-type="fig">C</xref> with yellow arrows pointing to the correct removal of these segments in SNAP and NeuroGPS-Tree and red arrows pointing to unsuccessful removal in G-Cut and TREES Toolbox). (2) Similar to (1), SNAP and NeuroGPS-Tree could be on the strict side in pruning(see in <xref rid="F8" ref-type="fig">Figures 8C</xref>–<xref rid="F8" ref-type="fig">E</xref> with orange arrows pointing to over-pruning). (3) In some cases, SNAP, G-cut, and TREES Toolbox have difficulty removing segments in conjunction region of two neurons (see in <xref rid="F8" ref-type="fig">Figure 8D</xref> with blue arrows pointing to under-pruning).</p>
      <fig position="float" id="F8">
        <label>Figure 8</label>
        <caption>
          <p>Performance comparison. <bold>(A–E)</bold> Visual examples. From left to right: the original image (displayed as MIP, same for the other sub-figures) overlaid with input APP2 results (red), SNAP (green), G-Cut (blue), TREES Toolbox (orange), NeuroGPS-Tree (purple), and BP results (yellow). The arrows point to locations of some differences between the algorithms: yellow arrows for correct removal, red arrows for unsuccessful removal, orange arrows for over-pruning, and blue arrows for under-pruning. <bold>(F)</bold> The four quantitative metrics based on segments for the four algorithms are shown with box plots. <bold>(G)</bold> Length-based metrics. In <bold>(F, G)</bold>, “N-GPS-T” is used as the abbreviation for “NeuroGPS Tree.”</p>
        </caption>
        <graphic xlink:href="fninf-17-1174049-g0008" position="float"/>
      </fig>
      <p><xref rid="F8" ref-type="fig">Figures 8F</xref>, <xref rid="F8" ref-type="fig">G</xref> show box plots of precision, sensitivity, F1-score, and MES for SNAP, G-Cut, NeuroGPS-Tree and TREES Toolbox. We can see G-Cut has best precision, and SNAP has the best sensitivity, F1-score, and MES scores. Since SNAP and G-Cut perform relatively comparable, we further counted how often SNAP or G-Cut algorithms performed better than the other for each neuron, and how often they performed equally well, based on these four metrics (<xref rid="T1" ref-type="table">Table 1</xref>). Overall, SNAP had relatively lower precision but better sensitivity, F1-score, and MES; hence, in general, SNAP outperformed the rest of the algorithms.</p>
      <table-wrap position="float" id="T1">
        <label>Table 1</label>
        <caption>
          <p>Tables of performance comparison results for SNAP and G-Cut.</p>
        </caption>
        <table frame="box" rules="all">
          <thead>
            <tr style="background-color:919498;color:ffffff">
              <th valign="top" align="left" colspan="4" rowspan="1">
                <bold>Segment-based</bold>
              </th>
            </tr>
            <tr style="background-color:#919497;color:#ffffff">
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>Metric</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>SNAP is better (%)</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>SNAP = G-Cut (%)</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>G-Cut is better (%)</bold>
              </td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Precision</td>
              <td valign="top" align="center" rowspan="1" colspan="1">33</td>
              <td valign="top" align="center" rowspan="1" colspan="1">19</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>48</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Sensitivity</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>60</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">39</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">F1-score</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>55</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">44</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MES</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>47</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">11</td>
              <td valign="top" align="center" rowspan="1" colspan="1">42</td>
            </tr>
            <tr style="background-color:#919497;color:#ffffff">
              <td valign="top" align="left" colspan="4" rowspan="1">
                <bold>Length-based</bold>
              </td>
            </tr>
            <tr style="background-color:#919497;color:#ffffff">
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>Metric</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>SNAP is better (%)</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>SNAP = G-Cut (%)</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>G-Cut is better (%)</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Precision</td>
              <td valign="top" align="center" rowspan="1" colspan="1">38</td>
              <td valign="top" align="center" rowspan="1" colspan="1">8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>54</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Sensitivity</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>61</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">38</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">F1-score</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>54</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">46</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MES</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>45</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">11</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>45</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>Both segment-based metrics (top) and length-based metrics (bottom) are presented. The bold numbers indicate the superior algorithm in the comparison.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>With the ability of separating target neuron from entangled reconstruction, SNAP natural achieved multiple neuron separation functionality in dense and entangled neuron reconstruction problems by pruning w.r.t. each of the neuron. One example was shown for its multiple neuron separation performance and compare it with that of G-Cut, NeuroGPS-Tree, and TREES toolbox. An image block with nine neurons (mostly dendrite portions) was reconstructed with APP2. As shown in <xref rid="F9" ref-type="fig">Figure 9</xref>, all nine neurons were entangled as one reconstruction. We applied the four algorithms in separating the nine neurons with soma locations given. <xref rid="F9" ref-type="fig">Figures 9B</xref>–<xref rid="F9" ref-type="fig">E</xref> show the SNAP results, the G-Cut results, the NeuroGPS-Tree results, and the TREES Toolbox. We can see that all algorithms could separate the neurons reasonably well. There are some differences within these results, and similar to examples in <xref rid="F8" ref-type="fig">Figure 8</xref> there are some over pruning and under pruning involved along the separation. <xref rid="F9" ref-type="fig">Figure 9F</xref> provides the Best Possible pruned results from APP2 results with manual annotation of the two neurons visible in this field of view (the rest of the neurons don't have manual annotations). From the visual comparison, we can see SNAP achieved good separation and pruning for this group of neurons.</p>
      <fig position="float" id="F9">
        <label>Figure 9</label>
        <caption>
          <p>A multiple neurons separating and pruning example. MIPs are displayed and overlaid with reconstruction. Different neurons are shown in different colors that are consistent across the results of all algorithms and manual annotation. <bold>(A)</bold> APP2 results as input. <bold>(B)</bold> SNAP results. <bold>(C)</bold> G-Cut results. <bold>(D)</bold> NeuroGPS-Tree results. <bold>(E)</bold> TREES Toolbox results. <bold>(F)</bold> Best Possible results of the two neurons with manual annotation. The red arrows point to locations with entanglements between neurons which some algorithms can remove and some cannot. Orange arrows point to locations of over-pruning.</p>
        </caption>
        <graphic xlink:href="fninf-17-1174049-g0009" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec id="s4">
    <title>4. Software availability</title>
    <p>This study was conducted with the support of the Vaa3D platform (v.3.601). The released binary and the source code for the Vaa3D platform are available through the GitHub release page of vaa3d.org (<ext-link xlink:href="https://github.com/Vaa3D" ext-link-type="uri">https://github.com/Vaa3D</ext-link>). The software implementation of the method presented here was developed in C++ and built as a plugin in the Vaa3D framework (Peng et al., <xref rid="B20" ref-type="bibr">2014</xref>) with Qt-4.7.2 installed. SNAP implementation was tested using both CentOS and Windows operating systems. It is available for download at <ext-link xlink:href="https://github.com/Vaa3D/vaa3d_tools/tree/master/hackathon/XuanZhao/SNAP" ext-link-type="uri">https://github.com/Vaa3D/vaa3d_tools/tree/master/hackathon/XuanZhao/SNAP</ext-link>. Guidance for use of the plugin is included in the README.txt file.</p>
  </sec>
  <sec id="s5">
    <title>5. Conclusion and discussion</title>
    <p>In this paper, we present SNAP, a structure-based neuron morphology reconstruction automated pruning pipeline. It incorporates statistical analysis and structure modeling into rules for removing erroneous extra segments, thereby improving neuron reconstruction workflow throughput. Experimental results, especially for quantitative evaluation with high precision and recall, demonstrate the effectiveness of SNAP. SNAP also achieved neuron separation in entangled neuron problems.</p>
    <p>Note that the methods in SNAP depend on statistical priors and use empirical values as thresholds. Here, it is important to point out that the prior knowledge drawn from careful study of gold-standard manual annotation data is on the different types of errors and structural models, which are independent of the choice of the automated reconstruction algorithm. SNAP can be applied to the results of any automated reconstruction algorithm.</p>
    <p>As SNAP reduces the number of wrong segments, manual curation can be speeded up. The results obtained with SNAP could serve as an improved basis for further post-processing algorithms, e.g., repair algorithms to make up the missing branches. SNAP could also be applied to manual annotation as a QC tool to identify segments that are possibly wrong. Hence, it is a powerful tool facilitating high-through neuron morphology reconstruction.</p>
  </sec>
  <sec sec-type="data-availability" id="s6">
    <title>Data availability statement</title>
    <p>The original contributions presented in the study are included in the article/<xref rid="SM1" ref-type="supplementary-material">Supplementary material</xref>, further inquiries can be directed to the corresponding authors.</p>
  </sec>
  <sec sec-type="author-contributions" id="s7">
    <title>Author contributions</title>
    <p>LD conceptualized the project, developed the algorithm with help from the team, and wrote the manuscript. XZ assisted with algorithm development and implemented the software. YL contributed to algorithm development. LL led the annotation of the gold-standard data. SG and YW contributed to algorithm development and manuscript writing. HP supervised the project. All authors contributed to the article and approved the submitted version.</p>
  </sec>
</body>
<back>
  <ack>
    <p>Thanks to SEU-Allen Joint Center annotation team for the gold standard annotation of whole-brain neuron morphology. Thanks to the Allen Institute for Brain Science for sparsely labeled mouse brains and Huazhong University of Science and Technology for fMOST images. Thanks for their practice in open science principles by sharing the data online. We thank Zhi Zhou, Yuanyuan Song, Lulu Yin, Shichen Zhang, Jintao Pan, Yanting Liu, Guodong Hong, Jia Yuan, Yanjun Duan, Yaping Wang, Qiang Ouyang, Zijun Zhao, Wan Wan, Peng Wang, Ping He, Lingsheng Kong, Feng Xiong, and other members in SEU-Allen Joint Center annotation team for their effort in gold-standard morphology data production. We thank Xin Chen, Bingjie Shao, Mengyu Wang, Haoyu Zeng, Gaoyu Wang, Yiwei Li, Nan Mo, Xiaoqin Gu, Xiaoxuan Jiang, and Hairuo Zhang in the annotation team for taking the annotation survey. We thank Zhangcan Ding, Ping He, Mengya Chen, and Peng Wang for their statistical analysis of error types. We thank Jie Xue for her help in performance evaluation.</p>
  </ack>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s9">
    <title>Publisher's note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
  <sec sec-type="supplementary-material" id="s10">
    <title>Supplementary material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link xlink:href="https://www.frontiersin.org/articles/10.3389/fninf.2023.1174049/full#supplementary-material" ext-link-type="uri">https://www.frontiersin.org/articles/10.3389/fninf.2023.1174049/full#supplementary-material</ext-link></p>
    <supplementary-material id="SM1" position="float" content-type="local-data">
      <media xlink:href="Data_Sheet_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ascoli</surname><given-names>G. A.</given-names></name><name><surname>Alonso-Nanclares</surname><given-names>L.</given-names></name><name><surname>Anderson</surname><given-names>S. A.</given-names></name><name><surname>Barrionuevo</surname><given-names>G.</given-names></name><name><surname>Benavides-Piccione</surname><given-names>R.</given-names></name><name><surname>Burkhalter</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>2008</year>). <article-title>Petilla terminology: nomenclature of features of gabaergic interneurons of the cerebral cortex</article-title>. <source>Nat. Rev. Neurosci.</source><volume>9</volume>, <fpage>557</fpage>–<lpage>568</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2402</pub-id><?supplied-pmid 18568015?><pub-id pub-id-type="pmid">18568015</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cannon</surname><given-names>R.</given-names></name><name><surname>Turner</surname><given-names>D.</given-names></name><name><surname>Pyapali</surname><given-names>G.</given-names></name><name><surname>Wheal</surname><given-names>H.</given-names></name></person-group> (<year>1998</year>). <article-title>An on-line archive of reconstructed hippocampal neurons</article-title>. <source>J. Neurosci. Methods</source>
<volume>84</volume>, <fpage>49</fpage>–<lpage>54</lpage>. <pub-id pub-id-type="doi">10.1016/s0165-0270(98)00091-0</pub-id><?supplied-pmid 9821633?><pub-id pub-id-type="pmid">9821633</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>H.</given-names></name><name><surname>Xiao</surname><given-names>H.</given-names></name><name><surname>Liu</surname><given-names>T.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name></person-group> (<year>2015</year>). <article-title>Smarttracing: self-learning-based neuron reconstruction</article-title>. <source>Brain Informatics</source>
<volume>2</volume>, <fpage>135</fpage>–<lpage>144</lpage>. <pub-id pub-id-type="doi">10.1007/s40708-015-0018-y</pub-id><?supplied-pmid 27747506?><pub-id pub-id-type="pmid">27747506</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cuntz</surname><given-names>H.</given-names></name><name><surname>Forstner</surname><given-names>F.</given-names></name><name><surname>Borst</surname><given-names>A.</given-names></name><name><surname>Häusser</surname><given-names>M.</given-names></name></person-group> (<year>2010</year>). <article-title>One rule to grow them all: a general theory of neuronal branching and its practical application</article-title>. <source>PLoS Comput. Biol.</source>
<volume>6</volume>, <fpage>1000877</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1000877</pub-id><?supplied-pmid 20700495?><pub-id pub-id-type="pmid">20700495</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>L.</given-names></name><name><surname>Liu</surname><given-names>S.</given-names></name><name><surname>Gou</surname><given-names>L.</given-names></name><name><surname>Hu</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Deng</surname><given-names>L.</given-names></name><etal/></person-group>. (<year>2022</year>). <article-title>Single-neuron projectome of mouse prefrontal cortex</article-title>. <source>Nat. Neurosci.</source><volume>25</volume>, <fpage>515</fpage>–<lpage>529</lpage>.<pub-id pub-id-type="pmid">35361973</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gong</surname><given-names>H.</given-names></name><name><surname>Xu</surname><given-names>D.</given-names></name><name><surname>Yuan</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Guo</surname><given-names>C.</given-names></name><name><surname>Peng</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>High-throughput dual-colour precision imaging for brain-wide connectome with cytoarchitectonic landmarks at the cellular level</article-title>. <source>Nat. Commun.</source><volume>7</volume>, <fpage>12142</fpage><pub-id pub-id-type="doi">10.1038/ncomms12142</pub-id><?supplied-pmid 27374071?><pub-id pub-id-type="pmid">27374071</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>C.</given-names></name><name><surname>Liu</surname><given-names>M.</given-names></name><name><surname>Guan</surname><given-names>T.</given-names></name><name><surname>Chen</surname><given-names>W.</given-names></name><name><surname>Wen</surname><given-names>H.</given-names></name><name><surname>Zeng</surname><given-names>T.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>Crossover structure separation with application to neuron tracing in volumetric images</article-title>. <source>IEEE Trans. Instrument. Measure.</source><volume>70</volume>, <fpage>1</fpage>–<lpage>13</lpage>. <pub-id pub-id-type="doi">10.1109/TIM.2021.3072119</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>S.</given-names></name><name><surname>Zhao</surname><given-names>X.</given-names></name><name><surname>Jiang</surname><given-names>S.</given-names></name><name><surname>Ding</surname><given-names>L.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name></person-group> (<year>2022</year>). <article-title>Image enhancement to leverage the 3d morphological reconstruction of single-cell neurons</article-title>. <source>Bioinformatics</source>
<volume>38</volume>, <fpage>503</fpage>–<lpage>512</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btab638</pub-id><?supplied-pmid 34515755?><pub-id pub-id-type="pmid">34515755</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Q.</given-names></name><name><surname>Cao</surname><given-names>T.</given-names></name><name><surname>Zeng</surname><given-names>S.</given-names></name><name><surname>Li</surname><given-names>A.</given-names></name><name><surname>Quan</surname><given-names>T.</given-names></name></person-group> (<year>2022</year>). <article-title>Minimizing probability graph connectivity cost for discontinuous filamentary structures tracing in neuron image</article-title>. <source>IEEE J. Biomed. Health Inform.</source>
<volume>26</volume>, <fpage>3092</fpage>–<lpage>3103</lpage>. <pub-id pub-id-type="doi">10.1109/JBHI.2022.3147512</pub-id><?supplied-pmid 35104232?><pub-id pub-id-type="pmid">35104232</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Q.</given-names></name><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>S.</given-names></name><name><surname>Xu</surname><given-names>C.</given-names></name><name><surname>Cao</surname><given-names>T.</given-names></name><name><surname>Xu</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Weakly supervised learning of 3d deep network for neuron reconstruction</article-title>. <source>Front. Neuroanat.</source><volume>14</volume>, <fpage>38</fpage>. <pub-id pub-id-type="doi">10.3389/fnana.2020.00038</pub-id><?supplied-pmid 32848636?><pub-id pub-id-type="pmid">32848636</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>S.</given-names></name><name><surname>Pan</surname><given-names>Z.</given-names></name><name><surname>Feng</surname><given-names>Z.</given-names></name><name><surname>Guan</surname><given-names>Y.</given-names></name><name><surname>Ren</surname><given-names>M.</given-names></name><name><surname>Ding</surname><given-names>Z.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Skeleton optimization of neuronal morphology based on three-dimensional shape restrictions</article-title>. <source>BMC Bioinformatics</source><volume>21</volume>, <fpage>395</fpage>. <pub-id pub-id-type="doi">10.1186/s12859-020-03714-z</pub-id><?supplied-pmid 32887543?><pub-id pub-id-type="pmid">32887543</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>R.</given-names></name><name><surname>Zhu</surname><given-names>M.</given-names></name><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Bienkowski</surname><given-names>M. S.</given-names></name><name><surname>Foster</surname><given-names>N. N.</given-names></name><name><surname>Xu</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Precise segmentation of densely interweaving neuron clusters using g-cut</article-title>. <source>Nat. Commun.</source><volume>10</volume>:<fpage>1549</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-09515-0</pub-id><?supplied-pmid 30948706?><pub-id pub-id-type="pmid">30948706</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>S.</given-names></name><name><surname>Quan</surname><given-names>T.</given-names></name><name><surname>Zhou</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>A.</given-names></name><name><surname>Fu</surname><given-names>L.</given-names></name><name><surname>Gong</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Research progress of neuron morphological reconstruction tools</article-title>. <source>Prog. Biochem. Biophys.</source><volume>46</volume>, <fpage>266</fpage>–<lpage>275</lpage>.</mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>H.</given-names></name><name><surname>Acton</surname><given-names>S. T.</given-names></name><name><surname>Weller</surname><given-names>D. S.</given-names></name></person-group> (<year>2017</year>). <article-title>“Content-aware neuron image enhancement,”</article-title> in <source>IEEE International Conference on Image Processing (ICIP)</source> (<publisher-loc>Beijing</publisher-loc>).<?supplied-pmid 30716037?></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y.</given-names></name></person-group> (<year>2011</year>). <article-title>The diadem and beyond</article-title>. <source>Neuroinformatics</source>
<volume>9</volume>, <fpage>99</fpage>–<lpage>102</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-011-9102-5</pub-id><?supplied-pmid 21431331?><pub-id pub-id-type="pmid">21431331</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Zhong</surname><given-names>Y.</given-names></name><name><surname>Zhao</surname><given-names>X.</given-names></name><name><surname>Liu</surname><given-names>L.</given-names></name><name><surname>Ding</surname><given-names>L.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name></person-group> (<year>2022</year>). <article-title>Tracing weak neuron fibers</article-title>. <source>Bioinformatics</source> 39, btac816. <pub-id pub-id-type="doi">10.1093/bioinformatics/btac816</pub-id><?supplied-pmid 36571479?><pub-id pub-id-type="pmid">36571479</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manubens-Gil</surname><given-names>L.</given-names></name><name><surname>Zhou</surname><given-names>Z.</given-names></name><name><surname>Chen</surname><given-names>H.</given-names></name><name><surname>Ramanathan</surname><given-names>A.</given-names></name><name><surname>Liu</surname><given-names>X.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2023</year>). <article-title>Bigneuron: a resource to benchmark and predict performance of algorithms for automated tracing of neurons in light microscopy datasets</article-title>. <source>Nat. Methods</source>. <pub-id pub-id-type="doi">10.1038/s41592-023-01848-5.</pub-id> [Epub ahead of print].<?supplied-pmid 37069271?><pub-id pub-id-type="pmid">37069271</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meijering</surname><given-names>E.</given-names></name></person-group> (<year>2010</year>). <article-title>Neuron tracing in perspective</article-title>. <source>Cytomet. A</source>
<volume>77</volume>, <fpage>693</fpage>–<lpage>704</lpage>. <pub-id pub-id-type="doi">10.1002/cyto.a.20895</pub-id><?supplied-pmid 20583273?><pub-id pub-id-type="pmid">20583273</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Halloran</surname><given-names>D. M.</given-names></name></person-group> (<year>2020</year>). <article-title>Module for swc neuron morphology file validation and correction enabled for high throughput batch processing</article-title>. <source>PLoS ONE</source>
<volume>15</volume>, <fpage>e0228091</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0228091</pub-id><?supplied-pmid 31971963?><pub-id pub-id-type="pmid">31971963</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>H.</given-names></name><name><surname>Bria</surname><given-names>A.</given-names></name><name><surname>Zhou</surname><given-names>Z.</given-names></name><name><surname>Iannello</surname><given-names>G.</given-names></name><name><surname>Long</surname><given-names>F.</given-names></name></person-group> (<year>2014</year>). <article-title>Extensible visualization and analysis for multidimensional images using vaa3d</article-title>. <source>Nat. Protoc.</source>
<volume>9</volume>, <fpage>193</fpage>–<lpage>208</lpage>. <pub-id pub-id-type="doi">10.1038/nprot.2014.011</pub-id><?supplied-pmid 24385149?><pub-id pub-id-type="pmid">24385149</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>H.</given-names></name><name><surname>Hawrylycz</surname><given-names>M.</given-names></name><name><surname>Roskams</surname><given-names>J.</given-names></name><name><surname>Hill</surname><given-names>S.</given-names></name><name><surname>Spruston</surname><given-names>N.</given-names></name><name><surname>Meijering</surname><given-names>E.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>Bigneuron: large-scale 3d neuron reconstruction from optical microscopy images</article-title>. <source>Neuron</source><volume>87</volume>, <fpage>252</fpage>–<lpage>256</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2015.06.036</pub-id><?supplied-pmid 26182412?><pub-id pub-id-type="pmid">26182412</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>H.</given-names></name><name><surname>Long</surname><given-names>F.</given-names></name><name><surname>Myers</surname><given-names>G. J. B.</given-names></name></person-group> (<year>2011</year>). <article-title>Automatic 3D neuron tracing using all-path pruning</article-title>. <source>Bioinformatics</source>
<volume>27</volume>, <fpage>i239</fpage>–<lpage>i247</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btr237</pub-id><?supplied-pmid 21685076?><pub-id pub-id-type="pmid">21685076</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>H.</given-names></name><name><surname>Xie</surname><given-names>P.</given-names></name><name><surname>Liu</surname><given-names>L.</given-names></name><name><surname>Kuang</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Qu</surname><given-names>L.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>Morphological diversity of single neurons in molecularly defined cell types</article-title>. <source>Nature</source><volume>598</volume>, <fpage>174</fpage>–<lpage>181</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-021-03941-1</pub-id><?supplied-pmid 34616072?><pub-id pub-id-type="pmid">34616072</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>H.</given-names></name><name><surname>Zhou</surname><given-names>Z.</given-names></name><name><surname>Meijering</surname><given-names>E.</given-names></name><name><surname>Zhao</surname><given-names>T.</given-names></name><name><surname>Ascoli</surname><given-names>G.</given-names></name><name><surname>Hawrylycz</surname><given-names>M.</given-names></name></person-group> (<year>2017</year>). <article-title>Automatic tracing of ultra-volumes of neuronal imagesnature methods</article-title>. <source>Nat. Methods</source>
<volume>14</volume>, <fpage>332</fpage>–<lpage>333</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.4233</pub-id><?supplied-pmid 28362437?><pub-id pub-id-type="pmid">28362437</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quan</surname><given-names>T.</given-names></name><name><surname>Zhou</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>S.</given-names></name><name><surname>Li</surname><given-names>A.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>Neurogps-tree: automatic reconstruction of large-scale neuronal populations with dense neurites</article-title>. <source>Nat. Methods</source><volume>13</volume>, <fpage>51</fpage>–<lpage>54</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.3662</pub-id><?supplied-pmid 26595210?><pub-id pub-id-type="pmid">26595210</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Santamaría-Pang</surname><given-names>A.</given-names></name><name><surname>Hernandez-Herrera</surname><given-names>P.</given-names></name><name><surname>Papadakis</surname><given-names>M.</given-names></name><name><surname>Saggau</surname><given-names>P.</given-names></name><name><surname>Kakadiaris</surname><given-names>I. A.</given-names></name></person-group> (<year>2015</year>). <article-title>Automatic morphological reconstruction of neurons from multiphoton and confocal microscopy images using 3D tubular models</article-title>. <source>Neuroinformatics</source>
<volume>13</volume>, <fpage>297</fpage>–<lpage>320</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-014-9253-2</pub-id><?supplied-pmid 25631538?><pub-id pub-id-type="pmid">25631538</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C.</given-names></name><name><surname>Lee</surname><given-names>Y.</given-names></name><name><surname>Pradana</surname><given-names>H.</given-names></name><name><surname>Zhou</surname><given-names>Z.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name></person-group> (<year>2017</year>). <article-title>Ensemble neuron tracer for 3d neuron reconstruction</article-title>. <source>Neuroinformatics</source>
<volume>15</volume>, <fpage>185</fpage>–<lpage>198</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-017-9325-1</pub-id><?supplied-pmid 28185058?><pub-id pub-id-type="pmid">28185058</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>Q.</given-names></name><name><surname>Liu</surname><given-names>L.</given-names></name></person-group> (<year>2019</year>). <article-title>TeraVR empowers precise reconstruction of complete 3-d neuronal morphology in the whole brain</article-title>. <source>Nat. Commun.</source>
<volume>10</volume>, <fpage>3474</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-11443-y</pub-id><?supplied-pmid 31375678?><pub-id pub-id-type="pmid">31375678</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>J.</given-names></name><name><surname>He</surname><given-names>Y.</given-names></name><name><surname>Yang</surname><given-names>Z.</given-names></name><name><surname>Guo</surname><given-names>C.</given-names></name><name><surname>Luo</surname><given-names>Q.</given-names></name><name><surname>Zhou</surname><given-names>W.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>3D brainCV: simultaneous visualization and analysis of cells and capillaries in a whole mouse brain with one-micron voxel resolution</article-title>. <source>NeuroImage</source><volume>87</volume>, <fpage>199</fpage>–<lpage>208</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.036</pub-id><?supplied-pmid 24185025?><pub-id pub-id-type="pmid">24185025</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>H.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name></person-group> (<year>2013</year>). <article-title>App2: automatic tracing of 3D neuron morphology based on hierarchical pruning of gray-weighted image distance-trees</article-title>. <source>Bioinformatics</source>
<volume>29</volume>, <fpage>1448</fpage>–<lpage>1454</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btt170</pub-id><?supplied-pmid 23603332?><pub-id pub-id-type="pmid">23603332</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>J.</given-names></name><name><surname>Zhao</surname><given-names>T.</given-names></name><name><surname>Lee</surname><given-names>T.</given-names></name><name><surname>Myers</surname><given-names>E.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name></person-group> (<year>2011</year>). <article-title>Anisotropic path searching for automatic neuron reconstruction</article-title>. <source>Med. Image Anal.</source>
<volume>15</volume>, <fpage>680</fpage>–<lpage>689</lpage>. <pub-id pub-id-type="doi">10.1016/j.media.2011.05.013</pub-id><?supplied-pmid 21669547?><pub-id pub-id-type="pmid">21669547</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>F.</given-names></name><name><surname>Liu</surname><given-names>M.</given-names></name><name><surname>Chen</surname><given-names>W.</given-names></name><name><surname>Zeng</surname><given-names>T.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name></person-group> (<year>2021</year>). <article-title>Automatic repair of 3d neuron reconstruction based on topological feature points and a most-based repairer</article-title>. <source>IEEE Trans. Instrument. Measure.</source>
<volume>70</volume>, <fpage>1</fpage>–<lpage>13</lpage>. <pub-id pub-id-type="doi">10.1109/TIM.2020.3033057</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>H.</given-names></name><name><surname>Sanes</surname><given-names>J. R.</given-names></name></person-group> (<year>2017</year>). <article-title>Neuronal cell-type classification: challenges, opportunities and the path forward</article-title>. <source>Nat. Rev. Neurosci.</source>
<volume>18</volume>, <fpage>530</fpage>–<lpage>546</lpage>. <pub-id pub-id-type="doi">10.1038/nrn.2017.85</pub-id><?supplied-pmid 28775344?><pub-id pub-id-type="pmid">28775344</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>T.</given-names></name><name><surname>Xie</surname><given-names>J.</given-names></name><name><surname>Amat</surname><given-names>F.</given-names></name><name><surname>Clack</surname><given-names>N.</given-names></name><name><surname>Ahammad</surname><given-names>P.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2011</year>). <article-title>Automated reconstruction of neuronal morphology based on local geometrical and global structural models</article-title>. <source>Neuroinformatics</source><volume>9</volume>, <fpage>247</fpage>–<lpage>261</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-011-9120-3</pub-id><?supplied-pmid 21547564?><pub-id pub-id-type="pmid">21547564</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>S.</given-names></name><name><surname>Li</surname><given-names>A.</given-names></name><name><surname>Huang</surname><given-names>Q.</given-names></name><name><surname>Xiong</surname><given-names>F.</given-names></name><name><surname>Li</surname><given-names>N.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>GTree: an open-source tool for dense reconstruction of brain-wide neuronal population</article-title>. <source>Neuroinformatics</source><volume>19</volume>, <fpage>305</fpage>–<lpage>317</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-020-09484-6</pub-id><?supplied-pmid 32844332?><pub-id pub-id-type="pmid">32844332</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Z.</given-names></name><name><surname>Sorensen</surname><given-names>S.</given-names></name><name><surname>Zeng</surname><given-names>H.</given-names></name><name><surname>Hawrylycz</surname><given-names>M.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name></person-group> (<year>2015</year>). <article-title>Adaptive image enhancement for tracing 3d morphologies of neurons and brain vasculatures</article-title>. <source>Neuroinformatics</source>
<volume>13</volume>, <fpage>153</fpage>–<lpage>166</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-014-9249-y</pub-id><?supplied-pmid 25310965?><pub-id pub-id-type="pmid">25310965</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
