<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName archivearticle.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Behav Neurosci</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Behav Neurosci</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Behav. Neurosci.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Behavioral Neuroscience</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1662-5153</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8581673</article-id>
    <article-id pub-id-type="doi">10.3389/fnbeh.2021.750894</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Behavioral Neuroscience</subject>
        <subj-group>
          <subject>Methods</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepBhvTracking: A Novel Behavior Tracking Method for Laboratory Animals Based on Deep Learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Sun</surname>
          <given-names>Guanglong</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1434890/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lyu</surname>
          <given-names>Chenfei</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Cai</surname>
          <given-names>Ruolan</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yu</surname>
          <given-names>Chencen</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sun</surname>
          <given-names>Hao</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Schriver</surname>
          <given-names>Kenneth E.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1479668/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gao</surname>
          <given-names>Lixia</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/648949/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Xinjian</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1426126/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Department of Neurology of the Second Affiliated Hospital, Interdisciplinary Institute of Neuroscience and Technology, Zhejiang University School of Medicine</institution>, <addr-line>Hangzhou</addr-line>, <country>China</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Key Laboratory of Medical Neurobiology of Zhejiang Province</institution>, <addr-line>Hangzhou</addr-line>, <country>China</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Key Laboratory of Biomedical Engineering of Ministry of Education, College of Biomedical Engineering and Instrument Science, Zhejiang University</institution>, <addr-line>Hangzhou</addr-line>, <country>China</country></aff>
    <aff id="aff4"><sup>4</sup><institution>School of Brain Science and Brain Medicine, Zhejiang University School of Medicine</institution>, <addr-line>Hangzhou</addr-line>, <country>China</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Walter Adriani, National Institute of Health (ISS), Italy</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Stefano Gaburro, Independent Researcher, Buguggiate, Italy; Antonio Jorge Fontenele, Federal University of Rio Grande do Norte, Brazil</p>
      </fn>
      <corresp id="c001">*Correspondence: Xinjian Li <email>lxjbio@zju.edu.cn</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Individual and Social Behaviors, a section of the journal Frontiers in Behavioral Neuroscience</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>28</day>
      <month>10</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>15</volume>
    <elocation-id>750894</elocation-id>
    <history>
      <date date-type="received">
        <day>31</day>
        <month>7</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>24</day>
        <month>9</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2021 Sun, Lyu, Cai, Yu, Sun, Schriver, Gao and Li.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Sun, Lyu, Cai, Yu, Sun, Schriver, Gao and Li</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Behavioral measurement and evaluation are broadly used to understand brain functions in neuroscience, especially for investigations of movement disorders, social deficits, and mental diseases. Numerous commercial software and open-source programs have been developed for tracking the movement of laboratory animals, allowing animal behavior to be analyzed digitally. <italic toggle="yes">In vivo</italic> optical imaging and electrophysiological recording in freely behaving animals are now widely used to understand neural functions in circuits. However, it is always a challenge to accurately track the movement of an animal under certain complex conditions due to uneven environment illumination, variations in animal models, and interference from recording devices and experimenters. To overcome these challenges, we have developed a strategy to track the movement of an animal by combining a deep learning technique, the You Only Look Once (YOLO) algorithm, with a background subtraction algorithm, a method we label DeepBhvTracking. In our method, we first train the detector using manually labeled images and a pretrained deep-learning neural network combined with YOLO, then generate bounding boxes of the targets using the trained detector, and finally track the center of the targets by calculating their centroid in the bounding box using background subtraction. Using DeepBhvTracking, the movement of animals can be tracked accurately in complex environments and can be used in different behavior paradigms and for different animal models. Therefore, DeepBhvTracking can be broadly used in studies of neuroscience, medicine, and machine learning algorithms.</p>
    </abstract>
    <kwd-group>
      <kwd>movement tracking</kwd>
      <kwd>behavioral assessment</kwd>
      <kwd>deep learning</kwd>
      <kwd>YOLO</kwd>
      <kwd>background subtraction</kwd>
    </kwd-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="1"/>
      <equation-count count="0"/>
      <ref-count count="40"/>
      <page-count count="10"/>
      <word-count count="7449"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>Introduction</title>
    <p>Behavior measurement and evaluation is one of the key methods to understand brain functions in neuroscience, especially with respect to movement and social behaviors. Different behavior paradigms (e.g., treadmill, open field, y maze, water maze, elevated plus maze, and three-chambered maze) have been developed and used to evaluate the movement, anxiety, social behavior, disease development, sleep disorder, the effect of medication, etc., of an animal (Feng et al., <xref rid="B9" ref-type="bibr">2020</xref>; Yu et al., <xref rid="B40" ref-type="bibr">2020</xref>). More importantly, with technical developments in electrophysiological recording, optical imaging, and optogenetics manipulating in freely behaving animals, we can study brain function in neural microcircuits. To understand the behavior of an animal systematically, it is essential to accurately and quickly quantify the movement of the animal (e.g., direction, speed, distance, and range of motion). However, due to the complexity of laboratory conditions and interference from the camera or other experimental devices used in behavioral recording, it is a significant challenge to track animal locomotion efficiently and precisely.</p>
    <p>Given the importance of movement tracking of laboratory animals, numerous open-source programs and commercial systems have been developed for recording and analyzing animal behavior, such as Limelight (Actimetrics, USA) (Jimenez et al., <xref rid="B21" ref-type="bibr">2018</xref>; Ishii et al., <xref rid="B20" ref-type="bibr">2019</xref>; Takemoto and Song, <xref rid="B35" ref-type="bibr">2019</xref>), ANY-maze (Stoelting Co, USA) (Morin and Studholme, <xref rid="B25" ref-type="bibr">2011</xref>; Rodrigues et al., <xref rid="B32" ref-type="bibr">2019</xref>; Feng et al., <xref rid="B9" ref-type="bibr">2020</xref>; Scarsi et al., <xref rid="B34" ref-type="bibr">2020</xref>), Ethovision® XT (Noldus, The Netherlands) (Noldus et al., <xref rid="B27" ref-type="bibr">2001</xref>; Yu et al., <xref rid="B40" ref-type="bibr">2020</xref>), TopScan (Clever Sys Inc., USA) (Grech et al., <xref rid="B14" ref-type="bibr">2019</xref>; Griffiths et al., <xref rid="B15" ref-type="bibr">2019</xref>), Super-maze (Shanghai Xinruan information Technology Co., China) (Hao et al., <xref rid="B17" ref-type="bibr">2012</xref>; Qiao et al., <xref rid="B29" ref-type="bibr">2017</xref>), and others (Samson et al., <xref rid="B33" ref-type="bibr">2015</xref>; Gulyás et al., <xref rid="B16" ref-type="bibr">2016</xref>; Bello-Arroyo et al., <xref rid="B3" ref-type="bibr">2018</xref>; Hewitt et al., <xref rid="B18" ref-type="bibr">2018</xref>). In previous studies, motion tracking in videos captured by the video camera is the most common and low-cost approach to achieve tracking of multiple parameters. Most tracking methods are based on background subtraction algorithms and were developed for rodents. With such an algorithm, an accurate route map can be calculated and drawn based on the contour of targets in a high-definition video image. However, these algorithms are subject to breakdown as both experimental paradigms and laboratory conditions become more complex. In addition, modern technical methods such as electrophysiological recording, optical imaging, and optical stimulation are now widely used with freely behaving animals. High background noise becomes a significant problem, making it difficult to quantify the movement of an animal. The use of background subtraction algorithms alone often cannot effectively separate the target from the high background noise. To overcome these challenges, many alternative methods, such as microwave Doppler radar (Giansanti et al., <xref rid="B11" ref-type="bibr">2005</xref>) and RFID technology (Lewejohann et al., <xref rid="B23" ref-type="bibr">2009</xref>; Catarinucci et al., <xref rid="B4" ref-type="bibr">2014</xref>), have been proposed for tracking animal motion. However, those systems tend to involve additional devices attached to the head of an animal which may be unstable or have a negative influence on the flexibility of the movement of the animal. Also, those systems are expensive and difficult to modify by the user because of high integration and low flexibility.</p>
    <p>Recently, researchers in the field of computer vision have advanced a number of algorithms to process image data, including some novel solutions for the detection of moving animals and humans. Some machine learning algorithms have shown high precision in object detection, such as deformable parts models (Felzenszwalb et al., <xref rid="B8" ref-type="bibr">2010</xref>; Unger et al., <xref rid="B36" ref-type="bibr">2017</xref>), R-CNN (Girshick et al., <xref rid="B12" ref-type="bibr">2014</xref>), and deep neural networks (Geuther et al., <xref rid="B10" ref-type="bibr">2019</xref>; Yoon et al., <xref rid="B39" ref-type="bibr">2019</xref>). Using those algorithms, several toolboxes were developed for precisely calculating the postures of laboratory animals during movements, such as DeepLabCut (Mathis et al., <xref rid="B24" ref-type="bibr">2018</xref>; Nath et al., <xref rid="B26" ref-type="bibr">2019</xref>), LEAP (Wang et al., <xref rid="B38" ref-type="bibr">2017</xref>; Pereira et al., <xref rid="B28" ref-type="bibr">2019</xref>), DeepPoseKit (Graving et al., <xref rid="B13" ref-type="bibr">2019</xref>), TRex (Walter and Couzin, <xref rid="B37" ref-type="bibr">2021</xref>), and DANNCE (Dunn et al., <xref rid="B7" ref-type="bibr">2021</xref>; Karashchuk et al., <xref rid="B22" ref-type="bibr">2021</xref>), which greatly simplify and speed up the analysis of multiple behaviors. Although these algorithms may be used to track the gross movement of an animal, they are time-consuming and insufficiently accurate for our purposes because the exact centers cannot be obtained in the process of creating a training dataset.</p>
    <p>YOLO is a new generation of deep learning algorithm based on convolutional neural networks (CNN) for object detection (Redmon et al., <xref rid="B30" ref-type="bibr">2016</xref>; Redmon and Farhadi, <xref rid="B31" ref-type="bibr">2017</xref>). Compared with R-CNN, a previous detection algorithm that selects a region of interest (ROI) for possible targets and then identifies targets by classification, YOLO transforms the detection process to a regression problem, predicting the coordinates of the bounding box of the target and classifying the probabilities (<italic toggle="yes">p</italic>-value) of the target directly from the full image through a single network, making it easier to optimize for better performance. Using YOLO, we can predict both the species and locations of experimental animal subjects in a video. However, YOLO only provides the position of an area around the animal (bounding box) instead of the actual position of the animal. The range or position of the bounding box may change abruptly between two sequential frames of the video, even with subtle animal movement. Therefore, it is also difficult to accurately track the position of an animal using only YOLO. Considering the advantages and disadvantages of previous algorithms, we postulated that if YOLO and the background subtraction method were combined, the animal motion could be tracked more accurately and efficiently.</p>
    <p>In this study, we propose a laboratory animal behavior tracking method named “DeepBhvTracking” based on both a deep learning algorithm (YOLO) and a background subtraction algorithm. To successfully track animal motion, we first obtain the approximate location of the experimental animal by drawing a bounding box using YOLO, and then, we measure the position of an animal based on the background subtraction algorithm. With our method, movement can be tracked under complex conditions accurately and quickly. All codes with respect to DeepBhvTracking are open-source; the scripts can be customized, and different experimental animal detection models can be easily trained. Overall, DeepBhvTracking is a widely applicable and high-powered behavior tracking method for laboratory animals.</p>
  </sec>
  <sec sec-type="materials and methods" id="s2">
    <title>Materials and Methods</title>
    <sec>
      <title>Materials</title>
      <p>All experimental procedures were approved by the Animal Use and Care Committee of Zhejiang University following the National Institutes of Health (NIH) guidelines. Adult wild-type C57BL/6 mice (<italic toggle="yes">n</italic> = 6) were used for most experiments. For movement comparison, mice with <italic toggle="yes">PRRT2</italic> (<italic toggle="yes">n</italic> = 6) and <italic toggle="yes">FMR1</italic> (<italic toggle="yes">n</italic> = 6) mutations were used. Our tracking method was also tested in adult common marmosets (<italic toggle="yes">n</italic> = 3).</p>
      <p>For data storage and processing, a high-performance computer (Dell, USA) was used (CPU Intel(R) Core (TM) i7-10700 CPU @ 2.90 GHz, RAM 64 GB, GPU Inter(R) UHD Graphics 630 8 GB). The DeepBhvTracking program was run within MATLAB R2020a with deep learning toolbox, computer vision toolbox, and pretrained deep neural networks: resnet18, mobilenetv2, and resnet50.</p>
      <p>In this study, all test videos were taken using a standard webcam (1,080 p for origin videos). For fast processing speed, original videos were read and resized to the resolution of 360<sup>*</sup>640<sup>*</sup>3 pixels for further analysis.</p>
    </sec>
    <sec>
      <title>Methods</title>
      <sec>
        <title>Model Training</title>
        <list list-type="order">
          <list-item>
            <p>Image labeling: Because YOLO is a supervised algorithm, manually labeled images are required for model training (Redmon et al., <xref rid="B30" ref-type="bibr">2016</xref>). To provide training data using a wide range of animal behaviors, images with RGB format were extracted randomly from videos, and a rectangular region around the animal was marked in each image using the Image Label App in the computer vision toolbox of MATLAB. The details of using the Image Label App are available online at <ext-link xlink:href="https://www.mathworks.com/help/vision/ref/imagelabeler-app.html" ext-link-type="uri">https://www.mathworks.com/help/vision/ref/imagelabeler-app.html</ext-link>. After testing under laboratory conditions, we found that around 300 labeled images were usually required for accurate detection under each condition.</p>
          </list-item>
          <list-item>
            <p>Image preparation: To test prediction accuracy, the dataset of labeled images was randomly divided into three sets: training (70%), validation (10%), and test (20%). Labeled images in the training set were transformed and resized, including color distortion and information dropping for broad adaptability. The original validation and test sets were retained to evaluate the accuracy of the model.</p>
          </list-item>
          <list-item>
            <p>Detector training: In this task, a pretrained deep neural network was transferred and combined with the YOLO algorithm for target detection. For YOLO, the images were normalized to the same resolution for feature extraction. Previous studies indicated that the normalized image size and differences among the pretrained networks have a significant impact on the accuracy and speed of training and tracking. To test this possibility, different pretrained networks including resnet18, mobilenetv2, and resnet50 were tested for each of the following normalized image sizes: 224<sup>*</sup>224<sup>*</sup>3, 320<sup>*</sup>320<sup>*</sup>3, 416<sup>*</sup>416<sup>*</sup>3, and 512<sup>*</sup>512<sup>*</sup>3. In this task, the detector was trained by mini-batch gradient descent (batch size is 16 frames), and the parameters of the network were updated after several iterations via back-propagation. The number of total epochs is 20 and the learning rate is 0.0001. Detailed principles and algorithm derivation follow from previous studies (Redmon et al., <xref rid="B30" ref-type="bibr">2016</xref>; Redmon and Farhadi, <xref rid="B31" ref-type="bibr">2017</xref>). After training, the detector was used for tracking evaluations.</p>
          </list-item>
        </list>
      </sec>
      <sec>
        <title>Video Tracking</title>
        <p>The position of an animal was tracked by combining the deep learning algorithm YOLO with a background subtraction algorithm. Our strategy was to define the bounding box of the target using YOLO and then to obtain the centroid of the target by background subtraction inside the bounding box. Background subtraction tracked moving animals through a pixel-by-pixel comparison of the present image with a background image, as described in detail by others (Barnich and Van Droogenbroeck, <xref rid="B2" ref-type="bibr">2011</xref>).</p>
        <p>First, to avoid interference from objects outside the maze, we manually defined the tracking area and set areas outside the tracking area to the background color. Second, the detector trained by YOLO was used to track the position of the animal with a bounding box. In many cases, multiple boxes were detected in one image. In this case, the bounding box with the highest <italic toggle="yes">p</italic>-value was chosen for future use. Next, the bounding box was enlarged 1.5 times to completely cover the whole animal. Finally, a traditional background subtraction method was used to obtain the contour of the animal in the bounding box, and the centroid of the animal was calculated based on the contour.</p>
      </sec>
      <sec>
        <title>Laboratory Animal Tested by DeepBhvTracking</title>
        <p>To evaluate the effectiveness of our tracking method—DeepBhvTracking, black or white mice and marmosets were tested in different behavior paradigms: open field, elevated plus maze, L maze, inverted V-shape maze, and treadmill. We also compared the performance of four tracking methods (background subtraction, YOLO detection, DeepLabCut, and DeepBhvTracking) in three classical behavior paradigms with different noise levels: open field, L maze, and three-chambered maze. Open field is a high signal-to-noise ratio scenario without the interference of wires or operation of an experimenter; in contrast, L maze involves interference from electric wires and hands of the experimenter because of behavior training and calcium imaging. Three-chambered maze is a low signal-to-noise ratio scenario due to the similar color between mice and background. To avoid bias from the training dataset on the performance of different algorithms, 300 images extracted from six videos based on a K-means algorithm were used to train the models of the three deep learning methods: YOLO detection, DeepLabCut, and DeepBhvTracking. In this study, we also compared movement differences among movement-deficit mice (<italic toggle="yes">PRRT2, FMR1</italic>) and wild-type mice. The movement of each animal was recorded for 8 min and tracked by DeepBhvTracking.</p>
      </sec>
      <sec>
        <title>Comparison With Other Methods</title>
        <p>To test tracking efficiency, we compared the training time, tracking speed, error to ground truth, and movement speed of animals detected by the different methods. To compare training speed, both DeepLabCut and DeepBhvTracking models were trained using the same dataset, pretrained neural network (resnet50), and parameters (image number: 300; batch size: 8; iterations: 2000). Tracking speed reflected the video processing speed (frames/second, fps). Error to ground truth was used to estimate the distance between the real location and the estimated location of the target.</p>
      </sec>
    </sec>
    <sec>
      <title>Statistical Analysis</title>
      <p>Error bars in all figures represent mean ± SEM, and the number (<italic toggle="yes">n</italic>) of samples employed is indicated in the legends. All data were analyzed by ANOVA followed by LSD test for multiple comparisons, <sup>*</sup> indicates <italic toggle="yes">p</italic> &lt; 0.05, <sup>**</sup> indicates <italic toggle="yes">p</italic> &lt; 0.01, <sup>***</sup> indicates <italic toggle="yes">p</italic> &lt; 0.001.</p>
      <p>The codes in this manuscript are open-source, and the detailed list is shown in <xref rid="T1" ref-type="table">Table 1</xref>. Users can download the codes at GitHub online (<ext-link xlink:href="https://github.com/SunGL001/DeepBhvTracking" ext-link-type="uri">https://github.com/SunGL001/DeepBhvTracking</ext-link>).</p>
      <table-wrap position="float" id="T1">
        <label>Table 1</label>
        <caption>
          <p>Detailed list of codes related to DeepBhvTracking.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Name</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Description</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>Training</bold>
              </td>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic toggle="yes">dbt_dataset</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">Get datasets for training</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic toggle="yes">dbt_training</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">Training a detector</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic toggle="yes">augmentData</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">Supporting function used in <italic toggle="yes">dbt_training</italic> for data augmentation</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic toggle="yes">preprocessData</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">Supporting function used in <italic toggle="yes">dbt_training</italic> for resizing image and bounding boxes to the target size</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>Tracking</bold>
              </td>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic toggle="yes">dbt_singleTracking</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">Tracking for single file</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic toggle="yes">dbt_batchTracking</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">Tracking for batch files</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic toggle="yes">dbt_manualTracking</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">Manual click tracking undetected and incorrect frames after tracking</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic toggle="yes">dbt_bhvread</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">Supporting function for read video data, supporting for multiple formats</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic toggle="yes">dbt_createLabeledVideo</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">Create the labeled video</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic toggle="yes">dbt_optimize</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">Save undetected and incorrect frames for optimizing model</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>Results</title>
    <sec>
      <title>Training and Tracking Optimization</title>
      <p>In this task, the position of an animal was tracked by combining pretrained neural networks with the YOLO algorithm and background subtraction (<xref rid="F1" ref-type="fig">Figure 1</xref>). To accurately track the position of different kinds of animals, a well-trained detector was required. We found that the training time was longer, and the tracking accuracy decreased if we trained the model on different kinds of animals together (data not shown). Additionally, we found that the tracking accuracy was highly correlated with the color of the animals and uncorrelated with behavior paradigms. So, we trained the detectors separately for different kinds of animals, namely, white mice, black mice, and marmosets (<xref rid="SM2" ref-type="supplementary-material">Supplementary Table 1</xref>). Initially, the tracking accuracy was low due to the training dataset that was randomly chosen; those images did not accurately represent the complex postures of the animals. To address this, we added a feedback method to merge undetected images in the training dataset for a better detector. After several iterations, an improved detector was achieved (<xref rid="F1" ref-type="fig">Figure 1A</xref>). We used 1,991 labeled images from different behavior assays for detector training in black mice, 1,458 images in white mice, and 400 images in marmosets (<xref rid="SM2" ref-type="supplementary-material">Supplementary Table 1</xref>).</p>
      <fig position="float" id="F1">
        <label>Figure 1</label>
        <caption>
          <p>Diagram image showing workflow of DeepBhvTracking. <bold>(A)</bold> Detector training workflow combining a pretrained deep neural network and YOLO algorithm. The parameters were updated via backpropagation. If the detector is not satisfied, wrong images were labeled, and merged into pretrained dataset for detector training again. <bold>(B)</bold> Target tracking workflow. Left, detecting the bounding box using YOLO deep learning method. Right, obtaining centroid with background subtraction method. <bold>(C)</bold> Schematic diagrams showing the tracking process of one frame from an open-field video. The numbers in <bold>C</bold> match the corresponding number in the tracking workflow of <bold>(B)</bold>.</p>
        </caption>
        <graphic xlink:href="fnbeh-15-750894-g0001" position="float"/>
      </fig>
      <p>During tracking, we found that the deep learning algorithms only provided a bounding box around the target, where the tracking center is the center of the bounding box instead of that of the animal. Moreover, occasionally multiple bounding boxes were obtained or the bounding box did not completely cover the animal, causing the location of the tracking center to change abruptly, resulting in a discontinuous motion trace after analysis. To overcome these limitations, we first detected the bounding box of the target by YOLO at a low threshold. Then, we enlarged the bounding box to completely cover the animal. Third, the contour of the animal was calculated based on background subtraction in the region of the enlarged bounding box (<xref rid="F1" ref-type="fig">Figures 1B</xref> left,<xref rid="F1" ref-type="fig">C</xref>). Last, the centroid of the animal was determined from the center of the contour (<xref rid="F1" ref-type="fig">Figures 1B</xref> right,<xref rid="F1" ref-type="fig">C</xref>).</p>
      <p>Three pretrained deep neural networks were evaluated with different image sizes. We found the training time increased with increasing image size (<xref rid="F2" ref-type="fig">Figure 2A</xref>). Of the three neural networks, resnet50 took the longest time at all image sizes (<xref rid="F2" ref-type="fig">Figure 2A</xref>) during detector training (two-way ANOVA, <italic toggle="yes">F</italic> = 360.75, <italic toggle="yes">p</italic> &lt; 0.001). In the detection step, tracking speed (two-way ANOVA, <italic toggle="yes">F</italic> = 92.93, <italic toggle="yes">p</italic> &lt; 0.001) and accuracy (two-way ANOVA, <italic toggle="yes">F</italic> = 197.00, <italic toggle="yes">p</italic> &lt; 0.001) were highly correlated with image size (<xref rid="F2" ref-type="fig">Figure 2B</xref>, see legend). Compared with other networks, resnet50 showed higher precision, and resnet18 showed faster processing speed (<xref rid="F2" ref-type="fig">Figure 2B</xref>). Considering the tradeoff between speed and precision, we set the resnet50 at a resolution of 480<sup>*</sup>480<sup>*</sup>3 pixels (one of the preset parameters used during detector training) as the pretrained deep neural network for constructing our detector. However, resnet18 at 512<sup>*</sup>512<sup>*</sup>3 pixels was also a potentially useful network for simple scenarios (e.g., mice in open-field maze) because it had a high detection speed at relatively high precision. The number of training images used in those three detectors is shown in <xref rid="SM2" ref-type="supplementary-material">Supplementary Table 1</xref>.</p>
      <fig position="float" id="F2">
        <label>Figure 2</label>
        <caption>
          <p>Performance comparison of three pretrained deep neural networks. <bold>(A)</bold> Comparison of training time with different image sizes for three neural networks. Training time increased with image size for all networks. Of the three, resnet50 took the longest time for all image sizes. <bold>(B)</bold> Detection precision plot against training speed for three neural networks at different image sizes. Data point diameter in <bold>(B)</bold> represents the image size from 224 (smallest), 320, 416, and 512 pixels (largest). Note that because of over-fitting, resnet50 is unable to train at an image size of 512 pixels; 480 × 480 images were used instead. Detection precision increased for all networks with increasing image size. In our training model, resnet50 at 480 pixels was used for all conditions.</p>
        </caption>
        <graphic xlink:href="fnbeh-15-750894-g0002" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>Smooth Movement Map Tracked by DeepBhvTracking</title>
      <p><italic toggle="yes">In vivo</italic> imaging and electrophysiological recording in freely behaving animals are widely used to understand the neural mechanisms of a particular behavior. Inherent with these techniques are human interference and recording wires that may be captured by the video camera during motion tracking. To address these issues, we compared the tracking accuracy and speed for four tracking methods in three tasks with different kinds of environmental noise (<xref rid="F3" ref-type="fig">Figure 3</xref>). In these tasks, the open field is a simple task without other observable interference; the L maze involves wire and hand image interference due to the wire cable of freely moving calcium imaging; and the three-chambered maze involves conditions with a white mouse in a brightly lit room light which resulted in low contrast between target and environment (<xref rid="F3" ref-type="fig">Figure 3</xref>).</p>
      <fig position="float" id="F3">
        <label>Figure 3</label>
        <caption>
          <p>Comparison of three different tracking results for the same video during calcium imaging in a freely behaving mouse. <bold>(A)</bold> Left: Tracking result of one example image with background subtraction only. Black region in red box indicates the detected pixels. Red dot indicates the center of the target. Due to the similar color between the wire and the target animal, the wire has been detected after removing the background. With this method, the target center is marked outside of the tracking area. Right: Blue line represents the resulting movement trace of a mouse in a 5,000 frame video. <bold>(B)</bold> Movement trace tracked by YOLO only. There are many abnormal movements (red arrow) in the tracking trace due to the random jumping of the bounding box. The number above the mouse is the <italic toggle="yes">p</italic>-value predicted by YOLO. <bold>(C)</bold> Movement trace tracked by DeepLabCut. Three of theframes failed to detect according to the trajectory. <bold>(D)</bold> Movement trace tracked by DeepBhvTracking. The movement trace is smooth and represents the actual movement of the animal. <bold>(E–H)</bold> Movement trace in the three-chambered task tracked by background subtraction, YOLO detection, DeepLabCut, and DeepBhvTracking, respectively. <bold>(I)</bold> Comparison of training time between two algorithms. Both models are trained using the same dataset, pretrained neural network, and parameters. Image number: 300; neural network: resnet50; batch size: 8; iterations: 2,000. <bold>(J)</bold> Comparison of tracking time of same video between four algorithms. There are significant differences in processing speed between the four tracking methods. <bold>(K)</bold> Comparison of error to ground truth of four methods in different behavior assays. In these tasks: open field is a simple task without other observable interference; L maze involves wire interference due to the wire cable of free-moving calcium imaging; three-chambered maze involves the condition with a white mouse in bright room light. The sample size for each paradigm is six videos. DeepBhvTracking performed well in all paradigms. The original data are shown in <xref rid="SM3" ref-type="supplementary-material">Supplementary Tables 2–</xref><xref rid="SM5" ref-type="supplementary-material">4</xref>. *<italic toggle="yes">p</italic> &lt; 0.05, **<italic toggle="yes">p</italic> &lt; 0.01, ***<italic toggle="yes">p</italic> &lt; 0.001, tested by ANOVA with LSD test. <bold>(L)</bold> Comparison of movement speed of four methods in different behavior assays.</p>
        </caption>
        <graphic xlink:href="fnbeh-15-750894-g0003" position="float"/>
      </fig>
      <p>To test training efficiency, we first compared the training time and tracking speed between DeepLabCut and DeepBhvTracking using the same dataset, pretrained neural network (resnet50), and parameters (image number: 300; batch size: 8; iterations: 2000). DeepLabCut showed a slower training speed than YOLO during both training stages (<xref rid="F3" ref-type="fig">Figure 3I</xref>, two-way ANOVA followed by Bonferroni's test, <italic toggle="yes">p</italic> = 0.029) and tracking stage algorithms that are based on deep learning (<xref rid="F3" ref-type="fig">Figure 3J</xref>, two-way ANOVA, <italic toggle="yes">p</italic> &lt; 0.001) with the same computer environments. First, in high background noise conditions, we found that obvious tracking errors were obtained with the background subtraction method alone and that more than 1/10 of frames in one video were required for manual tracking (<xref rid="F3" ref-type="fig">Figures 3A,E</xref>). Using this method alone, the target mouse could be marked outside of the tracking area in some frames due to erroneous calculation of the center based on detected artifacts (<xref rid="F3" ref-type="fig">Figure 3A</xref>). Second, we found that the bounding box of the position of an animal can be easily captured using YOLO with better performance (<xref rid="F3" ref-type="fig">Figures 3B,F</xref>). However, calculating the center of the bounding does not accurately reflect the position of the animal as shown by obviously abnormal motion in the trace (<xref rid="F3" ref-type="fig">Figures 3B,F</xref>, red arrows). Clearly, there are multiple rectangles in the tracking trace which arise from the rapid reorientation of the bounding box. DeepLabCut tracked the center of mice directly and performed well in the three-chambered task (<xref rid="F3" ref-type="fig">Figure 3G</xref>), but there are multiple incorrect frames detected in L-maze which arise from the periodic detection of the hand of the experimenter (<xref rid="F3" ref-type="fig">Figure 3C</xref>). Also, this method cannot exclude systematic error introduced during training dataset preparation (human-defined centroid of the animal). Using DeepBhvTracking, the movement trace is smooth and most accurately represents the actual movement of the animal (<xref rid="F3" ref-type="fig">Figures 3D,H</xref>).</p>
      <p>Statistically, compared with DeepLabCut, we found that errors to ground truth, which was used to estimate the distance between the real location and the estimated location of the target, decreased both in the open field (<xref rid="F3" ref-type="fig">Figure 3K</xref>, one-way ANOVA, <italic toggle="yes">F</italic> = 23.93, <italic toggle="yes">p</italic> &lt; 0.001; <xref rid="F3" ref-type="fig">Figure 3L</xref>, one-way ANOVA, <italic toggle="yes">F</italic> = 7.886, <italic toggle="yes">p</italic> = 0.001) and L-maze (<xref rid="F3" ref-type="fig">Figure 3K</xref>, L maze, one-way ANOVA, <italic toggle="yes">F</italic> = 10.70, <italic toggle="yes">p</italic> &lt; 0.001) conditions. Movement speed was lowest in the open field and L maze (<xref rid="F3" ref-type="fig">Figure 3L</xref>, <italic toggle="yes">p</italic> &lt; 0.01) compared with the other three methods. While the YOLO detection algorithm could avoid the interference of wire and hand, the trajectory was not smooth enough because the center of the bounding box does not represent the center of animals (<xref rid="F3" ref-type="fig">Figures 3B,F,K</xref>). DeepLabCut and DeepBhvTracking have similar performances in the three-chambered maze (<xref rid="F3" ref-type="fig">Figure 3L</xref>, LSD test, <italic toggle="yes">p</italic> = 0.871). However, errors to ground truth tracked by DeepLabCut were higher than DeepBhvTracking (LSD test, <italic toggle="yes">p</italic> = 0.040) in the L maze; this may be due to the inaccurate center position of the animal during training dataset construction. In summary, DeepBhvTracking can provide a relatively precise tracking result with fast processing speed in a variety of paradigms (<xref rid="F3" ref-type="fig">Figure 3</xref>).</p>
    </sec>
    <sec>
      <title>Widely Applicable Tracking in Different Paradigms and Animal Models by DeepBhvTracking</title>
      <p>To check the applicability and flexibility of DeepBhvTracking in different paradigms, black C57BL/6 mice were tested in different environments including open field (<xref rid="SM1" ref-type="supplementary-material">Supplementary Figure 1A</xref>), L maze (<xref rid="F3" ref-type="fig">Figure 3C</xref>), treadmill (<xref rid="F4" ref-type="fig">Figure 4A</xref>), elevated plus maze (<xref rid="F4" ref-type="fig">Figure 4B</xref>), and inverted V-shape maze (<xref rid="F4" ref-type="fig">Figure 4D</xref>). We obtained smooth movement traces for all conditions (<xref rid="F3" ref-type="fig">Figures 3C</xref>, <xref rid="F4" ref-type="fig">4A,B,D</xref>; <xref rid="SM1" ref-type="supplementary-material">Supplementary Figure 1A</xref>). It is worth noting that DeepBhvTracking achieved good performance even in a low target-to-background contrast such as black mice on treadmill (<xref rid="F4" ref-type="fig">Figure 4A</xref>) or white mice in a white environment (<xref rid="F4" ref-type="fig">Figure 4C</xref>). Moreover, in the treadmill assay, animals run only in a restricted area because it will be punished by an electric shock if it falls behind the treadmill. It is usually very difficult to calculate the movement speed of the animal when performing neuronal decoding. Our DeepBhvTracking method overcomes this challenge and achieves a smooth movement trace in treadmill conditions. White mice were trained separately and were tracked in a three-chambered box (<xref rid="F4" ref-type="fig">Figure 4C</xref>) and open field (<xref rid="F4" ref-type="fig">Figure 4F</xref>). Accurate movement tracking was achieved for both conditions. In addition, the movement of marmosets in a 1 m<sup>3</sup> home cage was tested and a clear movement map was achieved by DeepBhvTracking. Finally, two animals were tracked by labeling each animal with a different color sticker during video recording (<xref rid="F4" ref-type="fig">Figure 4F</xref> blue and orange trace), which indicated our method may also be adapted to social behavior analysis. Hence, DeepBhvTracking is easy and feasible to use with different animal models and different behavior paradigms.</p>
      <fig position="float" id="F4">
        <label>Figure 4</label>
        <caption>
          <p>Movement tracking in diverse paradigms using DeepBhvTracking. <bold>(A)</bold> Black mice in treadmill task. With a black mouse on a black background, the ratio between signal and noise is very low. Moreover, the animal only runs in a restricted area because it has been trained not to fall behind the treadmill. It is very difficult to calculate the speed of the animal while performing neuronal decoding in this task. Our DeepBhvTracking method overcomes this challenge and achieves a smooth movement trace. <bold>(B)</bold> Black mice in elevated plus maze. <bold>(C)</bold> White mice in three-chambered box maze. <bold>(D)</bold> Black mice in inverted V-shape maze. <bold>(E)</bold> Marmoset movement trace in home cage. <bold>(F)</bold> Social behavior of white mice in open field. Blue trace: White mouse with a blue mark on its back. Orange trace: White mouse with a green mark on its back.</p>
        </caption>
        <graphic xlink:href="fnbeh-15-750894-g0004" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>DeepBhvTracking Can Be Used to Test Movement and Emotion Deficits</title>
      <p>The open-field test is one of the most widely used paradigms for assessing locomotor activity and anxiety in rodents. To further test the effectiveness of our tracking method, we performed open-field tests in C57BL/6 wild-type mice and in two widely used movement-deficit mutant animals: <italic toggle="yes">PRRT2</italic> (Chen et al., <xref rid="B5" ref-type="bibr">2011</xref>) and <italic toggle="yes">FMR1</italic> (Baba and Uitti, <xref rid="B1" ref-type="bibr">2005</xref>) (<xref rid="SM1" ref-type="supplementary-material">Supplementary Figure 1</xref>). For each animal, we first draw out the movement trace of the animal achieved by DeepBhvTracking (<xref rid="SM1" ref-type="supplementary-material">Supplementary Figure 1A</xref>) and manually checked for no frame losses. Then, spatial pseudo heat maps of movement time (<xref rid="SM1" ref-type="supplementary-material">Supplementary Figure 1B</xref>) and speed (<xref rid="SM1" ref-type="supplementary-material">Supplementary Figure 1C</xref>) were calculated. We found that animals stayed longer at the corners than the central area and run faster in the middle of the open field (<xref rid="SM1" ref-type="supplementary-material">Supplementary Figures 1B,E</xref>). Moreover, both mutants ran faster in the open field than the wild-type animals (<xref rid="SM1" ref-type="supplementary-material">Supplementary Figure 1D</xref>, one-way ANOVA, <italic toggle="yes">F</italic> = 4.356, <italic toggle="yes">p</italic> = 0.025; <xref rid="SM1" ref-type="supplementary-material">Supplementary Figure 1F</xref>, two-way ANOVA, <italic toggle="yes">F</italic> = 16.15, <italic toggle="yes">p</italic> &lt; 0.001). Open field can also be used to test the anxiety level base of an animal on the time spent in the corner or center. Based on our tracking method, mice with <italic toggle="yes">PRRT2</italic> mutants stayed at the corner for a shorter time than wild animals (<xref rid="SM1" ref-type="supplementary-material">Supplementary Figure 1E</xref>, two-way ANOVA, <italic toggle="yes">F</italic> = 18.11, <italic toggle="yes">p</italic> &lt; 0.001). These results may indicate a low anxiety level for <italic toggle="yes">PRRT2</italic> mutants in our open field conditions. Further experiments should be performed to confirm this result.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>Discussion</title>
    <p>Accurate behavioral measurement and evaluation are the key steps for pharmacology, neuroscience, and psychological studies. However, commercially available software and open-source programs have many limitations, especially when the experiments are performed under complex environmental conditions. To overcome these difficulties, we designed DeepBhvTracking to track the position of an animal combining deep learning with the YOLO algorithm and background subtraction using the widely used software MATLAB. By incorporating the YOLO detection algorithm, the detection effectiveness is improved by generating a bounding box of the tracked animal (<xref rid="F3" ref-type="fig">Figures 3B,D,F,H</xref>). Simultaneously, background subtraction was applied in the bounding box to acquire an exact location of the animal, which corrects for the slight position deviation inherent in YOLO alone (<xref rid="F3" ref-type="fig">Figure 3</xref>).</p>
    <p>We have previously used several commercial software packages: Limelight, ANY-maze, and open-source programs. Although they have superior GUIs, the accuracy is insufficient in dim light and complex environments, and they are confounded by interruption of the recording process, for example, a human hand. Time-consuming manual tracking is required under these circumstances. In addition, the software can only be used under certain predefined conditions and is exceedingly difficult to modify for new environments. Recently, several algorithms, such as DeepLabCut (Mathis et al., <xref rid="B24" ref-type="bibr">2018</xref>; Nath et al., <xref rid="B26" ref-type="bibr">2019</xref>), LEAP (Wang et al., <xref rid="B38" ref-type="bibr">2017</xref>; Pereira et al., <xref rid="B28" ref-type="bibr">2019</xref>), and DeepPoseKit (Graving et al., <xref rid="B13" ref-type="bibr">2019</xref>), based on deep learning have been developed to estimate the posture of an animal during movement. Undoubtedly, those methods can obtain detailed movement information about the targets and have been broadly used in multiple studies (Dooley et al., <xref rid="B6" ref-type="bibr">2020</xref>; Huang et al., <xref rid="B19" ref-type="bibr">2021</xref>). However, these methods have intrinsic limitations to accurate estimation of the centroid of irregular animal targets during training dataset preparation (human-defined center of an animal). Also, tracking speed is very slow with those methods (<xref rid="F3" ref-type="fig">Figure 3J</xref>). Although DeepBhvTracking is also a supervised algorithm, we used background subtraction to correct the systematic error of training dataset preparation. So, DeepBhvTracking is stable, more accurate (<xref rid="F3" ref-type="fig">Figure 3</xref>), less susceptible to background noise, and suitable for different kinds of animals and behavior paradigms (<xref rid="F4" ref-type="fig">Figure 4</xref>). Furthermore, using a feedback training strategy, one can easily improve the detector by adding more labeled images. In addition, DeepBhvTracking also takes advantage of a background subtraction algorithm that defines the centroid of an animal more precisely. With these improvements, we can further increase the tracking accuracy and effectiveness. Finally, DeepBhvTracking is capable of tracking two animals in one video (<xref rid="F4" ref-type="fig">Figure 4F</xref>), if animals are marked with different colors; this indicates that this method is also feasible for the study of social behavior. As a new method, DeepBhvTracking is well-suited to detect multiple types of animals in different scenarios (<xref rid="F4" ref-type="fig">Figure 4</xref>), and it is straightforward to train or optimize the detector according to individual needs. Based on the position tracked by the DeepBhvTracking, the movement distance, the elapsed time, and the speed of the animal can be calculated easily (<xref rid="SM1" ref-type="supplementary-material">Supplementary Figure 1</xref>).</p>
    <p>It is worth noting that DeepBhvTracking can only track the whole-body centroid of an animal; there is no information regarding head direction or body parts. But information of the contour of an animal remains, which makes it possible to define more fine details of an animal, such as the head or tail. In addition, the body parts of animals could be tracked by labeling them with different colors. For example, if we label the nose of a mouse with a red mark and its tail with a green mark, the location of the nose or tail could be tracked by DeepBhvTracking as long as the detector was previously trained to recognize the red and green marks separately.</p>
  </sec>
  <sec sec-type="conclusions" id="s5">
    <title>Conclusion</title>
    <p>We have designed a strategy to track the centroid of an animal combining deep learning with the YOLO algorithm and background subtraction, a tool we call DeepBhvTracking. With this improved method, the motion of laboratory animals can be tracked accurately in a variety of different behavioral paradigms. This in turn offers the potential to speed up many studies in neuroscience, medicine, and so on.</p>
  </sec>
  <sec sec-type="data-availability" id="s6">
    <title>Data Availability Statement</title>
    <p>The data and codes that support the finding of this paper is available at GitHub online (<ext-link xlink:href="https://github.com/SunGL001/DeepBhvTracking" ext-link-type="uri">https://github.com/SunGL001/DeepBhvTracking</ext-link>).</p>
  </sec>
  <sec id="s7">
    <title>Ethics Statement</title>
    <p>The animal study was reviewed and approved by the Animal Experimentation Ethics Committee of Zhejiang University.</p>
  </sec>
  <sec id="s8">
    <title>Author Contributions</title>
    <p>GS, CL, and RC performed the behavioral recording and finished the data analysis. GS and XL wrote the code. RC, CY, HS, and KS carefully read and edited the manuscript. XL designed the experiments and approved the draft. XL and GS wrote the manuscript. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec sec-type="funding-information" id="s9">
    <title>Funding</title>
    <p>This work was supported by the Natural Science Foundation of China (32071097, 31871056, 61703365, and 91732302), the National Key R&amp;D Program of China (2018YFC1005003), and Fundamental Research Funds for the Central Universities (2019XZZX001-01-20 and 2018QN81008). This work was also supported by the MOE Frontier Science Center for Brain Science &amp; Brain-Machine Integration, Zhejiang University.</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s10">
    <title>Publisher's Note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
</body>
<back>
  <ack>
    <p>We thank Xiangyao Li and Zhiying Wu for generously providing the <italic toggle="yes">PRRT2</italic> and <italic toggle="yes">FMR1</italic> mice. We thank Junqing Chen for helping during the code writing process.</p>
  </ack>
  <sec sec-type="supplementary-material" id="s11">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link xlink:href="https://www.frontiersin.org/articles/10.3389/fnbeh.2021.750894/full#supplementary-material" ext-link-type="uri">https://www.frontiersin.org/articles/10.3389/fnbeh.2021.750894/full#supplementary-material</ext-link></p>
    <supplementary-material id="SM1" position="float" content-type="local-data">
      <label>Supplementary Figure 1</label>
      <caption>
        <p>Movement for the mice with different genotypes using DeepBhvTracking. <bold>(A)</bold> Example imaging showing the movement of a C57BL/6 mice in open field, the center, and corner regions indicated by the different shaded squares. <bold>(B)</bold> Pseudo heat map showing the animal motion time in <bold>(A)</bold>. <bold>(C)</bold> Pseudo heat map showing animal speed in <bold>(A)</bold>. <bold>(D)</bold> Comparison of average speed among three genotypes of mice. <bold>(E)</bold> Comparison of the time spent in the corner and center among three genotypes of mice. <bold>(F)</bold> Comparison of average speed at the corner and center among three genotypes of mice. <sup>*</sup> represents <italic toggle="yes">p</italic> &lt; 0.05, <sup>**</sup> indicates <italic toggle="yes">p</italic> &lt; 0.01, tested by the ANOVA with LSD test.</p>
      </caption>
      <media xlink:href="Image_1.TIF">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="SM2" position="float" content-type="local-data">
      <label>Supplementary Table 1</label>
      <caption>
        <p>The number of label images were required for detector training of laboratory animal.</p>
      </caption>
      <media xlink:href="Table_1.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="SM3" position="float" content-type="local-data">
      <label>Supplementary Table 2</label>
      <caption>
        <p>Comparison of tracking time using four tracking methods in different behavior assays.</p>
      </caption>
      <media xlink:href="Table_1.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="SM4" position="float" content-type="local-data">
      <label>Supplementary Table 3</label>
      <caption>
        <p>Comparison of pixels change per frame using four tracking methods in different behavior assays.</p>
      </caption>
      <media xlink:href="Table_1.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="SM5" position="float" content-type="local-data">
      <label>Supplementary Table 4</label>
      <caption>
        <p>Comparison of error to ground-truth using four tracking methods in different behavior assays.</p>
      </caption>
      <media xlink:href="Table_1.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baba</surname><given-names>Y.</given-names></name><name><surname>Uitti</surname><given-names>R. J.</given-names></name></person-group> (<year>2005</year>). <article-title>Fragile X-associated tremor/ataxia syndrome and movements disorders</article-title>. <source>Curr. Opin. Neurol.</source>
<volume>18</volume>, <fpage>393</fpage>–<lpage>398</lpage>. <pub-id pub-id-type="doi">10.1097/01.wco.0000168332.99305.50</pub-id><?supplied-pmid 16003114?><pub-id pub-id-type="pmid">16003114</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnich</surname><given-names>O.</given-names></name><name><surname>Van Droogenbroeck</surname><given-names>M.</given-names></name></person-group> (<year>2011</year>). <article-title>ViBe: a universal background subtraction algorithm for video sequences</article-title>. <source>IEEE Trans. Image Process</source>. <volume>20</volume>, <fpage>1709</fpage>–<lpage>1724</lpage>. <pub-id pub-id-type="doi">10.1109/TIP.2010.2101613</pub-id><?supplied-pmid 21189241?><pub-id pub-id-type="pmid">21189241</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bello-Arroyo</surname><given-names>E.</given-names></name><name><surname>Roque</surname><given-names>H.</given-names></name><name><surname>Marcos</surname><given-names>A.</given-names></name><name><surname>Orihuel</surname><given-names>J.</given-names></name><name><surname>Higuera-Matas</surname><given-names>A.</given-names></name><name><surname>Desco</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>MouBeAT: a new and open toolbox for guided analysis of behavioral tests in mice</article-title>. <source>Front. Behav. Neurosci.</source><volume>12</volume>:<fpage>201</fpage>. <pub-id pub-id-type="doi">10.3389/fnbeh.2018.00201</pub-id><?supplied-pmid 30245618?><pub-id pub-id-type="pmid">30245618</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Catarinucci</surname><given-names>L.</given-names></name><name><surname>Colella</surname><given-names>R.</given-names></name><name><surname>Mainetti</surname><given-names>L.</given-names></name><name><surname>Patrono</surname><given-names>L.</given-names></name><name><surname>Tarricone</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>). <article-title>Smart RFID antenna system for indoor tracking and behavior analysis of small animals in colony cages</article-title>. <source>IEEE Sens. J.</source>
<volume>14</volume>, <fpage>1198</fpage>–<lpage>1206</lpage>. <pub-id pub-id-type="doi">10.1109/JSEN.2013.2293594</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>W. J.</given-names></name><name><surname>Lin</surname><given-names>Y.</given-names></name><name><surname>Xiong</surname><given-names>Z. Q.</given-names></name><name><surname>Wei</surname><given-names>W.</given-names></name><name><surname>Ni</surname><given-names>W.</given-names></name><name><surname>Tan</surname><given-names>G. H.</given-names></name><etal/></person-group>. (<year>2011</year>). <article-title>Exome sequencing identifies truncating mutations in PRRT2 that cause paroxysmal kinesigenic dyskinesia</article-title>. <source>Nat. Genet.</source><volume>43</volume>, <fpage>1252</fpage>–<lpage>1255</lpage>. <pub-id pub-id-type="doi">10.1038/ng.1008</pub-id><?supplied-pmid 22101681?><pub-id pub-id-type="pmid">22101681</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dooley</surname><given-names>J. C.</given-names></name><name><surname>Glanz</surname><given-names>R. M.</given-names></name><name><surname>Sokoloff</surname><given-names>G.</given-names></name><name><surname>Blumberg</surname><given-names>M. S.</given-names></name></person-group> (<year>2020</year>). <article-title>Self-generated whisker movements drive state-dependent sensory input to developing barrel cortex</article-title>. <source>Curr. Biol.</source>
<volume>30</volume>, <fpage>2404</fpage>–<lpage>2410 e4</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2020.04.045</pub-id><?supplied-pmid 32413304?><pub-id pub-id-type="pmid">32413304</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunn</surname><given-names>T. W.</given-names></name><name><surname>Marshall</surname><given-names>J. D.</given-names></name><name><surname>Severson</surname><given-names>K. S.</given-names></name><name><surname>Aldarondo</surname><given-names>D. E.</given-names></name><name><surname>Hildebrand</surname><given-names>D. G. C.</given-names></name><name><surname>Chettih</surname><given-names>S. N.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>Geometric deep learning enables 3D kinematic profiling across species and environments</article-title>. <source>Nat. Methods</source><volume>18</volume>, <fpage>564</fpage>–<lpage>573</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-021-01106-6</pub-id><?supplied-pmid 33875887?><pub-id pub-id-type="pmid">33875887</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felzenszwalb</surname><given-names>P. F.</given-names></name><name><surname>Girshick</surname><given-names>R. B.</given-names></name><name><surname>McAllester</surname><given-names>D.</given-names></name><name><surname>Ramanan</surname><given-names>D.</given-names></name></person-group> (<year>2010</year>). <article-title>Object detection with discriminatively trained part-based models</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
<volume>32</volume>, <fpage>1627</fpage>–<lpage>1645</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2009.167</pub-id><?supplied-pmid 20634557?><pub-id pub-id-type="pmid">20634557</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feng</surname><given-names>X.</given-names></name><name><surname>Zhao</surname><given-names>H. Y.</given-names></name><name><surname>Shao</surname><given-names>Y. J.</given-names></name><name><surname>Lou</surname><given-names>H. F.</given-names></name><name><surname>Zhu</surname><given-names>L. Y.</given-names></name><name><surname>Duan</surname><given-names>S.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Anxiolytic effect of increased NREM Sleep after acute social defeat stress in mice</article-title>. <source>Neurosci. Bull.</source><volume>36</volume>, <fpage>1137</fpage>–<lpage>1146</lpage>. <pub-id pub-id-type="doi">10.1007/s12264-020-00473-y</pub-id><?supplied-pmid 32096115?><pub-id pub-id-type="pmid">32096115</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geuther</surname><given-names>B. Q.</given-names></name><name><surname>Deats</surname><given-names>S. P.</given-names></name><name><surname>Fox</surname><given-names>K. J.</given-names></name><name><surname>Murray</surname><given-names>S. A.</given-names></name><name><surname>Braun</surname><given-names>R. E.</given-names></name><name><surname>White</surname><given-names>J. K.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Robust mouse tracking in complex environments using neural networks</article-title>. <source>Commun. Biol.</source><volume>2</volume>:<fpage>124</fpage>. <pub-id pub-id-type="doi">10.1038/s42003-019-0362-1</pub-id><?supplied-pmid 30937403?><pub-id pub-id-type="pmid">30937403</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giansanti</surname><given-names>D.</given-names></name><name><surname>Maccioni</surname><given-names>G.</given-names></name><name><surname>Macellari</surname><given-names>V.</given-names></name></person-group> (<year>2005</year>). <article-title>The development and test of a device for the reconstruction of 3-D position and orientation by means of a kinematic sensor assembly with rate gyroscopes and accelerometers</article-title>. <source>IEEE Trans. Biomed. Eng.</source>
<volume>52</volume>, <fpage>1271</fpage>–<lpage>1277</lpage>. <pub-id pub-id-type="doi">10.1109/TBME.2005.847404</pub-id><?supplied-pmid 16041990?><pub-id pub-id-type="pmid">16041990</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Girshick</surname><given-names>R.</given-names></name><name><surname>Donahue</surname><given-names>J.</given-names></name><name><surname>Darrell</surname><given-names>T.</given-names></name><name><surname>Malik</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>). <source>Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</source>. <publisher-loc>Columbus, OH</publisher-loc>: <publisher-name>CVPR</publisher-name>. <pub-id pub-id-type="doi">10.1109/CVPR.2014.81</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graving</surname><given-names>J. M.</given-names></name><name><surname>Chae</surname><given-names>D.</given-names></name><name><surname>Naik</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>L.</given-names></name><name><surname>Koger</surname><given-names>B.</given-names></name><name><surname>Costelloe</surname><given-names>B. R.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning</article-title>. <source>eLife</source><volume>8</volume>:<fpage>e47994</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.47994</pub-id><?supplied-pmid 31570119?><pub-id pub-id-type="pmid">31570119</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grech</surname><given-names>A. M.</given-names></name><name><surname>Du</surname><given-names>X.</given-names></name><name><surname>Murray</surname><given-names>S. S.</given-names></name><name><surname>Xiao</surname><given-names>J.</given-names></name><name><surname>Hill</surname><given-names>R. A.</given-names></name></person-group> (<year>2019</year>). <article-title>Sex-specific spatial memory deficits in mice with a conditional TrkB deletion on parvalbumin interneurons</article-title>. <source>Behav. Brain Res.</source>
<volume>372</volume>:<fpage>111984</fpage>. <pub-id pub-id-type="doi">10.1016/j.bbr.2019.111984</pub-id><?supplied-pmid 31150746?><pub-id pub-id-type="pmid">31150746</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffiths</surname><given-names>B. B.</given-names></name><name><surname>Sahbaie</surname><given-names>P.</given-names></name><name><surname>Rao</surname><given-names>A.</given-names></name><name><surname>Arvola</surname><given-names>O.</given-names></name><name><surname>Xu</surname><given-names>L.</given-names></name><name><surname>Liang</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Pre-treatment with microRNA-181a antagomir prevents loss of parvalbumin expression and preserves novel object recognition following mild traumatic brain injury</article-title>. <source>Neuromolecular Med.</source><volume>21</volume>, <fpage>170</fpage>–<lpage>181</lpage>. <pub-id pub-id-type="doi">10.1007/s12017-019-08532-y</pub-id><?supplied-pmid 30900118?><pub-id pub-id-type="pmid">30900118</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gulyás</surname><given-names>M.</given-names></name><name><surname>Bencsik</surname><given-names>N.</given-names></name><name><surname>Pusztai</surname><given-names>S.</given-names></name><name><surname>Liliom</surname><given-names>H.</given-names></name><name><surname>Schlett</surname><given-names>K.</given-names></name></person-group> (<year>2016</year>). <article-title>AnimalTracker: an imagej-based tracking API to create a customized behaviour analyser program</article-title>. <source>Neuroinformatics</source>
<volume>14</volume>, <fpage>479</fpage>–<lpage>481</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-016-9303-z</pub-id><?supplied-pmid 27166960?><pub-id pub-id-type="pmid">27166960</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hao</surname><given-names>D.</given-names></name><name><surname>Yang</surname><given-names>L.</given-names></name><name><surname>Chen</surname><given-names>S.</given-names></name><name><surname>Tian</surname><given-names>Y.</given-names></name><name><surname>Wu</surname><given-names>S.</given-names></name></person-group> (<year>2012</year>). <article-title>916 MHz electromagnetic field exposure affects rat behavior and hippocampal neuronal discharge</article-title>. <source>Neural Regen. Res.</source>
<volume>7</volume>, <fpage>1488</fpage>–<lpage>1492</lpage>. <pub-id pub-id-type="doi">10.3969/j.issn.1673-5374.2012.19.007</pub-id><?supplied-pmid 25657684?><pub-id pub-id-type="pmid">25657684</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hewitt</surname><given-names>B. M.</given-names></name><name><surname>Yap</surname><given-names>M. H.</given-names></name><name><surname>Hodson-Tole</surname><given-names>E. F.</given-names></name><name><surname>Kennerley</surname><given-names>A. J.</given-names></name><name><surname>Sharp</surname><given-names>P. S.</given-names></name><name><surname>Grant</surname><given-names>R. A.</given-names></name></person-group> (<year>2018</year>). <article-title>A novel automated rodent tracker (ART), demonstrated in a mouse model of amyotrophic lateral sclerosis</article-title>. <source>J. Neurosci. Methods</source>
<volume>300</volume>, <fpage>147</fpage>–<lpage>156</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2017.04.006</pub-id><?supplied-pmid 28414047?><pub-id pub-id-type="pmid">28414047</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>K.</given-names></name><name><surname>Han</surname><given-names>Y.</given-names></name><name><surname>Chen</surname><given-names>K.</given-names></name><name><surname>Pan</surname><given-names>H.</given-names></name><name><surname>Zhao</surname><given-names>G.</given-names></name><name><surname>Yi</surname><given-names>W.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>A hierarchical 3D-motion learning framework for animal spontaneous behavior mapping</article-title>. <source>Nat. Commun.</source><volume>12</volume>:<fpage>2784</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-021-22970-y</pub-id><?supplied-pmid 33986265?><pub-id pub-id-type="pmid">33986265</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ishii</surname><given-names>D.</given-names></name><name><surname>Matsuzawa</surname><given-names>D.</given-names></name><name><surname>Matsuda</surname><given-names>S.</given-names></name><name><surname>Tomizawa-Shinohara</surname><given-names>H.</given-names></name><name><surname>Sutoh</surname><given-names>C.</given-names></name><name><surname>Shimizu</surname><given-names>E.</given-names></name></person-group> (<year>2019</year>). <article-title>Spontaneous recovery of fear differs among early - late adolescent and adult male mice</article-title>. <source>Int. J. Neurosci.</source>
<volume>129</volume>, <fpage>1</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1080/00207454.2018.1501049</pub-id><?supplied-pmid 30010457?><pub-id pub-id-type="pmid">30010457</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jimenez</surname><given-names>J. C.</given-names></name><name><surname>Su</surname><given-names>K.</given-names></name><name><surname>Goldberg</surname><given-names>A. R.</given-names></name><name><surname>Luna</surname><given-names>V. M.</given-names></name><name><surname>Biane</surname><given-names>J. S.</given-names></name><name><surname>Ordek</surname><given-names>G.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Anxiety cells in a hippocampal-hypothalamic circuit</article-title>. <source>Neuron</source><volume>97</volume>, <fpage>670</fpage>–<lpage>683.e6</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2018.01.016</pub-id><?supplied-pmid 29397273?><pub-id pub-id-type="pmid">29397273</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karashchuk</surname><given-names>P.</given-names></name><name><surname>Tuthill</surname><given-names>J. C.</given-names></name><name><surname>Brunton</surname><given-names>B. W.</given-names></name></person-group> (<year>2021</year>). <article-title>The DANNCE of the rats: a new toolkit for 3D tracking of animal behavior</article-title>. <source>Nat. Methods</source>
<volume>18</volume>, <fpage>460</fpage>–<lpage>462</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-021-01110-w</pub-id><?supplied-pmid 33875886?><pub-id pub-id-type="pmid">33875886</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewejohann</surname><given-names>L.</given-names></name><name><surname>Hoppmann</surname><given-names>A. M.</given-names></name><name><surname>Kegel</surname><given-names>P.</given-names></name><name><surname>Kritzler</surname><given-names>M.</given-names></name><name><surname>Krüger</surname><given-names>A.</given-names></name><name><surname>Sachser</surname><given-names>N.</given-names></name></person-group> (<year>2009</year>). <article-title>Behavioral phenotyping of a murine model of Alzheimer's disease in a seminaturalistic environment using RFID tracking</article-title>. <source>Behav. Res. Methods</source>
<volume>41</volume>, <fpage>850</fpage>–<lpage>856</lpage>. <pub-id pub-id-type="doi">10.3758/BRM.41.3.850</pub-id><?supplied-pmid 19587201?><pub-id pub-id-type="pmid">19587201</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A.</given-names></name><name><surname>Mamidanna</surname><given-names>P.</given-names></name><name><surname>Cury</surname><given-names>K. M.</given-names></name><name><surname>Abe</surname><given-names>T.</given-names></name><name><surname>Murthy</surname><given-names>V. N.</given-names></name><name><surname>Mathis</surname><given-names>M. W.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nat. Neurosci.</source><volume>21</volume>, <fpage>1281</fpage>–<lpage>1289</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><?supplied-pmid 30127430?><pub-id pub-id-type="pmid">30127430</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morin</surname><given-names>L. P.</given-names></name><name><surname>Studholme</surname><given-names>K. M.</given-names></name></person-group> (<year>2011</year>). <article-title>Separation of function for classical and ganglion cell photoreceptors with respect to circadian rhythm entrainment and induction of photosomnolence</article-title>. <source>Neuroscience</source>
<volume>199</volume>, <fpage>213</fpage>–<lpage>224</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroscience.2011.09.057</pub-id><?supplied-pmid 21985934?><pub-id pub-id-type="pmid">21985934</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nath</surname><given-names>T.</given-names></name><name><surname>Mathis</surname><given-names>A.</given-names></name><name><surname>Chen</surname><given-names>A. C.</given-names></name><name><surname>Patel</surname><given-names>A.</given-names></name><name><surname>Bethge</surname><given-names>M.</given-names></name><name><surname>Mathis</surname><given-names>M. W.</given-names></name></person-group> (<year>2019</year>). <article-title>Using DeepLabCut for 3D markerless pose estimation across species and behaviors</article-title>. <source>Nat. Protoc.</source>
<volume>14</volume>, <fpage>2152</fpage>–<lpage>2176</lpage>. <pub-id pub-id-type="doi">10.1038/s41596-019-0176-0</pub-id><?supplied-pmid 31227823?><pub-id pub-id-type="pmid">31227823</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noldus</surname><given-names>L. P.</given-names></name><name><surname>Spink</surname><given-names>A. J.</given-names></name><name><surname>Tegelenbosch</surname><given-names>R. A.</given-names></name></person-group> (<year>2001</year>). <article-title>EthoVision: a versatile video tracking system for automation of behavioral experiments</article-title>. <source>Behav. Res. Methods Instruments Computers</source>
<volume>33</volume>, <fpage>398</fpage>–<lpage>414</lpage>. <pub-id pub-id-type="doi">10.3758/BF03195394</pub-id><?supplied-pmid 11591072?><pub-id pub-id-type="pmid">11591072</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>T. D.</given-names></name><name><surname>Aldarondo</surname><given-names>D. E.</given-names></name><name><surname>Willmore</surname><given-names>L.</given-names></name><name><surname>Kislin</surname><given-names>M.</given-names></name><name><surname>Wang</surname><given-names>S. S.</given-names></name><name><surname>Murthy</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Fast animal pose estimation using deep neural networks</article-title>. <source>Nat. Methods</source><volume>16</volume>, <fpage>117</fpage>–<lpage>125</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-018-0234-5</pub-id><?supplied-pmid 30573820?><pub-id pub-id-type="pmid">30573820</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiao</surname><given-names>M.</given-names></name><name><surname>Sun</surname><given-names>P.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Wei</surname><given-names>S.</given-names></name><name><surname>Wei</surname><given-names>X.</given-names></name><name><surname>Song</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Profiling proteins in the hypothalamus and hippocampus of a rat model of premenstrual syndrome irritability</article-title>. <source>Neural Plast.</source><volume>2017</volume>:<fpage>6537230</fpage>. <pub-id pub-id-type="doi">10.1155/2017/6537230</pub-id><?supplied-pmid 28255462?><pub-id pub-id-type="pmid">28255462</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Redmon</surname><given-names>J.</given-names></name><name><surname>Divvala</surname><given-names>S.</given-names></name><name><surname>Girshick</surname><given-names>R.</given-names></name><name><surname>Farhadi</surname><given-names>A.</given-names></name></person-group> (<year>2016</year>). <article-title>“You only look once: unified, real-time object detection,”</article-title> in <source>2016 IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Las Vegas, NV</publisher-loc>: <publisher-name>CVPR</publisher-name>), <fpage>779</fpage>–<lpage>788</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2016.91</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Redmon</surname><given-names>J.</given-names></name><name><surname>Farhadi</surname><given-names>A.</given-names></name></person-group> (<year>2017</year>). <article-title>“YOLO9000: better, faster, stronger,”</article-title> in <source>IEEE Conference on Computer Vision &amp; Pattern Recognition</source> (<publisher-loc>Honolulu, HI</publisher-loc>), <fpage>6517</fpage>–<lpage>6525</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2017.690</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodrigues</surname><given-names>P.</given-names></name><name><surname>Barbosa</surname><given-names>L. B.</given-names></name><name><surname>Bianchini</surname><given-names>A. E.</given-names></name><name><surname>Ferrari</surname><given-names>F. T.</given-names></name><name><surname>Baldisserotto</surname><given-names>B.</given-names></name><name><surname>Heinzmann</surname><given-names>B. M.</given-names></name></person-group> (<year>2019</year>). <article-title>Nociceptive-like behavior and analgesia in silver catfish (Rhamdia quelen)</article-title>. <source>Physiol. Behav.</source>
<volume>210</volume>:<fpage>112648</fpage>. <pub-id pub-id-type="doi">10.1016/j.physbeh.2019.112648</pub-id><?supplied-pmid 31408639?><pub-id pub-id-type="pmid">31408639</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samson</surname><given-names>A. L.</given-names></name><name><surname>Ju</surname><given-names>L.</given-names></name><name><surname>Ah Kim</surname><given-names>H.</given-names></name><name><surname>Zhang</surname><given-names>S. R.</given-names></name><name><surname>Lee</surname><given-names>J. A.</given-names></name><name><surname>Sturgeon</surname><given-names>S. A.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>MouseMove: an open source program for semi-automated analysis of movement and cognitive testing in rodents</article-title>. <source>Sci. Rep.</source><volume>5</volume>:<fpage>16171</fpage>. <pub-id pub-id-type="doi">10.1038/srep16171</pub-id><?supplied-pmid 26530459?><pub-id pub-id-type="pmid">26530459</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scarsi</surname><given-names>F.</given-names></name><name><surname>Scheggia</surname><given-names>D.</given-names></name><name><surname>Papaleo</surname><given-names>F.</given-names></name></person-group> (<year>2020</year>). <article-title>Automated two-chamber operon ID/ED task for mice</article-title>. <source>Curr. Protocols Neurosci.</source>
<volume>94</volume>:<fpage>e109</fpage>. <pub-id pub-id-type="doi">10.1002/cpns.109</pub-id><?supplied-pmid 33275835?><pub-id pub-id-type="pmid">33275835</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takemoto</surname><given-names>M.</given-names></name><name><surname>Song</surname><given-names>W. J.</given-names></name></person-group> (<year>2019</year>). <article-title>Cue-dependent safety and fear learning in a discriminative auditory fear conditioning paradigm in the mouse</article-title>. <source>Learn. Memory</source>
<volume>26</volume>, <fpage>284</fpage>–<lpage>290</lpage>. <pub-id pub-id-type="doi">10.1101/lm.049577.119</pub-id><?supplied-pmid 31308247?><pub-id pub-id-type="pmid">31308247</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Unger</surname><given-names>J.</given-names></name><name><surname>Mansour</surname><given-names>M.</given-names></name><name><surname>Kopaczka</surname><given-names>M.</given-names></name><name><surname>Gronloh</surname><given-names>N.</given-names></name><name><surname>Spehr</surname><given-names>M.</given-names></name><name><surname>Merhof</surname><given-names>D.</given-names></name></person-group> (<year>2017</year>). <article-title>An unsupervised learning approach for tracking mice in an enclosed area</article-title>. <source>BMC Bioinformatics</source>
<volume>18</volume>:<fpage>272</fpage>. <pub-id pub-id-type="doi">10.1186/s12859-017-1681-1</pub-id><?supplied-pmid 28545524?><pub-id pub-id-type="pmid">28545524</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walter</surname><given-names>T.</given-names></name><name><surname>Couzin</surname><given-names>I. D.</given-names></name></person-group> (<year>2021</year>). <article-title>TRex, a fast multi-animal tracking system with markerless identification, and 2D estimation of posture and visual fields</article-title>. <source>Elife</source>
<volume>10</volume>:<fpage>e64000</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.64000</pub-id><?supplied-pmid 33634789?><pub-id pub-id-type="pmid">33634789</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z. R.</given-names></name><name><surname>Wang</surname><given-names>P.</given-names></name><name><surname>Xing</surname><given-names>L.</given-names></name><name><surname>Mei</surname><given-names>L. P.</given-names></name><name><surname>Zhao</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>T.</given-names></name></person-group> (<year>2017</year>). <article-title>Leap Motion-based virtual reality training for improving motor functional recovery of upper limbs and neural reorganization in subacute stroke patients</article-title>. <source>Neural Regenerat. Res.</source>
<volume>12</volume>, <fpage>1823</fpage>–<lpage>1831</lpage>. <pub-id pub-id-type="doi">10.4103/1673-5374.219043</pub-id><?supplied-pmid 29239328?><pub-id pub-id-type="pmid">29239328</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoon</surname><given-names>K.</given-names></name><name><surname>Kim</surname><given-names>D. Y.</given-names></name><name><surname>Yoon</surname><given-names>Y. C.</given-names></name><name><surname>Jeon</surname><given-names>M.</given-names></name></person-group> (<year>2019</year>). <article-title>Data association for multi-object tracking via deep neural networks</article-title>. <source>Sensors</source>
<volume>19</volume>:<fpage>559</fpage>. <pub-id pub-id-type="doi">10.3390/s19030559</pub-id><?supplied-pmid 30700017?><pub-id pub-id-type="pmid">30700017</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>B.</given-names></name><name><surname>Yuan</surname><given-names>B.</given-names></name><name><surname>Dai</surname><given-names>J. K.</given-names></name><name><surname>Cheng</surname><given-names>T. L.</given-names></name><name><surname>Xia</surname><given-names>S. N.</given-names></name><name><surname>He</surname><given-names>L. J.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>reversal of social recognition deficit in adult mice with MECP2 duplication via normalization of MeCP2 in the medial prefrontal cortex</article-title>. <source>Neurosci. Bull.</source><volume>36</volume>, <fpage>570</fpage>–<lpage>584</lpage>. <pub-id pub-id-type="doi">10.1007/s12264-020-00467-w</pub-id><?supplied-pmid 32144612?><pub-id pub-id-type="pmid">32144612</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
