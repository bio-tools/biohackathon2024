<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Genet</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Genet</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Genet.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Genetics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1664-8021</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10014624</article-id>
    <article-id pub-id-type="publisher-id">1132018</article-id>
    <article-id pub-id-type="doi">10.3389/fgene.2023.1132018</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Genetics</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>iEnhancer-DCSV: Predicting enhancers and their strength based on DenseNet and improved convolutional block attention module</article-title>
      <alt-title alt-title-type="left-running-head">Jia et al.</alt-title>
      <alt-title alt-title-type="right-running-head">
        <ext-link xlink:href="https://doi.org/10.3389/fgene.2023.1132018" ext-link-type="uri">10.3389/fgene.2023.1132018</ext-link>
      </alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Jia</surname>
          <given-names>Jianhua</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Lei</surname>
          <given-names>Rufeng</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">*</xref>
        <uri xlink:href="https://loop.frontiersin.org/people/2151861/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Qin</surname>
          <given-names>Lulu</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wu</surname>
          <given-names>Genqiang</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1708964/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wei</surname>
          <given-names>Xin</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>School of Information Engineering</institution>, <institution>Jingdezhen Ceramic University</institution>, <addr-line>Jingdezhen</addr-line>, <country>China</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Business School</institution>, <institution>Jiangxi Institute of Fashion Technology</institution>, <addr-line>Nanchang</addr-line>, <country>China</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p><bold>Edited by:</bold><ext-link xlink:href="https://loop.frontiersin.org/people/827295/overview" ext-link-type="uri">Lei Chen</ext-link>, Shanghai Maritime University, China</p>
      </fn>
      <fn fn-type="edited-by">
        <p><bold>Reviewed by:</bold><ext-link xlink:href="https://loop.frontiersin.org/people/153656/overview" ext-link-type="uri">Guoxian Yu</ext-link>, Shandong University, China</p>
        <p><ext-link xlink:href="https://loop.frontiersin.org/people/1442135/overview" ext-link-type="uri">Qi Dai</ext-link>, Zhejiang Sci-Tech University, China</p>
        <p><ext-link xlink:href="https://loop.frontiersin.org/people/2173241/overview" ext-link-type="uri">Hongtao Lu</ext-link>, Shanghai Jiao Tong University, China</p>
      </fn>
      <corresp id="c001">*Correspondence: Jianhua Jia, <email>jjh163yx@163.com</email>; Rufeng Lei, <email>rufeng_lei@163.com</email>
</corresp>
      <fn fn-type="other">
        <p>This article was submitted to Computational Genomics, a section of the journal Frontiers in Genetics</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>01</day>
      <month>3</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>14</volume>
    <elocation-id>1132018</elocation-id>
    <history>
      <date date-type="received">
        <day>26</day>
        <month>12</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>13</day>
        <month>2</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2023 Jia, Lei, Qin, Wu and Wei.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Jia, Lei, Qin, Wu and Wei</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Enhancers play a crucial role in controlling gene transcription and expression. Therefore, bioinformatics puts many emphases on predicting enhancers and their strength. It is vital to create quick and accurate calculating techniques because conventional biomedical tests take too long time and are too expensive. This paper proposed a new predictor called iEnhancer-DCSV built on a modified densely connected convolutional network (DenseNet) and an improved convolutional block attention module (CBAM). Coding was performed using one-hot and nucleotide chemical property (NCP). DenseNet was used to extract advanced features from raw coding. The channel attention and spatial attention modules were used to evaluate the significance of the advanced features and then input into a fully connected neural network to yield the prediction probabilities. Finally, ensemble learning was employed on the final categorization findings <italic>via</italic> voting. According to the experimental results on the test set, the first layer of enhancer recognition achieved an accuracy of 78.95%, and the Matthews correlation coefficient value was 0.5809. The second layer of enhancer strength prediction achieved an accuracy of 80.70%, and the Matthews correlation coefficient value was 0.6609. The iEnhancer-DCSV method can be found at <ext-link xlink:href="https://github.com/leirufeng/iEnhancer-DCSV" ext-link-type="uri">https://github.com/leirufeng/iEnhancer-DCSV</ext-link>. It is easy to obtain the desired results without using the complex mathematical formulas involved.</p>
    </abstract>
    <kwd-group>
      <kwd>enhancer</kwd>
      <kwd>DenseNet</kwd>
      <kwd>channel attention</kwd>
      <kwd>spatial attention</kwd>
      <kwd>ensemble learning</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">
          <institution-wrap>
            <institution>National Natural Science Foundation of China
</institution>
            <institution-id institution-id-type="doi">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group>
        <funding-source id="cn002">
          <institution-wrap>
            <institution>Natural Science Foundation of Jiangxi Province
</institution>
            <institution-id institution-id-type="doi">10.13039/501100004479</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group>
        <funding-source id="cn003">
          <institution-wrap>
            <institution>Education Department of Jiangxi Province
</institution>
            <institution-id institution-id-type="doi">10.13039/501100009102</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <funding-statement>This work was partially supported by the National Natural Science Foundation of China (nos 61761023, 62162032, and 31760315), the Natural Science Foundation of Jiangxi Province, China (nos 20202BABL202004 and 20202BAB202007), and the Scientific Research Plan of the Department of Education of Jiangxi Province, China (GJJ190695 and GJJ212419). These funders had no role in the study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec id="s1">
    <title>1 Introduction</title>
    <p>Genes are functional areas of an organism’s DNA (<xref rid="B9" ref-type="bibr">Dai et al., 2018</xref>; <xref rid="B20" ref-type="bibr">Kong et al., 2020</xref>) that hold genetic information. The gene is transferred to the protein through a sequence of transcription (<xref rid="B27" ref-type="bibr">Maston et al., 2006</xref>) and translation (<xref rid="B44" ref-type="bibr">Xiao et al., 2016</xref>), and proteins control the organism’s exterior phenotypic shape (<xref rid="B4" ref-type="bibr">Buccitelli and Selbach, 2020</xref>). Transcription is one of the most crucial aspects of gene expression. The enhancer and promoter (<xref rid="B8" ref-type="bibr">Cvetesic and Lenhard, 2017</xref>) are the most significant sequence regions for transcriptional activity. An enhancer is a brief non-coding DNA fragment on DNA (<xref rid="B18" ref-type="bibr">Kim et al., 2010</xref>) and controls rapid and slow gene expression (<xref rid="B34" ref-type="bibr">Shrinivas et al., 2019</xref>). According to previous studies, several illnesses (<xref rid="B47" ref-type="bibr">Yang et al., 2022</xref>) are produced as a result of enhancer mutations and deletions (<xref rid="B10" ref-type="bibr">Emison et al., 2005</xref>; <xref rid="B25" ref-type="bibr">Liu G. et al., 2018</xref>; <xref rid="B2" ref-type="bibr">Boyd et al., 2018</xref>; <xref rid="B42" ref-type="bibr">Wu et al., 2019</xref>). In terms of the activities they express, the enhancers may be categorized into groups, such as strong and weak enhancers, closed (balanced) enhancers, and latent enhancers (<xref rid="B33" ref-type="bibr">Shlyueva et al., 2014</xref>). Therefore, understanding and recognizing these specific gene sequence segments is an urgent problem (<xref rid="B31" ref-type="bibr">Pennacchio et al., 2013</xref>).</p>
    <p>Traditional medical experimental methods (<xref rid="B46" ref-type="bibr">Yang et al., 2020</xref>) in bioinformatics are costly and time-consuming. Therefore, it is crucial to develop computational techniques and derive some excellent predictors (<xref rid="B13" ref-type="bibr">Firpi et al., 2010</xref>; <xref rid="B12" ref-type="bibr">Fernández and Miranda-Saavedra, 2012</xref>; <xref rid="B11" ref-type="bibr">Erwin et al., 2014</xref>; <xref rid="B14" ref-type="bibr">Ghandi et al., 2014</xref>; <xref rid="B19" ref-type="bibr">Kleftogiannis et al., 2015</xref>; <xref rid="B26" ref-type="bibr">Lu et al., 2015</xref>; <xref rid="B3" ref-type="bibr">Bu et al., 2017</xref>; <xref rid="B45" ref-type="bibr">Yang et al., 2017</xref>). However, these techniques have limitations in the prediction of strong and weak enhancers. <xref rid="B23" ref-type="bibr">Liu et al. (2015)</xref> developed a predictor called iEnhancer-2L based on the support vector machine (SVM) algorithm and used the sequence pseudo-K-tuple nucleotide composition (PseKNC) approach to encode features. Afterward, machine learning-based methods were applied to the prediction of enhancers, such as SVM (<xref rid="B17" ref-type="bibr">Jia and He, 2016</xref>; <xref rid="B15" ref-type="bibr">He and Jia, 2017</xref>), RF (<xref rid="B36" ref-type="bibr">Singh et al., 2013</xref>; <xref rid="B41" ref-type="bibr">Wang et al., 2021</xref>), and XGBoost (<xref rid="B5" ref-type="bibr">Cai et al., 2021</xref>), and many excellent predictors have been created. However, a single machine learning classifier has obvious performance drawbacks. A predictor based on an ensemble learning model (<xref rid="B24" ref-type="bibr">Liu B. et al., 2018</xref>) was developed to address this problem, which generally has a significantly better performance. The ensemble learning model has diversity and complexity in feature processing. For instance, <xref rid="B39" ref-type="bibr">Wang C. et al. (2022</xref>) developed a predictor called Enhancer-FRL, which used 10 feature methods for feature coding. The manual creation of feature coding is a relatively difficult problem, and the presence of many complex feature coding types can lead to dimensional disasters. Furthermore, the effectiveness of conventional machine learning models depends on the extracted complex features. Consequently, the development of a predictor that requires only simple features is crucial.</p>
    <p>Nowadays, deep learning is becoming increasingly popular. <xref rid="B29" ref-type="bibr">Nguyen et al. (2019</xref>) proposed the iEnhancer-ECNN model based on convolutional neural networks (CNNs). <xref rid="B30" ref-type="bibr">Niu et al. (2021)</xref> proposed a model called the iEnhancer-EBLSTM based on bi-directional long short-term memory (Bi-LSTM). They used one-hot and K-mers coding techniques to encode the enhancer sequences and then fed these features into the deep learning network to get relatively good prediction results. For example, in the iEnhancer-ECNN model, the ACC and MCC of enhancer recognition results were 0.769 and 0.537, and the ACC and MCC of enhancer strength prediction results were 0.678 and 0.368, respectively. However, there is a wide gap in prediction precision using a better deep learning model.</p>
    <p>In deep learning networks, CNNs with more convolutional layers extract more advanced local features but lead to the problem of gradient disappearance and network degradation. To solve this problem, the residual neural network (ResNet) (<xref rid="B22" ref-type="bibr">Li et al., 2022</xref>) uses a short-circuit connection structure, which allows the convolutional layers to be connected several layers apart and can solve the problem of network degradation to some extent. However, the densely connected convolutional network (DenseNet) (<xref rid="B16" ref-type="bibr">Huang et al., 2010</xref>) has been enhanced based on ResNet. DenseNet extracts richer feature information by reusing the features of each previous layer, and it is more effective than ResNet. The attention model is also increasingly used, and the essence of the attention model is to focus on more useful feature information and suppress useless feature information. Convolutional block attention module (CBAM) (<xref rid="B48" ref-type="bibr">Zhang et al., 2022</xref>) can focus on more useful feature information from channel and spatial dimensions. The current computational method has the disadvantages of poor performance and complex features. For this purpose, we developed a new predictor called iEnhancer-DCSV. The predictor is conducted using a modified DenseNet and an improved CBAM attention module. The DenseNet framework makes it easier to extract more advanced features. Experimental results show that our model outperforms the existing models. The iEnhancer-DCSV model is currently the optimal choice for predicting enhancers and their strengths.</p>
  </sec>
  <sec sec-type="materials|methods" id="s2">
    <title>2 Materials and methods</title>
    <sec id="s2-1">
      <title>2.1 Benchmark dataset</title>
      <p>The benchmark dataset was created by <xref rid="B23" ref-type="bibr">Liu et al. (2015)</xref>. They took the enhancer fragments from nine cell lines, removed 80% of the redundant sequences with the CD-HIT (<xref rid="B16" ref-type="bibr">Huang et al., 2010</xref>) and then calculated the ideal fragment length of 200 bp for each enhancer sequence to create the final dataset. The dataset is split into two sections: a training dataset for the model’s training and an independent test dataset for model testing. The independent test dataset is made up of 200 enhancer samples (with 100 strongly and 100 weakly enhancer samples) and 200 non-enhancer samples, whereas the training dataset is made up of 1,484 enhancer samples (with 742 strongly and 742 weakly enhancer samples) and 1,484 non-enhancer samples. All enhancer samples in the independent test dataset were different from the training dataset to guarantee that the samples are independent. The benchmark dataset is described in <xref rid="T1" ref-type="table">Table 1</xref> and may be downloaded conveniently from the website: <ext-link xlink:href="https://github.com/leirufeng/iEnhancer-DCSV" ext-link-type="uri">https://github.com/leirufeng/iEnhancer-DCSV</ext-link>.</p>
      <table-wrap position="float" id="T1">
        <label>TABLE 1</label>
        <caption>
          <p>Specifics of the benchmark dataset.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead valign="top">
            <tr>
              <th align="left" rowspan="1" colspan="1">Layer</th>
              <th align="left" rowspan="1" colspan="1">Original dataset</th>
              <th align="left" rowspan="1" colspan="1">Enhancer</th>
              <th align="left" rowspan="1" colspan="1">Non-enhancer</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td rowspan="2" align="left" colspan="1">First layer</td>
              <td align="left" rowspan="1" colspan="1">Training dataset</td>
              <td align="left" rowspan="1" colspan="1">1,484</td>
              <td align="left" rowspan="1" colspan="1">1,484</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Testing dataset</td>
              <td align="left" rowspan="1" colspan="1">200</td>
              <td align="left" rowspan="1" colspan="1">200</td>
            </tr>
          </tbody>
        </table>
        <table frame="hsides" rules="groups">
          <thead valign="top">
            <tr>
              <td align="left" rowspan="1" colspan="1"/>
              <td align="left" rowspan="1" colspan="1">Original dataset</td>
              <td align="left" rowspan="1" colspan="1">Strong enhancers</td>
              <td align="left" rowspan="1" colspan="1">Weak enhancers</td>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td rowspan="2" align="left" colspan="1">Second layer</td>
              <td align="left" rowspan="1" colspan="1">Training dataset</td>
              <td align="left" rowspan="1" colspan="1">742</td>
              <td align="left" rowspan="1" colspan="1">742</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Testing dataset</td>
              <td align="left" rowspan="1" colspan="1">100</td>
              <td align="left" rowspan="1" colspan="1">100</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="s2-2">
      <title>2.2 Feature coding schemes</title>
      <p>Two simple and effective coding techniques are used in this study: one-hot and NCP. Notably, these two coding techniques produce columns with a dimension of 200, so they can be feature-combined. For instance, an enhancer sequence with a length of 200 bp can obtain a 4 × 200 feature matrix and a 3 × 200 feature matrix after one-hot and NCP coding, respectively. Finally, combining these two matrices through feature fusion can yield a 7 × 200 feature matrix. In this study, the enhancer sequence is considered a gray image by the feature coding matrix. The 7 × 200 matrix is directly used as the original feature input.</p>
      <sec id="s2-2-1">
        <title>2.2.1 One-hot coding</title>
        <p>In the field of bioinformatics, one-hot coding is one of the most used coding techniques. The advantages of this coding technique are its feasibility, efficiency, and ability to assure that each nucleotide letter is coded independently. The method is effective in avoiding the expression of interdependencies. This coding technique is particularly popular in bioinformatics. The double helix structure (<xref rid="B35" ref-type="bibr">Sinden et al., 1998</xref>) of DNA is widely known, and it is made up of four nucleotides: A (adenine deoxyribonucleotide), C (cytosine deoxyribonucleotide), G (guanine deoxyribonucleotide), and T (thymine deoxyribonucleotide) (<xref rid="B7" ref-type="bibr">Chou, 1984</xref>). The enhancer sequences are DNA sequences designated “0,1,2,3” in the order “ACGT.” The nucleotides in the sequences are then coded, and the coding length is four nucleotides. The coding elements are 0 and 1. The position corresponding to the nucleotide letter marker is coded as 1, and the other positions are coded as 0. For instance, “A” is coded as (1,0,0,0), “C” is coded as (0,1,0,0), “G” is coded as (0,0,1,0), and “T” is coded as (0,0,0,1) (<xref rid="B48" ref-type="bibr">Zhang et al., 2022</xref>). The one-hot coding is shown in <xref rid="F1" ref-type="fig">Figure 1A</xref>.</p>
        <fig position="float" id="F1">
          <label>FIGURE 1</label>
          <caption>
            <p>Overview of the iEnhancer-DCSV model. <bold>(A)</bold> Feature coding. One-hot and NCP are used to encode the enhancer sequence, and a 7 × 200 matrix is produced. <bold>(B)</bold> Framework of the iEnhancer-DCSV model. The original features are input directly to the modified DenseNet structure (which includes four dense blocks, normalized layers, and transition layers), and the improved structure is used to extract advanced features. Modules for spatial and channel attention are introduced to assess the extracted advanced features’ importance. The two evaluated advanced feature maps are multiplied together at the corresponding positions. The fully connected neural network is used to output the prediction probabilities. <bold>(C)</bold> Ensemble model. The model uses fivefold cross-validation, where each fold is tested using an independent test set, each test enhancer sequence generates five prediction probabilities, and the final classification is voted using ensemble learning.</p>
          </caption>
          <graphic xlink:href="fgene-14-1132018-g001" position="float"/>
        </fig>
      </sec>
      <sec id="s2-2-2">
        <title>2.2.2 NCP coding</title>
        <p>The four DNA nucleotides are structurally different from each other and have different chemical molecular structures (<xref rid="B48" ref-type="bibr">Zhang et al., 2022</xref>). For instance, C and T contain one loop each, whereas A and G have two loops between the four nucleotides. G and T may be classified as ketone groups from the standpoint of chemical composition, whereas A and C can be classified as amino groups. A and T have two hydrogen bonds, but C and G have three hydrogen bonds. The strength between C and G is more powerful than that between A and T. The specific chemical properties between nucleotides are shown in <xref rid="T2" ref-type="table">Table 2</xref>.</p>
        <table-wrap position="float" id="T2">
          <label>TABLE 2</label>
          <caption>
            <p>Nucleotide chemical property.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead valign="top">
              <tr>
                <th align="left" rowspan="1" colspan="1">Chemical property</th>
                <th align="left" rowspan="1" colspan="1">Category</th>
                <th align="left" rowspan="1" colspan="1">Nucleotide</th>
              </tr>
            </thead>
            <tbody valign="top">
              <tr>
                <td rowspan="2" align="left" colspan="1">Ring structure</td>
                <td align="left" rowspan="1" colspan="1">Purine</td>
                <td align="left" rowspan="1" colspan="1">A, G</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Pyrimidine</td>
                <td align="left" rowspan="1" colspan="1">C, T</td>
              </tr>
              <tr>
                <td rowspan="2" align="left" colspan="1">Functional group</td>
                <td align="left" rowspan="1" colspan="1">Amino</td>
                <td align="left" rowspan="1" colspan="1">A, C</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Keto</td>
                <td align="left" rowspan="1" colspan="1">G, T</td>
              </tr>
              <tr>
                <td rowspan="2" align="left" colspan="1">Hydrogen bonding</td>
                <td align="left" rowspan="1" colspan="1">Strong</td>
                <td align="left" rowspan="1" colspan="1">C, G</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Weak</td>
                <td align="left" rowspan="1" colspan="1">A, T</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>Then, coding is performed based on the chemical characteristics. The nucleotide <inline-formula id="inf1"><mml:math id="m1" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is located in position <inline-formula id="inf2"><mml:math id="m2" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula> in the sequence. Three chemical characteristics of nucleotide <inline-formula id="inf3"><mml:math id="m3" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are “ring structure,” “functional group,” and “hydrogen bond strength” (<xref rid="B43" ref-type="bibr">Xiao et al., 2019</xref>). The vector representation of <inline-formula id="inf4"><mml:math id="m4" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula id="inf5"><mml:math id="m5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is expressed as<disp-formula id="e1"><mml:math id="m6" overflow="scroll"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><label>(1)</label></disp-formula>
</p>
        <p>A, C, G, and T may be encoded using this approach as (1,1,1), (0,1,0), (1,0,0), and (0,0,1). NCP coding is shown in <xref rid="F1" ref-type="fig">Figure 1A</xref>.</p>
      </sec>
    </sec>
    <sec id="s2-3">
      <title>2.3 Model construction</title>
      <p>In this study, we constructed a network framework to automatically learn advanced features called iEnhancer-DCSV. The framework of iEnhancer-DCSV is divided into three parts: (A) feature coding, (B) framework of iEnhancer-DCSV model, and (C) ensemble model. The details are shown in <xref rid="F1" ref-type="fig">Figure 1</xref>.</p>
      <sec id="s2-3-1">
        <title>2.3.1 DenseNet</title>
        <p>In this study, we modified the initial DenseNet structure. The original DenseNet consists of a convolutional layer, a dense block layer, and a transition layer. First, convolution is applied to the original features. Then, the convolution features are processed by the dense block and transition layers. The dense block layer is a dense connection of all the preceding layers to the following layers. In particular, each layer accepts all its preceding layers as its additional input, enabling feature reuse. The transition layer, which mainly connects two adjacent dense blocks, reduces the feature map size. Instead, we deleted the first convolutional layer and added a batch normalization layer between the dense block layer and the transition layer. This processing method can extract better-quality feature information and reduce the risk of overfitting.</p>
        <sec id="s2-3-1-1">
          <title>2.3.1.1 Dense block</title>
          <p>The traditional CNN network does not perform very well in extracting feature information. A convolutional structure called dense convolutional block extracts richer feature information by reusing previous features. Experimentally, the dense convolutional network feature extraction is proven better than traditional CNN. The structure diagram is shown in <xref rid="F2" ref-type="fig">Figure 2</xref>.</p>
          <fig position="float" id="F2">
            <label>FIGURE 2</label>
            <caption>
              <p>Structure of a dense block.</p>
            </caption>
            <graphic xlink:href="fgene-14-1132018-g002" position="float"/>
          </fig>
          <p>In the dense block, the input of layer <inline-formula id="inf6"><mml:math id="m7" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula> is related to not only the output of layer <inline-formula id="inf7"><mml:math id="m8" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, but the output of all the previous layers. The <inline-formula id="inf8"><mml:math id="m9" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> level is represented as follows:<disp-formula id="e2"><mml:math id="m10" overflow="scroll"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msub><mml:mrow><mml:mo>,</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mrow><mml:mo>,</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><label>(2)</label></disp-formula>where is denoted as layers <inline-formula id="inf10"><mml:math id="m12" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> to <inline-formula id="inf11"><mml:math id="m13" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> stitched together by the dimension of the channel. <italic>H</italic> is a non-linear combinatorial function. It is a combination of batch normalization, ReLU activation function, and convolution (3 × 3).</p>
          <p>In this study, we used four dense blocks, each containing three layers of convolution. The final extraction of features was <inline-formula id="inf12"><mml:math id="m14" overflow="scroll"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p>
        </sec>
        <sec id="s2-3-1-2">
          <title>2.3.1.2 Transition layer</title>
          <p>The <inline-formula id="inf13"><mml:math id="m15" overflow="scroll"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>-1 layers in front of the dense block are combined by channel dimension. As the number of channels in the <inline-formula id="inf14"><mml:math id="m16" overflow="scroll"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>-layer becomes larger, it leads to an explosion of parameters, along with a slow training speed. We can improve the efficiency by connecting a transition layer with the dense block layer. The transition layer consists of a 1 × 1 convolution and a 2 × 2 average pooling. It is a function of reducing the number of channels and parameters in the dense block layer by downsampling to compress the model.</p>
        </sec>
      </sec>
      <sec id="s2-3-2">
        <title>2.3.2 Batch normalization</title>
        <p>Gradient explosion and gradient disappearance are serious problems in deep learning training, and this phenomenon tends to occur more likely in the deeper network structure. If the shallow parameters are changed, their fluctuations during backpropagation may be significant, resulting in significant variable shifts in the deeper network. Batch normalization (<xref rid="B28" ref-type="bibr">Min et al., 2016</xref>) has been shown to improve the generalization ability of the model. The batch normalization is expressed as follows:<disp-formula id="e3"><mml:math id="m17" overflow="scroll"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:msqrt><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(3)</label></disp-formula>
<disp-formula id="e4"><mml:math id="m18" overflow="scroll"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><label>(4)</label></disp-formula>where <inline-formula id="inf15"><mml:math id="m19" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:math></inline-formula> is the set of the feature dataset <inline-formula id="inf16"><mml:math id="m20" overflow="scroll"><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula>, <inline-formula id="inf17"><mml:math id="m21" overflow="scroll"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:math></inline-formula> is the mean of dataset <inline-formula id="inf18"><mml:math id="m22" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula id="inf19"><mml:math id="m23" overflow="scroll"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is the variance of dataset <inline-formula id="inf20"><mml:math id="m24" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:math></inline-formula>. <inline-formula id="inf21"><mml:math id="m25" overflow="scroll"><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula id="inf22"><mml:math id="m26" overflow="scroll"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:math></inline-formula> are trainable parameters.</p>
      </sec>
      <sec id="s2-3-3">
        <title>2.3.3 Improved CBAM attention module</title>
        <p>The CBAM attention module comprises channel attention and spatial attention modules (<xref rid="B6" ref-type="bibr">Chen et al., 2017</xref>). First, we use the channel attention module to evaluate the original features. Second, we take the feature map output from the channel attention module and feed it back into the spatial attention module. Finally, we output the final feature maps from the spatial attention module. This serial connection of CBAM attention modules has the disadvantage that the attention modules are all computed in a specific way, and the computation of weights destroys the feature shape of the input. This leads to inaccurate weight calculation of the spatial attention modules and loss of channel weighting information in the final feature map. We change the original serial approach in the CBAM attention module to a parallel method. The principle is to input the original features into the channel attention module and the spatial attention module and let the output features be multiplied by their corresponding positions. By this method, the effect of each attention model after evaluation can be maximally preserved and the expressiveness of the features can be improved.</p>
        <sec id="s2-3-3-1">
          <title>2.3.3.1 Channel attention module</title>
          <p>In deep learning, the degree of importance varies between different feature map channels, so we use the channel attention module to calculate different weights for each channel. By weighting each channel of the feature map, the model automatically pays attention to the more useful channel information to achieve the fixation of channel dimension and compression of spatial dimension. The channel attention module comprises the max pooling layer, the average pooling layer, the MLP module, and the sigmoid activation function. The CBAM’s channel attention module structure is shown in <xref rid="F3" ref-type="fig">Figure 3</xref>.</p>
          <fig position="float" id="F3">
            <label>FIGURE 3</label>
            <caption>
              <p>CBAM’s channel attention module structure.</p>
            </caption>
            <graphic xlink:href="fgene-14-1132018-g003" position="float"/>
          </fig>
          <p>The channel attention module starts with the feature map passing through two parallel max pooling and average pooling layers, which are input into the fully connected neural network (MLP) module separately. Second, the two results of the MLP output are summed element by element, and the channel attention module weights are obtained using the sigmoid activation function. Finally, these weights are multiplied by the feature map to obtain the feature map of the channel attention model weighting. The CBAM’s channel attention model is expressed as follows:<disp-formula id="e5"><mml:math id="m27" overflow="scroll"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><label>(5)</label></disp-formula>
<disp-formula id="e6"><mml:math id="m28" overflow="scroll"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>∙</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><label>(6)</label></disp-formula>where pooling here is the global max pooling and the global average pooling. <inline-formula id="inf23"><mml:math id="m29" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> denotes each channel-specific value of <inline-formula id="inf24"><mml:math id="m30" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:math></inline-formula> multiplied by the weight <inline-formula id="inf25"><mml:math id="m31" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        </sec>
        <sec id="s2-3-3-2">
          <title>2.3.3.2 Spatial attention module</title>
          <p>In deep learning, different receptive fields have different degrees of value to the feature map, so we use a spatial attention model to calculate the weights between receptive fields. By weighting the receptive fields, we allow the model to focus on the more useful target location information to achieve a constant spatial dimension and a compressed channel dimension. The spatial attention model is implemented through a max pooling layer, an average pooling layer, a CNN module, and a sigmoid activation function. The CBAM’s spatial attention module structure is shown in <xref rid="F4" ref-type="fig">Figure 4</xref>.</p>
          <fig position="float" id="F4">
            <label>FIGURE 4</label>
            <caption>
              <p>CBAM’s spatial attention module structure.</p>
            </caption>
            <graphic xlink:href="fgene-14-1132018-g004" position="float"/>
          </fig>
          <p>The spatial attention model first passes the feature maps through two parallel max pooling and average pooling layers and performs a stitching operation on the two pooling feature maps. Then, the newly obtained features are input into the CNN module to be transformed into a feature map with channel number 1, and the spatial attention module weights are obtained by the sigmoid activation function. Finally, this weight is multiplied by the feature map to obtain the weighted feature map of the spatial attention model. The CBAM’s spatial attention model is expressed as follows:<disp-formula id="e7"><mml:math id="m32" overflow="scroll"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mn>7</mml:mn><mml:mo>×</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>;</mml:mo><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><label>(7)</label></disp-formula>
<disp-formula id="e8"><mml:math id="m33" overflow="scroll"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>∙</mml:mo><mml:mi>F</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><label>(8)</label></disp-formula>
</p>
          <p>Pooling here is the global max pooling and global average pooling. The size of the convolutional kernel used in the CNN module is 7 × 7. Finally, <inline-formula id="inf26"><mml:math id="m34" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> denotes each receptive field of <inline-formula id="inf27"><mml:math id="m35" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:math></inline-formula> multiplied by the weight <inline-formula id="inf28"><mml:math id="m36" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        </sec>
      </sec>
      <sec id="s2-3-4">
        <title>2.3.4 Fully connected neural network</title>
        <p>We used a fully connected neural network (<xref rid="B40" ref-type="bibr">Wang. et al., 2022b</xref>) to predict the enhancers and their strength. After we extracted the advanced features, the size of the advanced features was reduced using a pooling layer. Then, these features are flattened into vectors, which are later input into the fully connected neural network. Finally, the softmax function is used to calculate the predicted probability of the enhancers. The softmax formula is expressed as<disp-formula id="e9"><mml:math id="m37" overflow="scroll"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(9)</label></disp-formula>
</p>
        <p>where <inline-formula id="inf29"><mml:math id="m38" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula id="inf30"><mml:math id="m39" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> denote the weights in the fully connected neural network, <inline-formula id="inf31"><mml:math id="m40" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> denotes the sample, and <inline-formula id="inf32"><mml:math id="m41" overflow="scroll"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> is the number of categories. <inline-formula id="inf33"><mml:math id="m42" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the probability that <inline-formula id="inf34"><mml:math id="m43" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> is predicted to be <inline-formula id="inf35"><mml:math id="m44" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula>. This is a dichotomous problem, <inline-formula id="inf36"><mml:math id="m45" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula> = 0 or <inline-formula id="inf37"><mml:math id="m46" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula> = 1.</p>
      </sec>
      <sec id="s2-3-5">
        <title>2.3.5 Ensemble model</title>
        <p>There is an ensemble method called bagging (<xref rid="B1" ref-type="bibr">Bauer and Kohavi, 1999</xref>). It is accomplished by training several different models, allowing independent test data to calculate the predicted results using different models and then averaging them. This ensemble learning approach is called model averaging. The advantage of model averaging is that different models do not usually produce the same error on the test data, and it is a very powerful method for reducing generalization errors.</p>
        <p>In this study, we used a fivefold cross-validation method (<xref rid="B32" ref-type="bibr">Shang et al., 2022</xref>). The training dataset was divided into five parts: four for training and one for validation. We used an independent test set put into each fold in cross-validation, by which five predictions are obtained. Finally, the final prediction results are obtained by the voting method. The ensemble method is shown in <xref rid="F1" ref-type="fig">Figure 1C</xref>.</p>
      </sec>
    </sec>
    <sec id="s2-4">
      <title>2.4 Performance evaluation</title>
      <p>Scientific evaluation metrics are a measure of model performance. In this study, the evaluation of model performance contains four metrics: sensitivity (Sn), specificity (Sp), accuracy (Acc), and Mathew’s correlation coefficient (MCC) (<xref rid="B37" ref-type="bibr">Sokolova and Lapalme, 2009</xref>). The specific calculation formula is shown as follows:<disp-formula id="e10"><mml:math id="m47" overflow="scroll"><mml:mrow><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mtext> </mml:mtext><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext> </mml:mtext><mml:mi>S</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext> </mml:mtext><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext> </mml:mtext><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:math><label>(10)</label></disp-formula>where TP, TN, FP, and FN are the four metrics in the confusion matrix, representing true positive, true negative, false positive, and false negative, respectively (<xref rid="B30" ref-type="bibr">Niu et al., 2021</xref>). In addition, we added the ROC curve area AUC metric (<xref rid="B38" ref-type="bibr">Vacic et al., 2006</xref>) to evaluate the model, and higher values of these metrics indicate better model performance.</p>
    </sec>
  </sec>
  <sec sec-type="results|discussion" id="s3">
    <title>3 Results and discussion</title>
    <sec id="s3-1">
      <title>3.1 Construction of the first layer (enhancer recognition) model</title>
      <p>The recognition of enhancers in the first layer is very important to complete the prediction mission. For the first layer of enhancer recognition, we used the iEnhancer-DCSV network framework. The advanced feature extraction and weight assignment are performed automatically by the model’s iEnhancer-DCSV network framework. First, the enhancer sequences are encoded using the one-hot and NCP methods, and then feature coding is fed into the DenseNet to extract advanced features. These advanced features are input into the channel attention module and the spatial attention module, respectively. The two evaluated advanced feature maps are multiplied at the corresponding positions, and then the pooling layer is used to compress the feature size. Finally, a fully connected neural network is used to derive the predicted probabilities. We validate the model by putting independent test sets into each fold of the fivefold cross-validation. The aforementioned five-time results are passed through a soft voting mechanism to arrive at the final prediction. The whole process was cycled 10 times to verify the stability of the model, and the obtained individual performance metrics were averaged. The experimental results for SN, SP, Acc, and MCC were 80.25%, 77.65%, 78.95%, and 0.5809, respectively.</p>
    </sec>
    <sec id="s3-2">
      <title>3.2 Construction of the second layer (strong and weak enhancer prediction) model</title>
      <p>On the basis of the correct identification of enhancers in the first layer, the second layer predicts the strengths and weaknesses of enhancers. As the second layer has less training data and the complex network structure can lead to overfitting, we removed the attention module from the iEnhancer-DCSV network framework and used the same training as the first layer, with experimental results of 99.10%, 62.30%, 80.70%, and 0.6609 for SN, SP, Acc, and MCC, respectively.</p>
    </sec>
    <sec id="s3-3">
      <title>3.3 Comparison of different coding methods</title>
      <p>Currently, feature engineering has been a very important part of the process because building a model and using a simple and efficient coding method is crucial. In this study, we compared the one-hot + NCP coding, one-hot coding, and NCP coding to determine the final coding method. We input the three encoding methods into the two network frameworks, layer 1 and layer 2, respectively, and the results of the experiment are shown in <xref rid="T3" ref-type="table">Table 3</xref>. In the first layer (enhancer recognition), the one-hot + NCP coding was slightly better than the one-hot coding and better than the NCP coding. In the second layer (strong and weak enhancer prediction), the one-hot + NCP coding was much better than these two coding types. Therefore, we adopted one-hot + NCP coding as the final coding method in this study.</p>
      <table-wrap position="float" id="T3">
        <label>TABLE 3</label>
        <caption>
          <p>Comparison results of different coding schemes.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead valign="top">
            <tr>
              <th align="left" rowspan="1" colspan="1">Layer</th>
              <th align="left" rowspan="1" colspan="1">Coding</th>
              <th align="left" rowspan="1" colspan="1">SN (%)</th>
              <th align="left" rowspan="1" colspan="1">SP (%)</th>
              <th align="left" rowspan="1" colspan="1">Acc (%)</th>
              <th align="left" rowspan="1" colspan="1">MCC</th>
              <th align="left" rowspan="1" colspan="1">AUC</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td rowspan="2" align="left" colspan="1">First layer</td>
              <td align="left" rowspan="1" colspan="1">One-hot</td>
              <td align="char" char="." rowspan="1" colspan="1">81.70</td>
              <td align="char" char="." rowspan="1" colspan="1">75.50</td>
              <td align="char" char="." rowspan="1" colspan="1">78.60</td>
              <td align="char" char="." rowspan="1" colspan="1">0.5737</td>
              <td align="char" char="." rowspan="1" colspan="1">0.8275</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">NCP</td>
              <td align="char" char="." rowspan="1" colspan="1">83.25</td>
              <td align="char" char="." rowspan="1" colspan="1">70.50</td>
              <td align="char" char="." rowspan="1" colspan="1">76.88</td>
              <td align="char" char="." rowspan="1" colspan="1">0.5428</td>
              <td align="char" char="." rowspan="1" colspan="1">0.8168</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"/>
              <td align="left" rowspan="1" colspan="1">One-hot + NCP</td>
              <td align="char" char="." rowspan="1" colspan="1">80.25</td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>77.65</bold>
              </td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>78.95</bold>
              </td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>0.5809</bold>
              </td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>0.8527</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="3" align="left" colspan="1">Second layer</td>
              <td align="left" rowspan="1" colspan="1">One-hot</td>
              <td align="char" char="." rowspan="1" colspan="1">60.30</td>
              <td align="char" char="." rowspan="1" colspan="1">72.80</td>
              <td align="char" char="." rowspan="1" colspan="1">66.55</td>
              <td align="char" char="." rowspan="1" colspan="1">0.3418</td>
              <td align="char" char="." rowspan="1" colspan="1">0.7491</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">NCP</td>
              <td align="char" char="." rowspan="1" colspan="1">90.50</td>
              <td align="char" char="." rowspan="1" colspan="1">53.40</td>
              <td align="char" char="." rowspan="1" colspan="1">71.95</td>
              <td align="char" char="." rowspan="1" colspan="1">0.4780</td>
              <td align="char" char="." rowspan="1" colspan="1">0.7666</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">One-hot + NCP</td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>99.10</bold>
              </td>
              <td align="char" char="." rowspan="1" colspan="1">62.30</td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>80.70</bold>
              </td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>0.6609</bold>
              </td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>0.8686</bold>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="s3-4">
      <title>3.4 Comparison of different model frameworks</title>
      <p>In this study, we used six network frameworks: ResNet, DenseNet, DenseNet + channel attention model, DenseNet + spatial attention model, DenseNet + CBAM attention model, and DenseNet + improved CBAM attention model. We tested these five network frameworks in the first layer (enhancer recognition) task because the amount of data for the second layer (enhancer strength prediction) task was too small. The original features were extracted using each of these five network frameworks for the high-level features, and the best-performing network framework was selected based on the experimental results. The experimental comparison results are shown in <xref rid="T4" ref-type="table">Table 4</xref>. Adding an attention model behind the DenseNet is already very effective, and the improved CBAM attention model integrates the advantages of both attention models. However, the improved effect is limited because the shape of the feature map is too small. The results show that the DenseNet + improved CBAM attention network framework works better. Therefore, we finally chose the DenseNet + improved CBAM attentional network framework model.</p>
      <table-wrap position="float" id="T4">
        <label>TABLE 4</label>
        <caption>
          <p>Comparison with different architecture methods at layer 1 (enhancer recognition).</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead valign="top">
            <tr>
              <th align="left" rowspan="1" colspan="1">Model framework</th>
              <th align="left" rowspan="1" colspan="1">SN (%)</th>
              <th align="left" rowspan="1" colspan="1">SP (%)</th>
              <th align="left" rowspan="1" colspan="1">Acc (%)</th>
              <th align="left" rowspan="1" colspan="1">MCC</th>
              <th align="left" rowspan="1" colspan="1">AUC</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td align="left" rowspan="1" colspan="1">ResNet</td>
              <td align="char" char="." rowspan="1" colspan="1">69.80</td>
              <td align="char" char="." rowspan="1" colspan="1">77.90</td>
              <td align="char" char="." rowspan="1" colspan="1">73.85</td>
              <td align="char" char="." rowspan="1" colspan="1">0.4927</td>
              <td align="char" char="." rowspan="1" colspan="1">0.8211</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">DenseNet</td>
              <td align="char" char="." rowspan="1" colspan="1">83.10</td>
              <td align="char" char="." rowspan="1" colspan="1">68.50</td>
              <td align="char" char="." rowspan="1" colspan="1">75.80</td>
              <td align="char" char="." rowspan="1" colspan="1">0.5219</td>
              <td align="char" char="." rowspan="1" colspan="1">0.8108</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">DenseNet + channel attention</td>
              <td align="char" char="." rowspan="1" colspan="1">78.20</td>
              <td align="char" char="." rowspan="1" colspan="1">78.35</td>
              <td align="char" char="." rowspan="1" colspan="1">78.27</td>
              <td align="char" char="." rowspan="1" colspan="1">0.5686</td>
              <td align="char" char="." rowspan="1" colspan="1">0.8316</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">DenseNet + spatial attention</td>
              <td align="char" char="." rowspan="1" colspan="1">78.75</td>
              <td align="char" char="." rowspan="1" colspan="1">78.20</td>
              <td align="char" char="." rowspan="1" colspan="1">78.48</td>
              <td align="char" char="." rowspan="1" colspan="1">0.5717</td>
              <td align="char" char="." rowspan="1" colspan="1">0.8304</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">DenseNet + CBAM attention</td>
              <td align="char" char="." rowspan="1" colspan="1">83.70</td>
              <td align="char" char="." rowspan="1" colspan="1">67.25</td>
              <td align="char" char="." rowspan="1" colspan="1">75.48</td>
              <td align="char" char="." rowspan="1" colspan="1">0.5183</td>
              <td align="char" char="." rowspan="1" colspan="1">0.8046</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">DenseNet + improved CBAM attention</td>
              <td align="char" char="." rowspan="1" colspan="1">80.25</td>
              <td align="char" char="." rowspan="1" colspan="1">77.65</td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>78.95</bold>
              </td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>0.5809</bold>
              </td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>0.8527</bold>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="s3-5">
      <title>3.5 Performance of iEnhancer-DCSV on the training dataset</title>
      <p>To verify the performance of the iEnhancer-DCSV classifier, we cycled through 10 times of fivefold cross-validation, and the experimental results are shown in <xref rid="T5" ref-type="table">Table 5</xref>. We found that the values of the evaluation metrics fluctuated relatively steadily on the first (enhancer recognition) and second (enhancer strength prediction) layer tasks, indicating that the iEnhancer-DCSV model has good generalization capability. <xref rid="F5" ref-type="fig">Figure 5</xref> shows the ROC curves of the first layer (enhancer recognition) with a mean AUC value of 0.8527 in 10 experiments, and <xref rid="F6" ref-type="fig">Figure 6</xref> shows the ROC curves of the second layer (enhancer strength prediction) with a mean AUC value of 0.8686 in 10 experiments. The results show that our proposed iEnhancer-DCSV has good performance.</p>
      <table-wrap position="float" id="T5">
        <label>TABLE 5</label>
        <caption>
          <p>Performance of iEnhancer-DCSV in 10 trials.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead valign="top">
            <tr>
              <th align="left" rowspan="1" colspan="1">Layer</th>
              <th align="left" rowspan="1" colspan="1">Cycle index</th>
              <th align="left" rowspan="1" colspan="1">Sn (%)</th>
              <th align="left" rowspan="1" colspan="1">Sp (%)</th>
              <th align="left" rowspan="1" colspan="1">Acc (%)</th>
              <th align="left" rowspan="1" colspan="1">MCC</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td rowspan="10" align="left" colspan="1">First layer</td>
              <td align="left" rowspan="1" colspan="1">0</td>
              <td align="left" rowspan="1" colspan="1">78.50</td>
              <td align="left" rowspan="1" colspan="1">78.00</td>
              <td align="left" rowspan="1" colspan="1">78.25</td>
              <td align="left" rowspan="1" colspan="1">0.5650</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">1</td>
              <td align="left" rowspan="1" colspan="1">83.00</td>
              <td align="left" rowspan="1" colspan="1">75.50</td>
              <td align="left" rowspan="1" colspan="1">79.25</td>
              <td align="left" rowspan="1" colspan="1">0.5866</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">2</td>
              <td align="left" rowspan="1" colspan="1">80.50</td>
              <td align="left" rowspan="1" colspan="1">75.00</td>
              <td align="left" rowspan="1" colspan="1">77.75</td>
              <td align="left" rowspan="1" colspan="1">0.5558</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">3</td>
              <td align="left" rowspan="1" colspan="1">74.00</td>
              <td align="left" rowspan="1" colspan="1">85.50</td>
              <td align="left" rowspan="1" colspan="1">79.75</td>
              <td align="left" rowspan="1" colspan="1">0.5989</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">4</td>
              <td align="left" rowspan="1" colspan="1">77.00</td>
              <td align="left" rowspan="1" colspan="1">81.50</td>
              <td align="left" rowspan="1" colspan="1">79.25</td>
              <td align="left" rowspan="1" colspan="1">0.5855</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">5</td>
              <td align="left" rowspan="1" colspan="1">87.50</td>
              <td align="left" rowspan="1" colspan="1">68.00</td>
              <td align="left" rowspan="1" colspan="1">77.75</td>
              <td align="left" rowspan="1" colspan="1">0.5658</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">6</td>
              <td align="left" rowspan="1" colspan="1">80.50</td>
              <td align="left" rowspan="1" colspan="1">80.00</td>
              <td align="left" rowspan="1" colspan="1">80.25</td>
              <td align="left" rowspan="1" colspan="1">0.6050</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">7</td>
              <td align="left" rowspan="1" colspan="1">79.50</td>
              <td align="left" rowspan="1" colspan="1">80.00</td>
              <td align="left" rowspan="1" colspan="1">79.75</td>
              <td align="left" rowspan="1" colspan="1">0.5950</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">8</td>
              <td align="left" rowspan="1" colspan="1">80.50</td>
              <td align="left" rowspan="1" colspan="1">78.00</td>
              <td align="left" rowspan="1" colspan="1">79.25</td>
              <td align="left" rowspan="1" colspan="1">0.5851</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">9</td>
              <td align="left" rowspan="1" colspan="1">81.50</td>
              <td align="left" rowspan="1" colspan="1">75.00</td>
              <td align="left" rowspan="1" colspan="1">78.25</td>
              <td align="left" rowspan="1" colspan="1">0.5661</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"/>
              <td align="left" rowspan="1" colspan="1">Mean ± STD</td>
              <td align="left" rowspan="1" colspan="1">
                <bold>80.25 ± 3.39</bold>
              </td>
              <td align="left" rowspan="1" colspan="1">
                <bold>77.65 ± 4.47</bold>
              </td>
              <td align="left" rowspan="1" colspan="1">
                <bold>78.95 ± 0.84</bold>
              </td>
              <td align="left" rowspan="1" colspan="1">
                <bold>0.5809 ± 0.0158</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="10" align="left" colspan="1">Second layer</td>
              <td align="left" rowspan="1" colspan="1">0</td>
              <td align="left" rowspan="1" colspan="1">99.99</td>
              <td align="left" rowspan="1" colspan="1">61.99</td>
              <td align="left" rowspan="1" colspan="1">81.00</td>
              <td align="left" rowspan="1" colspan="1">0.6702</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">1</td>
              <td align="left" rowspan="1" colspan="1">95.99</td>
              <td align="left" rowspan="1" colspan="1">62.99</td>
              <td align="left" rowspan="1" colspan="1">79.50</td>
              <td align="left" rowspan="1" colspan="1">0.6250</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">2</td>
              <td align="left" rowspan="1" colspan="1">95.99</td>
              <td align="left" rowspan="1" colspan="1">64.99</td>
              <td align="left" rowspan="1" colspan="1">80.50</td>
              <td align="left" rowspan="1" colspan="1">0.6416</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">3</td>
              <td align="left" rowspan="1" colspan="1">99.99</td>
              <td align="left" rowspan="1" colspan="1">60.99</td>
              <td align="left" rowspan="1" colspan="1">80.50</td>
              <td align="left" rowspan="1" colspan="1">0.6624</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">4</td>
              <td align="left" rowspan="1" colspan="1">99.99</td>
              <td align="left" rowspan="1" colspan="1">56.99</td>
              <td align="left" rowspan="1" colspan="1">78.50</td>
              <td align="left" rowspan="1" colspan="1">0.6313</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">5</td>
              <td align="left" rowspan="1" colspan="1">99.99</td>
              <td align="left" rowspan="1" colspan="1">61.99</td>
              <td align="left" rowspan="1" colspan="1">81.00</td>
              <td align="left" rowspan="1" colspan="1">0.6702</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">6</td>
              <td align="left" rowspan="1" colspan="1">99.99</td>
              <td align="left" rowspan="1" colspan="1">60.99</td>
              <td align="left" rowspan="1" colspan="1">80.50</td>
              <td align="left" rowspan="1" colspan="1">0.6624</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">7</td>
              <td align="left" rowspan="1" colspan="1">99.99</td>
              <td align="left" rowspan="1" colspan="1">64.99</td>
              <td align="left" rowspan="1" colspan="1">82.50</td>
              <td align="left" rowspan="1" colspan="1">0.6938</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">8</td>
              <td align="left" rowspan="1" colspan="1">99.99</td>
              <td align="left" rowspan="1" colspan="1">58.99</td>
              <td align="left" rowspan="1" colspan="1">79.50</td>
              <td align="left" rowspan="1" colspan="1">0.6468</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">9</td>
              <td align="left" rowspan="1" colspan="1">98.99</td>
              <td align="left" rowspan="1" colspan="1">67.99</td>
              <td align="left" rowspan="1" colspan="1">83.50</td>
              <td align="left" rowspan="1" colspan="1">0.7047</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"/>
              <td align="left" rowspan="1" colspan="1">Mean ± STD</td>
              <td align="left" rowspan="1" colspan="1">
                <bold>99.10 ± 1.58</bold>
              </td>
              <td align="left" rowspan="1" colspan="1">
                <bold>62.30 ± 3.00</bold>
              </td>
              <td align="left" rowspan="1" colspan="1">
                <bold>80.70 ± 1.38</bold>
              </td>
              <td align="left" rowspan="1" colspan="1">
                <bold>0.6609 ± 0.0243</bold>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <fig position="float" id="F5">
        <label>FIGURE 5</label>
        <caption>
          <p>ROC curves for layer 1 (enhancer recognition).</p>
        </caption>
        <graphic xlink:href="fgene-14-1132018-g005" position="float"/>
      </fig>
      <fig position="float" id="F6">
        <label>FIGURE 6</label>
        <caption>
          <p>ROC curves for layer 2 (enhancer strength prediction).</p>
        </caption>
        <graphic xlink:href="fgene-14-1132018-g006" position="float"/>
      </fig>
    </sec>
    <sec id="s3-6">
      <title>3.6 Comparison of iEnhancer-DCSV with existing predictors</title>
      <p>The iEnhancer-DCSV predictor proposed in this study is compared with seven existing predictors. The performance of independent datasets under different methods is shown in <xref rid="T6" ref-type="table">Table 6</xref>. The iEnhancer-DCSV predictor has better Acc and MCC metrics compared with others. The improvement ranges for ACC and MCC in the first layer (enhancer recognition) were 1.95%–5.95% and 0.0202–0.1205, respectively, and the improvement ranges for ACC and MCC in the second layer (enhancer strength prediction) were 7.2%–25.7% and 0.1218–0.5588, respectively. Meanwhile, in the first and second layers, the SN and SP metrics also have some advantages, indicating that iEnhancer-DCSV is more balanced and has more stable and superior performance in identifying positive and negative samples. The iEnhancer-DCSV predictor is expected to be the most advanced and representative tool for predicting enhancement and its strengths and weaknesses.</p>
      <table-wrap position="float" id="T6">
        <label>TABLE 6</label>
        <caption>
          <p>Comparison with other methods on the same independent datasets.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead valign="top">
            <tr>
              <th align="left" rowspan="1" colspan="1">Layer</th>
              <th align="left" rowspan="1" colspan="1">Predictor</th>
              <th align="left" rowspan="1" colspan="1">SN</th>
              <th align="left" rowspan="1" colspan="1">SP</th>
              <th align="left" rowspan="1" colspan="1">Acc</th>
              <th align="left" rowspan="1" colspan="1">MCC</th>
              <th align="left" rowspan="1" colspan="1">AUC</th>
              <th align="left" rowspan="1" colspan="1">Source</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td rowspan="8" align="left" colspan="1">First layer</td>
              <td align="left" rowspan="1" colspan="1">iEnhancer-2L</td>
              <td align="char" char="." rowspan="1" colspan="1">71.00</td>
              <td align="char" char="." rowspan="1" colspan="1">75.00</td>
              <td align="char" char="." rowspan="1" colspan="1">73.00</td>
              <td align="char" char="." rowspan="1" colspan="1">0.4604</td>
              <td align="left" rowspan="1" colspan="1">0.8062</td>
              <td align="left" rowspan="1" colspan="1">
                <xref rid="B23" ref-type="bibr">Liu et al. (2015)</xref>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">EnhancerPred</td>
              <td align="char" char="." rowspan="1" colspan="1">73.50</td>
              <td align="char" char="." rowspan="1" colspan="1">74.50</td>
              <td align="char" char="." rowspan="1" colspan="1">74.00</td>
              <td align="char" char="." rowspan="1" colspan="1">0.4800</td>
              <td align="left" rowspan="1" colspan="1">0.8013</td>
              <td align="left" rowspan="1" colspan="1">
                <xref rid="B17" ref-type="bibr">Jia and He (2016)</xref>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">iEnhancer-EL</td>
              <td align="char" char="." rowspan="1" colspan="1">71.00</td>
              <td align="char" char="." rowspan="1" colspan="1">78.50</td>
              <td align="char" char="." rowspan="1" colspan="1">74.75</td>
              <td align="char" char="." rowspan="1" colspan="1">0.4964</td>
              <td align="left" rowspan="1" colspan="1">0.8173</td>
              <td align="left" rowspan="1" colspan="1">
                <xref rid="B24" ref-type="bibr">Liu et al. (2018a)</xref>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">iEnhancer-ECNN</td>
              <td align="char" char="." rowspan="1" colspan="1">78.50</td>
              <td align="char" char="." rowspan="1" colspan="1">75.20</td>
              <td align="char" char="." rowspan="1" colspan="1">76.90</td>
              <td align="char" char="." rowspan="1" colspan="1">0.5370</td>
              <td align="left" rowspan="1" colspan="1">0.8320</td>
              <td align="left" rowspan="1" colspan="1">
                <xref rid="B29" ref-type="bibr">Nguyen et (2019)</xref>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">iEnhancer-XG</td>
              <td align="char" char="." rowspan="1" colspan="1">75.75</td>
              <td align="char" char="." rowspan="1" colspan="1">74.00</td>
              <td align="char" char="." rowspan="1" colspan="1">77.50</td>
              <td align="char" char="." rowspan="1" colspan="1">0.5150</td>
              <td align="left" rowspan="1" colspan="1">—</td>
              <td align="left" rowspan="1" colspan="1">
                <xref rid="B5" ref-type="bibr">Cai et al. (2021)</xref>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">iEnhancer-EBLSTM</td>
              <td align="char" char="." rowspan="1" colspan="1">75.50</td>
              <td align="char" char="." rowspan="1" colspan="1">79.50</td>
              <td align="char" char="." rowspan="1" colspan="1">77.20</td>
              <td align="char" char="." rowspan="1" colspan="1">0.5340</td>
              <td align="left" rowspan="1" colspan="1">0.7720</td>
              <td align="left" rowspan="1" colspan="1">
                <xref rid="B30" ref-type="bibr">Niu et al. (2021)</xref>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Enhancer-FRL</td>
              <td align="char" char="." rowspan="1" colspan="1">80.50</td>
              <td align="char" char="." rowspan="1" colspan="1">75.50</td>
              <td align="char" char="." rowspan="1" colspan="1">78.00</td>
              <td align="char" char="." rowspan="1" colspan="1">0.5607</td>
              <td align="left" rowspan="1" colspan="1">0.8573</td>
              <td align="left" rowspan="1" colspan="1">
                <xref rid="B39" ref-type="bibr">Wang et al. (2022a)</xref>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">iEnhancer-DCSV</td>
              <td align="char" char="." rowspan="1" colspan="1">80.25</td>
              <td align="char" char="." rowspan="1" colspan="1">77.65</td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>78.95</bold>
              </td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>0.5809</bold>
              </td>
              <td align="left" rowspan="1" colspan="1">0.8527</td>
              <td align="left" rowspan="1" colspan="1">This study</td>
            </tr>
            <tr>
              <td rowspan="8" align="left" colspan="1">Second layer</td>
              <td align="left" rowspan="1" colspan="1">iEnhancer-2L</td>
              <td align="char" char="." rowspan="1" colspan="1">47.00</td>
              <td align="char" char="." rowspan="1" colspan="1">74.00</td>
              <td align="char" char="." rowspan="1" colspan="1">60.50</td>
              <td align="char" char="." rowspan="1" colspan="1">0.2181</td>
              <td align="left" rowspan="1" colspan="1">0.6678</td>
              <td align="left" rowspan="1" colspan="1">
                <xref rid="B23" ref-type="bibr">Liu et al. (2015)</xref>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">EnhancerPred</td>
              <td align="char" char="." rowspan="1" colspan="1">45.00</td>
              <td align="char" char="." rowspan="1" colspan="1">65.00</td>
              <td align="char" char="." rowspan="1" colspan="1">55.00</td>
              <td align="char" char="." rowspan="1" colspan="1">0.1021</td>
              <td align="left" rowspan="1" colspan="1">0.5790</td>
              <td align="left" rowspan="1" colspan="1">
                <xref rid="B17" ref-type="bibr">Jia and He (2016)</xref>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">iEnhancer-EL</td>
              <td align="char" char="." rowspan="1" colspan="1">54.00</td>
              <td align="char" char="." rowspan="1" colspan="1">68.00</td>
              <td align="char" char="." rowspan="1" colspan="1">61.00</td>
              <td align="char" char="." rowspan="1" colspan="1">0.2222</td>
              <td align="left" rowspan="1" colspan="1">0.6801</td>
              <td align="left" rowspan="1" colspan="1">
                <xref rid="B25" ref-type="bibr">Liu et al. (2018b)</xref>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">iEnhancer-ECNN</td>
              <td align="char" char="." rowspan="1" colspan="1">79.10</td>
              <td align="char" char="." rowspan="1" colspan="1">56.40</td>
              <td align="char" char="." rowspan="1" colspan="1">67.80</td>
              <td align="char" char="." rowspan="1" colspan="1">0.3680</td>
              <td align="left" rowspan="1" colspan="1">0.7480</td>
              <td align="left" rowspan="1" colspan="1">
                <xref rid="B29" ref-type="bibr">Nguyen et (2019)</xref>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">iEnhancer-XG</td>
              <td align="char" char="." rowspan="1" colspan="1">70.00</td>
              <td align="char" char="." rowspan="1" colspan="1">57.00</td>
              <td align="char" char="." rowspan="1" colspan="1">63.50</td>
              <td align="char" char="." rowspan="1" colspan="1">0.2720</td>
              <td align="left" rowspan="1" colspan="1">—</td>
              <td align="left" rowspan="1" colspan="1">
                <xref rid="B5" ref-type="bibr">Cai et al. (2021)</xref>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">iEnhancer-EBLSTM</td>
              <td align="char" char="." rowspan="1" colspan="1">81.20</td>
              <td align="char" char="." rowspan="1" colspan="1">53.60</td>
              <td align="char" char="." rowspan="1" colspan="1">65.80</td>
              <td align="char" char="." rowspan="1" colspan="1">0.3240</td>
              <td align="left" rowspan="1" colspan="1">0.6580</td>
              <td align="left" rowspan="1" colspan="1">
                <xref rid="B30" ref-type="bibr">Niu et al. (2021)</xref>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Enhancer-FRL</td>
              <td align="char" char="." rowspan="1" colspan="1">98.00</td>
              <td align="char" char="." rowspan="1" colspan="1">49.00</td>
              <td align="char" char="." rowspan="1" colspan="1">73.50</td>
              <td align="char" char="." rowspan="1" colspan="1">0.5391</td>
              <td align="left" rowspan="1" colspan="1">0.8723</td>
              <td align="left" rowspan="1" colspan="1">
                <xref rid="B40" ref-type="bibr">Wang et al. (2022b)</xref>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">iEnhancer-DCSV</td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>99.10</bold>
              </td>
              <td align="char" char="." rowspan="1" colspan="1">62.30</td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>80.70</bold>
              </td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>0.6609</bold>
              </td>
              <td align="left" rowspan="1" colspan="1">0.8686</td>
              <td align="left" rowspan="1" colspan="1">This study</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
  </sec>
  <sec sec-type="conclusion" id="s4">
    <title>4 Conclusion</title>
    <p>In this study, we propose a new predictor of enhancer recognition and its strength called iEnhancer-DCSV. It is based on DenseNet and an improved CBAM attention module approach. The experimental results demonstrate that the MCC value for enhancer identification on the independent test set is 0.5809, and the MCC value for enhancer strength prediction is 0.6609. This indicates that the iEnhancer-DCSV predictor has good performance and generalization ability, which is better than the existing prediction tools. We combine deep learning methods with enhancer research to innovate computational methods in the field of bioinformatics and enrich enhancer research. In the future, the iEnhancer-DCSV predictor not only is applicable to enhancer classification tasks but can also be used in different prediction tasks, making its use convenient for researchers.</p>
    <p>Of course, some deficiencies must be overcome in our proposed model. The current enhancer sample of data is small and fails to sufficiently promote the performance of the iEnhancer-DCSV model using a big data-driven approach. In addition, data enhancement strategies were not employed to augment our data samples, such as generative adversarial networks (GANs) (<xref rid="B21" ref-type="bibr">Li and Zhang, 2021</xref>). This will be our future work issue to address. However, as the research on enhancers progresses, the disadvantage of a small amount of data will gradually disappear, and better deep learning methods will be used in the research, creating more possibilities for future enhancer recognition and strength prediction.</p>
  </sec>
</body>
<back>
  <ack>
    <p>The authors are grateful for the constructive comments and suggestions made by the reviewers.</p>
  </ack>
  <sec sec-type="data-availability" id="s5">
    <title>Data availability statement</title>
    <p>The original contributions presented in the study are included in the article/Supplementary Material, further inquiries can be directed to the corresponding authors.</p>
  </sec>
  <sec id="s6">
    <title>Author contributions</title>
    <p>JJ and RL conceived and designed the experiments. RL implemented feature extraction, model construction, model training, and performance evaluation. RL, LQ, and XW drafted the manuscript and revised the manuscript. JJ supervised this study. All authors contributed to the content of this paper and approved the final manuscript.</p>
  </sec>
  <sec sec-type="COI-statement" id="s8">
    <title>Conflict of interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s9">
    <title>Publisher’s note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations or those of the publisher, the editors, and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bauer</surname><given-names>E.</given-names></name><name><surname>Kohavi</surname><given-names>R.</given-names></name></person-group> (<year>1999</year>). <article-title>An empirical comparison of voting classification algorithms: Bagging, boosting, and variants</article-title>. <source>Mach. Learn.</source>
<volume>36</volume>, <fpage>105</fpage>–<lpage>139</lpage>. <pub-id pub-id-type="doi">10.1023/a:1007515423169</pub-id>
</mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boyd</surname><given-names>M.</given-names></name><name><surname>Thodberg</surname><given-names>M.</given-names></name><name><surname>Vitezic</surname><given-names>M.</given-names></name><name><surname>Bornholdt</surname><given-names>J.</given-names></name><name><surname>Vitting-Seerup</surname><given-names>K.</given-names></name><name><surname>Chen</surname><given-names>Y.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Characterization of the enhancer and promoter landscape of inflammatory bowel disease from human colon biopsies</article-title>. <source>Nat. Commun.</source>
<volume>9</volume>, <fpage>1661</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-018-03766-z</pub-id>
<pub-id pub-id-type="pmid">29695774</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bu</surname><given-names>H.</given-names></name><name><surname>Gan</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Zhou</surname><given-names>S.</given-names></name><name><surname>Guan</surname><given-names>J.</given-names></name></person-group> (<year>2017</year>). <article-title>A new method for enhancer prediction based on deep belief network</article-title>. <source>BMC Bioinforma.</source>
<volume>18</volume>, <fpage>418</fpage>. <pub-id pub-id-type="doi">10.1186/s12859-017-1828-0</pub-id>
</mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buccitelli</surname><given-names>C.</given-names></name><name><surname>Selbach</surname><given-names>M.</given-names></name></person-group> (<year>2020</year>). <article-title>mRNAs, proteins and the emerging principles of gene expression control</article-title>. <source>Nat. Rev. Genet.</source>
<volume>21</volume>, <fpage>630</fpage>–<lpage>644</lpage>. <pub-id pub-id-type="doi">10.1038/s41576-020-0258-4</pub-id>
<pub-id pub-id-type="pmid">32709985</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>L.</given-names></name><name><surname>Ren</surname><given-names>X.</given-names></name><name><surname>Fu</surname><given-names>X.</given-names></name><name><surname>Peng</surname><given-names>L.</given-names></name><name><surname>Gao</surname><given-names>M.</given-names></name><name><surname>Zeng</surname><given-names>X.</given-names></name></person-group> (<year>2021</year>). <article-title>iEnhancer-XG: interpretable sequence-based enhancers and their strength predictor</article-title>. <source>Bioinformatics</source>
<volume>37</volume>, <fpage>1060</fpage>–<lpage>1067</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa914</pub-id>
<pub-id pub-id-type="pmid">33119044</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L.</given-names></name><name><surname>Zhang</surname><given-names>H.</given-names></name><name><surname>Xiao</surname><given-names>J.</given-names></name><name><surname>Nie</surname><given-names>L.</given-names></name><name><surname>Shao</surname><given-names>J.</given-names></name><name><surname>Liu</surname><given-names>W.</given-names></name><etal/></person-group> (Year). "<article-title>SCA-CNN: Spatial and channel-wise attention in convolutional networks for image captioning</article-title>", in: <year>2017</year>
<conf-name>Proceeding of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR))</conf-name>, <fpage>6298</fpage>–<lpage>6306</lpage>.</mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chou</surname><given-names>K. C.</given-names></name></person-group> (<year>1984</year>). <article-title>Low-frequency vibrations of DNA molecules</article-title>. <source>Biochem. J.</source>
<volume>221</volume>, <fpage>27</fpage>–<lpage>31</lpage>. <pub-id pub-id-type="doi">10.1042/bj2210027</pub-id>
<pub-id pub-id-type="pmid">6466317</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cvetesic</surname><given-names>N.</given-names></name><name><surname>Lenhard</surname><given-names>B.</given-names></name></person-group> (<year>2017</year>). <article-title>Core promoters across the genome</article-title>. <source>Nat. Biotechnol.</source>
<volume>35</volume>, <fpage>123</fpage>–<lpage>124</lpage>. <pub-id pub-id-type="doi">10.1038/nbt.3788</pub-id>
<pub-id pub-id-type="pmid">28178253</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dai</surname><given-names>Q.</given-names></name><name><surname>Bao</surname><given-names>C.</given-names></name><name><surname>Hai</surname><given-names>Y.</given-names></name><name><surname>Ma</surname><given-names>S.</given-names></name><name><surname>Zhou</surname><given-names>T.</given-names></name><name><surname>Wang</surname><given-names>C.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>MTGIpick allows robust identification of genomic islands from a single genome</article-title>. <source>Brief. Bioinform</source>
<volume>19</volume>, <fpage>361</fpage>–<lpage>373</lpage>. <pub-id pub-id-type="doi">10.1093/bib/bbw118</pub-id>
<pub-id pub-id-type="pmid">28025178</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emison</surname><given-names>E. S.</given-names></name><name><surname>Mccallion</surname><given-names>A. S.</given-names></name><name><surname>Kashuk</surname><given-names>C. S.</given-names></name><name><surname>Bush</surname><given-names>R. T.</given-names></name><name><surname>Grice</surname><given-names>E.</given-names></name><name><surname>Lin</surname><given-names>S.</given-names></name><etal/></person-group> (<year>2005</year>). <article-title>A common sex-dependent mutation in a RET enhancer underlies Hirschsprung disease risk</article-title>. <source>Nature</source>
<volume>434</volume>, <fpage>857</fpage>–<lpage>863</lpage>. <pub-id pub-id-type="doi">10.1038/nature03467</pub-id>
<pub-id pub-id-type="pmid">15829955</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erwin</surname><given-names>G. D.</given-names></name><name><surname>Oksenberg</surname><given-names>N.</given-names></name><name><surname>Truty</surname><given-names>R. M.</given-names></name><name><surname>Kostka</surname><given-names>D.</given-names></name><name><surname>Murphy</surname><given-names>K. K.</given-names></name><name><surname>Ahituv</surname><given-names>N.</given-names></name><etal/></person-group> (<year>2014</year>). <article-title>Integrating diverse datasets improves developmental enhancer prediction</article-title>. <source>PLoS Comput. Biol.</source>
<volume>10</volume>, <fpage>e1003677</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1003677</pub-id>
<pub-id pub-id-type="pmid">24967590</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernández</surname><given-names>M.</given-names></name><name><surname>Miranda-Saavedra</surname><given-names>D.</given-names></name></person-group> (<year>2012</year>). <article-title>Genome-wide enhancer prediction from epigenetic signatures using genetic algorithm-optimized support vector machines</article-title>. <source>Nucleic Acids Res.</source>
<volume>40</volume>, <fpage>e77</fpage>. <pub-id pub-id-type="doi">10.1093/nar/gks149</pub-id>
<pub-id pub-id-type="pmid">22328731</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Firpi</surname><given-names>H. A.</given-names></name><name><surname>Ucar</surname><given-names>D.</given-names></name><name><surname>Tan</surname><given-names>K.</given-names></name></person-group> (<year>2010</year>). <article-title>Discover regulatory DNA elements using chromatin signatures and artificial neural network</article-title>. <source>Bioinformatics</source>
<volume>26</volume>, <fpage>1579</fpage>–<lpage>1586</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btq248</pub-id>
<pub-id pub-id-type="pmid">20453004</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghandi</surname><given-names>M.</given-names></name><name><surname>Lee</surname><given-names>D.</given-names></name><name><surname>Mohammad-Noori</surname><given-names>M.</given-names></name><name><surname>Beer</surname><given-names>M. A.</given-names></name></person-group> (<year>2014</year>). <article-title>Enhanced regulatory sequence prediction using gapped k-mer features</article-title>. <source>PLoS Comput. Biol.</source>
<volume>10</volume>, <fpage>e1003711</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1003711</pub-id>
<pub-id pub-id-type="pmid">25033408</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>W.</given-names></name><name><surname>Jia</surname><given-names>C.</given-names></name></person-group> (<year>2017</year>). <article-title>EnhancerPred2.0: Predicting enhancers and their strength based on position-specific trinucleotide propensity and electron–ion interaction potential feature selection</article-title>. <source>Mol. Biosyst.</source>
<volume>13</volume>, <fpage>767</fpage>–<lpage>774</lpage>. <pub-id pub-id-type="doi">10.1039/c7mb00054e</pub-id>
<pub-id pub-id-type="pmid">28239713</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Y.</given-names></name><name><surname>Niu</surname><given-names>B.</given-names></name><name><surname>Gao</surname><given-names>Y.</given-names></name><name><surname>Fu</surname><given-names>L.</given-names></name><name><surname>Li</surname><given-names>W.</given-names></name></person-group> (<year>2010</year>). <article-title>CD-HIT suite: A web server for clustering and comparing biological sequences</article-title>. <source>Bioinformatics</source>
<volume>26</volume>, <fpage>680</fpage>–<lpage>682</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btq003</pub-id>
<pub-id pub-id-type="pmid">20053844</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>C.</given-names></name><name><surname>He</surname><given-names>W.</given-names></name></person-group> (<year>2016</year>). <article-title>EnhancerPred: A predictor for discovering enhancers based on the combination and selection of multiple features</article-title>. <source>Sci. Rep.</source>
<volume>6</volume>, <fpage>38741</fpage>. <pub-id pub-id-type="doi">10.1038/srep38741</pub-id>
<pub-id pub-id-type="pmid">27941893</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>T.-K.</given-names></name><name><surname>Hemberg</surname><given-names>M.</given-names></name><name><surname>Gray</surname><given-names>J. M.</given-names></name><name><surname>Costa</surname><given-names>A. M.</given-names></name><name><surname>Bear</surname><given-names>D. M.</given-names></name><name><surname>Wu</surname><given-names>J.</given-names></name><etal/></person-group> (<year>2010</year>). <article-title>Widespread transcription at neuronal activity-regulated enhancers</article-title>. <source>Nature</source>
<volume>465</volume>, <fpage>182</fpage>–<lpage>187</lpage>. <pub-id pub-id-type="doi">10.1038/nature09033</pub-id>
<pub-id pub-id-type="pmid">20393465</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleftogiannis</surname><given-names>D.</given-names></name><name><surname>Kalnis</surname><given-names>P.</given-names></name><name><surname>Bajic</surname><given-names>V. B.</given-names></name></person-group> (<year>2015</year>). <article-title>Deep: A general computational framework for predicting enhancers</article-title>. <source>Nucleic Acids Res.</source>
<volume>43</volume>, <fpage>e6</fpage>. <pub-id pub-id-type="doi">10.1093/nar/gku1058</pub-id>
<pub-id pub-id-type="pmid">25378307</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kong</surname><given-names>R.</given-names></name><name><surname>Xu</surname><given-names>X.</given-names></name><name><surname>Liu</surname><given-names>X.</given-names></name><name><surname>He</surname><given-names>P.</given-names></name><name><surname>Zhang</surname><given-names>M. Q.</given-names></name><name><surname>Dai</surname><given-names>Q.</given-names></name></person-group> (<year>2020</year>). <article-title>2SigFinder: The combined use of small-scale and large-scale statistical testing for genomic island detection from a single genome</article-title>. <source>BMC Bioinforma.</source>
<volume>21</volume>, <fpage>159</fpage>. <pub-id pub-id-type="doi">10.1186/s12859-020-3501-2</pub-id>
</mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>M.</given-names></name><name><surname>Zhang</surname><given-names>W.</given-names></name></person-group> (<year>2021</year>). <article-title>Phiaf: Prediction of phage-host interactions with GAN-based data augmentation and sequence-based feature fusion</article-title>. <source>Briefings Bioinforma.</source>
<volume>23</volume>, <fpage>bbab348</fpage>. <pub-id pub-id-type="doi">10.1093/bib/bbab348</pub-id>
</mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Han</surname><given-names>P.</given-names></name><name><surname>Chen</surname><given-names>W.</given-names></name><name><surname>Gao</surname><given-names>C.</given-names></name><name><surname>Wang</surname><given-names>S.</given-names></name><name><surname>Song</surname><given-names>T.</given-names></name><etal/></person-group> (<year>2022</year>). <article-title>Marppi: Boosting prediction of protein–protein interactions with multi-scale architecture residual network</article-title>. <source>Briefings Bioinforma.</source>
<volume>24</volume>, <fpage>bbac524</fpage>. <pub-id pub-id-type="doi">10.1093/bib/bbac524</pub-id>
</mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B.</given-names></name><name><surname>Fang</surname><given-names>L.</given-names></name><name><surname>Long</surname><given-names>R.</given-names></name><name><surname>Lan</surname><given-names>X.</given-names></name><name><surname>Chou</surname><given-names>K.-C.</given-names></name></person-group> (<year>2015</year>). <article-title>iEnhancer-2L: a two-layer predictor for identifying enhancers and their strength by pseudo k-tuple nucleotide composition</article-title>. <source>Bioinformatics</source>
<volume>32</volume>, <fpage>362</fpage>–<lpage>369</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btv604</pub-id>
<pub-id pub-id-type="pmid">26476782</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B.</given-names></name><name><surname>Li</surname><given-names>K.</given-names></name><name><surname>Huang</surname><given-names>D.-S.</given-names></name><name><surname>Chou</surname><given-names>K.-C.</given-names></name></person-group> (<year>2018a</year>). <article-title>iEnhancer-EL: identifying enhancers and their strength with ensemble learning approach</article-title>. <source>Bioinformatics</source>
<volume>34</volume>, <fpage>3835</fpage>–<lpage>3842</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bty458</pub-id>
<pub-id pub-id-type="pmid">29878118</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>G.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Xu</surname><given-names>J.</given-names></name><name><surname>Chen</surname><given-names>X.</given-names></name><name><surname>Bao</surname><given-names>Y.</given-names></name><etal/></person-group> (<year>2018b</year>). <article-title>Alzheimer’s disease rs11767557 variant regulates EPHA1 gene expression specifically in human whole blood</article-title>. <source>J. Alzheimer's Dis.</source>
<volume>61</volume>, <fpage>1077</fpage>–<lpage>1088</lpage>. <pub-id pub-id-type="doi">10.3233/JAD-170468</pub-id>
<pub-id pub-id-type="pmid">29332039</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>Y.</given-names></name><name><surname>Qu</surname><given-names>W.</given-names></name><name><surname>Shan</surname><given-names>G.</given-names></name><name><surname>Zhang</surname><given-names>C.</given-names></name></person-group> (<year>2015</year>). <article-title>Delta: A distal enhancer locating tool based on AdaBoost algorithm and shape features of chromatin modifications</article-title>. <source>PLoS One</source>
<volume>10</volume>, <fpage>e0130622</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0130622</pub-id>
<pub-id pub-id-type="pmid">26091399</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maston</surname><given-names>G. A.</given-names></name><name><surname>Evans</surname><given-names>S. K.</given-names></name><name><surname>Green</surname><given-names>M. R.</given-names></name></person-group> (<year>2006</year>). <article-title>Transcriptional regulatory elements in the human genome</article-title>. <source>Annu. Rev. Genomics Hum. Genet.</source>
<volume>7</volume>, <fpage>29</fpage>–<lpage>59</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.genom.7.080505.115623</pub-id>
<pub-id pub-id-type="pmid">16719718</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Min</surname><given-names>S.</given-names></name><name><surname>Lee</surname><given-names>B.</given-names></name><name><surname>Yoon</surname><given-names>S.</given-names></name></person-group> (<year>2016</year>). <article-title>Deep learning in bioinformatics</article-title>. <source>Briefings Bioinforma.</source>
<volume>18</volume>, <fpage>bbw068</fpage>–<lpage>869</lpage>. <pub-id pub-id-type="doi">10.1093/bib/bbw068</pub-id>
</mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>Q. H.</given-names></name><name><surname>Nguyen-Vo</surname><given-names>T. H.</given-names></name><name><surname>Le</surname><given-names>N. Q. K.</given-names></name><name><surname>Do</surname><given-names>T. T. T.</given-names></name><name><surname>Rahardja</surname><given-names>S.</given-names></name><name><surname>Nguyen</surname><given-names>B. P.</given-names></name></person-group> (<year>2019</year>). <article-title>iEnhancer-ECNN: identifying enhancers and their strength using ensembles of convolutional neural networks</article-title>. <source>BMC Genomics</source>
<volume>20</volume>, <fpage>951</fpage>. <pub-id pub-id-type="doi">10.1186/s12864-019-6336-3</pub-id>
<pub-id pub-id-type="pmid">31874637</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niu</surname><given-names>K.</given-names></name><name><surname>Luo</surname><given-names>X.</given-names></name><name><surname>Zhang</surname><given-names>S.</given-names></name><name><surname>Teng</surname><given-names>Z.</given-names></name><name><surname>Zhang</surname><given-names>T.</given-names></name><name><surname>Zhao</surname><given-names>Y.</given-names></name></person-group> (<year>2021</year>). <article-title>iEnhancer-EBLSTM: Identifying enhancers and strengths by ensembles of bidirectional long short-term memory</article-title>. <source>Front. Genet.</source>
<volume>12</volume>, <fpage>665498</fpage>. <pub-id pub-id-type="doi">10.3389/fgene.2021.665498</pub-id>
<pub-id pub-id-type="pmid">33833783</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pennacchio</surname><given-names>L. A.</given-names></name><name><surname>Bickmore</surname><given-names>W.</given-names></name><name><surname>Dean</surname><given-names>A.</given-names></name><name><surname>Nobrega</surname><given-names>M. A.</given-names></name><name><surname>Bejerano</surname><given-names>G.</given-names></name></person-group> (<year>2013</year>). <article-title>Enhancers: Five essential questions</article-title>. <source>Nat. Rev. Genet.</source>
<volume>14</volume>, <fpage>288</fpage>–<lpage>295</lpage>. <pub-id pub-id-type="doi">10.1038/nrg3458</pub-id>
<pub-id pub-id-type="pmid">23503198</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shang</surname><given-names>Y.</given-names></name><name><surname>Ye</surname><given-names>X.</given-names></name><name><surname>Futamura</surname><given-names>Y.</given-names></name><name><surname>Yu</surname><given-names>L.</given-names></name><name><surname>Sakurai</surname><given-names>T.</given-names></name></person-group> (<year>2022</year>). <article-title>Multiview network embedding for drug-target Interactions prediction by consistent and complementary information preserving</article-title>. <source>Briefings Bioinforma.</source>
<volume>23</volume>, <fpage>bbac059</fpage>. <pub-id pub-id-type="doi">10.1093/bib/bbac059</pub-id>
</mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shlyueva</surname><given-names>D.</given-names></name><name><surname>Stampfel</surname><given-names>G.</given-names></name><name><surname>Stark</surname><given-names>A.</given-names></name></person-group> (<year>2014</year>). <article-title>Transcriptional enhancers: From properties to genome-wide predictions</article-title>. <source>Nat. Rev. Genet.</source>
<volume>15</volume>, <fpage>272</fpage>–<lpage>286</lpage>. <pub-id pub-id-type="doi">10.1038/nrg3682</pub-id>
<pub-id pub-id-type="pmid">24614317</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shrinivas</surname><given-names>K.</given-names></name><name><surname>Sabari</surname><given-names>B. R.</given-names></name><name><surname>Coffey</surname><given-names>E. L.</given-names></name><name><surname>Klein</surname><given-names>I. A.</given-names></name><name><surname>Boija</surname><given-names>A.</given-names></name><name><surname>Zamudio</surname><given-names>A. V.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Enhancer features that drive formation of transcriptional condensates</article-title>. <source>Mol. Cell</source>
<volume>75</volume>, <fpage>549</fpage>–<lpage>561.e7</lpage>. <pub-id pub-id-type="doi">10.1016/j.molcel.2019.07.009</pub-id>
<pub-id pub-id-type="pmid">31398323</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sinden</surname><given-names>R. R.</given-names></name><name><surname>Pearson</surname><given-names>C. E.</given-names></name><name><surname>Potaman</surname><given-names>V. N.</given-names></name><name><surname>Ussery</surname><given-names>D. W.</given-names></name></person-group> (<year>1998</year>). “<article-title>Dna: Structure and function</article-title>,” in <source>Advances in genome biology</source>. Editor <person-group person-group-type="editor"><name><surname>VermaJAI</surname><given-names>R. S.</given-names></name></person-group>, <fpage>1</fpage>–<lpage>141</lpage>.</mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>M.</given-names></name><name><surname>Rajagopal</surname><given-names>N.</given-names></name><name><surname>Xie</surname><given-names>W.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Wagner</surname><given-names>U.</given-names></name><name><surname>Wang</surname><given-names>W.</given-names></name><etal/></person-group> (<year>2013</year>). <article-title>Rfecs: A random-forest based algorithm for enhancer identification from chromatin state</article-title>. <source>PLoS Comput. Biol.</source>
<volume>9</volume>, <fpage>e1002968</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1002968</pub-id>
<pub-id pub-id-type="pmid">23526891</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sokolova</surname><given-names>M.</given-names></name><name><surname>Lapalme</surname><given-names>G.</given-names></name></person-group> (<year>2009</year>). <article-title>A systematic analysis of performance measures for classification tasks</article-title>. <source>Inf. Process. Manag.</source>
<volume>45</volume>, <fpage>427</fpage>–<lpage>437</lpage>. <pub-id pub-id-type="doi">10.1016/j.ipm.2009.03.002</pub-id>
</mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vacic</surname><given-names>V.</given-names></name><name><surname>Iakoucheva</surname><given-names>L. M.</given-names></name><name><surname>Radivojac</surname><given-names>P.</given-names></name></person-group> (<year>2006</year>). <article-title>Two sample logo: A graphical representation of the differences between two sets of sequence alignments</article-title>. <source>Bioinformatics</source>
<volume>22</volume>, <fpage>1536</fpage>–<lpage>1537</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btl151</pub-id>
<pub-id pub-id-type="pmid">16632492</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C.</given-names></name><name><surname>Zou</surname><given-names>Q.</given-names></name><name><surname>Ju</surname><given-names>Y.</given-names></name><name><surname>Shi</surname><given-names>H.</given-names></name></person-group> (<year>2022a</year>). <article-title>Enhancer-FRL: Improved and robust identification of enhancers and their activities using feature representation learning</article-title>. <source>IEEE/ACM Trans. Comput. Biol. Bioinforma.</source>, <fpage>1</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1109/TCBB.2022.3204365</pub-id>
</mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Peng</surname><given-names>Q.</given-names></name><name><surname>Mou</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Li</surname><given-names>H.</given-names></name><name><surname>Han</surname><given-names>T.</given-names></name><etal/></person-group> (<year>2022b</year>). <article-title>A successful hybrid deep learning model aiming at promoter identification</article-title>. <source>BMC Bioinforma.</source>
<volume>23</volume>, <fpage>206</fpage>. <pub-id pub-id-type="doi">10.1186/s12859-022-04735-6</pub-id>
</mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Xu</surname><given-names>Y.</given-names></name><name><surname>Yang</surname><given-names>Z.</given-names></name><name><surname>Liu</surname><given-names>X.</given-names></name><name><surname>Dai</surname><given-names>Q.</given-names></name></person-group> (<year>2021</year>). <article-title>Using recursive feature selection with random forest to improve protein structural class prediction for low-similarity sequences</article-title>. <source>Comput. Math. Methods Med.</source>
<volume>2021</volume>, <fpage>5529389</fpage>. <pub-id pub-id-type="doi">10.1155/2021/5529389</pub-id>
<pub-id pub-id-type="pmid">34055035</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>S.</given-names></name><name><surname>Ou</surname><given-names>T.</given-names></name><name><surname>Xing</surname><given-names>N.</given-names></name><name><surname>Lu</surname><given-names>J.</given-names></name><name><surname>Wan</surname><given-names>S.</given-names></name><name><surname>Wang</surname><given-names>C.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Whole-genome sequencing identifies ADGRG6 enhancer mutations and FRS2 duplications as angiogenesis-related drivers in bladder cancer</article-title>. <source>Nat. Commun.</source>
<volume>10</volume>, <fpage>720</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-08576-5</pub-id>
<pub-id pub-id-type="pmid">30755618</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>X.</given-names></name><name><surname>Xu</surname><given-names>Z. C.</given-names></name><name><surname>Qiu</surname><given-names>W. R.</given-names></name><name><surname>Wang</surname><given-names>P.</given-names></name><name><surname>Ge</surname><given-names>H. T.</given-names></name><name><surname>Chou</surname><given-names>K. C.</given-names></name></person-group> (<year>2019</year>). <article-title>iPSW(2L)-PseKNC: A two-layer predictor for identifying promoters and their strength by hybrid features via pseudo K-tuple nucleotide composition</article-title>. <source>Genomics</source>
<volume>111</volume>, <fpage>1785</fpage>–<lpage>1793</lpage>. <pub-id pub-id-type="doi">10.1016/j.ygeno.2018.12.001</pub-id>
<pub-id pub-id-type="pmid">30529532</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>Z.</given-names></name><name><surname>Zou</surname><given-names>Q.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Yang</surname><given-names>X.</given-names></name></person-group> (<year>2016</year>). <article-title>Genome-wide assessment of differential translations with ribosome profiling data</article-title>. <source>Nat. Commun.</source>
<volume>7</volume>, <fpage>11194</fpage>. <pub-id pub-id-type="doi">10.1038/ncomms11194</pub-id>
<pub-id pub-id-type="pmid">27041671</pub-id></mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>B.</given-names></name><name><surname>Liu</surname><given-names>F.</given-names></name><name><surname>Ren</surname><given-names>C.</given-names></name><name><surname>Ouyang</surname><given-names>Z.</given-names></name><name><surname>Xie</surname><given-names>Z.</given-names></name><name><surname>Bo</surname><given-names>X.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>BiRen: Predicting enhancers with a deep-learning-based model using the DNA sequence alone</article-title>. <source>Bioinformatics</source>
<volume>33</volume>, <fpage>1930</fpage>–<lpage>1936</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btx105</pub-id>
<pub-id pub-id-type="pmid">28334114</pub-id></mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>S.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Dai</surname><given-names>Q.</given-names></name></person-group> (<year>2020</year>). <article-title>Masqc: Next generation sequencing assists third generation sequencing for quality control in N6-methyladenine DNA identification</article-title>. <source>Front. Genet.</source>
<volume>11</volume>, <fpage>269</fpage>. <pub-id pub-id-type="doi">10.3389/fgene.2020.00269</pub-id>
<pub-id pub-id-type="pmid">32269589</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Z.</given-names></name><name><surname>Yi</surname><given-names>W.</given-names></name><name><surname>Tao</surname><given-names>J.</given-names></name><name><surname>Liu</surname><given-names>X.</given-names></name><name><surname>Zhang</surname><given-names>M. Q.</given-names></name><name><surname>Chen</surname><given-names>G.</given-names></name><etal/></person-group> (<year>2022</year>). <article-title>HPVMD-C: A disease-based mutation database of human papillomavirus in China</article-title>. <source>Database J. Biol. Databases Curation</source>
<volume>2022</volume>. <pub-id pub-id-type="doi">10.1093/database/baac018</pub-id>
</mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>T.</given-names></name><name><surname>Tang</surname><given-names>Q.</given-names></name><name><surname>Nie</surname><given-names>F.</given-names></name><name><surname>Zhao</surname><given-names>Q.</given-names></name><name><surname>Chen</surname><given-names>W.</given-names></name></person-group> (<year>2022</year>). <article-title>DeepLncPro: An interpretable convolutional neural network model for identifying long non-coding RNA promoters</article-title>. <source>Briefings Bioinforma.</source>
<volume>23</volume>, <fpage>bbac447</fpage>. <pub-id pub-id-type="doi">10.1093/bib/bbac447</pub-id>
</mixed-citation>
    </ref>
  </ref-list>
</back>
