<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9449940</article-id>
    <article-id pub-id-type="publisher-id">4905</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-022-04905-6</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>GeneralizedDTA: combining pre-training and multi-task learning to predict drug-target binding affinity for unknown drug discovery</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Lin</surname>
          <given-names>Shaofu</given-names>
        </name>
        <address>
          <email>linshaofu@bjut.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Shi</surname>
          <given-names>Chengyu</given-names>
        </name>
        <address>
          <email>scy88@emails.bjut.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Chen</surname>
          <given-names>Jianhui</given-names>
        </name>
        <address>
          <email>chenjianhui@bjut.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.28703.3e</institution-id><institution-id institution-id-type="ISNI">0000 0000 9040 3743</institution-id><institution>Faculty of Information Technology, </institution><institution>Beijing University of Technology, </institution></institution-wrap>No. 100, Pingleyuan, Chaoyang District, Beijing, 100124 China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.28703.3e</institution-id><institution-id institution-id-type="ISNI">0000 0000 9040 3743</institution-id><institution>Beijing International Collaboration Base on Brain Informatics and Wisdom Services, </institution><institution>Beijing University of Technology, </institution></institution-wrap>No. 100, Pingleyuan, Chaoyang District, Beijing, 100124 China </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.28703.3e</institution-id><institution-id institution-id-type="ISNI">0000 0000 9040 3743</institution-id><institution>Beijing Key Laboratory of MRI and Brain Informatics, </institution><institution>Beijing University Of Technology, </institution></institution-wrap>No. 100, Pingleyuan, Chaoyang District, Beijing, 100124 China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>7</day>
      <month>9</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>7</day>
      <month>9</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>23</volume>
    <elocation-id>367</elocation-id>
    <history>
      <date date-type="received">
        <day>26</day>
        <month>1</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>23</day>
        <month>8</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Accurately predicting drug-target binding affinity (DTA) in silico plays an important role in drug discovery. Most of the computational methods developed for predicting DTA use machine learning models, especially deep neural networks, and depend on large-scale labelled data. However, it is difficult to learn enough feature representation from tens of millions of compounds and hundreds of thousands of proteins only based on relatively limited labelled drug-target data. There are a large number of unknown drugs, which never appear in the labelled drug-target data. This is a kind of out-of-distribution problems in bio-medicine. Some recent studies adopted self-supervised pre-training tasks to learn structural information of amino acid sequences for enhancing the feature representation of proteins. However, the task gap between pre-training and DTA prediction brings the catastrophic forgetting problem, which hinders the full application of feature representation in DTA prediction and seriously affects the generalization capability of models for unknown drug discovery.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">To address these problems, we propose the GeneralizedDTA, which is a new DTA prediction model oriented to unknown drug discovery, by combining pre-training and multi-task learning. We introduce self-supervised protein and drug pre-training tasks to learn richer structural information from amino acid sequences of proteins and molecular graphs of drug compounds, in order to alleviate the problem of high variance caused by encoding based on deep neural networks and accelerate the convergence of prediction model on small-scale labelled data. We also develop a multi-task learning framework with a dual adaptation mechanism to narrow the task gap between pre-training and prediction for preventing overfitting and improving the generalization capability of DTA prediction model on unknown drug discovery. To validate the effectiveness of our model, we construct an unknown drug data set to simulate the scenario of unknown drug discovery. Compared with existing DTA prediction models, the experimental results show that our model has the higher generalization capability in the DTA prediction of unknown drugs.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">The advantages of our model are mainly attributed to two kinds of pre-training tasks and the multi-task learning framework, which can learn richer structural information of proteins and drugs from large-scale unlabeled data, and then effectively integrate it into the downstream prediction task for obtaining a high-quality DTA prediction in unknown drug discovery.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>DTA prediction</kwd>
      <kwd>Pre-training task</kwd>
      <kwd>Multi-task learning</kwd>
      <kwd>Dual adaptation mechanism</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100012166</institution-id>
            <institution>National Key Research and Development Program of China</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2020YFB2104402</award-id>
        <award-id>2020YFB2104402</award-id>
        <award-id>2020YFB2104402</award-id>
        <principal-award-recipient>
          <name>
            <surname>Lin</surname>
            <given-names>Shaofu</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>Chengyu</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Jianhui</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100005089</institution-id>
            <institution>Beijing Municipal Natural Science Foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>4222022</award-id>
        <award-id>4222022</award-id>
        <principal-award-recipient>
          <name>
            <surname>Shi</surname>
            <given-names>Chengyu</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Jianhui</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par4">Drug discovery is very inefficient by traditional wet laboratory experiments [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>]. It usually spends 10–17 years and billions of dollars on research and experimental processes [<xref ref-type="bibr" rid="CR3">3</xref>]. Such an inefficient process is obviously difficult to meet the needs of rapidly developing diseases, such as COVID-19. In order to improve the efficiency of drug discovery, predicting drug-target interaction (DTI) in silico has attracted more and more attention [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR4">4</xref>–<xref ref-type="bibr" rid="CR7">7</xref>]. These computational DTI prediction methods not only have low cost but also can greatly accelerate the process of drug development [<xref ref-type="bibr" rid="CR8">8</xref>].</p>
    <p id="Par5">Predicting drug-target binding affinity (DTA) [<xref ref-type="bibr" rid="CR9">9</xref>] is a kind of special DTI prediction task. Unlike traditional DTI prediction based on binary classification, DTA prediction can obtain the quantitative binding affinity between drugs and targets, which provides more detailed descriptions about drug-target interactions. Related studies mainly adopted machine learning models to realize a two-stage modeling process, including encoding and decoding. The encoding process learns feature representations from drugs and various targets, such as proteins. The decoding process predicts the binding affinity based on these feature representations. Early studies often adopted shallow machine learning models to learn feature representations for DTA prediction. SimBoost [<xref ref-type="bibr" rid="CR10">10</xref>] calculated the affinity similarity between drug compounds and targets by using collaborative filtering and then used the similarity as the feature vector to predict DTA. KronRLS [<xref ref-type="bibr" rid="CR11">11</xref>] used kernel-based methods to generate molecular descriptors of drugs. With the rapid development of deep learning, the deep neural networks have been widely used in DTA prediction, especially in the encoding process. DeepDTA [<xref ref-type="bibr" rid="CR12">12</xref>] introduced deep learning into DTA prediction for the first time, which used convolutional neural network (CNN) to generate 1D representations of drugs and proteins. GraphDTA [<xref ref-type="bibr" rid="CR13">13</xref>] used the open source chemical informatics software RDKit to construct the molecular graph of drug compounds instead of the compound string, and learnt the feature vector of drug compounds by using graph neural network. MGraphDTA [<xref ref-type="bibr" rid="CR14">14</xref>] built a super-deep GNN with 27 graph convolutional layers to capture the local and global structure of the compound simultaneously. MATT<inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\_$$\end{document}</tex-math><mml:math id="M2"><mml:mi>_</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq1.gif"/></alternatives></inline-formula>DTI [<xref ref-type="bibr" rid="CR15">15</xref>] encoded the correlations between atoms of drug compounds by a relation-aware self-attention block and modeled the interaction of drug representations and target representations by the multi-head attention block. DeepNC [<xref ref-type="bibr" rid="CR16">16</xref>] learnt the features of drugs and targets by the layers of GNN and 1-D convolution network, respectively. MINN-DTI [<xref ref-type="bibr" rid="CR17">17</xref>] combined an interacting-transformer module with an improved Communicative Message Passing Neural Network (CMPNN) to better capture the two-way impact between drugs and targets. Besides feature coding of drugs and proteins, feature aggregation has also attracted attention. FusionDTA [<xref ref-type="bibr" rid="CR18">18</xref>] utilized a novel muti-head linear attention mechanism to aggregates global information based on attention weights.</p>
    <p id="Par6">All of the above studies are based on labelled drug-target data sets, such as Davis [<xref ref-type="bibr" rid="CR19">19</xref>] and Kiba [<xref ref-type="bibr" rid="CR20">20</xref>]. Compared with tens of millions of compounds and hundreds of thousands of proteins, labelled drug-target data are relatively limited. The Davis data set [<xref ref-type="bibr" rid="CR19">19</xref>] only contains 72 drugs and 442 targets. The KEGG data set [<xref ref-type="bibr" rid="CR21">21</xref>] only has a total of 4797 drug-target pairs. However, the ZINC15 database [<xref ref-type="bibr" rid="CR22">22</xref>] contains over 230 million compounds in ready-to-dock. It is difficult to learn feature representations covering all drugs and compounds only based on relatively small labelled drug-target data. Aiming at this problem, Hu et al. [<xref ref-type="bibr" rid="CR23">23</xref>] performed the protein pre-training task on large amounts of unlabelled data to obtain the robust protein encoding model with enhanced structural information of amino acid sequences, and then fine-tuned the encoding model on the decoding process, i.e., the DTA prediction modeling process, for fitting the relatively small labelled drug-target data. Owing to enhanced structural information, their DTA prediction model achieved excellent results.</p>
    <p id="Par7">However, Hu et al.’s model only obtained structural information about amino acid sequences by using the protein pre-training task and neglected structural information of molecular graphs of drug compounds. More importantly, there is a task gap between pre-training and DTA prediction. The goal of protein pre-training is to accurately predict masked amino acids based on context information of amino acid sequences, but the goal of DTA prediction is to accurately calculate the binding affinity between drug compounds and proteins. Hu et al. adopted a sequential structure to integrate the pre-training task and the DTA prediction task [<xref ref-type="bibr" rid="CR23">23</xref>]. The task gap between them can bring the catastrophic forgetting problem [<xref ref-type="bibr" rid="CR24">24</xref>]. As the number of fine-tuning iterations increases, the downstream prediction model increasingly focuses on the drugs and proteins appearing frequently in the labelled drug-target training data, resulting in poor prediction results on those unknown drugs, which never appear in the labelled drug-target data. This is a kind of out-of-distribution (OOD) problems in biomedicine [<xref ref-type="bibr" rid="CR25">25</xref>]. The DTA prediction model has the poor generalization capability [<xref ref-type="bibr" rid="CR26">26</xref>] on unknown drug discovery. This problem is particularly serious when labelled data are obviously smaller than unlabeled pre-training data.</p>
    <p id="Par8">However, existing studies on DTI and DTA prediction did not pay special attention to these unknown drugs. To our knowledge, the poor generalization ability of model in unknown drug discovery has not been studies. In order to prove the existence of this problem, we used the Davis data set to perform a DTA prediction task for unknown drug discovery. The original training and test sets were divided referring to Öztürk et al.’s work [<xref ref-type="bibr" rid="CR13">13</xref>]. We randomly selected 20<inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\%$$\end{document}</tex-math><mml:math id="M4"><mml:mo>%</mml:mo></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq2.gif"/></alternatives></inline-formula> of drugs in the original training set, a total of 14 kinds of drugs, as new drugs. All corresponding drug-target pairs were deleted from the original training set to construct an unknown drug training set. The corresponding 5178 drug-target pairs were extracted from the original test set to construct an unknown drug test set. The DTA prediction task in unknown drug discovery was performed on the unknown drug training and test sets. Using GraphDTA [<xref ref-type="bibr" rid="CR12">12</xref>] to iterate 1000 times, the results are as follows.</p>
    <p id="Par9">Figure <xref rid="Fig1" ref-type="fig">1</xref> shows the convergence curve of loss function in 1000 times of iterations. The horizontal axis represents the number of iterations and the vertical axis represents the value of loss function. As shown in this figure, the losses on the unknown drug training set and the original test set decrease significantly in the first 200 iterations, the loss on the unknown drug test set fluctuates repeatedly at 0.85 and has no downward trend. This indicates that GraphDTA is over fitted and lacks the sufficient generalization capability for unknown drug discovery. It is necessary to carry out special studies on this problem.<fig id="Fig1"><label>Fig. 1</label><caption><p>Convergence analysis of GraphDTA in unknown drug discovery</p></caption><graphic xlink:href="12859_2022_4905_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par10">In previous studies, overfitting of model can be intervened by means of data enhancement, feature removal, and so on. For the DTA prediction task, data enhancement is too expensive because it needs to increase labelled drug-target data. Feature removal may reduce the accuracy of model and deviates from the original intention of feature enhancement of pre-training. Based on the above observations, this study proposes a new DTA prediction model, called GeneralizedDTA, by combining self-supervised pre-training and multi-task learning. The main contributions can be summarized as follows: <list list-type="order"><list-item><p id="Par11">Firstly, this study introduces both protein and drug pre-training tasks into the DTA prediction task. By using these two kinds of pre-training tasks, structural information of both amino acid sequences of proteins and molecular graphs of drug compounds is learnt and integrated in the DTA prediction task for the first time.</p></list-item><list-item><p id="Par12">Secondly, this study develops a multi-task learning model with a dual adaptation mechanism for alleviating the catastrophic forgetting problem of pre-training parameters. By using the MAML-based updating strategy, pre-training parameters are adapted by a few gradient updates, and then with the updated parameters, the whole model is trained in the downstream DTA prediction task for accelerating convergence and preventing the model from falling into local optimality.</p></list-item><list-item><p id="Par13">Thirdly, this study constructs a group of unknown drug data sets to simulate a scenario of unknown drug discovery and performs comparative experiments on these data sets. The experimental results show that the generalization capability of our model has been significantly improved compared with existing DTA prediction models. It can be better adapted to DTA prediction in unknown drug discovery.</p></list-item></list></p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <p id="Par14">In order to realize DTA prediction in unknown drug discovery, this study proposes the GeneralizedDTA model by combining self-supervised pre-training and multi-task learning. Two kinds of protein pre-training tasks are adopted to learn structural information of amino acid sequences. A kind of new drug pre-training task is designed to learn structural information of molecular graphs of drug compounds. In order to alleviate the catastrophic forgetting problem of pre-training parameters, a multi-task learning framework with a dual adaptation mechanism is developed to prevent the prediction model from falling into overfitting. Figure <xref rid="Fig2" ref-type="fig">2</xref> gives the model architecture of GeneralizedDTA, which includes four modules: the protein encoding layer, the drug encoding layer, the DTA prediction layer, and the multi-task learning framework.<fig id="Fig2"><label>Fig. 2</label><caption><p>The model architecture of GeneralizedDTA</p></caption><graphic xlink:href="12859_2022_4905_Fig2_HTML" id="MO2"/></fig></p>
    <sec id="Sec3">
      <title>Protein encoding layer</title>
      <p id="Par15">The protein encoding layer encodes amino acid sequences of proteins as vectors by using protein pre-training tasks. Inspired by BERT [<xref ref-type="bibr" rid="CR27">27</xref>], this study adopts a transformer model with the multi-head attention as the encoder to receive amino acid sequences. Given a amino acid sequence <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t=\left[ t_{1}, \ldots , t_{n}\right]$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq3.gif"/></alternatives></inline-formula> where <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t_{i} \in \{21$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>∈</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>21</mml:mn></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq4.gif"/></alternatives></inline-formula> amino acid types<inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\}$$\end{document}</tex-math><mml:math id="M10"><mml:mo stretchy="false">}</mml:mo></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq5.gif"/></alternatives></inline-formula>, the transformer model converts it into <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z=\left[ z_{1}, \ldots , z_{n}\right]$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq6.gif"/></alternatives></inline-formula> as follows:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned}&amp;z={\text {Transformer}}(Q, K, V ; t)=\text {Concat}\left( \text {head}_{1}, \ldots , \text {head}_{n}\right) W^{\circ } \end{aligned}$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mtext>Transformer</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>;</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>Concat</mml:mtext><mml:mfenced close=")" open="("><mml:msub><mml:mtext>head</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mtext>head</mml:mtext><mml:mi>n</mml:mi></mml:msub></mml:mfenced><mml:msup><mml:mi>W</mml:mi><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned}&amp;\text{ head}_{i}=\text{ Attention }(Q, K, V) \end{aligned}$$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="0.333333em"/><mml:msub><mml:mtext>head</mml:mtext><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mspace width="0.333333em"/><mml:mtext>Attention</mml:mtext><mml:mspace width="0.333333em"/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned}&amp;{\text {Attention}}(Q, K, V)={\text {softmax}}\left( \frac{Q K^{T}}{\sqrt{d_{k}}}\right) V \end{aligned}$$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mtext>Attention</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>softmax</mml:mtext><mml:mfenced close=")" open="("><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mfenced><mml:mi>V</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq7"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Q \in R^{d_{1} \times d_{2}}, K \in R^{d_{1} \times d_{2}}, V \in R^{d_{1} \times d_{2}}$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq7.gif"/></alternatives></inline-formula> are the parameters of attention, <italic>n</italic> is the number of heads, <inline-formula id="IEq8"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W^{o} \in R^{d_{1}} \times d_{1}$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msup><mml:mo>×</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq8.gif"/></alternatives></inline-formula> is the weight of heads and <inline-formula id="IEq9"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sqrt{d_{k}}$$\end{document}</tex-math><mml:math id="M24"><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq9.gif"/></alternatives></inline-formula> is the dimension number of Q. The self-attention function is computed on the dot products of each queries with all keys simultaneously, and divided by a softmax function to obtain the weights on the values [<xref ref-type="bibr" rid="CR28">28</xref>]. It can be simplified as a parameterized function Transformer (<inline-formula id="IEq10"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\bullet$$\end{document}</tex-math><mml:math id="M26"><mml:mo>∙</mml:mo></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq10.gif"/></alternatives></inline-formula>) with the parameter set <inline-formula id="IEq11"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta$$\end{document}</tex-math><mml:math id="M28"><mml:mi>θ</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq11.gif"/></alternatives></inline-formula> :<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} z=\text{ Transformer }(\theta ; t), \theta =\left\{ Q, K, V, W^{o}\right\} \end{aligned}$$\end{document}</tex-math><mml:math id="M30" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mspace width="0.333333em"/><mml:mtext>Transformer</mml:mtext><mml:mspace width="0.333333em"/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="}" open="{"><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msup></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>Based on the transformer model, this study adopts two pre-training tasks to obtain structural information of amino acid sequences of proteins.</p>
      <p id="Par16">Masked Language Modeling (MLM) Task [<xref ref-type="bibr" rid="CR28">28</xref>]: this task screens some amino acids at random and predicts their types. Given a masked amino acid sequence <italic>t</italic> and a masked amino acid set <inline-formula id="IEq12"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m=\left\{ m_{1}, m_{2}, \ldots , m_{N}\right\}$$\end{document}</tex-math><mml:math id="M32"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="}" open="{"><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq12.gif"/></alternatives></inline-formula>, the MLM decoder calculates the log probability for <italic>t</italic> as follows:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} z= &amp; {} \text{ Transformer }(\theta ; t) \end{aligned}$$\end{document}</tex-math><mml:math id="M34" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mspace width="0.333333em"/><mml:mtext>Transformer</mml:mtext><mml:mspace width="0.333333em"/><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} m^{\prime }= &amp; {} F C\left( \theta _{1}; z\right) \end{aligned}$$\end{document}</tex-math><mml:math id="M36" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:mi>z</mml:mi></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq13"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F C(\bullet )$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>∙</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq13.gif"/></alternatives></inline-formula> is a fully connected neural network (FC) with the parameter <inline-formula id="IEq14"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta _{1}$$\end{document}</tex-math><mml:math id="M40"><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq14.gif"/></alternatives></inline-formula> and <inline-formula id="IEq15"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m^{\prime }=\left\{ m_{1}^{\prime }, m_{2}^{\prime }, \ldots , m_{N}^{\prime }\right\}$$\end{document}</tex-math><mml:math id="M42"><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mfenced close="}" open="{"><mml:msubsup><mml:mi>m</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq15.gif"/></alternatives></inline-formula> represents the predicted amino acid set for the whole masked amino acid set. Then the log-likelihood function is used as the evaluation metrics for the MLM task:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\mathcal {L}}^{\mathbf {M L M}}\left( \theta , \theta _{1} ; m\right) =-\left[ \sum _{i=1}^{N} m_{i}^{\prime } \ln m_{i}+\left( 1-m_{i}^{\prime }\right) \ln \left( 1-m_{i}\right) \right] \end{aligned}$$\end{document}</tex-math><mml:math id="M44" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">M</mml:mi><mml:mi mathvariant="bold">L</mml:mi><mml:mi mathvariant="bold">M</mml:mi></mml:mrow></mml:msup><mml:mfenced close=")" open="("><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:mi>m</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfenced close="]" open="["><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msubsup><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>ln</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mfenced close=")" open="("><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:mfenced><mml:mo>ln</mml:mo><mml:mfenced close=")" open="("><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>By the above MLM task, the transformer model could effectively learn the bidirectional contextual representation of amino acid sequences of proteins.</p>
      <p id="Par17">Same Family Prediction (SFP) Task [<xref ref-type="bibr" rid="CR29">29</xref>, <xref ref-type="bibr" rid="CR30">30</xref>]: this task enables the model to determine if two proteins belong to the same family. In order to pre-train the transformer model with the SFP task, this study selects two amino acid sequences <inline-formula id="IEq16"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t^{1}$$\end{document}</tex-math><mml:math id="M46"><mml:msup><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq16.gif"/></alternatives></inline-formula> and <inline-formula id="IEq17"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t^{2}$$\end{document}</tex-math><mml:math id="M48"><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq17.gif"/></alternatives></inline-formula> from the Pfam dataset. Random sampling is adopted to ensure the probabilities that they come from the same class and different classes are the same. Aiming at the protein pair <inline-formula id="IEq18"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\langle \mathrm {t}^{1}, \mathrm {t}^{2}\right\rangle$$\end{document}</tex-math><mml:math id="M50"><mml:mfenced close="〉" open="〈"><mml:msup><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq18.gif"/></alternatives></inline-formula>, a FC with dropout [<xref ref-type="bibr" rid="CR31">31</xref>] is used to calculate their similarity value:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\hat{c}}=F C\left( \theta _{2} ; z_p\right) \end{aligned}$$\end{document}</tex-math><mml:math id="M52" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq19"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta _{2} \in {\mathbb {R}}^{|z| \times 2}$$\end{document}</tex-math><mml:math id="M54"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq19.gif"/></alternatives></inline-formula> is the parameter of FC, <inline-formula id="IEq20"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z_p=\left[ z_{1}^{1}, \cdots , z_{1_{1}}^{1}, z_{1}^{2}, \cdots , z_{n_{2}}^{2}\right] z \in {\mathbb {R}}^{|z|\times 1}$$\end{document}</tex-math><mml:math id="M56"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:msub><mml:mn>1</mml:mn><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfenced><mml:mi>z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq20.gif"/></alternatives></inline-formula> is the vector representation of <inline-formula id="IEq21"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\langle \mathrm {t}^{1}, \mathrm {t}^{2}\right\rangle$$\end{document}</tex-math><mml:math id="M58"><mml:mfenced close="〉" open="〈"><mml:msup><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq21.gif"/></alternatives></inline-formula> and <inline-formula id="IEq22"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{c}} \in {\mathbb {R}}^{2 \times 1}$$\end{document}</tex-math><mml:math id="M60"><mml:mrow><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq22.gif"/></alternatives></inline-formula> is the predicted similarity value, i.e., a probability that the protein pair belongs to the same protein family. The SFP task trains the model to minimize the cross-entropy loss which is designed to deal with predicted errors on probabilities. Therefore, this study adopts the log-likelihood function to measure the SFP loss:<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\mathcal {L}}^{\mathrm {SFP}}\left( \theta , \theta _{2} ; t\right) =-\ln p\left( n=n_{i} \mid \theta , \theta _{2}\right) , \quad n_{i} \in [ \text{ same } \text{ family, } \text{ not } \text{ same } \text{ family}] \end{aligned}$$\end{document}</tex-math><mml:math id="M62" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mi mathvariant="normal">SFP</mml:mi></mml:msup><mml:mfenced close=")" open="("><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:mi>t</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mo>ln</mml:mo><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mspace width="0.333333em"/><mml:mtext>same</mml:mtext><mml:mspace width="0.333333em"/><mml:mspace width="0.333333em"/><mml:mtext>family,</mml:mtext><mml:mspace width="0.333333em"/><mml:mspace width="0.333333em"/><mml:mtext>not</mml:mtext><mml:mspace width="0.333333em"/><mml:mspace width="0.333333em"/><mml:mtext>same</mml:mtext><mml:mspace width="0.333333em"/><mml:mspace width="0.333333em"/><mml:mtext>family</mml:mtext><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula>As the transformer model is asked to produce the higher similarity value for proteins from the same family, the SFP task enables the transformer model to better absorb global structural information of amino acid sequences of proteins.</p>
    </sec>
    <sec id="Sec4">
      <title>Drug encoding layer</title>
      <p id="Par18">The drug encoding layer encodes molecular graphs of drug compounds as vectors by a brand-new drug pre-training task. It adopts GCN [<xref ref-type="bibr" rid="CR32">32</xref>] to mine potential relationships from molecular graphs of drug compounds.</p>
      <p id="Par19">Given a molecular graph of drug compound <inline-formula id="IEq23"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal {G}}=({\mathcal {V}}, {\mathcal {E}}, {\mathcal {X}}, {\mathcal {Z}})$$\end{document}</tex-math><mml:math id="M64"><mml:mrow><mml:mi mathvariant="script">G</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">V</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">E</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq23.gif"/></alternatives></inline-formula> where <inline-formula id="IEq24"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal {V}}$$\end{document}</tex-math><mml:math id="M66"><mml:mi mathvariant="script">V</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq24.gif"/></alternatives></inline-formula> is the chemical atom set, <inline-formula id="IEq25"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal {E}}$$\end{document}</tex-math><mml:math id="M68"><mml:mi mathvariant="script">E</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq25.gif"/></alternatives></inline-formula> is the chemical bond set, <inline-formula id="IEq26"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal {X}} \in {\mathbb {R}}^{|\nu | \times d_{v}}$$\end{document}</tex-math><mml:math id="M70"><mml:mrow><mml:mi mathvariant="script">X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>ν</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq26.gif"/></alternatives></inline-formula> and <inline-formula id="IEq27"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal {Z}} \in {\mathbb {R}}^{|\varepsilon | \times d_{e}}$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:mi mathvariant="script">Z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>ε</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq27.gif"/></alternatives></inline-formula> are the atom and bond feature sets, respectively. GCN is mainly involved with two key computations “update” and “aggregate” for each atom at every layer. They can be represented as a parameterized function <inline-formula id="IEq28"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Psi (\bullet )$$\end{document}</tex-math><mml:math id="M74"><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>∙</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq28.gif"/></alternatives></inline-formula> with the parameter <inline-formula id="IEq29"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\psi$$\end{document}</tex-math><mml:math id="M76"><mml:mi>ψ</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq29.gif"/></alternatives></inline-formula> :<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \begin{aligned} {\mathbf {h}}_{v}^{l}&amp;=\Psi (\psi ; {\mathcal {A}}, {\mathcal {X}}, {\mathcal {Z}})^{l}\\&amp;={\text {UPDATE}}\left( {\mathbf {h}}_{v}^{l-1}, {\text {AGGREGATE}}\left( \left\{ \left( {\mathbf {h}}_{v}^{l-1}, {\mathbf {h}}_{w}^{l-1}, {\mathbf {z}}_{w v}\right) : u \in {\mathcal {N}}_{v}\right\} \right) \right) \end{aligned} \end{aligned}$$\end{document}</tex-math><mml:math id="M78" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Ψ</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ψ</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="script">A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>l</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mtext>UPDATE</mml:mtext><mml:mfenced close=")" open="("><mml:msubsup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mtext>AGGREGATE</mml:mtext><mml:mfenced close=")" open="("><mml:mfenced close="}" open="{"><mml:mfenced close=")" open="("><mml:msubsup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi mathvariant="italic">wv</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mo>:</mml:mo><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mfenced></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq30"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$u, v \in {\mathcal {V}}$$\end{document}</tex-math><mml:math id="M80"><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">V</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq30.gif"/></alternatives></inline-formula> are two chemical atoms, <inline-formula id="IEq31"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z_{u v}$$\end{document}</tex-math><mml:math id="M82"><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi mathvariant="italic">uv</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq31.gif"/></alternatives></inline-formula> is the feature vector of the chemical bond (<italic>u</italic>, <italic>v</italic>), <inline-formula id="IEq32"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm {h}_{v}^{0}=\mathrm {x}_{v} \in {\mathcal {X}}$$\end{document}</tex-math><mml:math id="M84"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">h</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="script">X</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq32.gif"/></alternatives></inline-formula> is the input of GCN and represents the feature of atom <italic>v</italic>, <inline-formula id="IEq33"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm {h}_{v}^{l}$$\end{document}</tex-math><mml:math id="M86"><mml:msubsup><mml:mi mathvariant="normal">h</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq33.gif"/></alternatives></inline-formula> represents the feature of atom <italic>v</italic> on the l-th layer of GCN, <inline-formula id="IEq34"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal {A}}$$\end{document}</tex-math><mml:math id="M88"><mml:mi mathvariant="script">A</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq34.gif"/></alternatives></inline-formula> is the adjacency matrix of drug compound <inline-formula id="IEq35"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal {G}}$$\end{document}</tex-math><mml:math id="M90"><mml:mi mathvariant="script">G</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq35.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq36"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal {N}}_{v}$$\end{document}</tex-math><mml:math id="M92"><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq36.gif"/></alternatives></inline-formula> is the neighborhood atom set of atom <italic>v</italic>.</p>
      <p id="Par20">In order to get a representation of drug compound <inline-formula id="IEq37"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal {G}}$$\end{document}</tex-math><mml:math id="M94"><mml:mi mathvariant="script">G</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq37.gif"/></alternatives></inline-formula>, the POOLING function on the last GCN layer is used to transform the molecular graph into a vector:<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\mathbf {h}}_{{\mathcal {G}}}={\text {POOLING}}\left( \left\{ {\mathbf {h}}_{v}^{l} \mid v \in {\mathcal {V}}\right\} \right) \end{aligned}$$\end{document}</tex-math><mml:math id="M96" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi mathvariant="script">G</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>POOLING</mml:mtext><mml:mfenced close=")" open="("><mml:mfenced close="}" open="{"><mml:msubsup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mo>∣</mml:mo><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">V</mml:mi></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq38"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{{\mathcal {G}}}$$\end{document}</tex-math><mml:math id="M98"><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="script">G</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq38.gif"/></alternatives></inline-formula> is the vector representation of drug compound <inline-formula id="IEq39"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal {G}}$$\end{document}</tex-math><mml:math id="M100"><mml:mi mathvariant="script">G</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq39.gif"/></alternatives></inline-formula> , POOLING is a simple pooling function like the max or mean-pooling [<xref ref-type="bibr" rid="CR33">33</xref>, <xref ref-type="bibr" rid="CR34">34</xref>]. For simplicity, we represent GCN as follows:<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\mathbf {h}}_{{\mathcal {G}}}=G C N (\psi ; {\mathcal {G}}) \end{aligned}$$\end{document}</tex-math><mml:math id="M102" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi mathvariant="script">G</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ψ</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="script">G</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula>Based on the GCN model, this study designs a new pre-training task to learn structural information of molecular graphs of drug compounds.</p>
      <p id="Par21">Drug Pre-training (DP) Task: this new task is designed to improve the representation learning capability on drugs by encouraging the generation of similar embeddings for neighboring chemical atoms in the molecular graph of drug compounds [<xref ref-type="bibr" rid="CR35">35</xref>]. The aggregation is a key computation in each layer of GCN. In compound-level aggregation, the neighboring chemical atoms aggregate their information based on Eq. (<xref rid="Equ10" ref-type="">10</xref>) [<xref ref-type="bibr" rid="CR36">36</xref>, <xref ref-type="bibr" rid="CR37">37</xref>]. For each chemical atom <inline-formula id="IEq40"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$v \in {\mathcal {V}}$$\end{document}</tex-math><mml:math id="M104"><mml:mrow><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">V</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq40.gif"/></alternatives></inline-formula>, GCN gets its representation by <inline-formula id="IEq41"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm {h}_{v}$$\end{document}</tex-math><mml:math id="M106"><mml:msub><mml:mi mathvariant="normal">h</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq41.gif"/></alternatives></inline-formula> and <inline-formula id="IEq42"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Psi (\bullet )$$\end{document}</tex-math><mml:math id="M108"><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>∙</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq42.gif"/></alternatives></inline-formula> in Eq. (<xref rid="Equ10" ref-type="">10</xref>). Therefore, as shown in Fig.<xref rid="Fig3" ref-type="fig">3</xref>, given a random atom bond <italic>u</italic> as the center node, the self-supervised loss function [<xref ref-type="bibr" rid="CR38">38</xref>] is chosen to realize the DP task, i.e., encourage similar embeddings for neighboring chemical atoms:<fig id="Fig3"><label>Fig. 3</label><caption><p>The drug pre-training based on context prediction</p></caption><graphic xlink:href="12859_2022_4905_Fig3_HTML" id="MO15"/></fig></p>
      <p id="Par22"><disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\mathcal {L}}^{\text{ atom } }\left( \psi ; \mathcal {{\mathcal {G}}}\right) =\sum _{(u, v) \in {\mathcal {G}}}-\ln \left( \sigma \left( {\mathbf {h}}_{u}^{\top } {\mathbf {h}}_{v}\right) \right) -\ln \left( \sigma \left( -{\mathbf {h}}_{u}^{\top } {\mathbf {h}}_{v^{\prime }}\right) \right) \end{aligned}$$\end{document}</tex-math><mml:math id="M110" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>atom</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msup><mml:mfenced close=")" open="("><mml:mi>ψ</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="script">G</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mi mathvariant="script">G</mml:mi></mml:mrow></mml:munder><mml:mo>-</mml:mo><mml:mo>ln</mml:mo><mml:mfenced close=")" open="("><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:msubsup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mi>⊤</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mfenced></mml:mfenced><mml:mo>-</mml:mo><mml:mo>ln</mml:mo><mml:mfenced close=")" open="("><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mo>-</mml:mo><mml:msubsup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mi>⊤</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:msup><mml:mi>v</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula>where <italic>v</italic> is the context anchor node which is directly connected to the center node <italic>u</italic>, <inline-formula id="IEq43"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$v^{\prime }$$\end{document}</tex-math><mml:math id="M112"><mml:msup><mml:mi>v</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq43.gif"/></alternatives></inline-formula> is the negative context node which is not directly connected to <italic>u</italic>, <inline-formula id="IEq44"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\psi$$\end{document}</tex-math><mml:math id="M114"><mml:mi>ψ</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq44.gif"/></alternatives></inline-formula> is the parameter of GCN, and <inline-formula id="IEq45"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><mml:math id="M116"><mml:mi>σ</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq45.gif"/></alternatives></inline-formula> is the sigmoid function. By 5 layers of GCN, each atom embedding absorbs almost all small local structures in the molecular graph [<xref ref-type="bibr" rid="CR39">39</xref>, <xref ref-type="bibr" rid="CR40">40</xref>].</p>
    </sec>
    <sec id="Sec5">
      <title>DTA prediction layer</title>
      <p id="Par23">The DTA prediction layer is to associate the drug compound with the protein for predicting their binding affinity. This study adopts a FC for DTA prediction. For a given drug-protein pair <inline-formula id="IEq46"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\langle {\mathcal {G}}, t\right\rangle$$\end{document}</tex-math><mml:math id="M118"><mml:mfenced close="〉" open="〈"><mml:mi mathvariant="script">G</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq46.gif"/></alternatives></inline-formula> where <inline-formula id="IEq47"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal {G}}$$\end{document}</tex-math><mml:math id="M120"><mml:mi mathvariant="script">G</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq47.gif"/></alternatives></inline-formula> is a molecular graph of drug compound and <italic>t</italic> is an amino acid sequence, the corresponding drug compound vector <inline-formula id="IEq48"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\varvec{h}}_{\mathcal {G}}$$\end{document}</tex-math><mml:math id="M122"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq48.gif"/></alternatives></inline-formula> and the protein vector <inline-formula id="IEq49"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z_p$$\end{document}</tex-math><mml:math id="M124"><mml:msub><mml:mi>z</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq49.gif"/></alternatives></inline-formula> can be obtained by the drug encoding layer and the protein encoding layer. Then, the process of predicting their binding affinity <inline-formula id="IEq50"><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{y}}$$\end{document}</tex-math><mml:math id="M126"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq50.gif"/></alternatives></inline-formula> is shown as follows:<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\hat{y}}=F C\left( \gamma ; {\text {Concat}}\left( {\varvec{h}}_{\mathcal {G}}, z_p\right) \right) \end{aligned}$$\end{document}</tex-math><mml:math id="M128" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mfenced close=")" open="("><mml:mi>γ</mml:mi><mml:mo>;</mml:mo><mml:mtext>Concat</mml:mtext><mml:mfenced close=")" open="("><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq51"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma$$\end{document}</tex-math><mml:math id="M130"><mml:mi>γ</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq51.gif"/></alternatives></inline-formula> is the parameter of full connection layers and Concat (<inline-formula id="IEq52"><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\bullet$$\end{document}</tex-math><mml:math id="M132"><mml:mo>∙</mml:mo></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq52.gif"/></alternatives></inline-formula>) indicates that the input is the concatenated vector of <inline-formula id="IEq53"><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\varvec{h}}_{G}$$\end{document}</tex-math><mml:math id="M134"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>G</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq53.gif"/></alternatives></inline-formula> and <inline-formula id="IEq54"><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z_p$$\end{document}</tex-math><mml:math id="M136"><mml:msub><mml:mi>z</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq54.gif"/></alternatives></inline-formula>.</p>
      <p id="Par24">The DTA prediction task trains the model to minimize the loss function. This study adopts the mean squared error (MSE) as the loss function:<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M137">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\mathcal {L}}^{\text{ affinities } }(\theta , \psi , \gamma ;\left\langle {\mathcal {G}}, t\right\rangle )=\frac{1}{2}({\hat{y}}-y)^{2} \end{aligned}$$\end{document}</tex-math><mml:math id="M138" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>affinities</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo>;</mml:mo><mml:mfenced close="〉" open="〈"><mml:mi mathvariant="script">G</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mfenced><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>-</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq55"><alternatives><tex-math id="M139">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{y}}$$\end{document}</tex-math><mml:math id="M140"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq55.gif"/></alternatives></inline-formula> is the predicted binding affinity of drug-protein pair and <italic>y</italic> is the true value, <inline-formula id="IEq56"><alternatives><tex-math id="M141">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta , \psi , \gamma$$\end{document}</tex-math><mml:math id="M142"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq56.gif"/></alternatives></inline-formula> are combined as model parameters.</p>
    </sec>
    <sec id="Sec6">
      <title>Multi-task learning framework with a dual adaptation mechanism</title>
      <p id="Par25">This study adopts multi-task learning to link the encoder, i.e. the pre-training tasks and the decoder, i.e. the DTA prediction task, for preventing overfitting caused by the local optimality under a relatively small supervised samples. In order to make the overall model bias against the main task DTA prediction, this study adopts the updated strategy of MAML [<xref ref-type="bibr" rid="CR41">41</xref>].</p>
      <p id="Par26">The drug pre-training task is defined as the query set. For this task, we adjust the prior parameter <inline-formula id="IEq57"><alternatives><tex-math id="M143">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\psi$$\end{document}</tex-math><mml:math id="M144"><mml:mi>ψ</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq57.gif"/></alternatives></inline-formula> of compound-level aggregation with one or a few gradient descent steps. The learning rate is set to <inline-formula id="IEq58"><alternatives><tex-math id="M145">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M146"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq58.gif"/></alternatives></inline-formula> for dual adaptation. The new prior parameter <inline-formula id="IEq59"><alternatives><tex-math id="M147">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\psi ^{\prime }$$\end{document}</tex-math><mml:math id="M148"><mml:msup><mml:mi>ψ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq59.gif"/></alternatives></inline-formula> can be obtained as follows:<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M149">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \psi ^{\prime }=\psi -\alpha \frac{\partial {\mathcal {L}}^{\text{ atom } }\left( \psi ; {\mathcal {G}}\right) }{\partial \psi } \end{aligned}$$\end{document}</tex-math><mml:math id="M150" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>ψ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>ψ</mml:mi><mml:mo>-</mml:mo><mml:mi>α</mml:mi><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>atom</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msup><mml:mfenced close=")" open="("><mml:mi>ψ</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="script">G</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:mi>ψ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula>Then, the FC parameter <inline-formula id="IEq60"><alternatives><tex-math id="M151">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma$$\end{document}</tex-math><mml:math id="M152"><mml:mi>γ</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq60.gif"/></alternatives></inline-formula> in the DTA prediction layer, which is defined as the support set, will be updated as follows:<disp-formula id="Equ17"><label>17</label><alternatives><tex-math id="M153">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \gamma ^{\prime }=\gamma -\alpha \frac{\partial {\mathcal {L}}^{\text{ affinities } }\left( \psi ^{\prime }, \gamma ;( {\mathcal {G}}, t)\right) }{\partial \gamma } \end{aligned}$$\end{document}</tex-math><mml:math id="M154" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:mo>-</mml:mo><mml:mi>α</mml:mi><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>affinities</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msup><mml:mfenced close=")" open="("><mml:msup><mml:mi>ψ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">G</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ17.gif" position="anchor"/></alternatives></disp-formula>After that, all the parameters are updated through the backpropagation of the overall loss function of the multi-tasking learning. We define the overall loss function as follows:<disp-formula id="Equ18"><label>18</label><alternatives><tex-math id="M155">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\mathcal {L}}^{\text{ all } }=\lambda _{\text{ atom } } {\mathcal {L}}^{\text{ atom } }+{\mathcal {L}}^{\text{ affinities } } \end{aligned}$$\end{document}</tex-math><mml:math id="M156" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>all</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>atom</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>atom</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>affinities</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ18.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq61"><alternatives><tex-math id="M157">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda _{\text{ atom }}$$\end{document}</tex-math><mml:math id="M158"><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>atom</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq61.gif"/></alternatives></inline-formula> set manually is the weight of the loss function of drug pre-training task. This study updates all learnable parameters by gradient descent. Before the pre-training drug task, we record the original model parameters, and take the parameters (query set) updated for the first time in pre-training as the prior parameters of the subsequent DTA prediction. The comprehensive loss function of DTA prediction and the drug pre-training task is taken as the objective function of dual adaptation. Subsequent original parameters are updated through the multi-task learning framework. Different from the frozen-strategy, the updated model parameters are original parameters rather than prior parameters.</p>
      <p id="Par27">The dual adaptation mechanism needs to save all learnable parameters in the pre-training task. For multi-head transformers learning, this will bring a huge increase in training time. Furthermore, this study mainly focuses on unknown drugs and introduces the drug pre-training task into the DTA prediction. Therefore, the multi-task learning framework in this study only combines the drug pre-training task with the DTA prediction task by using the above dual adaptation mechanism.</p>
    </sec>
  </sec>
  <sec id="Sec7">
    <title>Results</title>
    <sec id="Sec8">
      <title>Data preparation</title>
      <p id="Par28">This study performed the pre-training tasks on the following two datasets:<list list-type="bullet"><list-item><p id="Par29">Protein pre-training dataset: The Pfam dataset [<xref ref-type="bibr" rid="CR42">42</xref>] was used for protein pre- training. It was produced at the European Bioinformatics Institute using a sequence database, which is based on UniProt. Over 21M amino acid sequences of proteins were clustered into 16,479 families based on the sequence similar- ity. This study performed two protein pre-training tasks on this dataset for learning structural information of amino acid sequences.</p></list-item><list-item><p id="Par30">Drug pre-training dataset: The ZINC15 database [<xref ref-type="bibr" rid="CR22">22</xref>] was used for drug pre-training. It is provided by the Irwin and Shoichet Laboratories at the University of California. In this study, 2 million unlabeled compounds was used for learning structural information of molecular graphs of drug compounds by the drug pre-training task.</p></list-item></list>The Davis [<xref ref-type="bibr" rid="CR19">19</xref>] and Kiba [<xref ref-type="bibr" rid="CR20">20</xref>] were selected for performance evaluation. The Davis dataset includes 30056 drug-target pairs and is involved with 442 proteins and 68 compounds. The Kiba dataset includes 118254 drug-target pairs and is involved with 229 proteins and 2068 compounds. Their binding affinities are indicated by the relevant inhibitors with their respective dissociation constant values. A group of unknown drug data sets were constructed for simulating the scenario of unknown drug discovery. The process includes the following two steps:<list list-type="bullet"><list-item><p id="Par31">Unknown drug compound/protein selection: This study selected unknown drug compounds and proteins based on the similarity. Referring to [<xref ref-type="bibr" rid="CR43">43</xref>], we performed the substructural features based k-means algorithm on all the drug compounds and selected outliers as unknown drug compounds. Referring to [<xref ref-type="bibr" rid="CR25">25</xref>], we selected unknown proteins based on the Pfam family. The proteins from the smallest 42 families were selected as unknown proteins.</p></list-item><list-item><p id="Par32">Unknown dataset construction: Those drug-target pairs containing any unknown compounds or any unknown protein were extracted as the unknown test set (unknown-TeS). The corresponding drug-target pairs were removed from original training set [<xref ref-type="bibr" rid="CR12">12</xref>] and the remaining data were used to construct the unknown training set (unknown-TrS).</p></list-item></list>The distribution of data is shown in Table <xref rid="Tab1" ref-type="table">1</xref>. Similarly, we can obtain the unknown drug data sets from Kiba, as shown in Table <xref rid="Tab2" ref-type="table">2</xref>. After constructing the unknown drug data sets, we removed all unknown drug compounds from the drug pre-training dataset to avoid data leakage.<table-wrap id="Tab1"><label>Table 1</label><caption><p>The data distribution in the unknown drug data sets from Davis</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Number of proteins</th><th align="left">Number of drugs</th><th align="left">Number of drug-target pairs</th></tr></thead><tbody><tr><td align="left">All data</td><td align="left">442</td><td align="left">68</td><td align="left">30056</td></tr><tr><td align="left">unknown-TrS</td><td align="left">369</td><td align="left">56</td><td align="left">20664</td></tr><tr><td align="left">unknown-TeS</td><td align="left">442</td><td align="left">68</td><td align="left">10409</td></tr></tbody></table></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption><p>The data distribution in the unknown drug data sets from Kiba</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Number of proteins</th><th align="left">Number of drugs</th><th align="left">Number of drug-target pairs</th></tr></thead><tbody><tr><td align="left">All data</td><td align="left">229</td><td align="left">2068</td><td align="left">118254</td></tr><tr><td align="left">unknown-TrS</td><td align="left">191</td><td align="left">1723</td><td align="left">82524</td></tr><tr><td align="left">unknown-TeS</td><td align="left">229</td><td align="left">2068</td><td align="left">32490</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec9">
      <title>Model parameters</title>
      <p id="Par33">Protein pre-training tasks were first performed alone, and then the drug pre-training task and the DTA prediction task were carried out at the same time, by using the multi-task learning framework and dual adaptation mechanism. For protein encoding, the dimension size of amino acid vector was set to 20, the number of self-attention heads was 12, the number of hidden layers was 12, and the dimension size of hidden layer was 768. For drug encoding, the lay number of GCN was set to 5, and the dimension size of hidden layer was 300. For DTA prediction, the layer number of FC was set to 3. For multi-task learning, the learning rate was set to 0.001 and the weight of drug pre-training was set to 0, 0.5, 1.0 and 2.0, respectively.</p>
    </sec>
    <sec id="Sec10">
      <title>Baseline methods</title>
      <p id="Par34">In order to prove the validity of model, this study compares the proposed GeneralizedDTA with the following baseline methods:<list list-type="bullet"><list-item><p id="Par35">DeepDTA [<xref ref-type="bibr" rid="CR12">12</xref>]: It used CNN and the pooling architecture to capture the potential interaction features between proteins and drugs. Research showed that the CNN network with a smaller number of parameters can be used to test overfitting of transformer. Therefore, this study adopted three layers of convolution for drug and protein encoding of DeepDTA, and the kernel sizes were set to 4,6,8, respectively.</p></list-item><list-item><p id="Par36">GraphDTA [<xref ref-type="bibr" rid="CR13">13</xref>]: It represented SMILES strings of drugs as short ASCII strings. In this study, drug encoding of GraphDTA adopted three layers of graph convolution and the numbers of feature dimensions of layers were set to 78,156,312, respectively. This kind of incremental parameter design can enhance the information transfer between atoms.</p></list-item><list-item><p id="Par37">SAGDTA [<xref ref-type="bibr" rid="CR44">44</xref>]: It exploited the self-attention mechanism on drug molecular graphs to obtain efficient representations of drugs. In this study, features of each atom node in the molecular graph and the SAG used the hierarchical pooing architecture with 3 blocks which has been demonstrated to absorb global information better.</p></list-item><list-item><p id="Par38">MGraphDTA [<xref ref-type="bibr" rid="CR14">14</xref>]: It adopted a deep multiscale graph neural network based on chemical intuition for DTA prediction. A super-deep GNN with 27 graph convolutional layers was built to capture the local and global structure of the compound simultaneously. In this study, learning ration and embedding size were set to 5e-4 and 128 respectively.</p></list-item></list></p>
    </sec>
    <sec id="Sec11">
      <title>Evaluation metrics</title>
      <p id="Par39">This study adopted MSE and R-squared (<inline-formula id="IEq62"><alternatives><tex-math id="M159">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R^{2}$$\end{document}</tex-math><mml:math id="M160"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq62.gif"/></alternatives></inline-formula>) [<xref ref-type="bibr" rid="CR45">45</xref>] to evaluate the prediction results of the model. MSE and <inline-formula id="IEq63"><alternatives><tex-math id="M161">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R^{2}$$\end{document}</tex-math><mml:math id="M162"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq63.gif"/></alternatives></inline-formula> are well-defined metrics to measure how close the fitted line is in the regression task. They can be calculated as follows:<disp-formula id="Equ19"><label>19</label><alternatives><tex-math id="M163">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \mathrm {MSE}= &amp; {} \frac{1}{n} \sum _{i=1}^{n}\left( y_{i}-{\hat{y}}_{i}\right) ^{2} \end{aligned}$$\end{document}</tex-math><mml:math id="M164" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="normal">MSE</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msup><mml:mfenced close=")" open="("><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ19.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ20"><label>20</label><alternatives><tex-math id="M165">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} R^{2}= &amp; {} 1-\frac{\sum _{i}\left( y_{i}-{\hat{y}}_{i}\right) ^{2}}{\sum _{i}\left( y_{i}-{\bar{y}}\right) ^{2}} \end{aligned}$$\end{document}</tex-math><mml:math id="M166" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mfenced close=")" open="("><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mfenced close=")" open="("><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_4905_Article_Equ20.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq64"><alternatives><tex-math id="M167">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{y}}_{i}$$\end{document}</tex-math><mml:math id="M168"><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq64.gif"/></alternatives></inline-formula> is the true value of binding affinity of the <italic>i</italic>-th drug-target pair, <inline-formula id="IEq65"><alternatives><tex-math id="M169">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{y}}$$\end{document}</tex-math><mml:math id="M170"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq65.gif"/></alternatives></inline-formula> is the corresponding predicted value, and <inline-formula id="IEq66"><alternatives><tex-math id="M171">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bar{y}}$$\end{document}</tex-math><mml:math id="M172"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq66.gif"/></alternatives></inline-formula> is the average of true values of all binding affinities.</p>
    </sec>
    <sec id="Sec12">
      <title>Performance evaluation on predicting drug-target binding affinity</title>
      <p id="Par40">Tables <xref rid="Tab3" ref-type="table">3</xref> and <xref rid="Tab4" ref-type="table">4</xref> give experimental results. It can be seen that SAGNet and our model with <inline-formula id="IEq67"><alternatives><tex-math id="M173">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda _{\text{ atom } }=0$$\end{document}</tex-math><mml:math id="M174"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>atom</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq67.gif"/></alternatives></inline-formula> have the worst performance in two datasets. It indicates that deeper networks without additional auxiliary constraints perform worse on unknown data. Our model with <inline-formula id="IEq68"><alternatives><tex-math id="M175">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda _{\text{ atom } }=0$$\end{document}</tex-math><mml:math id="M176"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>atom</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq68.gif"/></alternatives></inline-formula>, in which <inline-formula id="IEq69"><alternatives><tex-math id="M177">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda _{\text{ atom } }=0$$\end{document}</tex-math><mml:math id="M178"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>atom</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq69.gif"/></alternatives></inline-formula> means the unbinding between drug pre-training and DTA prediction, had the biggest MSE. This indicates that overfitting exists due to catastrophic forgetting between drug pre-training and DTA prediction. It is necessary to develop a multi-task learning framework for binding pre-training and prediction models. GraphDTA [<xref ref-type="bibr" rid="CR13">13</xref>] achieved the better performance than DeepDTA, indicating that structural information based on the molecular graph of drug compounds are valuable for DTA prediction. MGraphDTA [<xref ref-type="bibr" rid="CR14">14</xref>] achieved the best results in four baseline methods. This proves once again the importance of structure information of the compounds, which is the important motivation to introduce the graph-based drug pre-training task in this study. Our model with <inline-formula id="IEq70"><alternatives><tex-math id="M179">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda _{\text{ atom } }=0.5$$\end{document}</tex-math><mml:math id="M180"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>atom</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq70.gif"/></alternatives></inline-formula> and <inline-formula id="IEq71"><alternatives><tex-math id="M181">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda _{\text{ atom } }=1.0$$\end{document}</tex-math><mml:math id="M182"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>atom</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq71.gif"/></alternatives></inline-formula> achieved the best performance in terms of all evaluation metrics in the Davis dataset and the Kiba dataset respectively. This shows that our model, which adopts a new drug pre-training task and combines it with the DTA prediction task by a multi-task learning framework, has better generalization capability in unknown drug discovery. But, different optimization weights may be required for different data sets. The reason can be attributed to the different affinity measurement methods in different datasets.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Experimental results in the unknown drug data sets from Davis</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">MSE</th><th align="left"><inline-formula id="IEq72"><alternatives><tex-math id="M183">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R^{2}$$\end{document}</tex-math><mml:math id="M184"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq72.gif"/></alternatives></inline-formula></th></tr></thead><tbody><tr><td align="left">DeepDTA</td><td align="left">1.0271</td><td align="left">0.1454</td></tr><tr><td align="left">GraphDTA</td><td align="left">0.8872</td><td align="left">0.2037</td></tr><tr><td align="left">SAGDTA</td><td align="left">1.1324</td><td align="left">0.1654</td></tr><tr><td align="left">MGraphDTA</td><td align="left">0.8532</td><td align="left">0.2287</td></tr><tr><td align="left">Our method (<inline-formula id="IEq73"><alternatives><tex-math id="M185">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda _{\text{ atom } }=0$$\end{document}</tex-math><mml:math id="M186"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>atom</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq73.gif"/></alternatives></inline-formula>)</td><td align="left">1.2764</td><td align="left">0.1512</td></tr><tr><td align="left">Our method (<inline-formula id="IEq74"><alternatives><tex-math id="M187">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda _{\text{ atom } }=0.5$$\end{document}</tex-math><mml:math id="M188"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>atom</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq74.gif"/></alternatives></inline-formula>)</td><td align="left">0.8467</td><td align="left">0.2402</td></tr><tr><td align="left">Our method (<inline-formula id="IEq75"><alternatives><tex-math id="M189">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda _{\text{ atom } }=1.0$$\end{document}</tex-math><mml:math id="M190"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>atom</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq75.gif"/></alternatives></inline-formula>)</td><td align="left">0.9041</td><td align="left">0.1886</td></tr><tr><td align="left">Our method (<inline-formula id="IEq76"><alternatives><tex-math id="M191">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda _{\text{ atom } }=2.0$$\end{document}</tex-math><mml:math id="M192"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>atom</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2.0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq76.gif"/></alternatives></inline-formula>)</td><td align="left">0.8603</td><td align="left">0.2279</td></tr></tbody></table></table-wrap><table-wrap id="Tab4"><label>Table 4</label><caption><p>Experimental results in the unknown drug data sets from Kiba</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">MSE</th><th align="left"><inline-formula id="IEq77"><alternatives><tex-math id="M193">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R^{2}$$\end{document}</tex-math><mml:math id="M194"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq77.gif"/></alternatives></inline-formula></th></tr></thead><tbody><tr><td align="left">DeepDTA</td><td align="left">0.5437</td><td align="left">0.3605</td></tr><tr><td align="left">GraphDTA</td><td align="left">0.4950</td><td align="left">0.2953</td></tr><tr><td align="left">SAGDTA</td><td align="left">0.6237</td><td align="left">0.2311</td></tr><tr><td align="left">MGraphDTA</td><td align="left">0.4667</td><td align="left">0.3766</td></tr><tr><td align="left">Our method (<inline-formula id="IEq78"><alternatives><tex-math id="M195">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda _{\text{ atom } }=0$$\end{document}</tex-math><mml:math id="M196"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>atom</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq78.gif"/></alternatives></inline-formula>)</td><td align="left">0.7311</td><td align="left">0.2039</td></tr><tr><td align="left">Our method (<inline-formula id="IEq79"><alternatives><tex-math id="M197">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda _{\text{ atom } }=0.5$$\end{document}</tex-math><mml:math id="M198"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>atom</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq79.gif"/></alternatives></inline-formula>)</td><td align="left">0.4331</td><td align="left">0.2831</td></tr><tr><td align="left">Our method (<inline-formula id="IEq80"><alternatives><tex-math id="M199">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda _{\text{ atom } }=1.0$$\end{document}</tex-math><mml:math id="M200"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>atom</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq80.gif"/></alternatives></inline-formula>)</td><td align="left">0.4582</td><td align="left">0.3906</td></tr><tr><td align="left">Our method (<inline-formula id="IEq81"><alternatives><tex-math id="M201">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda _{\text{ atom } }=2.0$$\end{document}</tex-math><mml:math id="M202"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>atom</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2.0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq81.gif"/></alternatives></inline-formula>)</td><td align="left">0.6067</td><td align="left">0.1781</td></tr></tbody></table></table-wrap></p>
      <p id="Par41">Figure <xref rid="Fig4" ref-type="fig">4</xref> gives convergence analysis in the unknown-TeS from Davis. As shown in this figure, the proposed GeneralizedDTA can effectively converge on the unknown-TeS from Davis and has the highest generalization capability.<fig id="Fig4"><label>Fig. 4</label><caption><p>Convergence analysis in the unknown-TeS from Davis</p></caption><graphic xlink:href="12859_2022_4905_Fig4_HTML" id="MO24"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec13">
    <title>Discussion</title>
    <sec id="Sec14">
      <title>Ablation study</title>
      <p id="Par42">The proposed GeneralizedDTA combines pre-training and multi-task learning. It is involved with four core components, including protein pre-training, drug pre-training, multi-task framework and dual adaptation mechanism. In order to analyze their effectiveness, an ablation study is designed with 4 ablated variants, without protein pre-training, without drug pre-training, without multi-task framework and without dual adaptation mechanism. The variant without dual adaptation mechanism is to finish pre-training firstly, and then transfer the pre-trained components into DTA. Experiments were performed on the unknown drug data sets from Davis.</p>
      <p id="Par43">Figure <xref rid="Fig5" ref-type="fig">5</xref> gives the experimental results. As shown in this figure, our method is superior to all variants. This indicates that all of four components are effective for improving DTA prediction. The effect of the drug pre-training is biggest and that of the protein pre-training is smaller. This indicates that the structural information of drug compounds is more important in DTA prediction than that of proteins. Figure <xref rid="Fig5" ref-type="fig">5</xref> also shows that, the effect of the deep learning model can be significantly improved under the constraint of multi-task framework. Dual adaptation mechanism can prevent local optimality in parameter updating of deep learning model and improve performance of DTA prediction.<fig id="Fig5"><label>Fig. 5</label><caption><p>Experimental results in the ablation study</p></caption><graphic xlink:href="12859_2022_4905_Fig5_HTML" id="MO25"/></fig></p>
    </sec>
    <sec id="Sec15">
      <title>Comparative analysis on pre-training models</title>
      <p id="Par44">Pre-training is a core component of GeneralizedDTA. This study adopted transformer-based protein pre-training and GCN-based drug pre-training. At present, there are several state-of-the-art protein pre-training models and drug pre-training models:<list list-type="bullet"><list-item><p id="Par45">ESM [<xref ref-type="bibr" rid="CR46">46</xref>]: It is a protein pre-training model which uses a very large deep model framework with self-supervised task by masked language modeling and homology information relevant modeling.</p></list-item><list-item><p id="Par46">DISAE [<xref ref-type="bibr" rid="CR47">47</xref>]: It is also a protein pre-training model which utilizes all protein sequences and their multiple sequence alignment to capture functional relationships between proteins without the knowledge of structure and function.</p></list-item><list-item><p id="Par47">ContextPred [<xref ref-type="bibr" rid="CR48">48</xref>] : It is a drug pre-training model which explores distribution of graph structure in the node-level self-supervised task and sample subgraphs to predict their surrounding graph structures.</p></list-item><list-item><p id="Par48">GROVER [<xref ref-type="bibr" rid="CR49">49</xref>]: It is also a drug pre-training model which uses local random walk-based objectives to learn rich structural and semantic information by self-supervised tasks in node, edge and graph level.</p></list-item></list>In order to evaluate pre-training of GeneralizedDTA, this study uses the above pre-training models to replace the pre-training components of GeneralizedDTA respectively. Experiments were performed on the unknown drug data sets from Davis.</p>
      <p id="Par49">Figure <xref rid="Fig6" ref-type="fig">6</xref> gives the experimental results. As shown in the figure, for protein pre-training, the DTA prediction result based on our transformer-based protein pre-training differs little from that based on ESM and DISAE. This indicates that these state-of-the-art protein pre-training models, such as ESM, can slightly improve DTA prediction, but not significantly. Considering the demand of computing resources, our protein pre-training is appropriate, especially in low resource environments.</p>
      <p id="Par50">Our GCN-based drug pre-training adopts the node-level self-supervised task. By randomly masked nodes and edge attribute [<xref ref-type="bibr" rid="CR35">35</xref>], the GCN model can be trained to generate graph embedding which can distinguish the similarity of atoms. Based on this kind of graph embedding, the capability of downstream DTA prediction model can be effectively improved. As shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>, the DTA prediction results based on our GCN-based drug pre-training are similar to that based on ContextPred, but significantly better than GROVER. Because ContextPred also adopts the node-level self-supervised task, this indicates that node-level adaption surrounding neighbors in our GCN-based drug pre-training is more suitable for DTA prediction than the random walk strategy in GROVER. The reason may be that the random walk strategy pays too much attention to downstream irrelevant information.<fig id="Fig6"><label>Fig. 6</label><caption><p>Comparative Analysis with different pre-training in unknown drug discovery</p></caption><graphic xlink:href="12859_2022_4905_Fig6_HTML" id="MO26"/></fig></p>
      <p id="Par51">The results of our model with <inline-formula id="IEq82"><alternatives><tex-math id="M203">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda _{\text{ atom } }=0$$\end{document}</tex-math><mml:math id="M204"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>atom</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4905_Article_IEq82.gif"/></alternatives></inline-formula> in Table <xref rid="Tab3" ref-type="table">3</xref> show that, if unbinding pre-training with prediction, the model can learn existing drugs too finely because of lacking of constraints, and lost the prediction capability on unknown drugs. Therefore, this study develops a multi-task learning framework with a dual adaptation mechanism to bind the drug pre-training and DTA prediction. In our dual adaptation mechanism, the parameters of GCN are not fixed every time. This helps to avoid falling into local optimization brought by the small labelled data set. We also use the loss function of the pre-training task as the regular term of the DTA prediction task to further alleviate overfitting of model. Therefore, the multi-task learning framework with a dual adaptation mechanism is most critical factor for improving the generalization capability of model on the DTA prediction of unknown drugs. The above comparative analysis on pre-training models also shows that only replacing the pre-training models cannot significantly improve DTA prediction. Considering the calculation complexity, the study adopts current transformer-based protein pre-training and GCN-based drug pre-training.</p>
    </sec>
  </sec>
  <sec id="Sec16">
    <title>Conclusion</title>
    <p id="Par52">Digging into the benchmark dataset Davis, we notice that previous studies on DTA prediction didn’t consider the generalization capability of model in unknown drug discovery. To address this challenge, this study proposes a new DTA prediction model called GeneralizedDTA. We introduce two protein pre-training tasks and a brand-new drug pre-training task to learn richer structural information of proteins and drugs, for accelerating the convergence of model on small-scale labelled data. We also develop a multi-task learning framework with a dual adaptation mechanism to prevent the prediction model from falling into overfitting and improve the generalization capability of model in unknown drug discovery. A group of comparative experiments on the new unknown drug data sets validate the effectiveness of our model for DTA prediction in unknown drug discovery.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>All authors contributed to the study conception and design. Data collection, coding and analyses were performed by Chengyu Shi and Jianhui Chen. The first draft of the manuscript was written by Shaofu Lin and Chengyu Shi. All authors commented on previous versions of the manuscript. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>The work is supported by National Key Research and Development Program of China (Grant No. 2020YFB2104402) and Beijing Natural Science Foundation (No. 4222022).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The source codes are publicly available in the GitHub repository <ext-link ext-link-type="uri" xlink:href="https://github.com/Frank-39/GeneralizeDTA">https://github.com/Frank-39/GeneralizeDTA</ext-link>.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par53">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ezzat</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X-L</given-names>
          </name>
          <name>
            <surname>Kwoh</surname>
            <given-names>C-K</given-names>
          </name>
        </person-group>
        <article-title>Computational prediction of drug-target interactions using chemogenomic approaches: an empirical survey</article-title>
        <source>Brief Bioinform</source>
        <year>2018</year>
        <volume>20</volume>
        <fpage>1337</fpage>
        <lpage>1357</lpage>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Manoochehri</surname>
            <given-names>HE</given-names>
          </name>
          <name>
            <surname>Nourani</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Drug-target interaction prediction using semi-bipartite graph model and deep learning</article-title>
        <source>BMC Bioinformatics</source>
        <year>2020</year>
        <volume>21</volume>
        <issue>4</issue>
        <fpage>1</fpage>
        <lpage>16</lpage>
        <pub-id pub-id-type="pmid">31898485</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mullard</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>New drugs cost us \$2.6 billion to develop</article-title>
        <source>Nat Rev Drug Discov</source>
        <year>2014</year>
        <volume>13</volume>
        <issue>12</issue>
        <fpage>877</fpage>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bleakley</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Yamanishi</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Supervised prediction of drug-target interactions using bipartite local models</article-title>
        <source>Bioinformatics</source>
        <year>2009</year>
        <volume>25</volume>
        <issue>18</issue>
        <fpage>2397</fpage>
        <lpage>2403</lpage>
        <pub-id pub-id-type="pmid">19605421</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Mongia A, Jain V, Chouzenoux E, Majumdar A. Deep latent factor model for predicting drug target interactions. 2019. p. 1254–1258.</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Shang</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>A learning-based method for drug-target interaction prediction based on feature representation learning and deep neural network</article-title>
        <source>BMC Bioinformatics</source>
        <year>2020</year>
        <volume>21</volume>
        <issue>13</issue>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="pmid">31898485</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>C-C</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Drug-pathway association prediction: from experimental results to computational models</article-title>
        <source>Brief Bioinform</source>
        <year>2021</year>
        <volume>22</volume>
        <issue>3</issue>
        <fpage>061</fpage>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Luo</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Kuang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A network integration approach for drug-target interaction prediction and computational drug repositioning from heterogeneous information</article-title>
        <source>Nat Commun</source>
        <year>2017</year>
        <volume>8</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="pmid">28232747</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>CC</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Dai</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Drug-target interaction prediction: databases, web servers and computational models</article-title>
        <source>Brief Bioinform</source>
        <year>2016</year>
        <volume>17</volume>
        <issue>4</issue>
        <fpage>696</fpage>
        <lpage>712</lpage>
        <pub-id pub-id-type="pmid">26283676</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Heidemeyer</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ban</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Cherkasov</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ester</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Simboost: a read-across approach for predicting drug-target binding affinities using gradient boosting machines</article-title>
        <source>J Cheminformatics</source>
        <year>2017</year>
        <volume>9</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>14</lpage>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Corsello</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Bittker</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Gould</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>McCarren</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Hirschman</surname>
            <given-names>JE</given-names>
          </name>
          <name>
            <surname>Johnston</surname>
            <given-names>SE</given-names>
          </name>
          <name>
            <surname>Vrcic</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Asiedu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Narayan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Mader</surname>
            <given-names>CC</given-names>
          </name>
          <name>
            <surname>Subramanian</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Golub</surname>
            <given-names>TR</given-names>
          </name>
        </person-group>
        <article-title>The drug repurposing hub: a next-generation drug library and information resource</article-title>
        <source>Nat Med</source>
        <year>2017</year>
        <volume>23</volume>
        <issue>4</issue>
        <fpage>405</fpage>
        <lpage>408</lpage>
        <pub-id pub-id-type="pmid">28388612</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Öztürk</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Özgür</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ozkirimli</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Deepdta: deep drug-target binding affinity prediction</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <issue>17</issue>
        <fpage>821</fpage>
        <lpage>829</lpage>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nguyen</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Quinn</surname>
            <given-names>TP</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>TD</given-names>
          </name>
          <name>
            <surname>Venkatesh</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Graphdta: predicting drug-target binding affinity with graph neural networks</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>37</volume>
        <issue>8</issue>
        <fpage>1140</fpage>
        <lpage>1147</lpage>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhong</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>CC</given-names>
          </name>
        </person-group>
        <article-title>Mgraphdta: deep multiscale graph neural network for explainable drug-target binding affinity prediction</article-title>
        <source>Chem Sci</source>
        <year>2022</year>
        <volume>13</volume>
        <fpage>816</fpage>
        <pub-id pub-id-type="pmid">35173947</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zeng</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Deep drug-target binding affinity prediction with multiple attention blocks</article-title>
        <source>Brief Bioinform</source>
        <year>2021</year>
        <volume>22</volume>
        <issue>5</issue>
        <fpage>117</fpage>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tran</surname>
            <given-names>HNT</given-names>
          </name>
          <name>
            <surname>Thomas</surname>
            <given-names>JJ</given-names>
          </name>
          <name>
            <surname>Malim</surname>
            <given-names>NHAH</given-names>
          </name>
        </person-group>
        <article-title>Deepnc: a framework for drug-target interaction prediction with graph neural networks</article-title>
        <source>PeerJ</source>
        <year>2022</year>
        <volume>10</volume>
        <fpage>13163</fpage>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Li F, Zhang Z, Guan J, Zhou S. Effective drug-target interaction prediction with mutual interaction neural network. Bioinformatics 2022;btac377</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yuan</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>CY-C</given-names>
          </name>
        </person-group>
        <article-title>Fusiondta attention-based feature polymerizer and knowledge distillation for drug-target binding affinity prediction</article-title>
        <source>Brief Bioinform</source>
        <year>2022</year>
        <volume>23</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Davis</surname>
            <given-names>MI</given-names>
          </name>
          <name>
            <surname>Hunt</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Herrgard</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ciceri</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Wodicka</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Pallares</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Hocker</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Treiber</surname>
            <given-names>DK</given-names>
          </name>
          <name>
            <surname>Zarrinkar</surname>
            <given-names>PP</given-names>
          </name>
        </person-group>
        <article-title>Comprehensive analysis of kinase inhibitor selectivity</article-title>
        <source>Nat Biotechnol</source>
        <year>2011</year>
        <volume>29</volume>
        <issue>11</issue>
        <fpage>1046</fpage>
        <lpage>1051</lpage>
        <pub-id pub-id-type="pmid">22037378</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Szwajda</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Shakyawar</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hintsanen</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Wennerberg</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Aittokallio</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Making sense of large-scale kinase inhibitor bioactivity data sets: a comparative and integrative analysis</article-title>
        <source>J Chem Inf Model</source>
        <year>2014</year>
        <volume>54</volume>
        <issue>3</issue>
        <fpage>735</fpage>
        <lpage>743</lpage>
        <pub-id pub-id-type="pmid">24521231</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kanehisa</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Goto</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Hattori</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Aoki-Kinoshita</surname>
            <given-names>KF</given-names>
          </name>
          <name>
            <surname>Itoh</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kawashima</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Katayama</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Araki</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hirakawa</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>From genomics to chemical genomics: new developments in KEGG</article-title>
        <source>Nucleic Acids Res</source>
        <year>2006</year>
        <volume>34</volume>
        <issue>suppl–1</issue>
        <fpage>354</fpage>
        <lpage>357</lpage>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sterling</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Irwin</surname>
            <given-names>JJ</given-names>
          </name>
        </person-group>
        <article-title>Zinc 15-ligand discovery for everyone</article-title>
        <source>J Chem Inf Model</source>
        <year>2015</year>
        <volume>55</volume>
        <issue>11</issue>
        <fpage>2324</fpage>
        <lpage>2337</lpage>
        <pub-id pub-id-type="pmid">26479676</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Hu F, Hu Y, Zhang J, Wang D, Yin P. Structure enhanced protein-drug interaction prediction using transformer and graph embedding, 2020;1010–1014.</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kirkpatrick</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Pascanu</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Rabinowitz</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Veness</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Desjardins</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Rusu</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Milan</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Quan</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ramalho</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Grabska-Barwinska</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hassabis</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Clopath</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Kumaran</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Hadsell</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Overcoming catastrophic forgetting in neural networks</article-title>
        <source>Proc Natl Acad Sci</source>
        <year>2017</year>
        <volume>114</volume>
        <issue>13</issue>
        <fpage>3521</fpage>
        <lpage>3526</lpage>
        <pub-id pub-id-type="pmid">28292907</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Cai T, Xie L, Chen M, Liu Y, He D, Zhang S, Mura C, Bourne PE, Xie L. Exploration of dark chemical genomics space via portal learning: applied to targeting the undruggable genome and covid-19 anti-infective polypharmacology. 2021; arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2111.14283">arXiv:2111.14283</ext-link></mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Arjovsky M. Out of distribution generalization in machine learning. PhD thesis, New York University; 2020.</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Devlin J, Chang M-W, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding. In: Proceedings of the 2019 annual conference of the North American chapter of the association for computational linguistics: human language technologies (NAACL-HLT 2019). 2019. p. 4171– 4186 .</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Bepler T, Berger B. Learning protein sequence embeddings using information from structure. In: Proceedings of the seventh international conference on learning representations (ICLR 2019) 2019;</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Min</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>H-S</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Yoon</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Pre-training of deep bidirectional protein sequence representations with structural information</article-title>
        <source>IEEE Access</source>
        <year>2021</year>
        <volume>9</volume>
        <fpage>123912</fpage>
        <lpage>123926</lpage>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Elofsson</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sonnhammer</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>A comparison of sequence and structure protein domain families as a basis for structural genomics</article-title>
        <source>Bioinformatics</source>
        <year>1999</year>
        <volume>15</volume>
        <issue>6</issue>
        <fpage>480</fpage>
        <lpage>500</lpage>
        <pub-id pub-id-type="pmid">10383473</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Srivastava</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Salakhutdinov</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>
        <source>J Mach Learn Res</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>1</issue>
        <fpage>1929</fpage>
        <lpage>1958</lpage>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Kipf TN, Welling M. Semi-supervised classification with graph convolutional networks. In: Proceedings of the 5th international conference on learning representations (ICLR 2017) 2017.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Atwood J, Towsley D. Diffusion-convolutional neural networks. In: Proceedings of the 30th International Conference on Neural Information Processing Systems (NIPS 2016). 2016. p. 2001–9.</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Duvenaud D, Maclaurin D, Aguilera-Iparraguirre J, Gómez-Bombarelli R, Hirzel T, Aspuru-Guzik A, Adams RP. Convolutional networks on graphs for learning molecular fingerprints. In: Proceedings of the 29th International Conference on Neural Information Processing Systems (NIPS 2015). 2015. p. 2215–23.</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Tang J, Qu M, Wang M, Zhang M, Yan J, Mei Q. Line: large-scale information network embedding. In: Proceedings of the 24th international conference on World Wide Web (WWW’15). 2015. p. 1067–1077</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J. Distributed representations of words and phrases and their compositionality. In: Advances in neural information processing systems. 2013. p. 3111– 3119</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Ying R, He R, Chen K, Eksombatchai P, Hamilton WL, Leskovec J. Graph convolutional neural networks for web-scale recommender systems. In: Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining. 2018. p. 974– 983.</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Hamilton WL, Ying R, Leskovec J. Inductive representation learning on large graphs. In: Proceedings of the 31st international conference on neural information processing systems. 2017. p. 1025– 1035 .</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Bai Y, Ding H, Qiao Y, Marinovic A, Gu K, Chen T, Sun Y, Wang W. Unsupervised inductive whole-graph embedding by preserving graph proximity. In: Proceedings of the seventh international conference on learning representations (ICLR 2019). 2019.</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Velickovic P, Fedus W, Hamilton WL, Liò P, Bengio Y, Hjelm RD. Deep graph infomax. In: Proceedings of the seventh international conference on learning representations (ICLR 2019)(Poster). 2019.</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Finn C, Abbeel P, Levine S. Model-agnostic meta-learning for fast adaptation of deep networks. In: Proceedings of the 34th international conference on machine learning (PMLR 2017). 2017. p. 1126– 1135 .</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Finn</surname>
            <given-names>RD</given-names>
          </name>
          <name>
            <surname>Bateman</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Clements</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Penelope Coggill</surname>
            <given-names>RYE</given-names>
          </name>
          <name>
            <surname>Eddy</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>Heger</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hetherington</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Holm</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Mistry</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Sonnhammer</surname>
            <given-names>ELL</given-names>
          </name>
          <name>
            <surname>Tate</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Punta</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Pfam: the protein families database</article-title>
        <source>Nucleic Acids Res</source>
        <year>2014</year>
        <volume>42</volume>
        <issue>D1</issue>
        <fpage>222</fpage>
        <lpage>230</lpage>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Gindulyte</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Shoemaker</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>Thiessen</surname>
            <given-names>PA</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Pubchem 2019 update: improved access to chemical data</article-title>
        <source>Nucleic Acids Res</source>
        <year>2019</year>
        <volume>47</volume>
        <issue>D1</issue>
        <fpage>1102</fpage>
        <lpage>1109</lpage>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Sag-dta: prediction of drug-target affinity using self-attention graph network</article-title>
        <source>Int J Mol Sci</source>
        <year>2021</year>
        <volume>22</volume>
        <issue>16</issue>
        <fpage>8993</fpage>
        <pub-id pub-id-type="pmid">34445696</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cameron</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Windmeijer</surname>
            <given-names>FA</given-names>
          </name>
        </person-group>
        <article-title>An r-squared measure of goodness of fit for some common nonlinear regression models</article-title>
        <source>J Econom</source>
        <year>1997</year>
        <volume>77</volume>
        <issue>2</issue>
        <fpage>329</fpage>
        <lpage>342</lpage>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rives</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Meier</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Sercu</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Goyal</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ott</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zitnick</surname>
            <given-names>CL</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title>
        <source>Proc Natl Acad Sci</source>
        <year>2021</year>
        <volume>118</volume>
        <issue>15</issue>
        <fpage>e2016239118</fpage>
        <pub-id pub-id-type="pmid">33876751</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cai</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Lim</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Abbu</surname>
            <given-names>KA</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Nussinov</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Msa-regularized protein sequence transformer toward predicting genome-wide chemical-protein interactions: Application to gpcrome deorphanization</article-title>
        <source>J Chem Inf Model</source>
        <year>2021</year>
        <volume>61</volume>
        <issue>4</issue>
        <fpage>1570</fpage>
        <lpage>1582</lpage>
        <pub-id pub-id-type="pmid">33757283</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <mixed-citation publication-type="other">Hu W, Liu B, Gomes J, Zitnik M, Liang P, Pande V, Leskovec J. Strategies for pre-training graph neural networks. 2019; arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1905.12265">arXiv:1905.12265</ext-link></mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rong</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bian</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Self-supervised graph transformer on large-scale molecular data</article-title>
        <source>Adv Neural Inf Process Syst</source>
        <year>2020</year>
        <volume>33</volume>
        <fpage>12559</fpage>
        <lpage>12571</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
