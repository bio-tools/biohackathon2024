<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neuroinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neuroinform</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neuroinform.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1662-5196</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10166805</article-id>
    <article-id pub-id-type="doi">10.3389/fninf.2023.1082111</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Technology and Code</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>NeuroWRAP: integrating, validating, and sharing neurodata analysis workflows</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Bowen</surname>
          <given-names>Zac</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/445254/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Magnusson</surname>
          <given-names>Gudjon</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Diep</surname>
          <given-names>Madeline</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ayyangar</surname>
          <given-names>Ujjwal</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Smirnov</surname>
          <given-names>Aleksandr</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/2152393/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kanold</surname>
          <given-names>Patrick O.</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Losert</surname>
          <given-names>Wolfgang</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/425800/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Fraunhofer USA Center Mid-Atlantic</institution>, <addr-line>Riverdale, MD</addr-line>, <country>United States</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Institute for Physical Science and Technology, University of Maryland, College Park</institution>, <addr-line>College Park, MD</addr-line>, <country>United States</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Department of Biomedical Engineering, Johns Hopkins University</institution>, <addr-line>Baltimore, MD</addr-line>, <country>United States</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Ya-tang Li, Chinese Institute for Brain Research, (CIBR), China</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Xiang Liao, Chongqing University, China; Peter Herman, Yale University, United States</p>
      </fn>
      <corresp id="c001">*Correspondence: Zac Bowen, <email>zbowen@fraunhofer.org</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>25</day>
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>17</volume>
    <elocation-id>1082111</elocation-id>
    <history>
      <date date-type="received">
        <day>27</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>07</day>
        <month>4</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2023 Bowen, Magnusson, Diep, Ayyangar, Smirnov, Kanold and Losert.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Bowen, Magnusson, Diep, Ayyangar, Smirnov, Kanold and Losert</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Multiphoton calcium imaging is one of the most powerful tools in modern neuroscience. However, multiphoton data require significant pre-processing of images and post-processing of extracted signals. As a result, many algorithms and pipelines have been developed for the analysis of multiphoton data, particularly two-photon imaging data. Most current studies use one of several algorithms and pipelines that are published and publicly available, and add customized upstream and downstream analysis elements to fit the needs of individual researchers. The vast differences in algorithm choices, parameter settings, pipeline composition, and data sources combine to make collaboration difficult, and raise questions about the reproducibility and robustness of experimental results. We present our solution, called NeuroWRAP (<ext-link xlink:href="http://www.neurowrap.org" ext-link-type="uri">www.neurowrap.org</ext-link>), which is a tool that wraps multiple published algorithms together, and enables integration of custom algorithms. It enables development of collaborative, shareable custom workflows and reproducible data analysis for multiphoton calcium imaging data enabling easy collaboration between researchers. NeuroWRAP implements an approach to evaluate the sensitivity and robustness of the configured pipelines. When this sensitivity analysis is applied to a crucial step of image analysis, cell segmentation, we find a substantial difference between two popular workflows, CaImAn and Suite2p. NeuroWRAP harnesses this difference by introducing consensus analysis, utilizing two workflows in conjunction to significantly increase the trustworthiness and robustness of cell segmentation results.</p>
    </abstract>
    <kwd-group>
      <kwd>two-photon calcium imaging</kwd>
      <kwd>image analysis</kwd>
      <kwd>consensus</kwd>
      <kwd>workflow management</kwd>
      <kwd>reproducibility</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">
          <institution-wrap>
            <institution>National Institutes of Health</institution>
            <institution-id institution-id-type="doi">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id award-type="contract" rid="cn001">U19 NS107464</award-id>
      </award-group>
      <funding-statement>This work was supported by the NIH U19 NS107464.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="1"/>
      <equation-count count="3"/>
      <ref-count count="17"/>
      <page-count count="10"/>
      <word-count count="8294"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="S1">
    <title>Introduction</title>
    <p>Two-photon calcium imaging is a common brain imaging technique that allows for recording the activity of hundreds or thousands of neurons at single-cell resolution. However, two-photon calcium imaging data requires significant pre-processing of acquired images (motion correction, cell segmentation, and signal extraction) and post-processing of extracted signals to produce interpretable readout from each neuron. As a result, many algorithms and pipelines have been developed for the analysis of two-photon imaging data. While several algorithms and pipelines are published and available as open-source packages (<xref rid="B6" ref-type="bibr">Giovannucci et al., 2017</xref>, <xref rid="B5" ref-type="bibr">2019</xref>; <xref rid="B11" ref-type="bibr">Pachitariu et al., 2017</xref>; <xref rid="B17" ref-type="bibr">Zhou et al., 2018</xref>; <xref rid="B2" ref-type="bibr">Cantu et al., 2020</xref>), countless others are customized in individual research groups. These different analysis methods could be different ways to approach a task such as automatic cell segmentation, or algorithms may be tailored to specific use cases such as rigid versus non-rigid jitter correction. Beyond different use cases, each method or pipeline typically has a variety of input parameters such as thresholds, temporal window sizes, sampling rates, and other experimental metadata that may need to be tuned to each different dataset. The vast potential differences in algorithm choices, pipeline parameter settings, and data sources combine to create an immense challenge of analysis reproducibility and collaboration.</p>
    <p>Reproducibility, the ability to produce the same results as previous work, is a large problem in science with neuroscience being no exception (<xref rid="B10" ref-type="bibr">Miłkowski et al., 2018</xref>). For the purposes of this work, we are using the word reproducibility to describe repeatability, replicability, and reproducibility. Repeatability is the ability for the same researchers to produce the same results using the same experimental setup (or software configuration) over multiple trials, which is the easiest to achieve. Replicability is the ability for different researchers to produce the same results as other researchers using the same experiment setup (or software configuration) across multiple trials. Reproducibility is the ability for different researchers to produce the same results as other researchers using a different experimental setup (or software configuration) across multiple trials (<xref rid="B12" ref-type="bibr">Plesser, 2018</xref>). All three are important, with replicability and reproducibility being the typical challenge point in analysis workflows causing results to differ between different researchers.</p>
    <p>A recent study gave the same FMRI dataset to 70 different research labs with the same analysis goal and no restrictions on what analyses could be used (<xref rid="B1" ref-type="bibr">Botvinik-Nezer et al., 2020</xref>). All groups used different analysis pipelines and found vastly different results. The authors highlighted several solutions to this problem: (1) share less-processed data, (2) publicly share data and code (3) public pre-registration of hypothesis and analysis to be used, and (4) using consensus results from multiple pipelines. There exists the need for a robust neuroimaging data analysis platform that enforces record keeping in a way that facilitates easily reproducing past results. Efforts have been made to standardize data format in neuroscience (<xref rid="B15" ref-type="bibr">Teeters et al., 2015</xref>; <xref rid="B7" ref-type="bibr">Gorgolewski et al., 2016</xref>) as well as data storage and sharing in neuroscience (<xref rid="B13" ref-type="bibr">Rübel et al., 2021</xref>), but standardization of analysis techniques remains an ongoing challenge. Scientific results that can be reproduced reliably will elevate the quality of further produced work but also save trainees and early career researchers immense amounts of time when establishing data analysis workflows.</p>
    <p>Neuroscience researchers would greatly benefit from a way to explore different options in their analysis pipelines in a controlled and reproducible manner. While analyzing neuroimaging data, it is rarely the case that the first algorithm or the first set of parameters used produce the final publishable results. It requires a significant amount of time and effort from researchers to optimize an analysis pipeline for their specific datasets because of the vast amount of parameter choices and lack of interoperability between different analysis packages due to differences in programming languages or input and output structure. Furthermore, there is not always a definitive method to know which algorithms, methods, or parameter choices are best; requiring running analyses many times and manually gathering results to determine the best configuration.</p>
    <p>Here we present NeuroWRAP (see text footnote 1), an analysis platform and workflow integrator for reproducible analysis of two-photon calcium imaging data. NeuroWRAP enables researchers, even with no programming experience, to analyze their data and keep an extensive record of all algorithms and parameters that were used. It facilitates reproducible analysis achieved by the ability to track, organize, and compare analysis executions, as well as allowing sharing of data and analysis workflows. Furthermore, it allows for pipelines to be easily built using algorithms from different sources and in different programming languages, such as MATLAB and Python working together seamlessly. We demonstrate how NeuroWRAP’s features can aid in parameter selection in conjunction with a consensus analysis module which combines outputs of multiple algorithms to produce consistent and reliable results.</p>
  </sec>
  <sec sec-type="results" id="S2">
    <title>Results</title>
    <p>NeuroWRAP is a neurodata processing platform that contains a suite of pre-processing and analysis algorithms for two-photon calcium imaging data. It enables researchers to easily process and analyze imaging data in a reproducible and collaborative manner. NeuroWRAP focuses on analyses required to get from raw data to interpretable neuronal readout but additionally contains some downstream analysis modules. Furthermore, it is free and requires no programming experience making it accessible for all neuroscientists looking to analyze two-photon calcium imaging data. NeuroWRAP is currently available for Windows and Mac.</p>
    <sec id="S2.SS1">
      <title>Modular design</title>
      <p>NeuroWRAP operates by executing workflows which consist of modules. Modules are the basic building blocks of workflows and consist of individual processing steps or algorithms within a typical two-photon calcium imaging analysis pipeline.</p>
      <p>Modules are typically categorized by their processing step where the categories are as follows: (1) File acquisition, where raw image data and metadata is loaded with options for several acquisition systems such as ThorLabs,<sup><xref rid="footnote1" ref-type="fn">1</xref></sup> Bruker/Prairie,<sup><xref rid="footnote2" ref-type="fn">2</xref></sup> and simple TIF stacks or multipage TIFs, (2) Motion correction, which corrects motion artifacts or jitter in imaging movies, (3) Cell segmentation, where cells within the field of view are identified either automatically or manually, (4) Signal extraction, where fluorescence over time is extracted from the image stack for identified cells and baseline-corrected signals can be calculated, and finally (5) Analysis, where downstream analysis can be performed on the extracted signals. NeuroWRAP includes a library of validated modules for each of these five processing steps that can be downloaded and used (<xref rid="T1" ref-type="table">Table 1</xref>). Users of NeuroWRAP are presented with all available modules which can be individually downloaded and assigned to runtime environments as desired. The modules listed in <xref rid="T1" ref-type="table">Table 1</xref> have been validated to work with one another following the typical dataflow from less processed (raw) data to more processed data. NeuroWRAP users can modify existing modules or write new modules from scratch. Users can then share any custom modules to other users, allowing them to see and optionally download them.</p>
      <table-wrap position="float" id="T1">
        <label>TABLE 1</label>
        <caption>
          <p>Highlighted modules included in NeuroWRAP.</p>
        </caption>
        <table frame="box" rules="all" cellspacing="5" cellpadding="5">
          <thead>
            <tr>
              <td valign="top" align="left" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">Module name</td>
              <td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">Type</td>
              <td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">Language</td>
              <td valign="top" align="left" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">Description</td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Read Bruker/Prairie</td>
              <td valign="top" align="center" rowspan="1" colspan="1">File acquisition</td>
              <td valign="top" align="center" rowspan="1" colspan="1">MATLAB</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Reads raw files from Bruker/Prairie system</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Read TIFs</td>
              <td valign="top" align="center" rowspan="1" colspan="1">File acquisition</td>
              <td valign="top" align="center" rowspan="1" colspan="1">MATLAB</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Reads TIF files from a directory</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Read multipage TIF</td>
              <td valign="top" align="center" rowspan="1" colspan="1">File acquisition</td>
              <td valign="top" align="center" rowspan="1" colspan="1">MATLAB</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Reads images from a multipage TIF file</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Read ThorImage (Single-channel)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">File acquisition</td>
              <td valign="top" align="center" rowspan="1" colspan="1">MATLAB</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Reads raw files from a ThorImage system</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Read ThorImage (Multi-channel)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">File acquisition</td>
              <td valign="top" align="center" rowspan="1" colspan="1">MATLAB</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Reads raw files from a ThorImage system with multichannel acquisition</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Load example raw data</td>
              <td valign="top" align="center" rowspan="1" colspan="1">File acquisition</td>
              <td valign="top" align="center" rowspan="1" colspan="1">MATLAB or Python</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Loads an example image matrix to test workflows</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DFT image registration</td>
              <td valign="top" align="center" rowspan="1" colspan="1">Motion correction</td>
              <td valign="top" align="center" rowspan="1" colspan="1">MATLAB</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Motion correction via discrete fourier transform (DFT) registration (<xref rid="B9" ref-type="bibr">Guizar-Sicairos et al., 2008</xref>)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Suite2p Cell segmentation</td>
              <td valign="top" align="center" rowspan="1" colspan="1">Cell segmentation</td>
              <td valign="top" align="center" rowspan="1" colspan="1">Python</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Automatic cell segmentation/detection from Suite2p (<xref rid="B11" ref-type="bibr">Pachitariu et al., 2017</xref>)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">CaImAn cell segmentation</td>
              <td valign="top" align="center" rowspan="1" colspan="1">Cell segmentation</td>
              <td valign="top" align="center" rowspan="1" colspan="1">Python</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Automatic cell segmentation/detection from CaImAn (<xref rid="B5" ref-type="bibr">Giovannucci et al., 2019</xref>)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">CITE-On</td>
              <td valign="top" align="center" rowspan="1" colspan="1">Cell segmentation</td>
              <td valign="top" align="center" rowspan="1" colspan="1">Python</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Automatic cell segmentation/detection from Fellin lab (<xref rid="B14" ref-type="bibr">Sitá et al., 2022</xref>)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Manual cell selection</td>
              <td valign="top" align="center" rowspan="1" colspan="1">Cell segmentation</td>
              <td valign="top" align="center" rowspan="1" colspan="1">MATLAB</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Manual cell selection via clicking GUI</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Bandpass detection</td>
              <td valign="top" align="center" rowspan="1" colspan="1">Cell segmentation</td>
              <td valign="top" align="center" rowspan="1" colspan="1">MATLAB</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Automatically detects ROIs from a mean image using band pass filtering</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Extract fluorescence</td>
              <td valign="top" align="center" rowspan="1" colspan="1">Signal extraction</td>
              <td valign="top" align="center" rowspan="1" colspan="1">MATLAB</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Extracts fluorescence over time from detected ROIs</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Calculate dF/F (sliding-window)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">Signal extraction</td>
              <td valign="top" align="center" rowspan="1" colspan="1">MATLAB</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Calculates dF/F using a sliding window to estimate baseline</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Granger causality analysis</td>
              <td valign="top" align="center" rowspan="1" colspan="1">Analysis</td>
              <td valign="top" align="center" rowspan="1" colspan="1">MATLAB</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Detects granger causal relationships between neurons based on activity (<xref rid="B4" ref-type="bibr">Francis et al., 2018</xref>, <xref rid="B3" ref-type="bibr">2022</xref>)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Cell finder consensus</td>
              <td valign="top" align="center" rowspan="1" colspan="1">Analysis</td>
              <td valign="top" align="center" rowspan="1" colspan="1">Python</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Finds matching cells from two cell segmentation algorithms</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn>
            <p>This list is not exhaustive and the module library will continue to expand as we incorporate more algorithms and users share modules.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Using modules within NeuroWRAP is significantly simpler than using packages or algorithms outside of the platform. We achieve this by requiring minimal input parameters, with default values available and visible where appropriate, and allowing outputs of modules to be piped into downstream modules as inputs. We have shared “validated” modules under the “Fraunhofer” username in NeuroWRAP (<xref rid="T1" ref-type="table">Table 1</xref>), which ensures module inputs and outputs are compatible with other modules in our library. As a result, any validated modules within NeuroWRAP can be used together so long as it follows the typical data flow logic of processing pipelines (i.e., from raw, less-processed data to more-processed data).</p>
      <p>When configuring a module within a workflow, there are four different input types that can be used for input parameters:</p>
      <list list-type="simple">
        <list-item>
          <label>•</label>
          <p><bold>Manual (default):</bold> Manual inputs let you manually enter a value into a field when creating or editing a workflow. Manual inputs support all data types except matrices.</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p><bold>Runtime:</bold> Runtime inputs are just like manual inputs, but instead of entering a value while creating or editing a workflow, the value is entered prior to executing the workflow. Runtime inputs allow you to choose different input values every time you run a workflow, without editing the workflow. Runtime inputs are useful for quickly running the same analysis on many different datasets, such as when batch processing data.</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p><bold>Pipe:</bold> The pipe input type enables passing data between modules by allowing you to choose an output of a prior module in the workflow as the input value. The available and compatible (i.e., matches the input’s variable type) inputs are listed in a drop-down menu. Pipe inputs allow custom workflows to be set up quickly and efficiently while also acting as a guide for data flow between modules.</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p><bold>HDF5:</bold> HDF5 inputs let you choose an HDF5 file (hierarchical data format version 5)<sup><xref rid="footnote3" ref-type="fn">3</xref></sup> and then choose a variable from that file as the input value. This input type is useful if loading pre-processed data to perform downstream analysis, or for iterating different analysis techniques on a previously executed workflow since NeuroWRAP’s output files are stored in HDF5.</p>
        </list-item>
      </list>
      <p>When creating a workflow, all input types are viable, it is a matter of choosing which suit the user’s situation best. In most cases, early modules in a workflow will utilize mostly manual or runtime inputs, while later modules in a workflow will utilize many pipe inputs that use outputs from previous modules.</p>
    </sec>
    <sec id="S2.SS2">
      <title>Fully customizable workflows</title>
      <p>With the library of available modules, workflows can be constructed to create data analysis pipelines. Workflows can be as long or short as needed, whether it is a single module or several modules. NeuroWRAP also comes with pre-built workflows which can be used as-is or easily modified to fit individual needs. All configured workflows, whether downloaded or created from scratch, will appear in the “Run Workflows” section of NeuroWRAP where they can be repeatedly executed as needed. <xref rid="F1" ref-type="fig">Figure 1</xref> show three example workflows, where the boxes between arrows represent individual modules and modules are colored according to their processing step categorization. Here we illustrate the ability to easily string together diverse algorithms, including different interpreter languages, on various types of acquired data. Additionally, one can load pre-processed data directly from an HDF5 file (utilizing the <italic>HDF5</italic> input type) or MATLAB.mat file (using the <italic>Load from.mat</italic> module) and jump right to the relevant processing step (<xref rid="F1" ref-type="fig">Figure 1</xref>, <italic>bottom</italic>). <xref rid="F1" ref-type="fig">Figure 1</xref> represents just a small fraction of the possible workflow configurations with the available module library.</p>
      <fig position="float" id="F1">
        <label>FIGURE 1</label>
        <caption>
          <p>Example workflows within NeuroWRAP. These three example workflows contain modules available within NeuroWRAP. Color denotes type of processing step categorized by the column headers. Workflow 3 uses results from a previous execution to calculate dF/F using an alternative method followed by downstream analysis.</p>
        </caption>
        <graphic xlink:href="fninf-17-1082111-g001" position="float"/>
      </fig>
      <p>Multiple workflows can be queued up to be automatically run and each workflow will be executed in the order they are added to the queue. This workflow queue is useful if batch processing data by running the same pipeline on several different datasets or iteratively running a pipeline with altered parameters on the same dataset to explore algorithm performance. Additionally, workflows can include multiple modules from the same processing step if a comparison of results from different algorithms is desired. For example, one could place three different automatic cell segmentation modules in a workflow, which will be executed serially and then have all results available within and at the end of the workflow execution.</p>
    </sec>
    <sec id="S2.SS3">
      <title>Integration of algorithms and programming languages</title>
      <p>Most existing neuroscience analysis pipelines are self-contained in that their individual algorithms are not easily mixed with algorithms from other existing analysis pipelines due to differences in programming languages, data structure, and data flow. Furthermore, most existing packages are written in either MATLAB or Python, potentially excluding users who lack the sufficient proficiency in one of these languages to be able to setup and utilize algorithms. NeuroWRAP bridges these gaps by supporting modules in both MATLAB and Python and integrating algorithms from various sources to function within a single workflow. The diversity of the module library is evident from <xref rid="T1" ref-type="table">Table 1</xref>, but it is important to note that these modules can all work together. Furthermore, modules from different interpreter languages can be used seamlessly within a workflow. NeuroWRAP also includes the ability to view the module code, edit module code as needed, and write a custom module from scratch. When creating a custom module, one can specify the inputs and outputs which then generates a code template where custom procedures in python or MATLAB can be written.</p>
      <p>NeuroWRAP features automatic handling of runtime environments to aid users that may have limited experience in setting up and managing virtual environments. When opening NeuroWRAP for the first time, a user will indicate where runtime interpreter paths are located on their computer, depending on what type of modules they wish to run. The three options for runtime environments are MATLAB, Python, and Conda. Python and Conda environments can both be used for modules written in Python, but Python runtime environments set up module requirements within a virtual environment (venv) while Conda runtime environments set up module requirements within a conda environment, a common framework for software packages in the life sciences domain. For any modules that require a Python virtual environment, the module installer automatically downloads and installs necessary dependencies based on the requirements file included with the module. MATLAB runtime environments simply point to the local path for the desired MATLAB version.</p>
    </sec>
    <sec id="S2.SS4">
      <title>Reproducible, recorded, and collaborative data analysis</title>
      <p>NeuroWRAP automatically records every relevant piece of information from an analysis pipeline each time it is executed (<xref rid="F2" ref-type="fig">Figure 2</xref>; <italic>left</italic>). When a workflow is run, an execution record is created which stores all module configurations, input parameter settings, data that was parsed during execution, as well as any figures produced and resulting data. Execution records are managed within NeuroWRAP but also reside in a local directory which stores all data in an HDF5 file as well as images of any produced figures. A screenshot of the NeuroWRAP GUI illustrates the workflow setup page for this specified workflow (<xref rid="F2" ref-type="fig">Figure 2</xref>; <italic>right</italic>).</p>
      <fig position="float" id="F2">
        <label>FIGURE 2</label>
        <caption>
          <p>Example workflow execution. An analysis workflow featuring 4 modules. Below each module are its input and output variables, where required inputs are bolded and optional inputs are italicized. Dashed arrows represent output variables that have been piped as inputs to downstream modules. All pictured information is stored in the output file upon execution of this workflow. On the right, a screenshot of the NeuroWRAP GUI is shown illustrating the setup page for this specific workflow.</p>
        </caption>
        <graphic xlink:href="fninf-17-1082111-g002" position="float"/>
      </fig>
      <p>Configured workflows can be shared and downloaded via NeuroWRAP enabling collaboration between users. These shared workflows can serve different purposes depending on how they are configured when shared. A shared workflow could be fully configured with all parameters preset to serve as an instructive example, or it could utilize runtime inputs so that certain key parameters can be left open for any future user. The built-in record keeping of executions facilitates reproducibility and simplifies the process of reporting analysis configuration to colleagues or in publication submissions.</p>
    </sec>
    <sec id="S2.SS5">
      <title>Consensus analysis of cell segmentation algorithms</title>
      <p>Choosing an openly available algorithm to include in an analysis workflow can be difficult because even a well-configured algorithm can produce differing results from other openly available algorithms that aim to accomplish the same task. However, these differences in output of multiple algorithms can be leveraged to find which results are the most consistent and robust. To aid researchers with this challenge, we have introduced a module in NeuroWRAP that compares and combines the output of different cell segmentation algorithms, a necessary step in any calcium imaging analysis workflow. A common issue with cell segmentation algorithms that automatically detect regions of interest (ROIs) is the occurrence of false positives (non-cellular ROIs mislabeled as active neurons) and false negatives (active neurons that were not assigned ROIs). Both erroneously included ROIs or missed ROIs can negatively affect the population statistics and downstream analysis performed on the dataset. Furthermore, the occurrence of false positives and false negatives can differ between cell segmentation algorithms. An approach to solve this problem is to use the consensus of cell segmentation algorithms by utilizing detected cells that appear as accepted ROIs in both algorithms. Assuming these two unique algorithms have different types of error, consensus analysis will reveal which detected cells are the most consistent and robust because false positives are less likely to appear as a result in both algorithms. NeuroWRAP includes a cell detection consensus module which takes the output coordinates from two cell detection algorithms and returns the cells that are location-matched within a user-defined distance threshold. The distance threshold determines the maximum acceptable pixel separation of two cell locations for them to be resolved as one cell. Cells that meet the consensus criteria can then be passed to downstream modules for further processing. A NeuroWRAP workflow can be created that utilizes two separate cell detection algorithms, runs them serially, and then feeds their output to the cell detection consensus module (<xref rid="F3" ref-type="fig">Figure 3A</xref>; <italic>top</italic>). This module will also produce a visual output that shows the output of each individual cell detection module and which cells spatially matched (<xref rid="F3" ref-type="fig">Figure 3A</xref>; <italic>bottom</italic>). This example consensus analysis workflow of Suite2p and CaImAn has been shared within NeuroWRAP with the name “Suite2P + CaImAn cell detection consensus,” which can be downloaded and used by NeuroWRAP users.</p>
      <fig position="float" id="F3">
        <label>FIGURE 3</label>
        <caption>
          <p>Results are sensitive to algorithm choice and parameter configuration. <bold>(A)</bold> Example of the cell detection consensus module in NeuroWRAP. The top schematic illustrates an example usage of the cell segmentation consensus module. The lower image is example output of the consensus module using Suite2p and CaIman on publicly available Neurofinder data. <bold>(B)</bold> Number of cells detected by Suite2p and CaImAn while altering certain input parameters. <italic>Top left</italic>: “threshold_scaling,” <italic>top right</italic>: “max_overlap” (Suite2p), <italic>bottom left</italic>: “gSig” (CaImAn), <italic>bottom right</italic>: number of expected cells in field of view extrapolated from “K” (CaImAn). <bold>(C)</bold>
<italic>Left</italic>: Number of cells detected by CaImAn across a selected parameter space. <italic>Middle</italic>: Ratio of detected cells in CaImAn to Suite2p across a selected parameter space. White pixels indicate parameter configurations where roughly the same number of neurons were detected. <italic>Right</italic>: Percentage of cells that were spatially matched between Suite2p and CaImAn at various parameter configurations. <bold>(D)</bold> Cell-location consensus between ground truth data and CaImAn’s detected cells across a selected parameter space.</p>
        </caption>
        <graphic xlink:href="fninf-17-1082111-g003" position="float"/>
      </fig>
    </sec>
    <sec id="S2.SS6">
      <title>NeuroWRAP enables crucial parameter exploration</title>
      <p>An overarching concern when considering results from neuroscience data analysis is that due to various reasons, researchers may not explore many different analysis procedures and settings on all of their data. This may be due to difficulty in tracking results across many different configurations, lacking of understanding of how sensitive parameters may be, or simply a lack of time. As a result, researchers may use parameter settings based on what was historically used in their lab or parameter settings used in well-cited literature, regardless of whether these settings are appropriate for their newly acquired data. Furthermore, slight differences in parameter choices can produce drastically different results, leading to a propagation of variability with downstream analyses and ultimately different answers to the scientific question being tested. Effort spent exploring and recording results from different algorithms and parameter settings could lead to more robust and reproducible science.</p>
      <p>The flexibility of modular workflows in NeuroWRAP is designed to enable users to easily compare different algorithms and parameter choices when constructing an analysis pipeline. To demonstrate this and further motivate the value of exploring different modules and different parameter settings within analysis pipelines, we constructed example workflows that are identical up to the point of cell segmentation, where they differ. We use automatic cell segmentation algorithms from two popular imaging analysis packages, Suite2p (<xref rid="B11" ref-type="bibr">Pachitariu et al., 2017</xref>) and CaImAn (<xref rid="B5" ref-type="bibr">Giovannucci et al., 2019</xref>). These workflows were run on the same publicly available dataset which has a ground truth of 330 cells determined from anatomical labeling (see section “Materials and methods”). To start, we view how the results from these modules change when sensitive parameters are adjusted. For Suite2p, we tested the number of cells detected as we altered two parameters: the <italic>threshold_scaling</italic> parameter which controls the threshold requirement for the signal-to-noise ratio on each ROI, and the <italic>max_overlap</italic> parameter which determines the amount of overlap two ROIs can have to be deemed unique. We found that the <italic>threshold_scaling</italic> parameter drastically affects the number of cells detected (<xref rid="F3" ref-type="fig">Figure 3B</xref>; <italic>top left</italic>) in an expected trend with lower values significantly overestimating the cell count and higher values underestimating the cell count. The <italic>max_overlap</italic> parameter followed an expected trend as well, yet with changes that were less drastic in magnitude (<xref rid="F3" ref-type="fig">Figure 3B</xref>; <italic>top right</italic>). For CaImAn, we again tested the number of cells detected as we altered two parameters: the <italic>gSig</italic> parameter which is the expected half-size of cells and <italic>K</italic> which is the number of expected cells per patch (approximated from total estimate in the entire field of view). We found that the <italic>gSig</italic> parameter has a large effect on the number of cells detected, with lower, unrealistic values drastically overestimating the number of cells, with mid-range values producing values closer to ground truth, though still with variation (<xref rid="F3" ref-type="fig">Figure 3B</xref>; bottom left). The <italic>K</italic> parameter had more surprising results, with certain values causing CaImAn to find more cells than estimated, while other values lead CaImAn to find fewer cells than estimated (<xref rid="F3" ref-type="fig">Figure 3B</xref>; bottom right).</p>
      <p>These examples of parameter exploration highlight the sensitivity of workflow configuration when it comes to producing consistent results. A simple exploration of a reduced range of these values could help users make informed decisions on parameter selections. For example, if a researcher knows the size of the field of view and the average cell size, an approximation can be made about how many cells are expected in the field of view. Utilizing NeuroWRAP to iteratively analyze data at a range of parameter values to explore how many cells are found in a recorded and reproducible manner can aid researchers in choosing an appropriate parameter setting.</p>
    </sec>
    <sec id="S2.SS7">
      <title>Consensus analysis can aid in parameter configuration of modules</title>
      <p>We have shown an example of how individual parameters can drastically alter results and need to be carefully considered when setting up analysis pipelines. We next explore how this information can be further utilized by comparing and combining the outputs of both modules and using the consensus analysis of detected cells, utilizing the unique advantages and features of NeuroWRAP. We test how these two example cell detection algorithms, Suite2p and CaImAn, agree or deviate from one another under various parameter configurations. To make the results comparison as close as possible, we use the number of cells found by Suite2p to seed CaImAn’s expected number of cells detected (parameter <italic>K</italic> from the previous section). We choose to alter the two most sensitive parameters from the previous section: <italic>threshold</italic>_scaling within Suite2p and <italic>gSig</italic> within CaImAn. We find that the number of cells detected by CaImAn expectedly decrease with higher threshold_scaling and gSig values, following the trend found in each individual algorithm (<xref rid="F3" ref-type="fig">Figure 3C</xref>; <italic>left</italic>). However, with the ratio of number of cells detected by CaImAn to number of cells detected by Suite2p (<xref rid="F3" ref-type="fig">Figure 3C</xref>; <italic>middle</italic>), we find that there is a narrow region of the parameter space (diagonally from top left to bottom right) where the two algorithms are finding roughly the same number of cells. This narrow region of the parameter space where two different algorithms agree gives more confidence in how parameters should be configured for robust results. However, the number of cells on its own may not be fully informative since the cell locations may differ. Therefore, we utilize the consensus module and determine which neurons are spatially matched at each joint parameter configuration (<xref rid="F3" ref-type="fig">Figure 3C</xref>; <italic>right</italic>). We find a similar region of high consensus between the two algorithms indicating how parameters in these two algorithms can be tuned to produce consistent output.</p>
      <p>Together these results point toward favorable regions within the parameter space (rather than precise single values) where different algorithms reach high consensus. Parameter selection can then be made according to regions of high consensus (<xref rid="F3" ref-type="fig">Figure 3C</xref>; <italic>middle</italic>, <italic>right</italic>) and whether the researcher wants more or fewer cells detected (<xref rid="F3" ref-type="fig">Figure 3C</xref>; <italic>left</italic>) depending on their research context. For example, if studying which cells are highly responsive across many trials in a stimulus-based experimental paradigm, one may wish to pick a portion of this favorable region where fewer cells are being detected (toward the bottom right), assuming highly active cells take precedent in these particular cell detection algorithms. Conversely, if studying population dynamics at a fine temporal scale, one may utilize a portion of this region to capture the activity of as many cells as possible (toward the top left) to capture the joint activity of all possible neurons. In either case, using a reduced and more coarsely sampled range of parameters in conjunction with consensus analysis could serve as a useful tool when determining how best to configure analysis on one’s data. This process is made significantly easier within the context of NeuroWRAP where all configurations are recorded with each workflow execution.</p>
      <p>Utilizing the ground truth data from anatomical labeling, we next compare the cell locations found by CaImAn across the parameter space to assess their spatial accuracy (<xref rid="F3" ref-type="fig">Figure 3D</xref>) by computing the true positive rate (TPR) as follows:</p>
      <disp-formula id="S2.Ex1">
        <mml:math id="M1" overflow="scroll">
          <mml:mrow>
            <mml:mrow>
              <mml:mi>T</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>P</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mpadded width="+5pt">
                <mml:mi>R</mml:mi>
              </mml:mpadded>
              <mml:mo>⁢</mml:mo>
              <mml:mi>C</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>a</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>I</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>m</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>A</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mpadded width="+3.3pt">
                <mml:mi>n</mml:mi>
              </mml:mpadded>
            </mml:mrow>
            <mml:mo rspace="10.8pt">=</mml:mo>
            <mml:mrow>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mpadded width="+5pt">
                    <mml:mi mathvariant="normal">#</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>o</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>f</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>C</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>a</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>I</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>m</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>A</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>n</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>c</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>e</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>l</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>l</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>s</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>m</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>a</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>t</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>c</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>h</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>i</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>n</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>g</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>g</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>r</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>o</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>u</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>n</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>d</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>t</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>r</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>u</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>t</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>h</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>o</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>t</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>a</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>l</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi mathvariant="normal">#</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>o</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>f</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>g</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>r</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>o</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>u</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>n</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>d</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>t</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>r</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>u</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>t</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>h</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>c</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>e</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>l</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>l</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>s</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mn>330</mml:mn>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                </mml:mrow>
              </mml:mfrac>
              <mml:mo>*</mml:mo>
              <mml:mn>100</mml:mn>
            </mml:mrow>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <p>We find a bias such that the more cells that are detected by CaImAn, the more likely there are to be matches with the ground truth data. Given that there is finite space within the field of view and a restriction on overlap of cells, the regions of high percentage matched with the ground truth data should still be taken as favorable to those regions with a low percentage. However, in the pursuit of proper parameter selection for cell detection, ground truth labeling is typically absent and thus consensus analysis may hold more merit.</p>
      <p>Lastly, we utilize the ground truth data in this example dataset to determine whether consensus cells are more likely to be real and therefore more robust. We first look at how many of CaImAn’s detected cells are spatially matched with ground truth at each parameter value (<xref rid="F4" ref-type="fig">Figure 4A</xref>) as follows:</p>
      <disp-formula id="S2.Ex2">
        <mml:math id="M2" overflow="scroll">
          <mml:mrow>
            <mml:mrow>
              <mml:mi>C</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>a</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>I</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>m</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>A</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mpadded width="+5pt">
                <mml:mi>n</mml:mi>
              </mml:mpadded>
              <mml:mo>⁢</mml:mo>
              <mml:mi>o</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>v</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>e</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>r</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>l</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>a</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mpadded width="+5pt">
                <mml:mi>p</mml:mi>
              </mml:mpadded>
              <mml:mo>⁢</mml:mo>
              <mml:mi>w</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>i</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>t</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mpadded width="+5pt">
                <mml:mi>h</mml:mi>
              </mml:mpadded>
              <mml:mo>⁢</mml:mo>
              <mml:mi>g</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>r</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>o</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>u</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>n</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mpadded width="+5pt">
                <mml:mi>d</mml:mi>
              </mml:mpadded>
              <mml:mo>⁢</mml:mo>
              <mml:mi>t</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>r</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>u</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>t</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mpadded width="+3.3pt">
                <mml:mi>h</mml:mi>
              </mml:mpadded>
            </mml:mrow>
            <mml:mo>=</mml:mo>
            <mml:mi/>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <disp-formula id="S2.Ex3">
        <mml:math id="M4" overflow="scroll">
          <mml:mrow>
            <mml:mstyle displaystyle="true">
              <mml:mfrac>
                <mml:mrow>
                  <mml:mpadded width="+5pt">
                    <mml:mi mathvariant="normal">#</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>o</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>f</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>C</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>a</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>I</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>m</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>A</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>n</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>c</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>e</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>l</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>l</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>s</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>m</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>a</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>t</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>c</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>h</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>i</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>n</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>g</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>g</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>r</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>o</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>u</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>n</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>d</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>t</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>r</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>u</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>t</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>h</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>o</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>t</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>a</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>l</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi mathvariant="normal">#</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>o</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>f</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>d</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>e</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>t</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>e</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>c</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>t</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>e</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>d</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>C</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>a</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>I</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>m</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>A</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>n</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>c</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>e</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>l</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>l</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>s</mml:mi>
                </mml:mrow>
              </mml:mfrac>
            </mml:mstyle>
            <mml:mo>*</mml:mo>
            <mml:mn>100</mml:mn>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <fig position="float" id="F4">
        <label>FIGURE 4</label>
        <caption>
          <p>Cell detection comparisons to ground truth data. <bold>(A)</bold> Proportion of CaImAn’s detected cells that overlap with ground truth cell locations at a range of parameter configurations. <bold>(B)</bold> Proportion of consensus analysis (Suite2p and CaImAn) cell locations that overlap with ground truth cell locations at a range of parameter configurations. <bold>(C)</bold> Scatter plot of each pixel value in panels plots <bold>(A,B)</bold>, comparing the percentage overlap with ground truth from CaImAn alone to that of CaImAn’s consensus with Suite2p.</p>
        </caption>
        <graphic xlink:href="fninf-17-1082111-g004" position="float"/>
      </fig>
      <p>We find that for most of the parameter space, only 75% or less of CaImAn’s cells are spatially matched with ground truth. It’s important to note that CaImAn is tailored toward detecting active cells, and some of the ground truth data may contain inactive cells, which may account for some of the discrepancy. Additionally, the extremes of this parameter space may be drastically overestimating or underestimating the number of cells, accounting for further deviations from ground truth. We next look at what proportion of the consensus cells, meaning the spatially matched cells that CaImAn and Suite2p both found, also overlap with ground truth cell locations (<xref rid="F4" ref-type="fig">Figure 4B</xref>) as follows:</p>
      <disp-formula id="S2.Ex4">
        <mml:math id="M6" overflow="scroll">
          <mml:mrow>
            <mml:mrow>
              <mml:mi>C</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>o</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>n</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>s</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>e</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>n</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>s</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>u</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mpadded width="+5pt">
                <mml:mi>s</mml:mi>
              </mml:mpadded>
              <mml:mo>⁢</mml:mo>
              <mml:mi>o</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>v</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>e</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>r</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>l</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>a</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mpadded width="+5pt">
                <mml:mi>p</mml:mi>
              </mml:mpadded>
              <mml:mo>⁢</mml:mo>
              <mml:mi>w</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>i</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>t</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mpadded width="+5pt">
                <mml:mi>h</mml:mi>
              </mml:mpadded>
              <mml:mo>⁢</mml:mo>
              <mml:mi>g</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>r</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>o</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>u</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>n</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mpadded width="+5pt">
                <mml:mi>d</mml:mi>
              </mml:mpadded>
              <mml:mo>⁢</mml:mo>
              <mml:mi>t</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>r</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>u</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mi>t</mml:mi>
              <mml:mo>⁢</mml:mo>
              <mml:mpadded width="+3.3pt">
                <mml:mi>h</mml:mi>
              </mml:mpadded>
            </mml:mrow>
            <mml:mo>=</mml:mo>
            <mml:mi/>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <disp-formula id="S2.Ex5">
        <mml:math id="M8" overflow="scroll">
          <mml:mrow>
            <mml:mstyle displaystyle="true">
              <mml:mfrac>
                <mml:mrow>
                  <mml:mpadded width="+5pt">
                    <mml:mi mathvariant="normal">#</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>o</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>f</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>C</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>o</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>n</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>s</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>e</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>n</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>s</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>u</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>s</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>c</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>e</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>l</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>l</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>s</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>m</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>a</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>t</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>c</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>h</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>i</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>n</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>g</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>g</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>r</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>o</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>u</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>n</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>d</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>t</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>r</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>u</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>t</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>h</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>o</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>t</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>a</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>l</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi mathvariant="normal">#</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>o</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>f</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>C</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>o</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>n</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>s</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>e</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>n</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>s</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>u</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mpadded width="+5pt">
                    <mml:mi>s</mml:mi>
                  </mml:mpadded>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>c</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>e</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>l</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>l</mml:mi>
                  <mml:mo>⁢</mml:mo>
                  <mml:mi>s</mml:mi>
                </mml:mrow>
              </mml:mfrac>
            </mml:mstyle>
            <mml:mo>*</mml:mo>
            <mml:mn>100</mml:mn>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <p>We see that consensus cell locations have a high overlap with ground truth (greater than 75%) at a wide range of the parameter space. We next compare the ground truth overlap of CaImAn and consensus cell locations by plotting the percent overlap with ground truth of each against one another at each parameter configuration (<xref rid="F4" ref-type="fig">Figure 4C</xref>). We see that most points lie above the diagonal, indicating that consensus has a higher proportion of its cells spatially matched with ground truth, regardless of parameter configuration.</p>
      <p>Consensus analysis allows researchers to be more lenient with parameter selection since the agreement cells are more likely to real and robust, while outliers or false positives would have to be produced by both individual algorithms to exist in the consensus results. When comparing to ground truth, we find that there is no single optimal parameter value, but rather a range of reasonable parameters that produce results consistent with ground truth. In a real analysis scenario, ground truth data will likely be unavailable, but the results in our example use case indicate that using the consensus cell locations rather than one cell detection algorithm in isolation award greater confidence in the detected cell locations, especially when exploring several different parameter configurations.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="S3">
    <title>Discussion</title>
    <p>Here we have presented NeuroWRAP, a workflow integrator for reproducible analysis of two-photon data. NeuroWRAP allows researchers to process and analyze multiphoton calcium imaging data while allowing them to explore, record, and share every aspect of their analysis pipeline. NeuroWRAP provides an analysis environment that contains a suite of options for each processing step in a calcium imaging data analysis pipeline while promoting reproducibility and collaboration between researchers. Furthermore, NeuroWRAP encourages researchers to explore ways to test their data analysis workflows and make them more robust. We have motivated this point with consensus analysis and parameter selection (<xref rid="F3" ref-type="fig">Figure 3</xref>).</p>
    <p>Many options exist for analysis of calcium imaging data, both in the form of individual algorithms that handle a single processing step, to suites that handle the full analysis pipeline. NeuroWRAP does not aim to replace existing algorithms, but rather aggregate and wrap existing tools in single environment where they can work together. For example, Suite2p (<xref rid="B11" ref-type="bibr">Pachitariu et al., 2017</xref>) and CaImAn (<xref rid="B5" ref-type="bibr">Giovannucci et al., 2019</xref>) are two popular options that each contain all of the analysis needed to process calcium imaging data. However, these tools are not intended to be modularized and used interchangeably with other algorithms. NeuroWRAP accomplishes interoperability between these algorithms as well as many more (see <xref rid="T1" ref-type="table">Table 1</xref>). A very similar workflow tool worth mentioning is Nipype (<xref rid="B8" ref-type="bibr">Gorgolewski et al., 2011</xref>) which is a Python package that allows constructing analysis pipelines using MATLAB and Python tools together, similar to NeuroWRAP. While the mission statement is similar, the major difference is that the available packages in Nipype are tailored toward magnetic resonance imaging data, in contrast to NeuroWRAP’s focus on multiphoton imaging data. Furthermore, NeuroWRAP focuses on workflow construction with an easy-to-use GUI, while Nipype requires some technical expertise in programming. Porcupine (<xref rid="B16" ref-type="bibr">van Mourik et al., 2018</xref>) is a visual pipeline tool that facilitates the creation of Nipype pipelines using a GUI, though again the focus is currently on magnetic resonance imaging data.</p>
    <p>As the tools available for processing multiphoton imaging data continue to develop and be extended, NeuroWRAP is designed to enable future extension seamlessly. Users of NeuroWRAP can extend its library and capabilities simply by sharing modules that they create within NeuroWRAP or sharing workflows that they construct using existing modules. Our team will continue to incorporate new algorithms and pipelines into NeuroWRAP as they are popularized or requested.</p>
    <p>Comparative and consensus analysis aim to address the challenge of variabilities in the analysis pipelines and results. Comparative and consensus analysis (i) investigates how comparable analysis should be compared with one another, (ii) generates insights to how one result differs or is similar to another, and (iii) offers mechanisms to consolidate the results and generate a consensus in order to produce a more reliable/trustworthy result. For one example use case, we have demonstrated that simple consensus based on agreement of two algorithms is sufficient to significantly enhance the robustness of the cell segmentation pipeline to parameter choice, and to enhance the trustworthiness of the selected cells using ground truth experimental data. Ultimately, the consensus analysis leads us to recommend choosing cell segmentation parameters that slightly overestimated the number of cells and pruning based on consensus for the example data analyzed here. The cell finder consensus analysis module in NeuroWRAP allows for users to easily perform the same analysis on their data.</p>
    <p>Note that the application of comparative and consensus analysis can occur in any stage in the experimental pipeline; for example, performing such analysis on the pre-processing pipeline can result in a more robust downstream analysis. The concept of consensus analysis has been proposed in software engineering domain, including in machine learning. The N-version programming technique (i.e., multiple versions of functionally equivalent programs are independently developed from the same software specification) along with majority voting algorithm has been used in safety-critical software systems to make decisions. In machine learning, multiple predictors can be used to produce the final estimate or predictor–these are often referred to “agreement of experts” or “ensemble learning,” and various consensus rules exist.</p>
    <p>Future work on NeuroWRAP will focus on further integrating consensus analysis across the workflow and making the platform applicable for a broader range of research data, in particular incorporating more experimental metadata such as behavioral readout and trial classification which will provide the necessary experimental information to incorporate more downstream analysis modules as well.</p>
  </sec>
  <sec sec-type="materials|methods" id="S4">
    <title>Materials and methods</title>
    <sec id="S4.SS1">
      <title>NeuroWRAP requirements and implementation</title>
      <p>NeuroWRAP runs on Windows and Mac. It does not have strict minimum hardware requirements, but individual modules may require more resources (i.e., RAM on modules that do not make use of memory mapping). NeuroWRAP manages workflows and keeps track of execution metadata, but all data analysis is done by modules. Modules can be implemented in Python or MATLAB, requiring a Python or MATLAB interpreter to run each, respectively.</p>
      <p>NeuroWRAP is implemented in Python. It runs a local web server using the Tornado library.<sup><xref rid="footnote4" ref-type="fn">4</xref></sup> The user interface is implemented as a single page web application and users can access it in their browser when NeuroWRAP is running. We use PyInstaller<sup><xref rid="footnote5" ref-type="fn">5</xref></sup> to bundle the application as an executable.</p>
      <p>When a Python module is installed, NeuroWRAP creates a virtual environment to install dependencies and execute the Python code. Each module is executed in a separate process and NeuroWRAP communicates with the modules over a ZeroMQ socket.<sup><xref rid="footnote6" ref-type="fn">6</xref></sup> This allows modules to run with different Python versions and isolated dependencies.</p>
    </sec>
    <sec id="S4.SS2">
      <title>Consensus analysis test data</title>
      <p>Data used for figure production (<xref rid="F3" ref-type="fig">Figure 3</xref>) was downloaded from the publicly available datasets as part of the Neurofinder challenge.<sup><xref rid="footnote7" ref-type="fn">7</xref></sup>
<xref rid="F3" ref-type="fig">Figures 3</xref>, <xref rid="F4" ref-type="fig">4</xref> utilize dataset N00.00 from the Svoboda lab and was acquired from an awake head-fixed mouse expressing genetically encoded calcium indicator GCaMP6s. Ground truth labeled ROIs were used for comparison to detected cell locations, where the precise x- and y-coordinate of each ground truth ROI was calculated as the average of the ROI pixel locations.</p>
    </sec>
    <sec id="S4.SS3">
      <title>Cell-finder consensus analysis</title>
      <p>The cell-finder consensus module uses two sets of input cell coordinates and finds which points are within the user-defined pixel distance threshold. When two cell coordinates are within this distance threshold, the average of the two cell locations is taken as the consensus coordinate location. For the figures in this work, a pixel distance of 15 was used, meaning cell centers within 15 pixels (roughly one cell diameter) are merged into one. An example consensus workflow has been shared within NeuroWRAP called “Suite2P + CaImAn cell detection consensus.”</p>
    </sec>
    <sec id="S4.SS4">
      <title>Parameter exploration analysis</title>
      <p>For Suite2p analysis, parameters were kept close to default values except when altered for parameter exploration or to fit the characteristics of the data (such as image sampling rate). Parameter “threshold_scaling” was altered between 0.7 and 3.4 in 0.1 increments while max_overlap was held at the default value of 0.75. During alterations of the max_overlap value between 0 and 1 in 0.1 increments, the threshold_scaling parameter was held at 1.5.</p>
      <p>For CaImAn analysis, parameters <italic>K</italic>, <italic>rf</italic>, and <italic>stride</italic> were set according to the CaImAn documentation to appropriately estimate the cell density. <italic>gSig</italic> is the expected half-size of neurons in pixels (approximate neuronal radius). Parameter <italic>rf</italic> is the half-size of patches and was set to <italic>gSig</italic>*4, while parameter <italic>stride</italic> is the overlap between patches in pixels and was set to <italic>gSig</italic>*2. Parameter <italic>K</italic> is the expected number of components per patch which we computed as <italic>K = K_total/npatches</italic> where <italic>npatches</italic> was determined by patch size and <italic>K_total</italic> was set according to estimate number of components in the entire field of view. For gSig analysis in <xref rid="F3" ref-type="fig">Figure 3B</xref>, we set <italic>K_total = 330 neurons</italic> as it was the number of cells labeled in the ground truth dataset. For all other analysis, <italic>K_total</italic> was varied across a pre-defined range (<xref rid="F3" ref-type="fig">Figure 3B</xref>; <italic>bottom right</italic>) or according to the number of cells detected by Suite2p (<xref rid="F3" ref-type="fig">Figures 3C, D</xref>).</p>
    </sec>
  </sec>
  <sec sec-type="data-availability" id="S5">
    <title>Data availability statement</title>
    <p>The original contributions presented in this study are included in the article/supplementary material, further inquiries can be directed to the corresponding author.</p>
  </sec>
  <sec sec-type="author-contributions" id="S6">
    <title>Author contributions</title>
    <p>GM, MD, ZB, PK, and WL initiated the project. GM, UA, and ZB developed and maintain the software. ZB, MD, AS, GM, and UA tested the software. ZB performed the analysis and wrote the manuscript. ZB, MD, AS, and WL edited the manuscript. All authors contributed to the article and approved the submitted version.</p>
  </sec>
</body>
<back>
  <ack>
    <p>We thank Dulara De Zoysa for participating in discussions and testing the software. We thank Aminah Sheikh, Kate Maximov, and other members of Patrick Kanold’s lab for testing early versions of the software. We thank Shoutik Mukherjee, Daniel Winkowski, Kelson Shilling-Scrivo, Behtash Babadi’s lab, Patrick Kanold’s lab, Tommaso Fellin’s lab, and Dante Chialvo’s lab for contributing code to the module library. We also thank Shy Shoham’s and Dima Rinberg’s labs for helpful comments on the analysis.</p>
  </ack>
  <fn-group>
    <fn id="footnote1">
      <label>1</label>
      <p>
        <ext-link xlink:href="http://www.thorlabs.com" ext-link-type="uri">www.thorlabs.com</ext-link>
      </p>
    </fn>
    <fn id="footnote2">
      <label>2</label>
      <p>
        <ext-link xlink:href="http://www.bruker.com" ext-link-type="uri">www.bruker.com</ext-link>
      </p>
    </fn>
    <fn id="footnote3">
      <label>3</label>
      <p>
        <ext-link xlink:href="http://www.hdfgroup.org" ext-link-type="uri">www.hdfgroup.org</ext-link>
      </p>
    </fn>
    <fn id="footnote4">
      <label>4</label>
      <p>
        <ext-link xlink:href="https://www.tornadoweb.org" ext-link-type="uri">https://www.tornadoweb.org</ext-link>
      </p>
    </fn>
    <fn id="footnote5">
      <label>5</label>
      <p>
        <ext-link xlink:href="https://pyinstaller.org" ext-link-type="uri">https://pyinstaller.org</ext-link>
      </p>
    </fn>
    <fn id="footnote6">
      <label>6</label>
      <p>
        <ext-link xlink:href="https://zeromq.org" ext-link-type="uri">https://zeromq.org</ext-link>
      </p>
    </fn>
    <fn id="footnote7">
      <label>7</label>
      <p>
        <ext-link xlink:href="http://neurofinder.codeneuro.org/" ext-link-type="uri">http://neurofinder.codeneuro.org/</ext-link>
      </p>
    </fn>
  </fn-group>
  <sec sec-type="COI-statement" id="S8">
    <title>Conflict of interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="S9">
    <title>Publisher’s note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botvinik-Nezer</surname><given-names>R.</given-names></name><name><surname>Holzmeister</surname><given-names>F.</given-names></name><name><surname>Camerer</surname><given-names>C. F.</given-names></name><name><surname>Dreber</surname><given-names>A.</given-names></name><name><surname>Huber</surname><given-names>J.</given-names></name><name><surname>Johannesson</surname><given-names>M.</given-names></name><etal/></person-group> (<year>2020</year>). <article-title>Variability in the analysis of a single neuroimaging dataset by many teams.</article-title>
<source><italic>Nature</italic></source>
<volume>582</volume>
<fpage>84</fpage>–<lpage>88</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-020-2314-9</pub-id>
<?supplied-pmid 32483374?><pub-id pub-id-type="pmid">32483374</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cantu</surname><given-names>D. A.</given-names></name><name><surname>Wang</surname><given-names>B.</given-names></name><name><surname>Gongwer</surname><given-names>M. W.</given-names></name><name><surname>He</surname><given-names>C. X.</given-names></name><name><surname>Goel</surname><given-names>A.</given-names></name><name><surname>Suresh</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2020</year>). <article-title>EZcalcium: Open source toolbox for analysis of calcium imaging data.</article-title>
<source><italic>Biorxiv</italic></source> [<comment>Preprint</comment>]. <pub-id pub-id-type="doi">10.1101/2020.01.02.893198</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Francis</surname><given-names>N. A.</given-names></name><name><surname>Mukherjee</surname><given-names>S.</given-names></name><name><surname>Koçillari</surname><given-names>L.</given-names></name><name><surname>Panzeri</surname><given-names>S.</given-names></name><name><surname>Babadi</surname><given-names>B.</given-names></name><name><surname>Kanold</surname><given-names>P. O.</given-names></name></person-group> (<year>2022</year>). <article-title>Sequential transmission of task-relevant information in cortical neuronal networks.</article-title>
<source><italic>Cell Rep.</italic></source>
<volume>39</volume>:<issue>110878</issue>. <pub-id pub-id-type="doi">10.1016/j.celrep.2022.110878</pub-id>
<?supplied-pmid 35649366?><pub-id pub-id-type="pmid">35649366</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Francis</surname><given-names>N. A.</given-names></name><name><surname>Winkowski</surname><given-names>D. E.</given-names></name><name><surname>Sheikhattar</surname><given-names>A.</given-names></name><name><surname>Armengol</surname><given-names>K.</given-names></name><name><surname>Babadi</surname><given-names>B.</given-names></name><name><surname>Kanold</surname><given-names>P. O.</given-names></name></person-group> (<year>2018</year>). <article-title>Small networks encode decision-making in primary auditory cortex.</article-title>
<source><italic>Neuron</italic></source>
<volume>97</volume>
<fpage>885</fpage>–<lpage>897.e886</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2018.01.019</pub-id>
<?supplied-pmid 29398362?><pub-id pub-id-type="pmid">29398362</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giovannucci</surname><given-names>A.</given-names></name><name><surname>Friedrich</surname><given-names>J.</given-names></name><name><surname>Gunn</surname><given-names>P.</given-names></name><name><surname>Kalfon</surname><given-names>J.</given-names></name><name><surname>Brown</surname><given-names>B. L.</given-names></name><name><surname>Koay</surname><given-names>S. A.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>CaImAn an open source tool for scalable calcium imaging data analysis.</article-title>
<source><italic>Elife</italic></source>
<volume>8</volume>:<issue>e38173</issue>. <pub-id pub-id-type="doi">10.7554/eLife.38173</pub-id>
<?supplied-pmid 30652683?><pub-id pub-id-type="pmid">30652683</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giovannucci</surname><given-names>A.</given-names></name><name><surname>Friedrich</surname><given-names>J.</given-names></name><name><surname>Kaufman</surname><given-names>M.</given-names></name><name><surname>Churchland</surname><given-names>A.</given-names></name><name><surname>Chklovskii</surname><given-names>D.</given-names></name><name><surname>Paninski</surname><given-names>L.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Onacid: Online analysis of calcium imaging data in real time.</article-title>
<source><italic>Biorxiv</italic></source> [<comment>Preprint</comment>]. <pub-id pub-id-type="doi">10.1101/193383</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>K. J.</given-names></name><name><surname>Auer</surname><given-names>T.</given-names></name><name><surname>Calhoun</surname><given-names>V. D.</given-names></name><name><surname>Craddock</surname><given-names>R. C.</given-names></name><name><surname>Das</surname><given-names>S.</given-names></name><name><surname>Duff</surname><given-names>E. P.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments.</article-title>
<source><italic>Sci. Data</italic></source>
<volume>3</volume>:<issue>160044</issue>. <pub-id pub-id-type="doi">10.1038/sdata.2016.44</pub-id>
<?supplied-pmid 27326542?><pub-id pub-id-type="pmid">27326542</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>K.</given-names></name><name><surname>Burns</surname><given-names>C.</given-names></name><name><surname>Madison</surname><given-names>C.</given-names></name><name><surname>Clark</surname><given-names>D.</given-names></name><name><surname>Halchenko</surname><given-names>Y.</given-names></name><name><surname>Waskom</surname><given-names>M.</given-names></name><etal/></person-group> (<year>2011</year>). <article-title>Nipype: A flexible, lightweight and extensible neuroimaging data processing framework in python.</article-title>
<source><italic>Front. Neuroinform.</italic></source>
<volume>5</volume>:<issue>13</issue>. <pub-id pub-id-type="doi">10.3389/fninf.2011.00013</pub-id>
<?supplied-pmid 21897815?><pub-id pub-id-type="pmid">21897815</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guizar-Sicairos</surname><given-names>M.</given-names></name><name><surname>Thurman</surname><given-names>S. T.</given-names></name><name><surname>Fienup</surname><given-names>J. R.</given-names></name></person-group> (<year>2008</year>). <article-title>Efficient subpixel image registration algorithms.</article-title>
<source><italic>Opt. Lett.</italic></source>
<volume>33</volume>
<fpage>156</fpage>–<lpage>158</lpage>. <pub-id pub-id-type="doi">10.1364/OL.33.000156</pub-id><pub-id pub-id-type="pmid">18197224</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miłkowski</surname><given-names>M.</given-names></name><name><surname>Hensel</surname><given-names>W. M.</given-names></name><name><surname>Hohol</surname><given-names>M.</given-names></name></person-group> (<year>2018</year>). <article-title>Replicability or reproducibility? On the replication crisis in computational neuroscience and sharing only relevant detail.</article-title>
<source><italic>J. Comput. Neurosci.</italic></source>
<volume>45</volume>
<fpage>163</fpage>–<lpage>172</lpage>. <pub-id pub-id-type="doi">10.1007/s10827-018-0702-z</pub-id>
<?supplied-pmid 30377880?><pub-id pub-id-type="pmid">30377880</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M.</given-names></name><name><surname>Stringer</surname><given-names>C.</given-names></name><name><surname>Dipoppa</surname><given-names>M.</given-names></name><name><surname>Schröder</surname><given-names>S.</given-names></name><name><surname>Rossi</surname><given-names>L. F.</given-names></name><name><surname>Dalgleish</surname><given-names>H.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Suite2p: Beyond 10,000 neurons with standard two-photon microscopy.</article-title>
<source><italic>Biorxiv</italic></source> [<comment>Preprint</comment>]. <pub-id pub-id-type="doi">10.1101/061507</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Plesser</surname><given-names>H. E.</given-names></name></person-group> (<year>2018</year>). <article-title>Reproducibility vs. replicability: A brief history of a confused terminology.</article-title>
<source><italic>Front. Neuroinform.</italic></source>
<volume>11</volume>:<issue>76</issue>. <pub-id pub-id-type="doi">10.3389/fninf.2017.00076</pub-id>
<?supplied-pmid 29403370?><pub-id pub-id-type="pmid">29403370</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rübel</surname><given-names>O.</given-names></name><name><surname>Tritt</surname><given-names>A.</given-names></name><name><surname>Ly</surname><given-names>R.</given-names></name><name><surname>Dichter</surname><given-names>B. K.</given-names></name><name><surname>Ghosh</surname><given-names>S.</given-names></name><name><surname>Niu</surname><given-names>L.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>The neurodata without borders ecosystem for neurophysiological data science.</article-title>
<source><italic>Biorxiv</italic></source> [<comment>Preprint</comment>]. <pub-id pub-id-type="doi">10.1101/2021.03.13.435173</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sitá</surname><given-names>L.</given-names></name><name><surname>Brondi</surname><given-names>M.</given-names></name><name><surname>Lagomarsino de Leon Roig</surname><given-names>P.</given-names></name><name><surname>Curreli</surname><given-names>S.</given-names></name><name><surname>Panniello</surname><given-names>M.</given-names></name><name><surname>Vecchia</surname><given-names>D.</given-names></name><etal/></person-group> (<year>2022</year>). <article-title>A deep-learning approach for online cell identification and trace extraction in functional two-photon calcium imaging</article-title>. <source><italic>Nat. Commun</italic></source>. <volume>13</volume>:<issue>529</issue>. <pub-id pub-id-type="doi">10.1038/s41467-022-29180-0</pub-id>
<?supplied-pmid 35318335?><pub-id pub-id-type="pmid">35318335</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teeters</surname><given-names>J. L.</given-names></name><name><surname>Godfrey</surname><given-names>K.</given-names></name><name><surname>Young</surname><given-names>R.</given-names></name><name><surname>Dang</surname><given-names>C.</given-names></name><name><surname>Friedsam</surname><given-names>C.</given-names></name><name><surname>Wark</surname><given-names>B.</given-names></name><etal/></person-group> (<year>2015</year>). <article-title>Neurodata without borders: Creating a common data format for neurophysiology.</article-title>
<source><italic>Neuron</italic></source>
<volume>88</volume>
<fpage>629</fpage>–<lpage>634</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2015.10.025</pub-id>
<?supplied-pmid 26590340?><pub-id pub-id-type="pmid">26590340</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Mourik</surname><given-names>T.</given-names></name><name><surname>Snoek</surname><given-names>L.</given-names></name><name><surname>Knapen</surname><given-names>T.</given-names></name><name><surname>Norris</surname><given-names>D. G.</given-names></name></person-group> (<year>2018</year>). <article-title>Porcupine: A visual pipeline tool for neuroimaging analysis.</article-title>
<source><italic>PLoS Comput. Biol.</italic></source>
<volume>14</volume>:<issue>e1006064</issue>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006064</pub-id>
<?supplied-pmid 29746461?><pub-id pub-id-type="pmid">29746461</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>P.</given-names></name><name><surname>Resendez</surname><given-names>S. L.</given-names></name><name><surname>Rodriguez-Romaguera</surname><given-names>J.</given-names></name><name><surname>Jimenez</surname><given-names>J. C.</given-names></name><name><surname>Neufeld</surname><given-names>S. Q.</given-names></name><name><surname>Giovannucci</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Efficient and accurate extraction of in vivo calcium signals from microendoscopic video data.</article-title>
<source><italic>Elife</italic></source>
<volume>7</volume>:<issue>e28728</issue>. <pub-id pub-id-type="doi">10.7554/eLife.28728</pub-id>
<?supplied-pmid 29469809?><pub-id pub-id-type="pmid">29469809</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
