<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9805574</article-id>
    <article-id pub-id-type="pmid">36373962</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btac719</article-id>
    <article-id pub-id-type="publisher-id">btac719</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Bioimage Informatics</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Digitally predicting protein localization and manipulating protein activity in fluorescence images using 4D reslicing GAN</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-6390-2517</contrib-id>
        <name>
          <surname>Jiao</surname>
          <given-names>Yang</given-names>
        </name>
        <aff><institution>Department of Electrical and Computer Engineering, University of Nevada</institution>, Las Vegas, NV 89154, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-6364-0094</contrib-id>
        <name>
          <surname>Gu</surname>
          <given-names>Lingkun</given-names>
        </name>
        <aff><institution>School of Life Sciences, University of Nevada</institution>, Las Vegas, NV 89154, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-7453-9365</contrib-id>
        <name>
          <surname>Jiang</surname>
          <given-names>Yingtao</given-names>
        </name>
        <aff><institution>Department of Electrical and Computer Engineering, University of Nevada</institution>, Las Vegas, NV 89154, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Weng</surname>
          <given-names>Mo</given-names>
        </name>
        <aff><institution>School of Life Sciences, University of Nevada</institution>, Las Vegas, NV 89154, <country country="US">USA</country></aff>
        <xref rid="btac719-cor1" ref-type="corresp"/>
        <!--mo.weng@unlv.edu-->
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-9510-1079</contrib-id>
        <name>
          <surname>Yang</surname>
          <given-names>Mei</given-names>
        </name>
        <aff><institution>Department of Electrical and Computer Engineering, University of Nevada</institution>, Las Vegas, NV 89154, <country country="US">USA</country></aff>
        <xref rid="btac719-cor1" ref-type="corresp"/>
        <!--mei.yang@unlv.edu-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Peng</surname>
          <given-names>Hanchuan</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btac719-cor1">To whom correspondence should be addressed. Email: <email>mo.weng@unlv.edu</email> or <email>mei.yang@unlv.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-11-14">
      <day>14</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>14</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <volume>39</volume>
    <issue>1</issue>
    <elocation-id>btac719</elocation-id>
    <history>
      <date date-type="received">
        <day>06</day>
        <month>6</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>28</day>
        <month>9</month>
        <year>2022</year>
      </date>
      <date date-type="editorial-decision">
        <day>17</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="corrected-typeset">
        <day>30</day>
        <month>11</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btac719.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>While multi-channel fluorescence microscopy is a vital imaging method in biological studies, the number of channels that can be imaged simultaneously is limited by technical and hardware limitations such as emission spectra cross-talk. One solution is using deep neural networks to model the localization relationship between two proteins so that the localization of one protein can be digitally predicted. Furthermore, the input and predicted localization implicitly reflect the modeled relationship. Accordingly, observing the response of the prediction via manipulating input localization could provide an informative way to analyze the modeled relationships between the input and the predicted proteins.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We propose a protein localization prediction (PLP) method using a cGAN named 4D Reslicing Generative Adversarial Network (4DR-GAN) to digitally generate additional channels. 4DR-GAN models the joint probability distribution of input and output proteins by simultaneously incorporating the protein localization signals in four dimensions including space and time. Because protein localization often correlates with protein activation state, based on accurate PLP, we further propose two novel tools: digital activation (DA) and digital inactivation (DI) to digitally activate and inactivate a protein, in order to observing the response of the predicted protein localization. Compared with genetic approaches, these tools allow precise spatial and temporal control. A comprehensive experiment on six pairs of proteins shows that 4DR-GAN achieves higher-quality PLP than Pix2Pix, and the DA and DI responses are consistent with the known protein functions. The proposed PLP method helps simultaneously visualize additional proteins, and the developed DA and DI tools provide guidance to study localization-based protein functions.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The open-source code is available at <ext-link xlink:href="https://github.com/YangJiaoUSA/4DR-GAN" ext-link-type="uri">https://github.com/YangJiaoUSA/4DR-GAN</ext-link>.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>UNLV TTGRA</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>NIH</institution>
            <institution-id institution-id-type="DOI">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Pathway to Independence Award</institution>
          </institution-wrap>
        </funding-source>
        <award-id>K99/R00 HD088764</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>UNLV University Libraries Open Article Fund</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="10"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Fluorescence microscopy, where samples are labeled with fluorescent probes (a.k.a. fluorophores), is one of the most versatile optical imaging methods. It allows visualization and quantification of various aspects of the target proteins, including their level, localization, behavior and interaction with other proteins. Usually, each protein of interest is labeled by one type of fluorophore, and its signals are collected as one channel. In laser-scanning confocal microscopy, labeled samples can be imaged in a 3D volume with three spatial axes. Application to live tissues or organisms generates time-lapse datasets with the time axis. Furthermore, if multiple proteins are labeled and imaged, additional channels are obtained, which result in information-rich 5D datasets.</p>
    <p>Although multi-channel imaging is a powerful tool that is used to understand protein functions, in practice, the number of proteins that can be imaged simultaneously is limited. This is because the emission spectra of individual fluorophores are often too wide to be sufficiently separated. Additionally, the choice of fluorophores is limited by the quantum yield and photostability of fluorophores (<xref rid="btac719-B30" ref-type="bibr">Wall <italic toggle="yes">et al.</italic>, 2015</xref>), as well as the <italic toggle="yes">in vivo</italic> concentration of target proteins. When it comes to live imaging, the choice of fluorophores is particularly limited because signals from live samples are much weaker. These limitations contribute to the difficulties of simultaneously imaging more than two proteins in live samples. Beyond the choice of compatible fluorophores, the number of channels is also bound by the availability of laser lines and detectors of a microscope, the demand for acquisition speed, and the availability of genetically labeled proteins. Without a proper tool, simultaneously observing and even studying multiple proteins has been quite a challenge.</p>
    <p>One way to alleviate this challenge is to use machine learning methods to digitally predict the localization of unimaged proteins, using the localization information obtained from the imaged proteins. As a promising candidate model for this task, conditional generative adversarial networks (cGANs) are able to take an input image and generate the desired output image. A cGAN usually has a generator and a discriminator that are both convolutional neural networks. The generator uses network parameters to implicitly model the joint probability distribution of the inputs and the outputs so that it generates the desired output for any given new input. Theoretically, if enough samples and training time are offered, the modeled probability distribution can match the true distribution (<xref rid="btac719-B9" ref-type="bibr">Goodfellow <italic toggle="yes">et al.</italic>, 2020</xref>). The discriminator model work as a classifier to discriminate the realness of the generated output. When given a new input, the generator tries to produce an output that fools the discriminator. In biological image processing, cGANs are popular in multiple topics including data augmentation (<xref rid="btac719-B2" ref-type="bibr">Bailo <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac719-B3" ref-type="bibr">Baniukiewicz <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac719-B8" ref-type="bibr">Dirvanauskas <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac719-B21" ref-type="bibr">Osokin <italic toggle="yes">et al.</italic>, 2017</xref>), domain translation (<xref rid="btac719-B10" ref-type="bibr">Han and Yin, 2017</xref>; <xref rid="btac719-B28" ref-type="bibr">Tang <italic toggle="yes">et al.</italic>, 2020</xref>), resolution enhancement (<xref rid="btac719-B1" ref-type="bibr">Alam <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac719-B12" ref-type="bibr">Ishii <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac719-B32" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2022</xref>; <xref rid="btac719-B37" ref-type="bibr">Zhou <italic toggle="yes">et al.</italic>, 2020</xref>), virtual stain (<xref rid="btac719-B4" ref-type="bibr">Bayramoglu <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btac719-B16" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac719-B17" ref-type="bibr">Liu <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac719-B22" ref-type="bibr">Rana <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btac719-B25" ref-type="bibr">Rivenson <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac719-B29" ref-type="bibr">Vasiljević <italic toggle="yes">et al.</italic>, 2021</xref>), stain normalization (<xref rid="btac719-B6" ref-type="bibr">Cong <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac719-B35" ref-type="bibr">Zanjani <italic toggle="yes">et al.</italic>, 2018</xref>) and others (<xref rid="btac719-B14" ref-type="bibr">Isomura and Toyoizumi, 2021</xref>; <xref rid="btac719-B15" ref-type="bibr">Kench and Cooper, 2021</xref>; <xref rid="btac719-B33" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2021</xref>). Particularly, Pix2Pix (<xref rid="btac719-B13" ref-type="bibr">Isola <italic toggle="yes">et al.</italic>, 2017</xref>) is a successful example of cGANs that show effectiveness on multiple tasks such as image colorization and style transfer. A recent work (<xref rid="btac719-B26" ref-type="bibr">Shigene <italic toggle="yes">et al.</italic>, 2021</xref>) attempted to predict the localization of a protein using another protein from 2D fluorescence images with Pix2Pix.</p>
    <p>However, the Pix2Pix work failed to obtain pixel-wise accurate results, likely because it only considered the 2D correlation between proteins. In living cells, the localizations of different proteins are often correlated in 3D space as well as in time. Due to the direct and indirect interaction, the localization of one protein complex may play a role in the localization of another complex of different protein compositions. Many proteins form large super-molecular complexes or structures, which occupy, move and interact with other complexes in 3D space that are captured in multiple z-slices of a 3D image stack. The complexes that are not in the same 2D plane may interact and provide important information for accurate localization prediction. In addition, complex formation, movement and interaction can be temporally regulated in the cell, and result in drastic localization changes over time. Thus, the localizations of interacting proteins often show temporal correlations as well.</p>
    <p>To better incorporate 3D and time information in predicting the localization of proteins, we propose a protein localization prediction (PLP) method using a new cGAN named 4D Reslicing Generative Adversarial Network (4DR-GAN). 4DR-GAN models the joint probability distribution of imaged and unimaged proteins by incorporating the correlations between two protein localizations manifested in four dimensions, three in space and one in time. To our knowledge, this is the first work on applying cGANs to 4D information modeling. The generator of the 4DR-GAN is an end-to-end network that takes a 4D image as an input and incorporates spatial and temporal information via two encoding paths. Subsequently, the 5D feature maps are extracted from the two paths, and they are resliced to the same shape to be paired in space and time. The paired features are reconstructed to produce a 4D output image with identical size as the input. Altogether, this 4DR-GAN enables accurate prediction of protein localization that cannot be imaged together. Furthermore, with the new capability of accurate PLP of fluorescence images, it opens the door to digitally manipulating a protein’s localization and activation. When manipulating the input protein, the response of the predicted protein reveals the protein relationships. In this regard, we further propose two novel tools, digital activation (DA) and digital inactivation (DI). DA allows to observe the predicted protein response when digitally increases protein localization or protein activity, while DI serves the same purpose by digitally decreases protein localization or protein activity.</p>
    <p>DA and DI present advantages when compared with genetic knockout and knockdown in terms of protein activity manipulation. Essential in testing the function of a gene, genetic knockout removes the gene from the genome, and gene knockdown stops or decreases the expression of the targeted genes. However, gene knockout and knockdown have drawbacks that mostly originate from their limited spatial and temporal control capabilities. Applying genetic knockdown or knockout to undesired tissues or stages often complicates the analysis of gene functions. Another drawback is that these genetic approaches are unable to manipulate gene functions at subcellular levels, which is an important aspect in understanding the differential protein functions at multiple subcellular compartments. In contrast, DA and DI can manipulate gene functions with precise spatial and temporal control and induce immediate effects, allowing gene function to be digitally removed or activated in any cells and subcellular regions at any time point. If the protein manipulation consistently leads to changes in prediction, the changes reflect the local or global relationship between the input and the predicted proteins, making DA and DI desirable tools for protein functional relationship study.</p>
    <p>To evaluate the effectiveness of PLP along with DA and DI, we used 5D datasets from live imaging of Drosophila embryos that revealed the localization of two proteins in separate channels. These datasets offer rich temporal information since the subcellular localization of proteins change rapidly in a developing embryo. The high spatial and temporal resolutions of these datasets (pixel size: 0.1 µm; frame rate: 10 s) allow us to test prediction accuracy at subcellular levels. The proteins involved are well studied in their localizations and functions, and therefore, they offer a variety of evaluation criteria. We summarize our contributions in the following three aspects:
</p>
    <list list-type="order">
      <list-item>
        <p>To visualize more proteins simultaneously in fluorescence microscopy, we propose a PLP method to predict the localization of unimaged protein from imaged proteins using 4DR-GANs, a new cGAN developed solely for this work. 4DR-GAN can simultaneously incorporate 4D information for the purpose of PLP.</p>
      </list-item>
      <list-item>
        <p>Based on PLP, we developed two new tools to digitally manipulate protein localization and activation: DA and DI. These tools allow to precise spatial and temporal manipulation and induce an immediate response. A consistent response could provide clues to the functional relationship between the two proteins.</p>
      </list-item>
      <list-item>
        <p>A comprehensive experiment on six pairs of PLP shows the effectiveness of 4DR-GAN and the success of PLP. Compared with the existing network, the protein localization and dynamic behavior in our prediction results are closer to the ground truth (GT). Through performing DA and DI on multiple groups of proteins, we obtained responses in the predicted protein localization that are consistent with the known protein functions.</p>
      </list-item>
    </list>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 4D reslicing GAN</title>
      <p>The fundamental role of 4DR-GAN is to incorporate spatial and temporal information simultaneously in the input 4D image and produce realistic 4D output. In PLP, 4DR-GAN takes one protein localization as input, and predicts another protein localization as output. 4DR-GAN consists of a 4D-reslicing generator (<italic toggle="yes">G</italic>) that predicts the protein localization in 4D images, and a 4D-consistency discriminator (<italic toggle="yes">D</italic>) that assesses the realness of the prediction in terms of localization, temporal consistency, and the input-target correlation. <xref rid="btac719-F1" ref-type="fig">Figure 1</xref> demonstrates the structure of 4DR-GAN, where an input 4D image is denoted as <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyzt</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, and a target 4D image is denoted as <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>β</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyzt</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>. <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyzt</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is first resliced into XYZ-T view as <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyz</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> which sees the 4D image as XYZ-volumes with <italic toggle="yes">t</italic> frames, and XYT-Z view as <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyt</mml:mi><mml:mo>−</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, which sees it as XYT-volumes with <italic toggle="yes">z</italic> frames.</p>
      <fig position="float" id="btac719-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>4D fluorescence image prediction by 4D Reslicing GAN (4DR-GAN). The overall flow of training the generator and the discriminator of 4DR-GAN. The input and the target 4D images constitute two channels of a 5D fluorescence image, which visualize the localization of two proteins in the X-, Y-, Z- and T-axes. <italic toggle="yes">G</italic> is a dual-path network that separately encodes the XYZ-axis and XYT-axis information of the input 4D image. <italic toggle="yes">D</italic> justifies the realness of the predicted image by taking the 4D images that are resliced into XY(ZxT) view. Various types of arrows are used to distinguish different operations, as shown in the legend. More details of network implementation are in <xref rid="sup1" ref-type="supplementary-material">Supplementary Section 1.1</xref> and <xref rid="sup1" ref-type="supplementary-material">Figure S1</xref>. Training objectives and hyperparameters can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Section 1.2</xref></p>
        </caption>
        <graphic xlink:href="btac719f1" position="float"/>
      </fig>
      <p>Correspondingly, the generator <italic toggle="yes">G</italic> has two paths. In the XYZ-T path of <italic toggle="yes">G</italic>, the XYZ-volume of each <italic toggle="yes">t</italic> frame <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyz</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is sent to XYZ Encoder to obtain the feature maps, denoted as <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyz</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, while in the XYT-Z path, the XYT-volume of each <italic toggle="yes">z</italic> frame is sent to XYT Encoder to obtain the feature maps, denoted as <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyt</mml:mi><mml:mo>−</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyz</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyt</mml:mi><mml:mo>−</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are 4D maps that incorporate both spatial and temporal information. All the feature maps, <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyz</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyt</mml:mi><mml:mo>−</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, are further assembled into 5D feature maps denoted as <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyzt</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow/><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xytz</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow/><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, respectively. Subsequently, taking into account that <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyzt</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow/><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xytz</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow/><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> represent different views of the image, we reslice <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xytz</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow/><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> according to XYZ-T view to become <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyzt</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow/><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, which spatially and temporally matches <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyzt</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow/><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. This reslicing operation is detailed in the section of network implementation in <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>.</p>
      <p>To reconstruct a 4D output, the two 5D feature maps <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyzt</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow/><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyzt</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow/><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> in XYZ-T view will be independently decoded to obtain <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mi>β</mml:mi><mml:mrow><mml:mi mathvariant="italic">xyz</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> in all <italic toggle="yes">t</italic> frames. Specifically, the 5D feature maps <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyzt</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow/><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyzt</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow/><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are resliced into individual 4D feature maps <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyz</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow/><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi mathvariant="italic">xyz</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow/><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> in <italic toggle="yes">t</italic> frames, and the corresponding pairs in time are sent to the Decoder to reconstruct the XYZ-volume <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mi>β</mml:mi><mml:mrow><mml:mi mathvariant="italic">xyz</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Upon the completion of the reconstruction, the prediction in all <italic toggle="yes">t</italic> frames <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mi>β</mml:mi><mml:mrow><mml:mi mathvariant="italic">xyzt</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is obtained.</p>
      <p>The discriminator <italic toggle="yes">D</italic> justifies the realness of the prediction by taking the 4D images that are resliced into XY(ZxT) view such as <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>×</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>β</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>×</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, and <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mi>β</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>×</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>. D takes the localization of the input and the target proteins simultaneously, such as <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>×</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>β</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>×</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>α</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>×</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mi>β</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>×</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. In this way, <italic toggle="yes">D</italic> justifies the localization and the temporal consistency of proteins, as well as the interaction between proteins.</p>
    </sec>
    <sec>
      <title>2.2 Data acquisition</title>
      <p>We applied this 4DR-GAN to the 5D datasets collected from live imaging of Drosophila early embryos involving three proteins: Myosin (Myo), E-Cadherin (E-Cad) and Ajuba (Jub). Embryos were dechorionated in 4% sodium hypochlorite, washed in water, and mounted in glass-bottom Petri dishes by the natural affinity between the vitelline membrane and the glass. The dish chamber was then filled with water and covered by an oxygen-permeable membrane. The imaging was performed with a Zeiss LSM 800 confocal microscope equipped with high sensitivity GaAsp detectors. The 488- and 561-nm lasers were used to excite GFP and mCherry, respectively. Images were acquired using a plan-Apochromat 63×/1.40 oil objective with the pinhole set at 1 Airy unit and the pixel size set at 0.124 μm. The z-stacks start from the embryo surface to 7 μm deep with 0.5 μm increments. The time interval between stacks is 10 s.</p>
      <p>The original 5D fluorescence images have two channels and slight variations in shapes. The two channels are split as input and target, and then resized and cropped into training and testing samples. Accordingly, we performed four groups of PLP: from Myo to E-Cad, from E-Cad to Myo, from Jub to Myo, and from Jub to E-Cad.</p>
      <p>For each group of proteins, 516 samples with a size of 256 × 256 × 16 × 10 × 2 were used for training. Each sample had 256 pixels on the <italic toggle="yes">X</italic>- and <italic toggle="yes">Y</italic>-axis, 16 pixels on <italic toggle="yes">Z</italic>-axis and 10 time frames. Meanwhile, two channels were involved in each sample, where one channel was used as input, and another channel was as the target output (GT). These samples were cropped from 12 5D fluorescence images. For validation, 129 samples were cropped from another three images. Three images containing the Myo localization with a size of 256 × 256 × 16 × 40 × 1 were used for testing.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <p>We evaluate the PLP from 4DR-GAN with three approaches and compare the results with Pix2Pix prediction. Section 3.1 demonstrates the similarity and difference between prediction and GT based on the characteristics of imaged biological structures. Section 3.2 quantifies the similarity between the prediction and GT images using Fréchet Inception Distance (FID). Section 3.3 quantitatively evaluates the protein distribution and behavior as well as their changes with time. Section 3.4 shows the application of PLP in DA and DI.</p>
    <sec>
      <title>3.1 PLP accurately recapitulates protein localization at subcellular levels</title>
      <p>Because the function and activation state of the protein determine the subcellular localization, we evaluated the similarity between the prediction and target GT, using key biological characteristics of the subcellular localization. Our datasets recorded the ventral cells of fly embryos (<xref rid="btac719-F2" ref-type="fig">Fig. 2a</xref>, light purple) during a period when these cells turned from a flat sheet into a tube-like structure. This tissue shape change is driven by the combined action of Myo and E-Cad (<xref rid="btac719-F2" ref-type="fig">Fig. 2a</xref>). Myo is a molecular motor that generates the contractile physical force that changes cell shape while E-cad connects Myo filaments in individual cells into tissue-level network (<xref rid="btac719-B18" ref-type="bibr">Martin <italic toggle="yes">et al.</italic>, 2009</xref>, <xref rid="btac719-B19" ref-type="bibr">2010</xref>). During the imaging period, the amount of Myo proteins that are activated in ventral cells is increased, and the activation is restricted to the apical cortex of the cell beneath the cell membrane (<xref rid="btac719-F2" ref-type="fig">Fig. 2a</xref>, red filaments). In confocal microscopy images, the activated Myo complexes are visualized as filamentous networks of high concentration, which appears in the top slides of the image stacks (<xref rid="btac719-F2" ref-type="fig">Fig. 2a and c</xref>, top row input). The inactive pool of Myo appears to be uniform and at a low concentration since they diffuse freely in the cytoplasm of the cell. To apply the force generated by active Myo to change cell morphology, Myo networks in neighbor cells are connected through the interaction with E-Cad complexes. E-Cad complexes provide adhesions between neighboring cells. In the images, inactive E-Cad proteins uniformly diffuse on and label cell membranes with low intensity. In contrast, activated E-Cad proteins that are engaged in cell adhesion are assembled into higher-order complexes, and appear in images as high-intensity clusters along the cell–cell boundaries (<xref rid="btac719-F2" ref-type="fig">Fig. 2a and c</xref>, second row, target, 8 × 8 cells). By connecting to these E-Cad clusters, Myo filaments pull cell boundaries towards the center of the apical surface, therefore reducing cell apical surface areas (<xref rid="btac719-F2" ref-type="fig">Fig. 2a</xref>). Meanwhile, in response to the force experienced by the E-Cad complex, Jub is recruited to the cell adhesion complex and detected as spots overlapping with a portion of E-Cad clusters along cell–cell boundaries (<xref rid="btac719-B23" ref-type="bibr">Rauskolb <italic toggle="yes">et al.</italic>, 2014</xref>).</p>
      <fig position="float" id="btac719-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>The subcellular localization of the predicted proteins. (<bold>a</bold>) The known relationship between Myo and E-Cad. (<bold>b</bold>) The way of demonstrating 4D images with a z-slice and multiple time frames. (<bold>c</bold>) Localization prediction from Myo to E-Cad shown with the input Myo, the ground truth Ecad localization, our 4DR-GAN prediction results and the Pix2Pix prediction results. The red boxes show 4DR-GAN predicts better cell outlines and the yellow boxes show the prediction results are too smooth without showing E-Cad clusters. (<bold>d</bold>) Localization prediction from Jub to E-Cad. The red boxes show Pix2Pix predicts inaccurately resulting in extra cell boundaries. (<bold>e</bold>) Localization prediction from E-Cad to Myo. The red boxes show that Pix2Pix predicts inaccurately resulting in visible cell boundaries. (<bold>f</bold>) Temporal consistency of prediction results. Red arrows show inconsistent predictions in time. (<bold>g</bold>) Generating an additional channel that cannot be imaged together. Best view with zoom in (A color version of this figure appears in the online version of this article)</p>
        </caption>
        <graphic xlink:href="btac719f2" position="float"/>
      </fig>
      <p>To evaluate the subcellular localization of the predicted proteins, we picked a z-slice close to the apical surface including major Myo and E-Cad signals and demonstrated the changes in consecutive <italic toggle="yes">t</italic> frames (<xref rid="btac719-F2" ref-type="fig">Fig. 2b</xref>).</p>
      <p>First, we compared the morphology of the protein localization. <xref rid="btac719-F2" ref-type="fig">Figure 2c–e</xref> shows the PLP results, with four rows displaying the input protein localization, the GT of target protein localization and the prediction by Pix2Pix and our 4DR-GAN, respectively (additional cases in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figs. S5–S10</xref>). As discussed above, E-Cad signals largely label cell boundaries. Consistent with the GT, 4DR-GAN produces correct outlines of individual cells, whereas extra or missing cells are often present in the Pix2Pix prediction (<xref rid="btac719-F2" ref-type="fig">Fig. 2c</xref>, red rectangle). 4DR-GAN is also better at recapitulating the clustering behavior of E-Cad proteins. Because inactive E-Cad uniformly labels cell membrane and active E-Cad form clusters, the cell outlines visualized by E-Cad are dotted lines like the target GT. By comparison, the Pix2Pix results tend to be smooth lines without clusters (<xref rid="btac719-F2" ref-type="fig">Fig. 2c</xref>, yellow rectangle). This shows that 4DR-GAN predicts the localization of activated E-Cad better than Pix2Pix most likely because 4DR-GAN utilizes the temporal information and allows capturing of more information from the predicted proteins. The advantage becomes more obvious when input images are of low signal-to-noise ratios, such as Jub channel co-imaged with E-Cad. As shown in <xref rid="btac719-F2" ref-type="fig">Figure 2d</xref>, the results produced by Pix2Pix show an extensive amount of extra cell boundaries that do not exist in the GT (<xref rid="btac719-F2" ref-type="fig">Fig. 2d</xref>, red rectangle). In contrast, our 4DR-GAN prediction is able to generate correct cell boundaries for most cells.</p>
      <p>4DR-GAN also generates more faithful predictions of Myo from the E-Cad channel (<xref rid="btac719-F2" ref-type="fig">Fig. 2e</xref>) and Jub channel (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S8</xref>). 4DR-GAN is able to recapitulate both the active Myo pool (high-intensity network) and the inactive pool (uniform at low intensity). Among the active Myo, the majority localizes in the center of the apical surface (medial Myo), while a minor pool localizes to some E-Cad complexes (junctional Myo). In Pix2Pix results, the predicted Myo shows lower intensities overall than GT and 4DR-GAN prediction. Interestingly, this inaccurate prediction affects medial Myo and inactive Myo more than junctional Myo, which results in a lower intensity ratio between medial Myo and junctional Myo in the Pix2Pix prediction. In addition, the localization of junctional Myo excessively resembles that of the input E-Cad rather than the GT Myo: cell outlines are clearly visible in Pix2Pix predicted Myo images even though cell outlines are barely visible in Myo images from GT and 4DR-GAN prediction (<xref rid="btac719-F2" ref-type="fig">Fig. 2e</xref>, red box). These analyses show that 4DR-GAN gives rise to more accurate prediction of protein subcellular localization.</p>
      <p>Predicting Jub from E-Cad shows the expected cluster morphology along the cell boundary (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S10</xref>), but interestingly predicting Jub from Myo appears to be more challenging (<xref rid="sup1" ref-type="supplementary-material">Supplementary Figs. S7 and S11</xref>). It still predicts many aspects correctly. For example, Jub forms bright clusters similar to E-Cad, only in cells with high levels of active Myo, and in regions close to the apical cell surface. However, it could not precisely predict Jub localization to the cell boundary. This is surprising considering that E-Cad and Jub mostly localize together and predicting E-Cad from Myo is successful. One reason for the discrepancy may lie in the different reagents. Due to chromosome conflicts, the fluorescent Myo protein used in the Myo/Jub experiments is expressed from a different transgene than that used in the Myo/E-Cad experiments. The one used in Myo/Jub experiments appears to be expressed at a lower level. As a result, while junctional Myo is readily visible in Myo/E-Cad images, its signals are substantially weaker in the Myo/Jub images (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S11</xref>). This suggests that high-quality junctional Myo signals may be an important source of information for E-Cad prediction.</p>
      <p>Secondly, we evaluated the temporal consistency of the predicted signals. Our 4DR-GAN is temporally more stable in terms of both pixel intensity and object morphology (<xref rid="btac719-F2" ref-type="fig">Fig. 2f</xref>). In the GT and 4DR-GAN images, the pixel intensities of cell boundaries labeled by E-Cad are consistent between time frames. Whereas, in the predictions of Pix2Pix, the intensity of cell boundaries changes drastically, with some cell boundaries jumping from low to high intensity in a single time interval, only to drop in the next. Morphologically, it is observed that the shape of the same cell often changes sharply and cell boundaries can suddenly appear or disappear between time frames (red arrows in <xref rid="btac719-F2" ref-type="fig">Fig. 2f</xref>). These predictions are incorrect as the shape and existence of cell boundaries do not change this drastically with our 10-s frame rate. 4DR-GAN reduces these problems and maintains the temporal consistency of cell morphology.</p>
      <p>Lastly, to test the effectiveness of generating an additional channel that cannot be imaged together, we applied the trained 4DR-GAN to dual-channel datasets of Myo and Jub and generated E-Cad images as the third channel. Both Myo channel and Jub channel can successfully predict E-Cad channel, as shown in <xref rid="btac719-F2" ref-type="fig">Figure 2g</xref>. The predicted E-Cad gives rise to cell boundaries that not only are of appropriate sizes and shapes but also cover Jub signals, consistent with Jub localizing exclusively to a portion of the E-Cad complex. Lastly, consistent with the known spatial relationship of Myo and E-Cad localization, Myo appears mostly inside the cell boundaries labeled by predicted E-Cad.</p>
    </sec>
    <sec>
      <title>3.2 4DR-GAN generates high-quality localization that has better FID scores than the compared baseline</title>
      <p>To further evaluate the predictions generated by 4DR-GAN quantitatively, we employed the FID (<xref rid="btac719-B11" ref-type="bibr">Heusel <italic toggle="yes">et al.</italic>, 2017</xref>). It is a widely used metric that reflects the human perception of similarity because it employs a deep CNN layer closer to output nodes that correspond to real-world objects. In contrast, the traditional pixel-level metrics, such as mean square error (MSE), structural similarity index and peak signal-to-noise ratio, are mismatched with human perceptual preference (<xref rid="btac719-B36" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic>, 2018</xref>). Two widely used pre-trained CNNs are employed in our evaluation: InceptionV3 (<xref rid="btac719-B27" ref-type="bibr">Szegedy <italic toggle="yes">et al.</italic>, 2016</xref>) trained on ImageNet for image classification, and I3D (<xref rid="btac719-B5" ref-type="bibr">Carreira and Zisserman, 2017</xref>; <xref rid="btac719-B31" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2018</xref>) trained on Kinetics400 for video recognition. Because our prediction output is 4D, it is sliced on the <italic toggle="yes">Z</italic>-axis and <italic toggle="yes">T</italic>-axis into 2D images to fit InceptionV3 so that the similarity between prediction output and GT can be evaluated in the XY-plane. To fit I3D, the prediction output is sliced along <italic toggle="yes">Z</italic>-axis or <italic toggle="yes">T</italic>-axis, which results in the XYZ-volume and XYT-volume respectively. This allows the evaluation of volumetric and temporal consistency. We compared 4DR-GAN against Pix2Pix, and we further developed Pix2Pix from 2D to 3D to optimize its ability in PLP. More details of FID can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Section 1.3</xref>.</p>
      <p><xref rid="btac719-T1" ref-type="table">Table 1</xref> demonstrates the FID evaluation on the prediction of six groups of samples. To comprehensively evaluate 4DR-GAN as a dual-path network, an ablation study is conducted by separately evaluating the XYZ encoding path and the XYT encoding path while muting the fourth dimension. 4DR-GAN dual-path receives the best or the second best FID score in most evaluation cases than other networks. For the prediction quality in 2D, 4DR-GAN surpasses Pix2Pix as reflected on the FID with InceptionV3. This is consistent with the observation in Section 3.1 that the prediction of 4DR-GAN has more accurate cell outlines and protein clustering behavior. Since 4DR-GAN has a similar network depth and layer arrangement with Pix2Pix, the results show that predicting protein localization by incorporating 4D information helps improve the quality of prediction in 2D views. The FID with I3D(z) and I3D(t) further evaluates the volumetric consistency and temporal consistency, respectively, and in most cases, 4DR-GAN outperforms Pix2Pix. As expected, 4DR-GAN records a substantial improvement in temporal consistency because the temporal correlation is ignored in Pix2Pix. For example, in the case of predicting E-Cad from Myo, the score of volumetric consistency improves by 10.63%, from 0.762 to 0.681, and the score of temporal consistency improves even more, by 30.03%, from 1.612 to 1.130. Accordingly, the superior performance of 4DR-GAN on temporal and volumetric consistency supports the conclusion in Section 3.1 that the predictions of 4DR-GAN have stable pixel intensity and object morphology over time.</p>
      <table-wrap position="float" id="btac719-T1">
        <label>Table 1.</label>
        <caption>
          <p>Prediction quality evaluation by FID score</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Protein (A–&gt;B)</th>
              <th rowspan="1" colspan="1">Eval metrics: FID</th>
              <th colspan="4" align="center" rowspan="1">Networks<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">Pix2Pix</th>
              <th rowspan="1" colspan="1">4DR-GAN (XYZ path)</th>
              <th rowspan="1" colspan="1">4DR-GAN (XYT path)</th>
              <th rowspan="1" colspan="1">4DR-GAN (XYZ and XYT paths)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Myo –&gt; E-Cad</td>
              <td rowspan="1" colspan="1">InceptionV3</td>
              <td rowspan="1" colspan="1">2.243</td>
              <td rowspan="1" colspan="1">1.320</td>
              <td rowspan="1" colspan="1">
                <bold>0.932</bold>
              </td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>0.578</bold>
                </underline>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">I3D (Z)</td>
              <td rowspan="1" colspan="1">0.762</td>
              <td rowspan="1" colspan="1">0.850</td>
              <td rowspan="1" colspan="1">
                <bold>0.756</bold>
              </td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>0.681</bold>
                </underline>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">I3D (T)</td>
              <td rowspan="1" colspan="1">1.615</td>
              <td rowspan="1" colspan="1">
                <bold>1.366</bold>
              </td>
              <td rowspan="1" colspan="1">1.590</td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>1.130</bold>
                </underline>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">E-Cad –&gt; Myo</td>
              <td rowspan="1" colspan="1">InceptionV3</td>
              <td rowspan="1" colspan="1">2.872</td>
              <td rowspan="1" colspan="1">0.631</td>
              <td rowspan="1" colspan="1">
                <bold>0.524</bold>
              </td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>0.393</bold>
                </underline>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">I3D (Z)</td>
              <td rowspan="1" colspan="1">0.638</td>
              <td rowspan="1" colspan="1">0.677</td>
              <td rowspan="1" colspan="1">
                <bold>0.590</bold>
              </td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>0.557</bold>
                </underline>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">I3D (T)</td>
              <td rowspan="1" colspan="1">1.261</td>
              <td rowspan="1" colspan="1">1.056</td>
              <td rowspan="1" colspan="1">
                <bold>1.013</bold>
              </td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>0.882</bold>
                </underline>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Jub –&gt; Myo</td>
              <td rowspan="1" colspan="1">InceptionV3</td>
              <td rowspan="1" colspan="1">1.861</td>
              <td rowspan="1" colspan="1">0.610</td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>0.377</bold>
                </underline>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.415</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">I3D (Z)</td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>0.920</bold>
                </underline>
              </td>
              <td rowspan="1" colspan="1">1.083</td>
              <td rowspan="1" colspan="1">0.993</td>
              <td rowspan="1" colspan="1">
                <bold>0.976</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">I3D (T)</td>
              <td rowspan="1" colspan="1">1.700</td>
              <td rowspan="1" colspan="1">1.536</td>
              <td rowspan="1" colspan="1">
                <bold>1.409</bold>
              </td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>1.379</bold>
                </underline>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Myo –&gt; Jub</td>
              <td rowspan="1" colspan="1">InceptionV3</td>
              <td rowspan="1" colspan="1">3.083</td>
              <td rowspan="1" colspan="1">
                <bold>1.867</bold>
              </td>
              <td rowspan="1" colspan="1">1.903</td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>1.434</bold>
                </underline>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">I3D (Z)</td>
              <td rowspan="1" colspan="1">4.547</td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>1.397</bold>
                </underline>
              </td>
              <td rowspan="1" colspan="1">4.529</td>
              <td rowspan="1" colspan="1">
                <bold>1.475</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">I3D (T)</td>
              <td rowspan="1" colspan="1">4.985</td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>1.775</bold>
                </underline>
              </td>
              <td rowspan="1" colspan="1">4.733</td>
              <td rowspan="1" colspan="1">
                <bold>1.920</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Jub –&gt; E-Cad</td>
              <td rowspan="1" colspan="1">InceptionV3</td>
              <td rowspan="1" colspan="1">4.253</td>
              <td rowspan="1" colspan="1">1.404</td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>1.118</bold>
                </underline>
              </td>
              <td rowspan="1" colspan="1">
                <bold>1.157</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">I3D (Z)</td>
              <td rowspan="1" colspan="1">1.389</td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>1.205</bold>
                </underline>
              </td>
              <td rowspan="1" colspan="1">1.388</td>
              <td rowspan="1" colspan="1">
                <bold>1.207</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">I3D (T)</td>
              <td rowspan="1" colspan="1">2.129</td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>1.547</bold>
                </underline>
              </td>
              <td rowspan="1" colspan="1">
                <bold>1.646</bold>
              </td>
              <td rowspan="1" colspan="1">1.714</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">E-Cad –&gt; Jub</td>
              <td rowspan="1" colspan="1">InceptionV3</td>
              <td rowspan="1" colspan="1">5.689</td>
              <td rowspan="1" colspan="1">
                <bold>1.717</bold>
              </td>
              <td rowspan="1" colspan="1">2.090</td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>1.301</bold>
                </underline>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">I3D (Z)</td>
              <td rowspan="1" colspan="1">
                <bold>1.031</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.984</bold>
              </td>
              <td rowspan="1" colspan="1">1.167</td>
              <td rowspan="1" colspan="1">1.061</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">I3D (T)</td>
              <td rowspan="1" colspan="1">1.510</td>
              <td rowspan="1" colspan="1">
                <bold>1.309</bold>
              </td>
              <td rowspan="1" colspan="1">1.464</td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>1.264</bold>
                </underline>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic toggle="yes">Note</italic>: Lower is better. The best results are in underline bold and the second best results are in bold. The orders of magnitude of InceptionV3, I3D (Z), and I3D (T) FID scores are 10<sup>3</sup>, 10<sup>1</sup>, 10<sup>1</sup>, respectively.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>From the ablation study, overall, 4DR-GAN dual-path receives the best or the second best FID score in most evaluation cases: best score for 55.56% cases and the second best score for 33.33% of the case. When 4DR-GAN is the second best, its score is often very close to the best score and outperforms Pix2Pix. In addition, 4DR-GAN XYZ-path and XYT-path outperform Pix2Pix in most cases. When comparing 4DR-GAN XYZ-path and XYT-path with Pix2Pix, we observe that the prediction performance improvement is not constrained by the encoded dimensions in the generator. For example, in the case of predicting Myo from E-Cad, we observe that 4DR-GAN XYZ-path performs well on both volumetric (XYZ) and temporal (XYT) consistency. The discriminator is the key to the improvement because it comprehensively justifies the localization and the temporal consistency of proteins, as well as the interaction between proteins. Although the 4DR-GAN XYZ-path focuses on encoding XYZ dimensions in the generator, the discriminator forces the generator to learn the T dimension to reduce the adversarial loss.</p>
      <p>We observe that the score scales vary with networks used in FID. It is common for different pre-trained networks to result in scores that differ by a few orders of magnitude (<xref rid="btac719-B11" ref-type="bibr">Heusel <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btac719-B31" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2018</xref>). The biological images used in our study differ from the pre-trained datasets of Inception V3 and I3D, and result in different score scales. In addition, image defects such as inconsistent brightness and contrast across samples can affect the evaluation score scale as well.</p>
      <p>The experimental result demonstrates that when two functionally related proteins are correlated in 4D, 4DR-GAN incorporates the information in all four dimensions and achieves high-quality prediction.</p>
    </sec>
    <sec>
      <title>3.3 PLP predicts protein localization dynamics with high fidelity</title>
      <p>Protein subcellular localization is dynamic during development and can change dramatically. We next evaluated the quality of the temporal dynamics of 4DR-GAN predicted protein localization. We used the Myo channel predicted from the E-Cad channel since Myo subcellular localization changes in all five dimensions during the live imaging periods. The details of evaluation implementation are in <xref rid="sup1" ref-type="supplementary-material">Supplementary Section 1.4</xref> and <xref rid="sup1" ref-type="supplementary-material">Figures S2–S4</xref>.</p>
      <p>First, we compared Myo intensity between GT and prediction in Z and T dimensions (<xref rid="btac719-F3" ref-type="fig">Fig. 3a</xref>). Similar to GT, in the 4DR-GAN predicted channel, high-intensity Myo is only detected on the apical surface (the first several z slices) and becomes more and more intense during the imaging time frame. This is true for both medial and junctional Myo. Although the Pix2Pix prediction follows a similar pattern, the intensity of predicted Myo is lower. This is especially prominent for medial Myo, consistent with the 2D analysis that finds a lower medial: junctional Myo ratio (<xref rid="btac719-F2" ref-type="fig">Fig. 2e</xref>). This becomes clearer when analyzing the intensity profiles along Z at a given time point or the intensity increase with time at a given z (<xref rid="btac719-F3" ref-type="fig">Fig. 3b and c</xref>). While GT and 4DR-GAN closely resemble each other, the profiles generated by P2P show lower intensities and somewhat deviated curves.</p>
      <fig position="float" id="btac719-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>The localization dynamic of the predicted Myo from E-Cad. (<bold>a</bold>) The predicted Myo intensity in Z and T dimensions compared with GT. Medial and junctional Myo signals are separately reviewed because they have differential relationships with the input E-Cad. (<bold>b</bold>) The predicted Myo intensity dynamics in Z dimension with a fixed time frame (<italic toggle="yes">t</italic> = 30) compared with GT. The example images show the GT signal in Z dimension. (<bold>c</bold>) The predicted Myo intensity dynamics in T dimension with a fixed z-slice (<italic toggle="yes">z</italic> = 4) compared with GT. The example images show the GT signal in T dimension. (<bold>d</bold>) The relationship between cell area and average Myo intensity in prediction results compared with GT. The example images demonstrate that the cells with smaller apical surface areas are more likely to have higher active Myo. (<bold>e</bold>) The relationship between cell area decreasing (DECR) ratio and average Myo intensity (INTST) increasing (INCR) ratio. The example images demonstrate that when the cell apical surface area decreases continuously, the cell is more likely to have increasing active Myo. Best view with zoom in</p>
        </caption>
        <graphic xlink:href="btac719f3" position="float"/>
      </fig>
      <p>Secondly, we examined whether the predicted Myo localization is consistent with its biological function. Higher concentration (intensity) of filamentous Myo is correlated with higher contractile force (<xref rid="btac719-B34" ref-type="bibr">Xie and Martin, 2015</xref>). By connecting to E-Cad complex, the contractile force reduces the apical surface area. Therefore, at a given time point, cells with smaller apical surface area are more likely to have more active Myo. <xref rid="btac719-F3" ref-type="fig">Figure 3d</xref> quantifies the average Myo intensities for cells of different sizes during the 10 consecutive time points when Myo is extensively activated. These histograms show that Myo is indeed at higher levels in cells of smaller size. The histogram profile of 4DR-GAN prediction is closer to that of GT than the Pix2Pix result. Similarly, a given cell usually has more active Myo when its area is reduced. <xref rid="btac719-F3" ref-type="fig">Figure 3e</xref> shows the changing rate of Myo in cells of decreasing sizes at three time points. Within this short time frame (30 s), 4DR-GAN prediction and GT have over 30% of cells that show more than 10% increase in Myo. This parameter in Pix2Pix prediction is 22% which is 30–40% less than that of 4DR-GAN prediction and GT.</p>
    </sec>
    <sec>
      <title>3.4 DA and DI predict correct consequences of protein loss-of-function and gain-of-function</title>
      <p>The above analysis shows that, by integrating the information from all four dimensions, 4DR-GAN can accurately predict protein subcellular localization and concentration. Protein localization and concentration are not only the input and output of PLP, but are also closely related to protein activation states. For example, a higher concentration of Myo indicates more activated Myo proteins and correlates with higher physical tension generated by Myo. Therefore, we reason that it is possible to digitally control protein activities, by altering their localization and concentration in the images. This inspired us to develop effective DA and DI methods (refer to the DA and DI practice in <xref rid="sup1" ref-type="supplementary-material">Supplementary Section 1.6</xref>) to digitally manipulate protein activities and predict functional consequences. The predicted channel of 4DR-GAN-based PLP should respond to the input change in a way consistent with their functional relationship.</p>
      <p>To test the effectiveness of these digital operations, we first used Myo as the input channel where Myo activities cause the change in cell apical surface area. We performed DI of Myo in the circled region by erasing Myo signals (<xref rid="btac719-F4" ref-type="fig">Fig. 4a</xref>, second row. More results in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S12</xref>). Since Myo produces contractile tension to reduce the cell apical surface, removing active Myo should lead to the relaxation of apical surface area, which appears in the image as bigger cells outlined by E-Cad. Indeed, an immediate response of the predicted E-Cad around the Myo knockout region is labeled: the cell outline marked by E-Cad in the new prediction becomes bigger (red) than those in the prediction before the DI (cyan). This is consistent with observations from biological loss-of-function experiments where breaking Myo filaments with high-power lasers leads to relaxation and expansion of the cell apical surface (<xref rid="btac719-B19" ref-type="bibr">Martin <italic toggle="yes">et al.</italic>, 2010</xref>). <xref rid="btac719-F4" ref-type="fig">Figure 4b</xref> shows the effect of DA of Myo by increasing Myo intensity in the circle, which represents an increase in the contractile force and should lead to a reduction in the cell apical surface area. The cell outline marked by E-Cad in the new prediction becomes smaller (red) than that in the prediction before the DA (cyan). Again, this is in line with the observations from biological experiments (<xref rid="btac719-B7" ref-type="bibr">Dawes-Hoang <italic toggle="yes">et al.</italic>, 2005</xref>), where forced Myo activation using genetic approaches induces cell apical surface area reduction.</p>
      <fig position="float" id="btac719-F4">
        <label>Fig. 4.</label>
        <caption>
          <p>Digital inactivation and digital activation results. (<bold>a</bold>) Observing E-Cad and cell response when digitally inactivating Myo subcellular localization. (<bold>b</bold>) Observing E-Cad and cell response when digitally activating Myo subcellular localization. (<bold>c</bold>) Observing Myo response when digitally inactivating E-Cad and stopping shrinking of apical areas. (<bold>d</bold>) Observing Myo response when digitally activating E-Cad and increasing shrinking of apical areas. (<bold>e</bold>) Observing E-Cad response when digitally inactivating Jub. (<bold>f</bold>) Observing E-Cad response when digitally activating Jub. The gray circles and arrows highlight the DI or DA manipulation, the orange dotted lines show the apical surface area manipulation, and the gray rectangles and red arrows highlight the responses. Best view with zoom in (A color version of this figure appears in the online version of this article)</p>
        </caption>
        <graphic xlink:href="btac719f4" position="float"/>
      </fig>
      <p>In the above case, Myo is the driving force causing cell apical area to change. Next we ask, whether we can predict the localization pattern of the required Myo when we digitally manipulate the apical area. Specifically, we tested whether keeping cells from decreasing their apical areas would decrease the predicted Myo intensity and whether forcing cells to shrink would increase the predicted Myo intensity. Both operations generated the expected results around the digitally altered regions. In the result, compared with the prediction without manipulation, Myo is weaker when cell areas are kept from being reduced (<xref rid="btac719-F4" ref-type="fig">Fig. 4c</xref>. More results in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S13</xref>), while Myo increases when one large cell is digitally split into two smaller ones that shrink in the apical area (<xref rid="btac719-F4" ref-type="fig">Fig. 4d</xref>).</p>
      <p>To test whether this approach can be applied to a variety of proteins, we digitally activated and inactivated Jub and observed how E-Cad responds (<xref rid="btac719-F4" ref-type="fig">Fig. 4e and f</xref>. More results in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S14</xref>). Jub diffuses in cytoplasm when inactive. It can be activated in response to Myo-generated tension and recruited to E-Cad clusters (<xref rid="btac719-B23" ref-type="bibr">Rauskolb <italic toggle="yes">et al.</italic>, 2014</xref>). This recruitment of Jub into clusters is hypothesized to stabilize cell adhesion provided by E-Cad complexes (<xref rid="btac719-B24" ref-type="bibr">Razzell <italic toggle="yes">et al.</italic>, 2018</xref>). Based on Jub localization properties, we digitally activated or inactivated Jub by strengthening or weakening Jub cluster intensity in the input images. It is observed that a weakened Jub cluster is translated into a weakened E-Cad cluster, and the two clusters overlap with each other. On the other hand, a Jub cluster of increased intensity is translated into a higher-intensity E-Cad cluster (<xref rid="btac719-F4" ref-type="fig">Fig. 4e and f</xref>). This indicates that Jub and E-Cad not only colocalize in the images due to their molecular interaction but there is also a strong correlation between Jub and E-Cad clusters’ intensity, consistent with Jub’s role in stabilizing E-Cad-based cell adhesion (<xref rid="btac719-B23" ref-type="bibr">Rauskolb <italic toggle="yes">et al.</italic>, 2014</xref>; <xref rid="btac719-B24" ref-type="bibr">Razzell <italic toggle="yes">et al.</italic>, 2018</xref>). This also shows that DA and DI are versatile approaches applicable to proteins with a network-like localization (Myo) and proteins with a cluster-like localization (E-Cad and Jub).</p>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <p>Compared with Pix2Pix, PLP generated by 4DR-GAN is more accurate in subcellular localization, temporal consistency and dynamics. The experiment results demonstrate the importance of incorporating information from all spatial and temporal dimensions in the prediction of protein localization, which allows 4DR-GAN to capture more relationship features between two protein localizations. Noticeably, there are often different pools of the same protein that change with time and show differential localizations. When the pools of a protein play different roles in protein relationships, taking advantage of four dimensions simultaneously is the key for accurate prediction. For example, predicting E-Cad from Myo localization requires the network to differentiate between junctional Myo and medial Myo. 4DR-GAN is able to do so and accurately predict the localization of E-Cad when junctional and medial Myo signals change dramatically with time. Similarly, the prediction from Jub to E-Cad requires 4DR-GAN to learn the relationship between Jub, activate E-Cad (high-intensity clusters) and inactivate E-Cad (low intensity uniform membrane). These experimental cases suggest that 4DR-GAN can learn complex spatial and temporal relationships.</p>
    <p>The proposed PLP method will benefit a variety of fluorescence microscopies, especially live imaging where fluorophore choices are limited. For example, in our experiments, imaging Jub and E-Cad together is already challenging due to low <italic toggle="yes">in vivo</italic> protein concentration and fluorophore limitations, let alone imaging three proteins. With PLP, we successfully predicted E-Cad from the imaged Myo channel in Myo-Jub datasets, which leads to high-quality signals for all three proteins. PLP can also be instrumental in case of hardware limitations such as the availability of laser lines and detectors on a microscope by predicting other channels from imaged channels.</p>
    <p>Based on 4DR-GAN-based PLP, DA and DI are two novel tools we propose for protein functional relationship study. A key feature of DA and DI is the capacity to precisely manipulate a protein localization in space and time. When doing so, DA and DI reflect the immediate effect on the output protein localization and therefore shed light on the protein relationships locally and globally. The experimental results not only demonstrate that DA and DI can predict the correct consequences of protein loss-of-function and gain-of-function, but also suggest that the 4DR-GAN-based PLP learns correct protein relationships. DA and DI require manipulation designs appropriate for protein functions. For unknown protein functions, multiple designs should be considered and analyzed.</p>
    <p>Another key feature of DA and DI is that the outputs respond to input manipulation regardless of causality between proteins or the structure labeled by the protein. When the upstream protein is manipulated digitally, it mimics biological loss-of-function and gain-of-function experiments and the downstream protein responds in the predicted localization. However, the reverse experiment is different. When manipulation is applied to the downstream protein, the upstream protein does not respond in biological experiments, but it will respond in digital experiments. Specifically, when the downstream protein is digitally manipulated into a certain localization, the prediction provides a clue on the localization of the upstream protein localization that is required to drive the downstream protein into this certain localization. Therefore, DA and DI provide additional information in such cases. Overall, the realization of DA and DI provides a convenient and low-cost way to study protein functions and relationships and guides experimental designs in biological studies of unknown proteins.</p>
    <p>Besides DA and DI, there are other possible applications of PLP. For example, in the cases of predicting the localizations of E-Cad from that of Myo and backward, we observe that the prediction of E-Cad from Myo has better perceptual quality than the prediction of Myo. The high prediction quality of E-Cad could imply that Myo is a major factor affecting E-Cad localization, while the information from E-Cad alone is insufficient and factors other than E-Cad contribute significantly to Myo localization. This observation also suggests potential future works of PLP. By adopting multiple informational sources, the performance of PLP for proteins that have multiple factors can be improved. In addition, analyzing the prediction quality may provide clues to the causality between proteins or cellular structures labeled by proteins.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the UNLV TTGRA and NIH Pathway to Independence Award [K99/R00 HD088764]. The publication fees for this article were partially supported by the UNLV University Libraries Open Article Fund.</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btac719_Supplementary_Data</label>
      <media xlink:href="btac719_supplementary_data.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <sec sec-type="data-availability">
    <title>Data availability</title>
    <p>The data underlying this article will be shared on reasonable request to the corresponding author.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btac719-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alam</surname><given-names>M.S.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Super-resolution enhancement method based on generative adversarial network for integral imaging microscopy</article-title>. <source>Sensors</source>, <volume>21</volume>, <fpage>2164</fpage>.<pub-id pub-id-type="pmid">33808866</pub-id></mixed-citation>
    </ref>
    <ref id="btac719-B2">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bailo</surname><given-names>O.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) Red blood cell image generation for data augmentation using conditional generative adversarial networks. In: <italic toggle="yes">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</italic>, <italic toggle="yes">Long Beach, CA, USA</italic>, pp. <fpage>1039</fpage>–<lpage>1048</lpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baniukiewicz</surname><given-names>P.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Generative adversarial networks for augmenting training data of microscopic cell images</article-title>. <source>Front. Comput. Sci</source>., <volume>1</volume>, <fpage>10</fpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B4">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bayramoglu</surname><given-names>N.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) Towards virtual H&amp;E staining of hyperspectral lung histology images using conditional generative adversarial networks. In: <italic toggle="yes">Proceedings of the IEEE International Conference on Computer Vision Workshops, Venice, Italy</italic>, pp. <fpage>64</fpage>–<lpage>71</lpage></mixed-citation>
    </ref>
    <ref id="btac719-B5">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Carreira</surname><given-names>J.</given-names></string-name>, <string-name><surname>Zisserman</surname><given-names>A.</given-names></string-name></person-group> (<year>2017</year>) Quo vadis, action recognition? A new model and the kinetics dataset. In: <italic toggle="yes">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>, pp. <fpage>6299</fpage>–<lpage>6308</lpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B6">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Cong</surname><given-names>C.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) Texture enhanced generative adversarial network for stain normalisation in histopathology images. In: <italic toggle="yes">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), Nice, France</italic>, IEEE, pp. <fpage>1949</fpage>–<lpage>1952</lpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dawes-Hoang</surname><given-names>R.E.</given-names></string-name></person-group><etal>et al</etal> (<year>2005</year>) <article-title>Folded gastrulation, cell shape change and the control of myosin localization</article-title>. <source>Development</source>, <volume>132</volume>, <fpage>4165</fpage>–<lpage>4178</lpage>.<pub-id pub-id-type="pmid">16123312</pub-id></mixed-citation>
    </ref>
    <ref id="btac719-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dirvanauskas</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>HEMIGEN: human embryo image generator based on generative adversarial networks</article-title>. <source>Sensors</source>, <volume>19</volume>, <fpage>3578</fpage>.<pub-id pub-id-type="pmid">31426441</pub-id></mixed-citation>
    </ref>
    <ref id="btac719-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goodfellow</surname><given-names>I.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Generative adversarial networks</article-title>. <source>Commun. ACM</source>, <volume>63</volume>, <fpage>139</fpage>–<lpage>144</lpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B10">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Han</surname><given-names>L.</given-names></string-name>, <string-name><surname>Yin</surname><given-names>Z.</given-names></string-name></person-group> (<year>2017</year>) Transferring microscopy image modalities with conditional generative adversarial networks. In: <italic toggle="yes">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, Honolulu, HI, USA</italic>, pp. <fpage>99</fpage>–<lpage>107</lpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B11">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Heusel</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In: Advances in Neural Information Processing Systems, p. <fpage>30</fpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ishii</surname><given-names>S.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Generative and discriminative model-based approaches to microscopic image restoration and segmentation</article-title>. <source>Microscopy</source>, <volume>69</volume>, <fpage>79</fpage>–<lpage>91</lpage>.<pub-id pub-id-type="pmid">32215571</pub-id></mixed-citation>
    </ref>
    <ref id="btac719-B13">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Isola</surname><given-names>P.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) Image-to-image translation with conditional adversarial networks. In: <italic toggle="yes">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA</italic>, pp. <fpage>1125</fpage>–<lpage>1134</lpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Isomura</surname><given-names>T.</given-names></string-name>, <string-name><surname>Toyoizumi</surname><given-names>T.</given-names></string-name></person-group> (<year>2021</year>) <article-title>Dimensionality reduction to maximize prediction generalization capability</article-title>. <source>Nat. Mach. Intell</source>., <volume>3</volume>, <fpage>434</fpage>–<lpage>446</lpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kench</surname><given-names>S.</given-names></string-name>, <string-name><surname>Cooper</surname><given-names>S.J.</given-names></string-name></person-group> (<year>2021</year>) <article-title>Generating three-dimensional structures from a two-dimensional slice with generative adversarial network-based dimensionality expansion</article-title>. <source>Nat. Mach. Intell</source>., <volume>3</volume>, <fpage>299</fpage>–<lpage>305</lpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Deep learning for virtual histological staining of bright-field microscopic images of unlabeled carotid artery tissue</article-title>. <source>Mol. Imaging Biol</source>., <volume>22</volume>, <fpage>1301</fpage>–<lpage>1309</lpage>.<pub-id pub-id-type="pmid">32514884</pub-id></mixed-citation>
    </ref>
    <ref id="btac719-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>S.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Unpaired stain transfer using pathology-consistent constrained generative adversarial networks</article-title>. <source>IEEE Trans. Med. Imaging</source>, <volume>40</volume>, <fpage>1977</fpage>–<lpage>1989</lpage>.<pub-id pub-id-type="pmid">33784619</pub-id></mixed-citation>
    </ref>
    <ref id="btac719-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Martin</surname><given-names>A.C.</given-names></string-name></person-group><etal>et al</etal> (<year>2009</year>) <article-title>Pulsed contractions of an actin–myosin network drive apical constriction</article-title>. <source>Nature</source>, <volume>457</volume>, <fpage>495</fpage>–<lpage>499</lpage>.<pub-id pub-id-type="pmid">19029882</pub-id></mixed-citation>
    </ref>
    <ref id="btac719-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Martin</surname><given-names>A.C.</given-names></string-name></person-group><etal>et al</etal> (<year>2010</year>) <article-title>Integration of contractile forces during tissue invagination</article-title>. <source>J. Cell Biol</source>., <volume>188</volume>, <fpage>735</fpage>–<lpage>749</lpage>.<pub-id pub-id-type="pmid">20194639</pub-id></mixed-citation>
    </ref>
    <ref id="btac719-B21">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Osokin</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) GANs for biological image synthesis. In: <italic toggle="yes">Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy</italic>, pp. <fpage>2233</fpage>–<lpage>2242</lpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B22">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Rana</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) Computational histological staining and destaining of prostate core biopsy RGB images with generative adversarial neural networks. In: <italic toggle="yes">2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA), Orlando, FL, USA</italic>, IEEE, pp. <fpage>828</fpage>–<lpage>834</lpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rauskolb</surname><given-names>C.</given-names></string-name></person-group><etal>et al</etal> (<year>2014</year>) <article-title>Cytoskeletal tension inhibits Hippo signaling through an Ajuba-Warts complex</article-title>. <source>Cell</source>, <volume>158</volume>, <fpage>143</fpage>–<lpage>156</lpage>.<pub-id pub-id-type="pmid">24995985</pub-id></mixed-citation>
    </ref>
    <ref id="btac719-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Razzell</surname><given-names>W.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>The force-sensitive protein Ajuba regulates cell adhesion during epithelial morphogenesis</article-title>. <source>J. Cell Biol</source>., <volume>217</volume>, <fpage>3715</fpage>–<lpage>3730</lpage>.<pub-id pub-id-type="pmid">30006462</pub-id></mixed-citation>
    </ref>
    <ref id="btac719-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rivenson</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>PhaseStain: the digital staining of label-free quantitative phase microscopy images using deep learning</article-title>. <source>Light Sci. Appl</source>., <volume>8</volume>, <fpage>1</fpage>–<lpage>11</lpage>.<pub-id pub-id-type="pmid">30622704</pub-id></mixed-citation>
    </ref>
    <ref id="btac719-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shigene</surname><given-names>K.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Translation of cellular protein localization using convolutional networks</article-title>. <source>Front. Cell Dev. Biol</source>., <volume>9</volume>, <fpage>635231</fpage>.<pub-id pub-id-type="pmid">34422790</pub-id></mixed-citation>
    </ref>
    <ref id="btac719-B27">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Szegedy</surname><given-names>C.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) Rethinking the inception architecture for computer vision. In: <italic toggle="yes">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA</italic>, pp. <fpage>2818</fpage>–<lpage>2826</lpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B28">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Tang</surname><given-names>Z.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) 3D conditional adversarial learning for synthesizing microscopic neuron image using skeleton-to-neuron translation. In: <italic toggle="yes">2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), Iowa City, IA, USA</italic>, IEEE, pp. <fpage>1775</fpage>–<lpage>1779</lpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vasiljević</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Towards histopathological stain invariance by unsupervised domain augmentation using generative adversarial networks</article-title>. <source>Neurocomputing</source>, <volume>460</volume>, <fpage>277</fpage>–<lpage>291</lpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wall</surname><given-names>K.P.</given-names></string-name></person-group><etal>et al</etal> (<year>2015</year>) <article-title>Fluorescence quantum yield measurements of fluorescent proteins: a laboratory experiment for a biochemistry or molecular biophysics laboratory course</article-title>. <source>Biochem. Mol. Biol. Educ</source>., <volume>43</volume>, <fpage>52</fpage>–<lpage>59</lpage>.<pub-id pub-id-type="pmid">25395254</pub-id></mixed-citation>
    </ref>
    <ref id="btac719-B31">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>T.-C.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) Video-to-video synthesis. In: <italic toggle="yes">Advances in Neural Information Processing Systems 31 (NeurIPS 2018), Montréal, Canada.</italic></mixed-citation>
    </ref>
    <ref id="btac719-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>W.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>Resolution enhancement in microscopic imaging based on generative adversarial network with unpaired data</article-title>. <source>Opt. Commun</source>., <volume>503</volume>, <fpage>127454</fpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>Z.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Global voxel transformer networks for augmented microscopy</article-title>. <source>Nat. Mach. Intell</source>., <volume>3</volume>, <fpage>161</fpage>–<lpage>171</lpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xie</surname><given-names>S.</given-names></string-name>, <string-name><surname>Martin</surname><given-names>A.C.</given-names></string-name></person-group> (<year>2015</year>) <article-title>Intracellular signalling and intercellular coupling coordinate heterogeneous contractile events to facilitate tissue folding</article-title>. <source>Nat. Commun</source>., <volume>6</volume>, <fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B35">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zanjani</surname><given-names>F.G.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) Stain normalization of histopathology images using generative adversarial networks. In: <italic toggle="yes">2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018), Washington, DC, USA</italic>, IEEE, pp. <fpage>573</fpage>–<lpage>577</lpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B36">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) The unreasonable effectiveness of deep features as a perceptual metric. In: <italic toggle="yes">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA</italic>, pp. <fpage>586</fpage>–<lpage>595</lpage>.</mixed-citation>
    </ref>
    <ref id="btac719-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname><given-names>H.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>3D high resolution generative deep-learning network for fluorescence microscopy imaging</article-title>. <source>Opt. Lett</source>., <volume>45</volume>, <fpage>1695</fpage>–<lpage>1698</lpage>.<pub-id pub-id-type="pmid">32235976</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
