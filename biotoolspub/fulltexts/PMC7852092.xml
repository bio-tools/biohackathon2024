<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7852092</article-id>
    <article-id pub-id-type="pmid">33522898</article-id>
    <article-id pub-id-type="publisher-id">3952</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-020-03952-1</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepGRN: prediction of transcription factor binding site across cell-types using attention-based deep neural networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Chen</surname>
          <given-names>Chen</given-names>
        </name>
        <address>
          <email>ccm3x@mail.missouri.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hou</surname>
          <given-names>Jie</given-names>
        </name>
        <address>
          <email>jie.hou@slu.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Shi</surname>
          <given-names>Xiaowen</given-names>
        </name>
        <address>
          <email>shix@missouri.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yang</surname>
          <given-names>Hua</given-names>
        </name>
        <address>
          <email>yanghu@missouri.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Birchler</surname>
          <given-names>James A.</given-names>
        </name>
        <address>
          <email>birchlerj@missouri.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0305-2853</contrib-id>
        <name>
          <surname>Cheng</surname>
          <given-names>Jianlin</given-names>
        </name>
        <address>
          <email>chengji@missouri.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.134936.a</institution-id><institution-id institution-id-type="ISNI">0000 0001 2162 3504</institution-id><institution>Electrical Engineering and Computer Science Department, </institution><institution>University of Missouri, </institution></institution-wrap>Columbia, MO 65211 USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.262962.b</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 9342</institution-id><institution>Department of Computer Science, </institution><institution>Saint Louis University, </institution></institution-wrap>St. Louis, MO 63103 USA </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.134936.a</institution-id><institution-id institution-id-type="ISNI">0000 0001 2162 3504</institution-id><institution>Division of Biological Sciences, </institution><institution>University of Missouri, </institution></institution-wrap>Columbia, MO 65211 USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>1</day>
      <month>2</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>1</day>
      <month>2</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>22</volume>
    <elocation-id>38</elocation-id>
    <history>
      <date date-type="received">
        <day>9</day>
        <month>3</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>29</day>
        <month>12</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Due to the complexity of the biological systems, the prediction of the potential DNA binding sites for transcription factors remains a difficult problem in computational biology. Genomic DNA sequences and experimental results from parallel sequencing provide available information about the affinity and accessibility of genome and are commonly used features in binding sites prediction. The attention mechanism in deep learning has shown its capability to learn long-range dependencies from sequential data, such as sentences and voices. Until now, no study has applied this approach in binding site inference from massively parallel sequencing data. The successful applications of attention mechanism in similar input contexts motivate us to build and test new methods that can accurately determine the binding sites of transcription factors.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">In this study, we propose a novel tool (named DeepGRN) for transcription factors binding site prediction based on the combination of two components: single attention module and pairwise attention module. The performance of our methods is evaluated on the ENCODE-DREAM in vivo Transcription Factor Binding Site Prediction Challenge datasets. The results show that DeepGRN achieves higher unified scores in 6 of 13 targets than any of the top four methods in the DREAM challenge. We also demonstrate that the attention weights learned by the model are correlated with potential informative inputs, such as DNase-Seq coverage and motifs, which provide possible explanations for the predictive improvements in DeepGRN.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">DeepGRN can automatically and effectively predict transcription factor binding sites from DNA sequences and DNase-Seq coverage. Furthermore, the visualization techniques we developed for the attention modules help to interpret how critical patterns from different types of input features are recognized by our model.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Transcription factor</kwd>
      <kwd>Attention mechanism</kwd>
      <kwd>DNA binding site prediction</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100008982</institution-id>
            <institution>National Science Foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>IOS1545780</award-id>
        <award-id>DBI1149224</award-id>
        <principal-award-recipient>
          <name>
            <surname>Birchler</surname>
            <given-names>James A.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>Jianlin</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>U.S. Department of Energy</institution>
        </funding-source>
        <award-id>DE-SC0020400</award-id>
        <principal-award-recipient>
          <name>
            <surname>Cheng</surname>
            <given-names>Jianlin</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par8">Transcription factors (TFs) are proteins that bind to specific genomic sequences and affect numerous cellular processes. They regulate the rates of transcriptional activities of downstream genes through such binding events, thus acting as activators or repressors in the gene regulatory networks by controlling the expression level and the protein abundance of their targeted genes [<xref ref-type="bibr" rid="CR1">1</xref>]. Chromatin immunoprecipitation-sequencing (ChIP-Seq) is the golden standard to determine the interactions of a TF and all its potential binding regions on genomic sequences. However, ChIP-Seq experiments usually require reagents and materials that are infeasible to acquire, such as antibodies targeting specific TF of interest. Thus, predictions of potential binding sites through computational methods are considered as alternative solutions. Also, the prediction of binding sites of TFs would facilitate many biological studies by providing resources as reference for experimental validation.</p>
    <p id="Par9">Many algorithms have been developed to infer the potential binding sites of different TFs, including hidden Markov models [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR3">3</xref>], hierarchical mixture models [<xref ref-type="bibr" rid="CR4">4</xref>], support vector machines [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR6">6</xref>], discriminative maximum conditional likelihood [<xref ref-type="bibr" rid="CR7">7</xref>] and random forest [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR9">9</xref>]. These methods usually rely on prior knowledge about sequence preference, such as position weight matrix [<xref ref-type="bibr" rid="CR10">10</xref>]. However, these features may be less reliable if they are generated from inference based methods (such as de-novo motif discovery) when no prior knowledge is available [<xref ref-type="bibr" rid="CR7">7</xref>].</p>
    <p id="Par10">More recently, methods based on deep neural networks (DNNs), such as DeepBind, TFImpute, and DeepSEA, have shown performances superior to traditional models [<xref ref-type="bibr" rid="CR11">11</xref>–<xref ref-type="bibr" rid="CR13">13</xref>]. Compared with the conventional methods, deep learning models have their advantages at learning high-level features from data with huge sizes. This property makes them ideal for the binding site prediction task since a genome-wide binding profile of a TF can be generated from each ChIP-Seq experiment. Unlike many existing models that rely on the quality of the input data and labor-intensive feature engineering, deep learning requires less domain knowledge or data pre-processing and is more powerful when there is little or no prior knowledge of potential binding regions. Current studies in the protein binding site prediction tasks usually involve the combination of two deep learning architectures: convolutional neural networks (CNN) and recurrent neural networks (RNN). The convolutional layer has the potential to extract local features from different genomic signals and regions [<xref ref-type="bibr" rid="CR14">14</xref>], while the recurrent layer is better at utilizing useful information across the entire sequences of data. Several popular methods for binding prediction, such as DanQ [<xref ref-type="bibr" rid="CR15">15</xref>], DeeperBind [<xref ref-type="bibr" rid="CR16">16</xref>], and FactorNet [<xref ref-type="bibr" rid="CR17">17</xref>], are built on such model architecture.</p>
    <p id="Par11">Recently, the concept of attention mechanism has achieved great success in neural machine translation [<xref ref-type="bibr" rid="CR18">18</xref>] and sentiment analysis [<xref ref-type="bibr" rid="CR19">19</xref>]. It enhances the ability of DNNs by focusing on the information that is highly valuable to successful prediction. Combining with RNNs, it allows models to learn the high-level representations of input sequences with long-range dependencies. For example, long short-term memory (LSTM) models with attention mechanism have been proposed in relation classification [<xref ref-type="bibr" rid="CR20">20</xref>] and sentence compression [<xref ref-type="bibr" rid="CR21">21</xref>]. Because of the input context similarities between language processing (sentences) and the DNA binding site prediction (sequences and results from massively parallel sequencing), similar approaches can be applied improve the performance of existing methods [<xref ref-type="bibr" rid="CR22">22</xref>–<xref ref-type="bibr" rid="CR24">24</xref>].</p>
    <p id="Par12">Interrogating the input–output relationships for complex models is another important task in machine learning. The weights of a deep neural network are usually difficult to interpret directly due to their redundancy and nonlinear relationship with the output. Saliency maps and feature importance scores are conventional approaches for model interpretation in machine learning involving genomics data [<xref ref-type="bibr" rid="CR25">25</xref>]. With the application of attention mechanism, we are also interested in testing its ability to enhance the interpretability of existing CNN-RNN architecture models.</p>
    <p id="Par13">In this paper, we develop a TF binding prediction tool (DeepGRN) that is based on deep learning with attention mechanism. The experimental results demonstrate that our approach is competitive among the current state-of-the-art methods. Also, our work can be extended to explain the input–output relationships through the learning process. We show that the utilization of informative patterns in both DNase-Seq and DNA sequences is important for accurate prediction.</p>
  </sec>
  <sec id="Sec2">
    <title>Implementation</title>
    <sec id="Sec3">
      <title>Datasets from ENCODE-DREAM challenge</title>
      <p id="Par14">The datasets used for model training and benchmarking are from the 2016 ENCODE-DREAM in vivo Transcription Factor Binding Site Prediction Challenge. The detailed description of the pre-processing of the data can be found at <ext-link ext-link-type="uri" xlink:href="https://www.synapse.org/#!Synapse:syn6131484/">https://www.synapse.org/#!Synapse:syn6131484/</ext-link>.</p>
      <p id="Par15">For all TF and cell-types provided in the challenge datasets, the label of the binding status of the TFs is generated from ChIP-Seq experiments and used as ground truth. Chromatin accessibility information (DNase-Seq data), and RNA-Seq data are provided as input features for model training.</p>
      <p id="Par16">For model training, we follow the rules and restrictions of the DREAM challenge: the models are trained on all chromosomes except 1, 8, and 21, and chromosome 11 is used as validation. The model with the best performance in validation data is used for final prediction if no “leaderboard” dataset is provided by the challenge. The leaderboard data are available for some TFs for benchmarking, and each participant can test the performance on these TFs with up to ten submissions. Thus, if such data are provided, we pick the top 10 best models from the first step as an optional model selection step. The final performance of our models is reported based on the final test data that are used to determine the rank of the submissions in the challenge (Figure S1 and Table S1, see Additional file <xref rid="MOESM1" ref-type="media">1</xref>). We use the similar organization of input features introduced by FactorNet [<xref ref-type="bibr" rid="CR17">17</xref>]: DNA Primary sequence, Chromatin accessibility information (DNase-Seq data) are transformed into sequential features and become the input of the convolution layers at the first part of the models. Gene expression and annotations are transformed into non-sequential features and feed into the intermediate dense layers of the model (Details are described in the “<xref rid="Sec10" ref-type="sec">Deep neural network models with attention modules</xref>” section).</p>
      <p id="Par17">We also collected DNase and ChIP profiles for additional cell lines from the Encode Project (<ext-link ext-link-type="uri" xlink:href="https://www.encodeproject.org">https://www.encodeproject.org</ext-link>) and Roadmap Epigenomics databases (<ext-link ext-link-type="uri" xlink:href="http://www.roadmapepigenomics.org/data/">http://www.roadmapepigenomics.org/data/</ext-link>) to improve the capability of generalization of our model. The performance of models trained with and without external datasets are evaluated separately.</p>
    </sec>
    <sec id="Sec4">
      <title>Transcription factor binding data</title>
      <p id="Par18">Transcription factor binding data from ChIP-Seq experiments is the target for our prediction. The whole genome is divided into bins of 200 bp with a sliding step size of 50 bp (i.e., 250-450 bp, 300-500 bp). Each bin falls into one of the three types: bound, unbound, or ambiguous, which is determined from the ChIP-Seq results. Bins overlapping with peaks and passing the Irreproducible Discovery Rate (IDR) check with a threshold of 5% [<xref ref-type="bibr" rid="CR26">26</xref>] are labeled as bound. Bins that overlap with peaks but fail to pass the reproducibility threshold are labeled as ambiguous. All other bins are labeled as unbound. We do not use any ambiguous bins during the training or validation process according to the common practice. Therefore, each bin in the genomic sequence will either be a positive site (bounded) or a negative site (unbounded).</p>
    </sec>
    <sec id="Sec5">
      <title>DNA primary sequence</title>
      <p id="Par19">Human genome release hg19/GRCh37 is used as the reference genome. In concordance with the common practice of algorithms that perform feature extraction from chromatin profile, such as FactorNet[<xref ref-type="bibr" rid="CR17">17</xref>], DeepSea[<xref ref-type="bibr" rid="CR12">12</xref>], and DanQ[<xref ref-type="bibr" rid="CR15">15</xref>], we expand each bin by 400 bp in both upstream and downstream, resulting in a 1000 bp input region. In addition, we have evaluated the performance of different selections of input ranges and showed that range above 600 bp is sufficient to acquire stable prediction performance (Figure S2). The sequence of this region is represented by a 1000 × 4 bit matrix by 1-hot encoding, with each row represented a nucleotide. Since low mappability sequences may introduce bias in parallel sequencing experiments, sequence uniqueness (also known as “mappability”) is closely related to the quality of sequencing data [<xref ref-type="bibr" rid="CR27">27</xref>]. Thus, we select Duke 35 bp uniqueness score (<ext-link ext-link-type="uri" xlink:href="https://genome.ucsc.edu/cgi-bin/hgFileUi?db=hg19&amp;g=wgEncodeMapability">https://genome.ucsc.edu/cgi-bin/hgFileUi?db=hg19&amp;g=wgEncodeMapability</ext-link>) as an extra feature. Scores ranging from 0 to 1 are assigned to each position as the inverse of occurrences of a sequence with the exceptions that the scores of unique sequences are 1 and scores of sequences occurring more than four times are 0 [<xref ref-type="bibr" rid="CR28">28</xref>]. As a result, the sequence uniqueness is represented by a 1000 × 1 vector for each input bin. The ENCODE Project Consortium has provided a blacklist of genomic regions that produce artifact signals in NGS experiments [<xref ref-type="bibr" rid="CR29">29</xref>]. We exclude input bins overlapping with these regions from training data and set their prediction scores to 0 automatically if they are in target regions of prediction.</p>
    </sec>
    <sec id="Sec6">
      <title>DNase-Seq data</title>
      <p id="Par20">Chromatin accessibility refers to the accessibility of regions on a chromosome and is highly correlated with TF binding events [<xref ref-type="bibr" rid="CR4">4</xref>]. DNase-Seq experiment can be used to obtain genome-wide maps of chromatin accessibility information as chromatin accessible regions are usually more sensitive to the endonuclease DNase-I than non-accessible regions [<xref ref-type="bibr" rid="CR30">30</xref>]. DNase-Seq results for all cell-types are provided in the Challenge datasets in the BigWig format. Normalized 1 × coverage score is generated from the BAM files using deepTools [<xref ref-type="bibr" rid="CR31">31</xref>] with bin size = 1 and is represented by a 1000 × 1 vector for each input bin.</p>
    </sec>
    <sec id="Sec7">
      <title>Gene expression and annotation</title>
      <p id="Par21">The annotation feature for each bin is encoded as a binary vector of length 6, with each value represent if there is an overlap between the input bin and each of the six genomic features (coding regions, intron, promoter, 5′/3′-UTR, and CpG island). We also include RNA-Seq data since they can be used to characterize the differences in gene expression levels among different cell-types. Principal Component Analysis (PCA) is performed on the Transcripts per Million (TPM) normalized counts from RNA-Seq data of all cell-types provided by the Challenge. The first eight principal components of a cell-type are used as expression scores for all inputs from that cell-type, generating a vector of length 8. The processed data files for these features are provided in the FactorNet Repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/uci-cbcl/FactorNet/tree/master/resources">https://github.com/uci-cbcl/FactorNet/tree/master/resources</ext-link>). These non-sequential features are fused into the first dense layer in the model.</p>
    </sec>
    <sec id="Sec8">
      <title>PhastCons genome conservation tracks</title>
      <p id="Par22">We use the 100-way PhastCons conservation tracks [<xref ref-type="bibr" rid="CR32">32</xref>] as a feature for additional models. The PhastCons scores are represented as base-by-base conservation scores generated from multiple alignments of 99 vertebrates to the human genome. Conserved elements along the genome are recognized from phylogenetic models, and the conservation score for each base is computed as the probability that it locates in such conserved regions. For each input bin, the PhastCons scores are represented as a vector of L × 1 with a range from 0 to 1.</p>
    </sec>
    <sec id="Sec9">
      <title>CpG island feature profiling</title>
      <p id="Par23">We use the CGI score derived from Mocap [<xref ref-type="bibr" rid="CR33">33</xref>] to profile the epigenomic environment for each input region. The CGI score can be calculated as:<disp-formula id="Equa"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$CGI\left( {N_{CpG} ,N_{C} ,N_{G} ,L} \right) = \left\{ {\begin{array}{*{20}c} {1\ if \frac{{N_{CpG} L}}{{((N_{C} + N_{G} )/2)^{2} }} &gt; 0.6\ and\ \frac{{N_{C} + N_{G} }}{L} &gt; 0.5} \\ {0 \ otherwise } \\ \end{array} } \right.$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mi>C</mml:mi><mml:mi>G</mml:mi><mml:mi>I</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="italic">CpG</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="italic">CpG</mml:mi></mml:mrow></mml:msub><mml:mi>L</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">/</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mo>&gt;</mml:mo><mml:mn>0.6</mml:mn><mml:mspace width="4pt"/><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mspace width="4pt"/><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow><mml:mi>L</mml:mi></mml:mfrac><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mspace width="4pt"/><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2020_3952_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par24">For each input bin, the CGI scores are represented as a vector of L × 1 with binary values of 0 or 1.</p>
    </sec>
    <sec id="Sec10">
      <title>Deep neural network models with attention modules</title>
      <p id="Par25">The shape of each sequential input is L × (4 + 1 + 1) for each region with length L after combining all sequential features (DNA sequence, sequence uniqueness, and Chromatin accessibility). Sequential inputs are generated for both the forward strand and the reverse complement strand. The weights in all layers of the model are shared between both inputs to form a “Siamese” architecture [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR34">34</xref>, <xref ref-type="bibr" rid="CR35">35</xref>]. Vectors of non-sequential features from gene expression data and genomic annotations are fused into the model at the first dense layer. The overall architecture of our model is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. The model is built with two major modules: single attention and pairwise attention. They use the same input and architecture except for their internal attention mechanism. The final result of our model is the average of the output of two modules.<fig id="Fig1"><label>Fig. 1</label><caption><p>The general framework of the two attention modules of DeepGRN. The diagram of the deep neural network architecture. Convolutional and bidirectional LSTM layers use both forward and reverse complement features as inputs. In the single attention module, attention weights are computed from hidden outputs of LSTM and are used to generate the weighted representation through an element-wise multiplication. In the pairwise attention module, three components: Q(query), K(key), and V(value) are computed from LSTM output. The multiplication of Q and transpose of K are used to calculate the attention weights for each position of V. The multiplication of V and attention scores is the output of the pairwise attention module. Outputs from attention layers are flattened and fused with non-sequential features (genomic annotation and gene expression). The final score is computed through dense layers with sigmoid activation and merging of both forward and reverse complement inputs. The dimensions of each layer are shown beside each component</p></caption><graphic xlink:href="12859_2020_3952_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par26">The first part of our model is a 1D convolutional layer, which is a common practice for feature extraction in deep learning models involving genomics data [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR17">17</xref>]. We use Bidirectional Long Short-term Memory (Bi-LSTM) nodes as recurrent units in our model. The computation steps in an LSTM unit can be written as:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_{t} = \sigma \left( {W_{f} \cdot \left[ {h_{t - 1} ,x_{t} } \right] + b_{f} } \right)$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2020_3952_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i_{t} = \sigma \left( {W_{i} \cdot \left[ {h_{t - 1} ,x_{t} } \right] + b_{i} } \right)$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2020_3952_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widetilde{{C_{t} }} = tanh\left( {W_{C} \cdot \left[ {h_{t - 1} ,x_{t} } \right] + b_{C} } \right)$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>C</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2020_3952_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widetilde{{C_{t} }} = f_{t} * C_{t - 1} + i_{t} * \widetilde{{C_{t} }}$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>C</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow/><mml:mo>∗</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow/><mml:mo>∗</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>C</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:math><graphic xlink:href="12859_2020_3952_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$o_{t} = \sigma \left( {W_{o} \cdot \left[ {h_{t - 1} ,x_{t} } \right] + b_{o} } \right)$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2020_3952_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{t} = o_{t} * tanh\left( {\widetilde{{C_{t} }}} \right)$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow/><mml:mo>∗</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mfenced close=")" open="("><mml:mover accent="true"><mml:msub><mml:mi>C</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2020_3952_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq1"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_{t}$$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq1.gif"/></alternatives></inline-formula>, <inline-formula id="IEq2"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i_{t}$$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq2.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq3"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$o_{t}$$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq3.gif"/></alternatives></inline-formula> are the forget gate, input gate, and output gate. <inline-formula id="IEq4"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{t - 1}$$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq4.gif"/></alternatives></inline-formula> and <inline-formula id="IEq5"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{t}$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq5.gif"/></alternatives></inline-formula> are the hidden state vectors at position <inline-formula id="IEq6"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t - 1$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq6.gif"/></alternatives></inline-formula> and <inline-formula id="IEq7"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t$$\end{document}</tex-math><mml:math id="M28"><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq7.gif"/></alternatives></inline-formula>. <inline-formula id="IEq8"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{t}$$\end{document}</tex-math><mml:math id="M30"><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq8.gif"/></alternatives></inline-formula> is the input vector at position <inline-formula id="IEq9"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t$$\end{document}</tex-math><mml:math id="M32"><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq9.gif"/></alternatives></inline-formula>. <inline-formula id="IEq10"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left[ {h_{t - 1} ,x_{t} } \right]$$\end{document}</tex-math><mml:math id="M34"><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq10.gif"/></alternatives></inline-formula> stands for vector concatenation operation. <inline-formula id="IEq11"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_{t - 1}$$\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq11.gif"/></alternatives></inline-formula>, <inline-formula id="IEq12"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widetilde{{C_{t} }}$$\end{document}</tex-math><mml:math id="M38"><mml:mover accent="true"><mml:msub><mml:mi>C</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq12.gif"/></alternatives></inline-formula> and <inline-formula id="IEq13"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_{t}$$\end{document}</tex-math><mml:math id="M40"><mml:msub><mml:mi>C</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq13.gif"/></alternatives></inline-formula> are output cell state at position <inline-formula id="IEq14"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t - 1$$\end{document}</tex-math><mml:math id="M42"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq14.gif"/></alternatives></inline-formula>, new cell state at position <italic>t,</italic> and output cell state at position <inline-formula id="IEq15"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t$$\end{document}</tex-math><mml:math id="M44"><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq15.gif"/></alternatives></inline-formula>, respectively. <inline-formula id="IEq16"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W_{f}$$\end{document}</tex-math><mml:math id="M46"><mml:msub><mml:mi>W</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq16.gif"/></alternatives></inline-formula>, <inline-formula id="IEq17"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W_{i}$$\end{document}</tex-math><mml:math id="M48"><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq17.gif"/></alternatives></inline-formula>, <inline-formula id="IEq18"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W_{C}$$\end{document}</tex-math><mml:math id="M50"><mml:msub><mml:mi>W</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq18.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq19"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W_{o}$$\end{document}</tex-math><mml:math id="M52"><mml:msub><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq19.gif"/></alternatives></inline-formula> are learned weight matrices. <inline-formula id="IEq20"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$b_{f}$$\end{document}</tex-math><mml:math id="M54"><mml:msub><mml:mi>b</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq20.gif"/></alternatives></inline-formula>, <inline-formula id="IEq21"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$b_{i}$$\end{document}</tex-math><mml:math id="M56"><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq21.gif"/></alternatives></inline-formula>, <inline-formula id="IEq22"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$b_{C}$$\end{document}</tex-math><mml:math id="M58"><mml:msub><mml:mi>b</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq22.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq23"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$b_{o}$$\end{document}</tex-math><mml:math id="M60"><mml:msub><mml:mi>b</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq23.gif"/></alternatives></inline-formula> are learned bias vector parameters for each gate. <inline-formula id="IEq24"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><mml:math id="M62"><mml:mi>σ</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq24.gif"/></alternatives></inline-formula> and <inline-formula id="IEq25"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$tanh$$\end{document}</tex-math><mml:math id="M64"><mml:mrow><mml:mi mathvariant="italic">tanh</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq25.gif"/></alternatives></inline-formula> are sigmoid function and hyperbolic tangent function, respectively.</p>
      <p id="Par27">In Bi-LSTM layers, two copies of the inputs of LSTM are rearranged into two directions: one for the forward direction and one for the backward direction, and they go into the LSTM unit separately. The outputs from two directions are concatenated at the last dimension. Thus, the last dimension of the Bi-LSTM output is two times of the last dimension of the input.</p>
      <p id="Par28">In the single attention module, suppose its input vector <inline-formula id="IEq26"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h$$\end{document}</tex-math><mml:math id="M66"><mml:mi>h</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq26.gif"/></alternatives></inline-formula> has shape <inline-formula id="IEq27"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l$$\end{document}</tex-math><mml:math id="M68"><mml:mi>l</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq27.gif"/></alternatives></inline-formula> by <inline-formula id="IEq28"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r$$\end{document}</tex-math><mml:math id="M70"><mml:mi>r</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq28.gif"/></alternatives></inline-formula>, we first computed the unnormalized attention score <inline-formula id="IEq29"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e = M \times h{ }$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>h</mml:mi><mml:mrow/></mml:mrow></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq29.gif"/></alternatives></inline-formula> where <inline-formula id="IEq30"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$M$$\end{document}</tex-math><mml:math id="M74"><mml:mi>M</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq30.gif"/></alternatives></inline-formula> is a weight matrix with shape <inline-formula id="IEq31"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l$$\end{document}</tex-math><mml:math id="M76"><mml:mi>l</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq31.gif"/></alternatives></inline-formula> by <inline-formula id="IEq32"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l$$\end{document}</tex-math><mml:math id="M78"><mml:mi>l</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq32.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq33"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e$$\end{document}</tex-math><mml:math id="M80"><mml:mi>e</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq33.gif"/></alternatives></inline-formula> has shape <inline-formula id="IEq34"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l$$\end{document}</tex-math><mml:math id="M82"><mml:mi>l</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq34.gif"/></alternatives></inline-formula> by <inline-formula id="IEq35"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r$$\end{document}</tex-math><mml:math id="M84"><mml:mi>r</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq35.gif"/></alternatives></inline-formula>. A learned bias of shape <inline-formula id="IEq36"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l$$\end{document}</tex-math><mml:math id="M86"><mml:mi>l</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq36.gif"/></alternatives></inline-formula> by <inline-formula id="IEq37"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r$$\end{document}</tex-math><mml:math id="M88"><mml:mi>r</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq37.gif"/></alternatives></inline-formula> is added to <inline-formula id="IEq38"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e$$\end{document}</tex-math><mml:math id="M90"><mml:mi>e</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq38.gif"/></alternatives></inline-formula> after the multiplication. This can be summarized as a dense layer operation <inline-formula id="IEq39"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_{att,r}$$\end{document}</tex-math><mml:math id="M92"><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq39.gif"/></alternatives></inline-formula> on input <inline-formula id="IEq40"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h$$\end{document}</tex-math><mml:math id="M94"><mml:mi>h</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq40.gif"/></alternatives></inline-formula>. Then, we apply the Softmax function along the first dimension of <inline-formula id="IEq41"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e$$\end{document}</tex-math><mml:math id="M96"><mml:mi>e</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq41.gif"/></alternatives></inline-formula> in order to get the normalized attention score <inline-formula id="IEq42"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M98"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq42.gif"/></alternatives></inline-formula>. Finally, the weighted output <inline-formula id="IEq43"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z$$\end{document}</tex-math><mml:math id="M100"><mml:mi>Z</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq43.gif"/></alternatives></inline-formula> will be computed based on the attention weight <inline-formula id="IEq44"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M102"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq44.gif"/></alternatives></inline-formula>. At dimension <inline-formula id="IEq45"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r$$\end{document}</tex-math><mml:math id="M104"><mml:mi>r</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq45.gif"/></alternatives></inline-formula> of input <inline-formula id="IEq46"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h$$\end{document}</tex-math><mml:math id="M106"><mml:mi>h</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq46.gif"/></alternatives></inline-formula>, these steps can be written as follows:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e_{r} = f_{att,r} \left( {h_{1,r} ,h_{2,r} ,...,h_{N,r} } \right)$$\end{document}</tex-math><mml:math id="M108" display="block"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2020_3952_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${ }\alpha_{i,r} = exp\left( {e_{i,r} } \right)/\mathop \sum \limits_{k = 1}^{N} exp\left( {e_{k,r} } \right){ }{\text{ }}$$\end{document}</tex-math><mml:math id="M110" display="block"><mml:mrow><mml:mrow/><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mo stretchy="false">/</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mrow/><mml:mspace width="0.333333em"/></mml:mrow></mml:math><graphic xlink:href="12859_2020_3952_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha_{i} = (\mathop \sum \limits_{r = 1}^{R} \alpha_{i,r} )/D$$\end{document}</tex-math><mml:math id="M112" display="block"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>R</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">/</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:math><graphic xlink:href="12859_2020_3952_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z_{i,r} = h_{i,r} {*}\alpha_{i}$$\end{document}</tex-math><mml:math id="M114" display="block"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow/><mml:mo>∗</mml:mo></mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><graphic xlink:href="12859_2020_3952_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par29">Here, <inline-formula id="IEq47"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e_{r}$$\end{document}</tex-math><mml:math id="M116"><mml:msub><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq47.gif"/></alternatives></inline-formula> is the unnormalized attention score at dimension <inline-formula id="IEq48"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r$$\end{document}</tex-math><mml:math id="M118"><mml:mi>r</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq48.gif"/></alternatives></inline-formula>. Vector <inline-formula id="IEq49"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha_{i,r}$$\end{document}</tex-math><mml:math id="M120"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq49.gif"/></alternatives></inline-formula> represents attention weight at dimension <inline-formula id="IEq50"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r$$\end{document}</tex-math><mml:math id="M122"><mml:mi>r</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq50.gif"/></alternatives></inline-formula> of position <inline-formula id="IEq51"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M124"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq51.gif"/></alternatives></inline-formula> and is normalized by Softmax function. The attention dimension <inline-formula id="IEq52"><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r$$\end{document}</tex-math><mml:math id="M126"><mml:mi>r</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq52.gif"/></alternatives></inline-formula> in our model will stay unchanged during the transformations. The dimension of the attention weights can be reduced from <inline-formula id="IEq53"><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N \times r$$\end{document}</tex-math><mml:math id="M128"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq53.gif"/></alternatives></inline-formula> to <inline-formula id="IEq54"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N \times 1$$\end{document}</tex-math><mml:math id="M130"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq54.gif"/></alternatives></inline-formula> by averaging at each position. The final output <inline-formula id="IEq55"><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z_{i,r}$$\end{document}</tex-math><mml:math id="M132"><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq55.gif"/></alternatives></inline-formula> is computed based on the corresponding attention score. After the attention layers, the prediction scores are computed from dense layers with sigmoid activation function and merged from both forward and reverse complement inputs.</p>
      <p id="Par30">In the pairwise attention module, there are three components: Q(query), K(key) and V(value). Their values are computed from LSTM output from three different trainable weight matrices. The dimension of the trained weights for Q, K and V are <inline-formula id="IEq56"><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l$$\end{document}</tex-math><mml:math id="M134"><mml:mi>l</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq56.gif"/></alternatives></inline-formula> by <inline-formula id="IEq57"><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_{k}$$\end{document}</tex-math><mml:math id="M136"><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq57.gif"/></alternatives></inline-formula>, <inline-formula id="IEq58"><alternatives><tex-math id="M137">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l$$\end{document}</tex-math><mml:math id="M138"><mml:mi>l</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq58.gif"/></alternatives></inline-formula> by <inline-formula id="IEq59"><alternatives><tex-math id="M139">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_{k}$$\end{document}</tex-math><mml:math id="M140"><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq59.gif"/></alternatives></inline-formula> and <inline-formula id="IEq60"><alternatives><tex-math id="M141">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l$$\end{document}</tex-math><mml:math id="M142"><mml:mi>l</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq60.gif"/></alternatives></inline-formula> by <inline-formula id="IEq61"><alternatives><tex-math id="M143">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_{v}$$\end{document}</tex-math><mml:math id="M144"><mml:msub><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq61.gif"/></alternatives></inline-formula> where <inline-formula id="IEq62"><alternatives><tex-math id="M145">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_{k}$$\end{document}</tex-math><mml:math id="M146"><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq62.gif"/></alternatives></inline-formula> and <inline-formula id="IEq63"><alternatives><tex-math id="M147">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_{v}$$\end{document}</tex-math><mml:math id="M148"><mml:msub><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq63.gif"/></alternatives></inline-formula> are set as 64 as the default setup described in [<xref ref-type="bibr" rid="CR36">36</xref>]. The multiplication of Q and transpose of K are used to compute the attention weights for each position of V after Softmax conversion and dimension normalization. The multiplication of V and attention weights are the output of the pairwise attention module. The output of the pairwise attention module is computed as:<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M149">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z = Softmax\left( {\frac{{Q \times K^{T} }}{{\sqrt {d_{k} } }}} \right) \times V$$\end{document}</tex-math><mml:math id="M150" display="block"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced close=")" open="("><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mfenced><mml:mo>×</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math><graphic xlink:href="12859_2020_3952_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par31">Since each position in the sequential features simultaneously flows through the pairwise attention module, the pairwise attention module itself is not able to sense the position and order from the sequential input. To address this, we add the positional encodings to the input of the pairwise attention. We expect this additional encoding will enhance the ability of the model to make use of the order of the sequence. The positional encodings have the same dimension <inline-formula id="IEq64"><alternatives><tex-math id="M151">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d$$\end{document}</tex-math><mml:math id="M152"><mml:mi>d</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq64.gif"/></alternatives></inline-formula> as the input of the pairwise attention module. In this work, we choose different frequencies sine and cosine functions [<xref ref-type="bibr" rid="CR37">37</xref>] to encode the positional information:<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M153">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$PE_{{\left( {pos,2i} \right)}} = sin\left( {pos/10000^{2i/d} } \right)$$\end{document}</tex-math><mml:math id="M154" display="block"><mml:mrow><mml:mi>P</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:msup><mml:mn>10000</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2020_3952_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M155">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$PE_{{\left( {pos,2i + 1} \right)}} = cos\left( {pos/10000^{2i/d} } \right)$$\end{document}</tex-math><mml:math id="M156" display="block"><mml:mrow><mml:mi>P</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:msup><mml:mn>10000</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2020_3952_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq65"><alternatives><tex-math id="M157">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$pos$$\end{document}</tex-math><mml:math id="M158"><mml:mrow><mml:mi mathvariant="italic">pos</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq65.gif"/></alternatives></inline-formula> is the position in the sequential input, and <inline-formula id="IEq66"><alternatives><tex-math id="M159">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M160"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq66.gif"/></alternatives></inline-formula> is the index of the last dimension of the model. The resulting positional encodings vector is added to its input. Through such encoding technique, the relative position information can be learned by the model since for any fixed offset <inline-formula id="IEq67"><alternatives><tex-math id="M161">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k$$\end{document}</tex-math><mml:math id="M162"><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq67.gif"/></alternatives></inline-formula>, <inline-formula id="IEq68"><alternatives><tex-math id="M163">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$PE_{{\left( {pos + k} \right)}}$$\end{document}</tex-math><mml:math id="M164"><mml:mrow><mml:mi>P</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfenced></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq68.gif"/></alternatives></inline-formula> can be represented as <inline-formula id="IEq69"><alternatives><tex-math id="M165">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$PE_{{\left( {pos,2i} \right)}} cos\left( {10000^{2k/d} } \right) + PE_{{\left( {pos,2i + 1} \right)}} sin\left( {10000^{2k/d} } \right)$$\end{document}</tex-math><mml:math id="M166"><mml:mrow><mml:mi>P</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:msub><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:msup><mml:mn>10000</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>k</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mfenced><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mfenced close=")" open="("><mml:msup><mml:mn>10000</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>k</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq69.gif"/></alternatives></inline-formula>, which is the linear combination of <inline-formula id="IEq70"><alternatives><tex-math id="M167">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$PE_{{\left( {pos} \right)}}$$\end{document}</tex-math><mml:math id="M168"><mml:mrow><mml:mi>P</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">pos</mml:mi></mml:mrow></mml:mfenced></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq70.gif"/></alternatives></inline-formula>. Similarly, this also applies to dimensions of <inline-formula id="IEq71"><alternatives><tex-math id="M169">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2i + 1$$\end{document}</tex-math><mml:math id="M170"><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq71.gif"/></alternatives></inline-formula> as well.</p>
      <p id="Par32">The single attention module is designed to represent the importance of different regions along with the sequential input, while the pairwise attention module seeks to attend the importance between each pair of positions across the sequential input. We expect this difference in architecture will help to improve the learning ability of the model in a complementary manner.</p>
      <p id="Par33">We tested different configurations for typical hyperparameters (learning rate, network depth, dropout rates) and the hyperparameters specific to our model (the dimension of attention weights, merging function the two output scores) during training. The complete description of hyperparameters and their possible options are summarized in Table S2 [see Additional file <xref rid="MOESM1" ref-type="media">1</xref>]. We train one model for each TF, resulting in 12 models in total. The single and pairwise attention module will always use the same configuration rather than train separately.</p>
      <p id="Par34">There are 51,676,736 bins in total on training chromosomes in the labels, resulting in <inline-formula id="IEq72"><alternatives><tex-math id="M171">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$51676736 \times n$$\end{document}</tex-math><mml:math id="M172"><mml:mrow><mml:mn>51676736</mml:mn><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq72.gif"/></alternatives></inline-formula> potential training samples for each TF, where <inline-formula id="IEq73"><alternatives><tex-math id="M173">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n$$\end{document}</tex-math><mml:math id="M174"><mml:mi>n</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq73.gif"/></alternatives></inline-formula> is the number of available cell-types for training. Due to limited computing capacity, we use the iterative training process. During training, the training data is the mixture of all positives (labeled as “B”) with downsampled negatives (labeled as “U”) [<xref ref-type="bibr" rid="CR17">17</xref>]. In the traditional model training in deep learning, all input data are used to update the model weights exactly once for each epoch. However, this is not applicable in our task since the negative samples (regions do not bind to TFs) are much more abundant than the positive samples (regions bind to TFs), and use all negative samples for training in one epoch is not practical since the number of them is extremely huge (as they cover most of the human genome). Thus, in each epoch during model training, we first sample negative samples with numbers proportional to the number of all positive samples, and combine these negative samples with all positive samples for training. We will re-sample the negative bins and start another round of model training (next epoch). To make the training process more effective, we use a different strategy to generate positive training samples for transcription factors that have a large number of positive labels (CTCF, FOXA1, HNF4A, MAX, REST and JUND). For these TFs, we randomly sample a 200-bp region from each ChIP-Seq peak in the narrow peak data as positive instances for training instead of using all positive samples in each epoch. We use the Adam [<xref ref-type="bibr" rid="CR38">38</xref>] optimizer with binary cross-entropy as the loss function. The default number of epochs is set to 60, but the training will be early stopped if there are no improvements in validation auPRC for five consecutive epochs. For detailed instructions about data retrieving, training, prediction, and visualization with our programs, please see Additional file <xref rid="MOESM2" ref-type="media">2</xref>.</p>
    </sec>
  </sec>
  <sec id="Sec11">
    <title>Results</title>
    <sec id="Sec12">
      <title>Overall benchmarking on evaluation data</title>
      <p id="Par35">We list the performance of our model as four metrics used in the DREAM Challenge (Table <xref rid="Tab1" ref-type="table">1</xref>) and compare them with the unified score from the top four teams in the final leaderboard of the ENCODE-DREAM Challenge (Table <xref rid="Tab2" ref-type="table">2</xref>). The unified score for each TF and cell-type is based on the rank of each metric and is computed as: <inline-formula id="IEq74"><alternatives><tex-math id="M175">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sum ln\left( {r/\left( 6 \right)} \right){ }$$\end{document}</tex-math><mml:math id="M176"><mml:mrow><mml:mo>∑</mml:mo><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mfenced close=")" open="("><mml:mn>6</mml:mn></mml:mfenced></mml:mrow></mml:mfenced><mml:mrow/></mml:mrow></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq74.gif"/></alternatives></inline-formula> where <inline-formula id="IEq75"><alternatives><tex-math id="M177">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r$$\end{document}</tex-math><mml:math id="M178"><mml:mi>r</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3952_Article_IEq75.gif"/></alternatives></inline-formula> is the rank of the method for one specific performance measure (auROC, auPRC, Recall at 50% FDR and Recall at 10% FDR). Thus, smaller scores indicate better performance. The TFs, chromosomes, and cell-types for evaluation are the same as those used for the final rankings. DeepGRN typically achieves auROC scores above 98% for most of the TF/cell type pairs, reaching as low.<table-wrap id="Tab1"><label>Table 1</label><caption><p>The performance of DeepGRN with four metrics used in the DREAM Challenge</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">TF name</th><th align="left">Cell-type</th><th align="left">auROC</th><th align="left">auPRC</th><th align="left">Recall at 50% FDR</th><th align="left">Recall at 10% FDR</th></tr></thead><tbody><tr><td align="left">CTCF</td><td align="left">PC-3</td><td char="." align="char">0.987</td><td char="." align="char">0.767</td><td char="." align="char">0.766</td><td char="." align="char">0.603</td></tr><tr><td align="left">CTCF</td><td align="left">induced pluripotent stem cell</td><td char="." align="char">0.998</td><td char="." align="char">0.902</td><td char="." align="char">0.945</td><td char="." align="char">0.744</td></tr><tr><td align="left">E2F1</td><td align="left">K562</td><td char="." align="char">0.989</td><td char="." align="char">0.404</td><td char="." align="char">0.388</td><td char="." align="char">0.100</td></tr><tr><td align="left">EGR1</td><td align="left">liver</td><td char="." align="char">0.993</td><td char="." align="char">0.405</td><td char="." align="char">0.318</td><td char="." align="char">0.021</td></tr><tr><td align="left">FOXA1</td><td align="left">liver</td><td char="." align="char">0.985</td><td char="." align="char">0.546</td><td char="." align="char">0.584</td><td char="." align="char">0.164</td></tr><tr><td align="left">FOXA2</td><td align="left">liver</td><td char="." align="char">0.984</td><td char="." align="char">0.548</td><td char="." align="char">0.588</td><td char="." align="char">0.143</td></tr><tr><td align="left">GABPA</td><td align="left">liver</td><td char="." align="char">0.991</td><td char="." align="char">0.516</td><td char="." align="char">0.488</td><td char="." align="char">0.154</td></tr><tr><td align="left">HNF4A</td><td align="left">liver</td><td char="." align="char">0.971</td><td char="." align="char">0.636</td><td char="." align="char">0.700</td><td char="." align="char">0.263</td></tr><tr><td align="left">JUND</td><td align="left">liver</td><td char="." align="char">0.983</td><td char="." align="char">0.535</td><td char="." align="char">0.585</td><td char="." align="char">0.027</td></tr><tr><td align="left">MAX</td><td align="left">liver</td><td char="." align="char">0.990</td><td char="." align="char">0.425</td><td char="." align="char">0.349</td><td char="." align="char">0.004</td></tr><tr><td align="left">NANOG</td><td align="left">induced pluripotent stem cell</td><td char="." align="char">0.996</td><td char="." align="char">0.499</td><td char="." align="char">0.515</td><td char="." align="char">0.035</td></tr><tr><td align="left">REST</td><td align="left">liver</td><td char="." align="char">0.986</td><td char="." align="char">0.482</td><td char="." align="char">0.527</td><td char="." align="char">0.030</td></tr><tr><td align="left">TAF1</td><td align="left">liver</td><td char="." align="char">0.989</td><td char="." align="char">0.424</td><td char="." align="char">0.393</td><td char="." align="char">0.000</td></tr></tbody></table></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption><p>The unified scores of DeepGRN and the top four algorithms in the DREAM Challenge</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">TF</th><th align="left">Cell</th><th align="left">Anchor</th><th align="left">FactorNet</th><th align="left">Cheburashka</th><th align="left">Catchitt</th><th align="left">DeepGRN</th></tr></thead><tbody><tr><td align="left">CTCF</td><td align="left">PC-3</td><td char="." align="char">0.67</td><td char="." align="char">0.17</td><td char="." align="char">0.83</td><td char="." align="char">0.5</td><td char="." align="char">0.33</td></tr><tr><td align="left">CTCF</td><td align="left">induced pluripotent stem cell</td><td char="." align="char">0.83</td><td char="." align="char">0.33</td><td char="." align="char">0.67</td><td char="." align="char">0.5</td><td char="." align="char"><bold>0.17</bold></td></tr><tr><td align="left">E2F1</td><td align="left">K562</td><td char="." align="char">0.5</td><td char="." align="char">0.83</td><td char="." align="char">0.67</td><td char="." align="char">0.17</td><td char="." align="char">0.33</td></tr><tr><td align="left">EGR1</td><td align="left">liver</td><td char="." align="char">0.17</td><td char="." align="char">0.83</td><td char="." align="char">0.67</td><td char="." align="char">0.33</td><td char="." align="char">0.5</td></tr><tr><td align="left">FOXA1</td><td align="left">liver</td><td char="." align="char">0.67</td><td char="." align="char">0.33</td><td char="." align="char">0.83</td><td char="." align="char">0.5</td><td char="." align="char"><bold>0.17</bold></td></tr><tr><td align="left">FOXA2</td><td align="left">liver</td><td char="." align="char">0.33</td><td char="." align="char">0.83</td><td char="." align="char">0.67</td><td char="." align="char">0.5</td><td char="." align="char"><bold>0.17</bold></td></tr><tr><td align="left">GABPA</td><td align="left">liver</td><td char="." align="char">0.33</td><td char="." align="char">0.83</td><td char="." align="char">0.67</td><td char="." align="char">0.5</td><td char="." align="char"><bold>0.17</bold></td></tr><tr><td align="left">HNF4A</td><td align="left">liver</td><td char="." align="char">0.67</td><td char="." align="char">0.33</td><td char="." align="char">0.83</td><td char="." align="char">0.5</td><td char="." align="char"><bold>0.17</bold></td></tr><tr><td align="left">JUND</td><td align="left">liver</td><td char="." align="char">0.17</td><td char="." align="char">0.83</td><td char="." align="char">0.67</td><td char="." align="char">0.5</td><td char="." align="char">0.33</td></tr><tr><td align="left">MAX</td><td align="left">liver</td><td char="." align="char">0.17</td><td char="." align="char">0.83</td><td char="." align="char">0.33</td><td char="." align="char">0.67</td><td char="." align="char">0.5</td></tr><tr><td align="left">NANOG</td><td align="left">induced pluripotent stem cell</td><td char="." align="char">0.33</td><td char="." align="char">0.5</td><td char="." align="char">0.83</td><td char="." align="char">0.67</td><td char="." align="char"><bold>0.17</bold></td></tr><tr><td align="left">REST</td><td align="left">liver</td><td char="." align="char">0.67</td><td char="." align="char">0.33</td><td char="." align="char">0.83</td><td char="." align="char">0.5</td><td char="." align="char"><bold>0.17</bold></td></tr><tr><td align="left">TAF1</td><td align="left">liver</td><td char="." align="char">0.17</td><td char="." align="char">0.5</td><td char="." align="char">0.67</td><td char="." align="char">0.33</td><td char="." align="char">0.83</td></tr></tbody></table><table-wrap-foot><p>Bold scores denote the TF and cell-types that DeepGRN rank as the highest</p></table-wrap-foot></table-wrap></p>
      <p id="Par36">as 97.1% for HNF4A/liver. The scores of auPRC have a more extensive range of values, from 40.4% for E2F1/ K562 to 90.2% for CTCF/iPSC.</p>
      <p id="Par37">For each TF and cell-type combination, our attention model has better performance on 69% (9/13) of the prediction targets than Anchor [<xref ref-type="bibr" rid="CR39">39</xref>], 85% (11/13) than FactorNet [<xref ref-type="bibr" rid="CR17">17</xref>], 85% (11/13) than Cheburashka [<xref ref-type="bibr" rid="CR7">7</xref>], and 77% (10/13) than Catchitt [<xref ref-type="bibr" rid="CR40">40</xref>]. Among all methods benchmarked, our method has the highest ranking in 7 out of 13 targets (CTCF/iPSC, FOXA1/liver, FOXA2/liver, GABPA/liver, HNF4A/liver, NANOG/iPSC, and REST/liver), with the best average score (0.31) across all TF/ cell-types pairs (Table <xref rid="Tab2" ref-type="table">2</xref>).</p>
      <p id="Par38">To precisely evaluate the capability of deepGRN under the restrictions of the ENCODE DREAM Challenge, we also compared the performance of deepGRN trained using datasets provided by the challenge with four available features: Genomic sequence features, DNase-Seq and RNA-Seq data. The results are summarized in Table S3 and S4. DeepGRN still achieves the highest ranking in 6 out of 13 targets, with the best average unified score (0.33) across all targets. We also compared our results with models without the attention component using the four challenge features. We built these models using the same architecture as deepGRN models, except for the attention component and trained them with the same hyperparameter selection process. The results are shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. DeepGRN with attention mechanism outperforms the models without attention in 11 out of 13 targets by the auPRC metric, with the largest difference from target REST (0.168).<fig id="Fig2"><label>Fig. 2</label><caption><p>Comparision of the deep learning models with and without attention mechanism</p></caption><graphic xlink:href="12859_2020_3952_Fig2_HTML" id="MO2"/></fig></p>
    </sec>
    <sec id="Sec13">
      <title>Performance comparison between two attention modules</title>
      <p id="Par39">In addition to the comparisons with the top 4 methods in the challenge, we also benchmarked the individual performance of the single and pairwise attention module (Table S5, see Additional file <xref rid="MOESM1" ref-type="media">1</xref>). In general, the results extracted from the single attention module have similar performances. For all 13 TF and cell-type pairs, the single attention module has higher auROC in 6 targets while the pairwise attention module has higher auROC in 3 targets. The rest of the targets are tied. The final output of the model is the ensemble of these two modules by averaging, and it outperforms any of the individual attention modules in 10 of 13 targets (Table <xref rid="Tab1" ref-type="table">1</xref>). The largest improvements from ensemble (as auPRC) come from FOXA2 (0.34), REST (0.09) and FOXA1 (0.09). We also found that the performance of the two attention modules have the same trend across all TF and cell-types in all four performance measures (Fig. <xref rid="Fig3" ref-type="fig">3</xref>), suggesting that the capability of learning from features are coherent between the two modules.<fig id="Fig3"><label>Fig. 3</label><caption><p>Performance comparison between single and pairwise attention mechanism. The performance of each TF and cell-type pairs of the output of the individual module are shown in four measures: (auROC, auPRC, recall at 50% FDR and Recall at 10% FDR). ρ: Pearson Correlation Coefficient, σ: Spearman Correlation Coefficient</p></caption><graphic xlink:href="12859_2020_3952_Fig3_HTML" id="MO3"/></fig></p>
      <p id="Par40">We evaluated the importance of each feature between single and pairwise attention mechanism. For the prediction of each target, we set the values of each sequential feature (DNase-Seq, sequence, or uniqueness) to zero, or randomly switch the order of the vector for a non-sequential feature (genomic elements or RNA-Seq). The decrease of auPRC from these new predictions is used as the importance score of each feature (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). We found that across all TF and cell-types, the sequential features have the largest average importance scores: DNase-Seq (0.36), DNA sequence (0.21), and 35 bp uniqueness (0.21) while the scores for other features are much smaller. Similar trends have also been found using individual single and pair attention modules.<fig id="Fig4"><label>Fig. 4</label><caption><p>Importance score of features between single and pairwise attention mechanism. The values represented as the decrease of auPRC without using the specific feature for prediction. The negative value represents an increase of auPRC</p></caption><graphic xlink:href="12859_2020_3952_Fig4_HTML" id="MO4"/></fig></p>
    </sec>
    <sec id="Sec14">
      <title>Interpretation of attention scores with DNase-Seq and ChIP-Seq</title>
      <p id="Par41">In the single attention module, the output is a weighted sum of the input from the attention layer, and the attention scores are used as weights. These scores characterize a unified mapping between the importance of input feature with its relative position in the sequential input. To analyze the relationship between attention weights and the position of TF binding events, we extract the attention scores from the single attention module for both forward strand and reverse complement strand and compare them with the corresponding normalized ChIP-Seq fold changes in the same region that are predicted as positive (score &gt; 0.5). Similarly, we computed the saliency scores for the same input regions (The implementation details are described in Additional file <xref rid="MOESM1" ref-type="media">1</xref>). We found that the attention scores on the two DNA strands have a higher correlation (ρ = 0.90, σ = 0.79) than the saliency scores (ρ = 0.78, σ = 0.51) (Fig. <xref rid="Fig5" ref-type="fig">5</xref>a, b). Across all TF and cell-type pairs, we found that there is a positive correlation between the attention weights and normalized ChIP-Seq Fold (Fig. <xref rid="Fig5" ref-type="fig">5</xref>c), and such relationship is not detected globally in saliency scores (Fig. <xref rid="Fig5" ref-type="fig">5</xref>d). For all TF and cell-types in the benchmark datasets, we select at least four different genomic regions that have a clear ChIP-Seq peak signal in each target for demonstration. We show that the averaged attention weights put more focus on the actual binding region for each cell-type and these focusing points shift along with the shift of TF binding signals (see Additional file <xref rid="MOESM3" ref-type="media">3</xref>).<fig id="Fig5"><label>Fig. 5</label><caption><p>Analysis of attention weights and saliency scores. (<bold>a</bold>) Scatterplot of attention weights from positive strand and reverse strand. (<bold>b</bold>) Scatterplot of saliency scores from positive strand and reverse strand. (<bold>c</bold>) Scatterplot of ChIP-Seq fold change and mean attention weights from both strands. Z-score transformation is applied to both axes. (<bold>d</bold>) Distribution of the correlation between attention weights/saliency scores and ChIP-Seq fold change. The dashed line represents the mean of each group. The p-value is calculated using the Wilcoxon signed-rank test. The attention weights and saliency scores on the reverse complement strand are reversed before plotting. ρ: Spearman Correlation Coefficient, σ: Pearson Correlation Coefficient. The correlation between normalized ChIP-Seq Fold change and normalized saliency scores is 0.40 (Spearman) and 0.49 (Pearson)</p></caption><graphic xlink:href="12859_2020_3952_Fig5_HTML" id="MO5"/></fig></p>
      <p id="Par42">Since the accessibility of the genome plays an important role in TF binding, it is expected to find high DNase coverage for those openly accessible areas that can explain the binding event detected by the ChIP-Seq experiment. We run a genome-wide analysis on regions with high DNase-Seq peaks in the single attention module for transcription factor JUND, which is one of the most susceptible targets to DNase-Seq. We illustrate the distribution of normalized DNase coverage values from both the true positives that are false negatives without attention and true negatives that are false positives without attention (Fig. <xref rid="Fig6" ref-type="fig">6</xref>). The results show that the true positives that are only recognized by attention models generally have a smaller DNase coverage than those recognized by both models. This observation indicates that the predictive improvements of attention models may result from focusing on more informative DNase-Seq coverage values while ignoring irrelevant regions in negative samples.<fig id="Fig6"><label>Fig. 6</label><caption><p>Distribution of average normalized DNase coverage values of different regions with the inputs of JUND. The predictions from both models with and without attention from our training are evaluated by the true positive labels. Then the average normalized DNase coverage is calculated based on bins classified differently by the two models</p></caption><graphic xlink:href="12859_2020_3952_Fig6_HTML" id="MO6"/></fig></p>
    </sec>
    <sec id="Sec15">
      <title>Motif detection over high attention scores regions</title>
      <p id="Par43">For those positive samples without distinct DNase-Seq peaks, the patterns of genomic sequences are critical information for successful prediction. To test the ability of attention weights to recognize motifs that contribute to binding events from the genomic sequences, we use an approach similar to DeepBind [<xref ref-type="bibr" rid="CR13">13</xref>]. For the model trained for each TF, we first acquire the coordinates on the relative positions of maximum column sum of the attention weights from all positive bins in test datasets and extract a subsequence with a length of 20 bp around each coordinate. To exclude samples that can be easily classified from patterns of DNase-Seq signal, we only select positive bins that have no significant coverage peaks (ratio between the highest score and average scores &lt; 15). Then we run FIMO [<xref ref-type="bibr" rid="CR41">41</xref>] to detect known motifs relevant to the TF of the model in the JASPAR database [<xref ref-type="bibr" rid="CR42">42</xref>]. From the extracted subsequences, we discover motif MA0139.1 (CTCF) in the prediction for CTCF/induced pluripotent cell and MA0148.4 (FOXA1) in the prediction for FOXA1/liver cell. Figure <xref rid="Fig7" ref-type="fig">7</xref>a and b show the comparison between the sequence logo of the motif rebuilt from the subsequences and the actual known motifs. We also plot the attention scores of the samples that contain these subsequences (Fig. <xref rid="Fig7" ref-type="fig">7</xref>c, f) and the relative location of the regions with detected motifs in FIMO (Fig. <xref rid="Fig7" ref-type="fig">7</xref>d, g). Furthermore, we show that these maximum attention weights do not come from the DNase-Seq peaks near the motif regions by coincidence since no similar pattern is detected from the normalized DNase scores in the same regions (Fig. <xref rid="Fig7" ref-type="fig">7</xref>e, h). We illustrate the similar trends found in the single attention module in Figure S3 [see Additional file <xref rid="MOESM1" ref-type="media">1</xref>].<fig id="Fig7"><label>Fig. 7</label><caption><p>Comparisons of known motifs and matching motifs learned by pairwise attention module in CTCF and FOXA1. (<bold>a</bold>) Sequence logo built from subsequences detected in CTCF/induced pluripotent cell prediction (left) and motif MA0139.1/ CTCF (right). (<bold>b</bold>) The attention scores of the samples selected from CTCF/induced pluripotent cell prediction with hits of MA0139.1/ CTCF in FIMO. (<bold>c</bold>)The relative positions of the detected motifs in the same region of (<bold>b</bold>). (<bold>d</bold>) The normalized DNase-Seq scores in the same region of (<bold>b</bold>). (<bold>e</bold>) Sequence logo built from subsequences detected in FOXA1/liver cell prediction (left) and motif MA0148.4/ FOXA1 (right). (<bold>f</bold>) The attention scores of the samples selected from FOXA1/liver cell prediction with hits of MA0148.4/ FOXA1 in FIMO. (<bold>g</bold>) The relative positions of the detected motifs in the same region of (<bold>f</bold>). (<bold>h</bold>) The normalized DNase-Seq scores in the same region of (f)</p></caption><graphic xlink:href="12859_2020_3952_Fig7_HTML" id="MO7"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec16">
    <title>Discussion</title>
    <p id="Par44">The attention mechanism is attractive in various machine learning studies and has achieved superior performance in image caption generation and natural language processing tasks [<xref ref-type="bibr" rid="CR37">37</xref>, <xref ref-type="bibr" rid="CR43">43</xref>]. Recurrent neural network models with attention mechanism are particularly good at tasks with long-range dependency in input data. Inspired by these works, we introduce the attention mechanism to DNN models for TF binding site prediction.</p>
    <p id="Par45">The benchmark result using ENCODE-DREAM Challenge datasets shows that the performances of our model are competitive with the current state-of-the-art methods. It is worth mentioning that the DNase-Seq scores are the most critical feature in the attention mechanism from our experiments according to the feature importance analysis. Many prediction tools for binding site prediction before the challenge, such as DeepBind or TFImpute, are not able to utilize the DNase-Seq data and are not as suitable as the four methods that we used for benchmarking in this study. However, the methods we benchmarked in this study share the similar concepts with these existing tools (For example, FactorNet is built with similar architecture as the TFImpute with additional support for the DNase-Seq data) and may reflect the potential of them using the same set of features.</p>
    <p id="Par46">The attention weights learned by the models provide an alternative approach to exploring the dependencies between input and output other than saliency maps. By comparing true ChIP-Seq fold change peaks with attention weights, we show how attention weights shift when the fold change peaks move along the DNA sequence. We also demonstrate that our attention model has the ability to learn from known motifs related to specific TFs.</p>
    <p id="Par47">Due to the rules of the DREAM Challenge, we only use very limited types of features in this work. However, if more types of features (such as sequence conservation or epigenetic modifications) are available, they can possibly be transformed into sequential formats and may further improve the prediction performance through our attention architecture. The attention mechanism itself is also evolving rapidly. For example, the multi-head attention introduced by Transformer [<xref ref-type="bibr" rid="CR37">37</xref>] showed that high-level features could be learned by attention without relying on any recurrent or convolution layers. We expect that better prediction for the TF binding may also be benefited from these novel deep learning architectures in both accuracy and efficacy.</p>
  </sec>
  <sec id="Sec17">
    <title>Conclusions</title>
    <p id="Par48">In this study, we propose a new tool (DeepGRN) that incorporates the attention mechanism with the CNNs-RNNs based architecture. The result shows that the performances of our models are competitive with the top 4 methods in the Challenge leaderboard. We demonstrate that the attention modules in our model help to interpret how critical patterns from different types of input features are recognized.</p>
  </sec>
  <sec id="Sec18">
    <title>Availability and requirements</title>
    <p id="Par49">
      <list list-type="bullet">
        <list-item>
          <p id="Par50">Project name: DeepGRN</p>
        </list-item>
        <list-item>
          <p id="Par51">Project home page: https://github.com/jianlin-cheng/DeepGRN.</p>
        </list-item>
        <list-item>
          <p id="Par52">Operating system(s): Linux, Mac OS, Windows.</p>
        </list-item>
        <list-item>
          <p id="Par53">Programming language: Python, R.</p>
        </list-item>
        <list-item>
          <p id="Par54">Other requirements: Python version 3.6.0 or higher, R version 3.3.0 or higher.</p>
        </list-item>
        <list-item>
          <p id="Par55">License: GNU GPL.</p>
        </list-item>
        <list-item>
          <p id="Par56">Any restrictions to use by non-academics: None.</p>
        </list-item>
      </list>
    </p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec19">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2020_3952_MOESM1_ESM.pdf">
            <caption>
              <p><bold>Additional file 1.</bold> Supplementary figures and tables. Including all supplementary figures and tables referenced in the main text.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="12859_2020_3952_MOESM2_ESM.pdf">
            <caption>
              <p><bold>Additional file 2.</bold> Instructions of training, prediction, and visualization data with DeepGRN. Including data retrieving, training, prediction with DeepGRN and the implementation details of the visualization scripts used in the main text.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="12859_2020_3952_MOESM3_ESM.pdf">
            <caption>
              <p><bold>Additional file 3.</bold> Visualization of the relationship between ChIP-Seq peak and attention weights. For each genomic region, the plot on the left represents the attention weights, and the plot on the right represents the enrichment of ChIP-Seq signal fold changes in the same region. Since the lengths of attention weights are reduced by the convolution and pooling layers, their lengths are less than the fold change values. Thus, the plots are aligned on the X-axis to represent the relative position of fold change and averaged attention weights.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>TF</term>
        <def>
          <p id="Par4">Transcription factor</p>
        </def>
      </def-item>
      <def-item>
        <term>Bi-LSTM</term>
        <def>
          <p id="Par5">Bidirectional long short-term memory</p>
        </def>
      </def-item>
      <def-item>
        <term>DNase-Seq</term>
        <def>
          <p id="Par6">DNase I hypersensitive sites sequencing</p>
        </def>
      </def-item>
      <def-item>
        <term>ChIP-Seq</term>
        <def>
          <p id="Par7">Chromatin immunoprecipitation sequencing</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary Information</title>
    <p>The online version contains supplementary material available at 10.1186/s12859-020-03952-1.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>We wish to thank the organizers of ENCODE-DREAM in vivo Transcription Factor Binding Site Prediction Challenge.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>JC conceived of the project. CC and JH designed the experiment. CC implemented the method and gathered the results. CC, JH, XS, HY, and JB wrote the manuscript. All authors edited and approved the manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work has been supported by NSF grants (IOS1545780 and DBI1149224) and the U.S. Department of Energy (DOE) grant “Deep Green: Structural and Functional Genomic Characterization of Conserved Unannotated Green Lineage Proteins” (DE-SC0020400). The funders (NSF and DOE) does not play a role in conducting this research.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The datasets used in this study and the source code of DeepGRN are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jianlin-cheng/DeepGRN">https://github.com/jianlin-cheng/DeepGRN</ext-link>.</p>
  </notes>
  <notes id="FPar1">
    <title>Ethics approval and consent to participate</title>
    <p id="Par57">Not applicable.</p>
  </notes>
  <notes id="FPar2">
    <title>Consent for publication</title>
    <p id="Par58">Not applicable.</p>
  </notes>
  <notes id="FPar3" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par59">The authors declare they have no conflict of interest.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hobert</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>Gene regulation by transcription factors and microRNAs</article-title>
        <source>Science</source>
        <year>2008</year>
        <volume>319</volume>
        <issue>5871</issue>
        <fpage>1785</fpage>
        <lpage>1786</lpage>
        <pub-id pub-id-type="doi">10.1126/science.1151651</pub-id>
        <pub-id pub-id-type="pmid">18369135</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mehta</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Schwab</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Sengupta</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Statistical mechanics of transcription-factor binding site discovery using hidden markov models</article-title>
        <source>J Stat Phys</source>
        <year>2011</year>
        <volume>142</volume>
        <issue>6</issue>
        <fpage>1187</fpage>
        <lpage>1205</lpage>
        <pub-id pub-id-type="doi">10.1007/s10955-010-0102-x</pub-id>
        <pub-id pub-id-type="pmid">22851788</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mathelier</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Wasserman</surname>
            <given-names>WW</given-names>
          </name>
        </person-group>
        <article-title>The next generation of transcription factor binding site prediction</article-title>
        <source>PLoS Comput Biol</source>
        <year>2013</year>
        <volume>9</volume>
        <issue>9</issue>
        <fpage>e1003214</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1003214</pub-id>
        <pub-id pub-id-type="pmid">24039567</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pique-Regi</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Degner</surname>
            <given-names>JF</given-names>
          </name>
          <name>
            <surname>Pai</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Gaffney</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Gilad</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Pritchard</surname>
            <given-names>JK</given-names>
          </name>
        </person-group>
        <article-title>Accurate inference of transcription factor binding from DNA sequence and chromatin accessibility data</article-title>
        <source>Genome Res</source>
        <year>2011</year>
        <volume>21</volume>
        <issue>3</issue>
        <fpage>447</fpage>
        <lpage>455</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.112623.110</pub-id>
        <pub-id pub-id-type="pmid">21106904</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>TY</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Abe</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Horton</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Mann</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Bussemaker</surname>
            <given-names>HJ</given-names>
          </name>
          <name>
            <surname>Gordan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Rohs</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Quantitative modeling of transcription factor binding specificities using DNA shape</article-title>
        <source>P Natl Acad Sci USA</source>
        <year>2015</year>
        <volume>112</volume>
        <issue>15</issue>
        <fpage>4654</fpage>
        <lpage>4659</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1422023112</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Djordjevic</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sengupta</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Shraiman</surname>
            <given-names>BI</given-names>
          </name>
        </person-group>
        <article-title>A biophysical approach to transcription factor binding site discovery</article-title>
        <source>Genome Res</source>
        <year>2003</year>
        <volume>13</volume>
        <issue>11</issue>
        <fpage>2381</fpage>
        <lpage>2390</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.1271603</pub-id>
        <pub-id pub-id-type="pmid">14597652</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Keilwagen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Posch</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Grau</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Accurate prediction of cell type-specific transcription factor binding</article-title>
        <source>Genome Biol</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>1</issue>
        <fpage>9</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-018-1614-y</pub-id>
        <pub-id pub-id-type="pmid">30630522</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xiao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Segal</surname>
            <given-names>MR</given-names>
          </name>
        </person-group>
        <article-title>Identification of yeast transcriptional regulation networks using multivariate random forests</article-title>
        <source>PLoS Comput Biol</source>
        <year>2009</year>
        <volume>5</volume>
        <issue>6</issue>
        <fpage>e1000414</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1000414</pub-id>
        <pub-id pub-id-type="pmid">19543377</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hooghe</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Broos</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Van Roy</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>De Bleser</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>A flexible integrative approach based on random forest improves prediction of transcription factor binding sites</article-title>
        <source>Nucleic Acids Res</source>
        <year>2012</year>
        <volume>40</volume>
        <issue>14</issue>
        <fpage>e106</fpage>
        <lpage>e106</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gks283</pub-id>
        <pub-id pub-id-type="pmid">22492513</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sherwood</surname>
            <given-names>RI</given-names>
          </name>
          <name>
            <surname>Hashimoto</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>O'Donnell</surname>
            <given-names>CW</given-names>
          </name>
          <name>
            <surname>Lewis</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Barkal</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>van Hoff</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Karun</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Jaakkola</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Gifford</surname>
            <given-names>DK</given-names>
          </name>
        </person-group>
        <article-title>Discovery of directional and nondirectional pioneer transcription factors by modeling DNase profile magnitude and shape</article-title>
        <source>Nat Biotechnol</source>
        <year>2014</year>
        <volume>32</volume>
        <issue>2</issue>
        <fpage>171</fpage>
        <lpage>178</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt.2798</pub-id>
        <pub-id pub-id-type="pmid">24441470</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zeng</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Edwards</surname>
            <given-names>MD</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Gifford</surname>
            <given-names>DKJB</given-names>
          </name>
        </person-group>
        <article-title>Convolutional neural network architectures for predicting DNA–protein binding</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>32</volume>
        <issue>12</issue>
        <fpage>i121</fpage>
        <lpage>i127</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btw255</pub-id>
        <pub-id pub-id-type="pmid">27307608</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Troyanskaya</surname>
            <given-names>OG</given-names>
          </name>
        </person-group>
        <article-title>Predicting effects of noncoding variants with deep learning-based sequence model</article-title>
        <source>Nat Methods</source>
        <year>2015</year>
        <volume>12</volume>
        <issue>10</issue>
        <fpage>931</fpage>
        <lpage>934</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.3547</pub-id>
        <pub-id pub-id-type="pmid">26301843</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alipanahi</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Delong</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Weirauch</surname>
            <given-names>MT</given-names>
          </name>
          <name>
            <surname>Frey</surname>
            <given-names>BJ</given-names>
          </name>
        </person-group>
        <article-title>Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning</article-title>
        <source>Nat Biotechnol</source>
        <year>2015</year>
        <volume>33</volume>
        <issue>8</issue>
        <fpage>831</fpage>
        <lpage>838</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt.3300</pub-id>
        <pub-id pub-id-type="pmid">26213851</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kalkatawi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Magana-Mora</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Jankovic</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Bajic</surname>
            <given-names>VB</given-names>
          </name>
        </person-group>
        <article-title>DeepGSR: an optimized deep-learning structure for the recognition of genomic signals and regions</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>35</volume>
        <issue>7</issue>
        <fpage>1125</fpage>
        <lpage>1132</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty752</pub-id>
        <pub-id pub-id-type="pmid">30184052</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Quang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>DanQ: a hybrid convolutional and recurrent deep neural network for quantifying the function of DNA sequences</article-title>
        <source>Nucleic Acids Res</source>
        <year>2016</year>
        <volume>44</volume>
        <issue>11</issue>
        <fpage>e107</fpage>
        <lpage>e107</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkw226</pub-id>
        <pub-id pub-id-type="pmid">27084946</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Hassanzadeh HR, Wang M. DeeperBind: Enhancing prediction of sequence specificities of DNA binding proteins. In: IEEE international conference on bioinformatics and biomedicine (BIBM): 2016. 178–183.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Quang D, Xie X. FactorNet: A deep learning framework for predicting cell type specific transcription factor binding from nucleotide-resolution sequential data. Methods 2019.</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Luong M-T, Pham H, Manning CD. Effective approaches to attention-based neural machine translation. In: Proceedings of the 2015 conference on empirical methods in natural language processing<italic>: 2015</italic>.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Wang Y, Huang M, Zhao L: Attention-based lstm for aspect-level sentiment classification. In: Proceedings of the 2016 conference on empirical methods in natural language processing: 2016. 606–615.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Zhou P, Shi W, Tian J, Qi Z, Li B, Hao H, Xu B. Attention-based bidirectional long short-term memory networks for relation classification. In<italic>: </italic>Aug 2016; Berlin, Germany. Association for Computational Linguistics: 207–212.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Tran N-T, Luong V-T, Nguyen NL-T, Nghiem M-Q: Effective attention-based neural architectures for sentence compression with bidirectional long short-term memory. In: Proceedings of the Seventh Symposium on Information and Communication Technology; Ho Chi Minh City, Vietnam. 3011111: ACM 2016: 123–130.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Singh</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lanchantin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Sekhon</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Qi</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Attend and predict: understanding gene regulation by selective attention on chromatin</article-title>
        <source>Adv Neural Inf Process Syst</source>
        <year>2017</year>
        <volume>30</volume>
        <fpage>6785</fpage>
        <lpage>6795</lpage>
        <?supplied-pmid 30147283?>
        <pub-id pub-id-type="pmid">30147283</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shen</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Bao</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>D-S</given-names>
          </name>
        </person-group>
        <article-title>Recurrent neural network for predicting transcription factor binding sites</article-title>
        <source>Sci Rep</source>
        <year>2018</year>
        <volume>8</volume>
        <issue>1</issue>
        <fpage>15270</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-018-33321-1</pub-id>
        <pub-id pub-id-type="pmid">30323198</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Park</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Koh</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Jeon</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Yeo</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Kang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Enhancing the interpretability of transcription factor binding site prediction using attention mechanism</article-title>
        <source>Sci Rep</source>
        <year>2020</year>
        <volume>10</volume>
        <issue>1</issue>
        <fpage>13413</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-020-70218-4</pub-id>
        <pub-id pub-id-type="pmid">32770026</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Eraslan</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Avsec</surname>
            <given-names>Ž</given-names>
          </name>
          <name>
            <surname>Gagneur</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Theis</surname>
            <given-names>FJ</given-names>
          </name>
        </person-group>
        <article-title>Deep learning: new computational modelling techniques for genomics</article-title>
        <source>Nat Rev Genet</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>7</issue>
        <fpage>389</fpage>
        <lpage>403</lpage>
        <pub-id pub-id-type="doi">10.1038/s41576-019-0122-6</pub-id>
        <pub-id pub-id-type="pmid">30971806</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>QH</given-names>
          </name>
          <name>
            <surname>Brown</surname>
            <given-names>JB</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>HY</given-names>
          </name>
          <name>
            <surname>Bickel</surname>
            <given-names>PJ</given-names>
          </name>
        </person-group>
        <article-title>Measuring reproducibility of high-throughput experiments</article-title>
        <source>Ann Appl Stat</source>
        <year>2011</year>
        <volume>5</volume>
        <issue>3</issue>
        <fpage>1752</fpage>
        <lpage>1779</lpage>
        <pub-id pub-id-type="doi">10.1214/11-AOAS466</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sholtis</surname>
            <given-names>SJ</given-names>
          </name>
          <name>
            <surname>Noonan</surname>
            <given-names>JP</given-names>
          </name>
        </person-group>
        <article-title>Gene regulation and the origins of human biological uniqueness</article-title>
        <source>Trends Genet</source>
        <year>2010</year>
        <volume>26</volume>
        <issue>3</issue>
        <fpage>110</fpage>
        <lpage>118</lpage>
        <pub-id pub-id-type="doi">10.1016/j.tig.2009.12.009</pub-id>
        <pub-id pub-id-type="pmid">20106546</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Derrien</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Estelle</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Marco Sola</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Knowles</surname>
            <given-names>DG</given-names>
          </name>
          <name>
            <surname>Raineri</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Guigo</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Ribeca</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Fast computation and applications of genome mappability</article-title>
        <source>PLoS ONE</source>
        <year>2012</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>e30377</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0030377</pub-id>
        <pub-id pub-id-type="pmid">22276185</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <collab>Consortium EP</collab>
        </person-group>
        <article-title>An integrated encyclopedia of DNA elements in the human genome</article-title>
        <source>Nature</source>
        <year>2012</year>
        <volume>489</volume>
        <issue>7414</issue>
        <fpage>57</fpage>
        <lpage>74</lpage>
        <pub-id pub-id-type="doi">10.1038/nature11247</pub-id>
        <pub-id pub-id-type="pmid">22955616</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Madrigal</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Krajewski</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Current bioinformatic approaches to identify DNase I hypersensitive sites and genomic footprints from DNase-seq data</article-title>
        <source>Front Genet</source>
        <year>2012</year>
        <volume>3</volume>
        <fpage>230</fpage>
        <pub-id pub-id-type="doi">10.3389/fgene.2012.00230</pub-id>
        <pub-id pub-id-type="pmid">23118738</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ramirez</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Dundar</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Diehl</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gruning</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>Manke</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>deepTools: a flexible platform for exploring deep-sequencing data</article-title>
        <source>Nucleic Acids Res</source>
        <year>2014</year>
        <volume>42</volume>
        <fpage>W187</fpage>
        <lpage>191</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gku365</pub-id>
        <pub-id pub-id-type="pmid">24799436</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pollard</surname>
            <given-names>KS</given-names>
          </name>
          <name>
            <surname>Hubisz</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Rosenbloom</surname>
            <given-names>KR</given-names>
          </name>
          <name>
            <surname>Siepel</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Detection of nonneutral substitution rates on mammalian phylogenies</article-title>
        <source>Genome Res</source>
        <year>2010</year>
        <volume>20</volume>
        <issue>1</issue>
        <fpage>110</fpage>
        <lpage>121</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.097857.109</pub-id>
        <pub-id pub-id-type="pmid">19858363</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Carriero</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Silva</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Bonneau</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Mocap: large-scale inference of transcription factor binding sites from chromatin accessibility</article-title>
        <source>Nucleic Acids Res</source>
        <year>2017</year>
        <volume>45</volume>
        <issue>8</issue>
        <fpage>4315</fpage>
        <lpage>4329</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkx174</pub-id>
        <pub-id pub-id-type="pmid">28334916</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Mueller J, Thyagarajan A. Siamese recurrent architectures for learning sentence similarity. In: Thirtieth AAAI conference on artificial intelligence<italic>: 2016</italic>.</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qin</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Imputation for transcription factor binding predictions based on deep learning</article-title>
        <source>PLoS Comput Biol</source>
        <year>2017</year>
        <volume>13</volume>
        <issue>2</issue>
        <fpage>e1005403</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1005403</pub-id>
        <pub-id pub-id-type="pmid">28234893</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. In: Proceedings of the 31st international conference on neural information processing systems; Long Beach, California, USA. Curran Associates Inc. 2017: 6000–6010.</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. In: Advances in neural information processing systems<italic>: 2017</italic>. 5998–6008.</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J: Adam: A method for stochastic optimization. CoRR, abs/1412.6980.</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Quang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Guan</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Anchor: trans-cell type prediction of transcription factor binding sites</article-title>
        <source>Genome Res</source>
        <year>2019</year>
        <volume>29</volume>
        <issue>2</issue>
        <fpage>281</fpage>
        <lpage>292</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.237156.118</pub-id>
        <pub-id pub-id-type="pmid">30567711</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Preselection of training cell types improves prediction of transcription factor binding sites</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bailey</surname>
            <given-names>TL</given-names>
          </name>
          <name>
            <surname>Elkan</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Fitting a mixture model by expectation maximization to discover motifs in biopolymers</article-title>
        <source>Proc Int Conf Intell Syst Mol Biol</source>
        <year>1994</year>
        <volume>2</volume>
        <fpage>28</fpage>
        <lpage>36</lpage>
        <?supplied-pmid 7584402?>
        <pub-id pub-id-type="pmid">7584402</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Khan</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Fornes</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Stigliani</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gheorghe</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Castro-Mondragon</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>van der Lee</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Bessy</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cheneby</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kulkarni</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>Tan</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>JASPAR 2018: update of the open-access database of transcription factor binding profiles and its web framework</article-title>
        <source>Nucleic Acids Res</source>
        <year>2018</year>
        <volume>46</volume>
        <issue>D1</issue>
        <fpage>D260</fpage>
        <lpage>D266</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkx1126</pub-id>
        <pub-id pub-id-type="pmid">29140473</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Yang Z, Yang D, Dyer C, He X, Smola A, Hovy E. Hierarchical attention networks for document classification. In: Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies:<italic> 2016</italic>. 1480–1489.</mixed-citation>
    </ref>
  </ref-list>
</back>
