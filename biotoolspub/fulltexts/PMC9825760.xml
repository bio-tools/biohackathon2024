<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9825760</article-id>
    <article-id pub-id-type="pmid">36458923</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btac779</article-id>
    <article-id pub-id-type="publisher-id">btac779</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Sequence Analysis</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepCellEss: cell line-specific essential protein prediction with attention-based interpretable deep learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Yiming</given-names>
        </name>
        <aff><institution>Hunan Provincial Key Lab on Bioinformatics, School of Computer Science and Engineering, Central South University</institution>, Changsha 410083, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1726-0955</contrib-id>
        <name>
          <surname>Zeng</surname>
          <given-names>Min</given-names>
        </name>
        <aff><institution>Hunan Provincial Key Lab on Bioinformatics, School of Computer Science and Engineering, Central South University</institution>, Changsha 410083, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Fuhao</given-names>
        </name>
        <aff><institution>Hunan Provincial Key Lab on Bioinformatics, School of Computer Science and Engineering, Central South University</institution>, Changsha 410083, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-4593-9332</contrib-id>
        <name>
          <surname>Wu</surname>
          <given-names>Fang-Xiang</given-names>
        </name>
        <aff><institution>Division of Biomedical Engineering, Department of Computer Science, Department of Mechanical Engineering University of Saskatchewan</institution>, Saskatoon, SK S7N 5A9, <country country="CA">Canada</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0188-1394</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>Min</given-names>
        </name>
        <aff><institution>Hunan Provincial Key Lab on Bioinformatics, School of Computer Science and Engineering, Central South University</institution>, Changsha 410083, <country country="CN">China</country></aff>
        <xref rid="btac779-cor1" ref-type="corresp"/>
        <!--limin@mail.csu.edu.cn-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Alkan</surname>
          <given-names>Can</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btac779-cor1">To whom correspondence should be addressed. <email>limin@mail.csu.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-12-02">
      <day>02</day>
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>02</day>
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <volume>39</volume>
    <issue>1</issue>
    <elocation-id>btac779</elocation-id>
    <history>
      <date date-type="received">
        <day>17</day>
        <month>6</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>25</day>
        <month>11</month>
        <year>2022</year>
      </date>
      <date date-type="editorial-decision">
        <day>29</day>
        <month>11</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>01</day>
        <month>12</month>
        <year>2022</year>
      </date>
      <date date-type="corrected-typeset">
        <day>12</day>
        <month>12</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btac779.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Protein essentiality is usually accepted to be a conditional trait and strongly affected by cellular environments. However, existing computational methods often do not take such characteristics into account, preferring to incorporate all available data and train a general model for all cell lines. In addition, the lack of model interpretability limits further exploration and analysis of essential protein predictions.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>In this study, we proposed DeepCellEss, a sequence-based interpretable deep learning framework for cell line-specific essential protein predictions. DeepCellEss utilizes a convolutional neural network and bidirectional long short-term memory to learn short- and long-range latent information from protein sequences. Further, a multi-head self-attention mechanism is used to provide residue-level model interpretability. For model construction, we collected extremely large-scale benchmark datasets across 323 cell lines. Extensive computational experiments demonstrate that DeepCellEss yields effective prediction performance for different cell lines and outperforms existing sequence-based methods as well as network-based centrality measures. Finally, we conducted some case studies to illustrate the necessity of considering specific cell lines and the superiority of DeepCellEss. We believe that DeepCellEss can serve as a useful tool for predicting essential proteins across different cell lines.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The DeepCellEss web server is available at <ext-link xlink:href="http://csuligroup.com:8000/DeepCellEss" ext-link-type="uri">http://csuligroup.com:8000/DeepCellEss</ext-link>. The source code and data underlying this study can be obtained from <ext-link xlink:href="https://github.com/CSUBioGroup/DeepCellEss" ext-link-type="uri">https://github.com/CSUBioGroup/DeepCellEss</ext-link>.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>62225209</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Hunan Provincial Science and Technology Program</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Hunan Province</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2021RC4008</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="9"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Essential genes are indispensable for the survival of a single-celled organism, a cell line or a multicellular organism (<xref rid="btac779-B3" ref-type="bibr">Bartha <italic toggle="yes">et al.</italic>, 2018</xref>). Essential proteins are products of essential genes, which perform the basic functions in the biological processes, and can be used to facilitate drug discovery and disease treatment (<xref rid="btac779-B16" ref-type="bibr">Ji <italic toggle="yes">et al.</italic>, 2019</xref>). The traditional biological experiments of essential protein identification include transposon mutagenesis, single-gene knockout, RNA interference and recent CRISPR gene-editing technology (<xref rid="btac779-B28" ref-type="bibr">Peters <italic toggle="yes">et al.</italic>, 2016</xref>; <xref rid="btac779-B30" ref-type="bibr">Rancati <italic toggle="yes">et al.</italic>, 2018</xref>). However, these wet-lab experiments are expensive, time-consuming and labor-intensive. Thus, it is urgent to develop effective and accurate computational methods to predict essential proteins.</p>
    <p>The computational methods can be roughly divided into two categories: network-based centrality measures and machine learning-based methods. Network-based centrality measures usually rely on a constructed biological network and design a scoring function to assign essential scores for each node in the constructed biological network. The Centrality-Lethality Rule was first proposed by <xref rid="btac779-B15" ref-type="bibr">Jeong <italic toggle="yes">et al.</italic> (2001)</xref>, which points out highly connected proteins in a protein–protein network are more likely to be essential proteins. After that, a lot of network-based centrality measures such as betweenness centrality (BC), closeness centrality (CC), eigenvector centrality (EC), local average centrality (LAC) and maximum neighborhood component (MNC) were proposed to identify essential proteins (<xref rid="btac779-B20" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2016</xref>; <xref rid="btac779-B26" ref-type="bibr">Lin <italic toggle="yes">et al.</italic>, 2008</xref>). Considering that some biological information is very important for protein essentiality, researchers incorporated various biological information sources in their scoring functions, including protein subcellular localization information, gene expression profiles, orthologous information and RNA-Seq data (<xref rid="btac779-B19" ref-type="bibr">Lei <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btac779-B23" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2014</xref>, <xref rid="btac779-B20" ref-type="bibr">2016</xref>; <xref rid="btac779-B34" ref-type="bibr">Tang <italic toggle="yes">et al.</italic>, 2014</xref>).</p>
    <p>With the rapid development of high-throughput sequencing technology, more and more essential protein data are accumulated, which provide a data foundation of machine learning-based methods. <xref rid="btac779-B6" ref-type="bibr">Deng <italic toggle="yes">et al.</italic> (2011)</xref> proposed a machine learning-based integrative model that uses Naïve Bayes, logistical regression, C4.5 decision tree and CN2 rule to estimate essentiality. <xref rid="btac779-B10" ref-type="bibr">Guo <italic toggle="yes">et al.</italic> (2017)</xref> adopted a support vector machine (SVM) to construct a prediction model from nucleotide composition and association information. <xref rid="btac779-B18" ref-type="bibr">Kuang <italic toggle="yes">et al.</italic> (2021)</xref> developed a machine learning model which combines gradient-boosted tree, SVM and multi-layer perceptron (MLP) to predict essential genes. <xref rid="btac779-B39" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> (2021)</xref> developed an ensemble deep learning model by integrating multiple gradient boosting decision tree (GBDT) base classifiers for accurate prediction. Recently, deep learning techniques have achieved great success in the bioinformatics field (<xref rid="btac779-B8" ref-type="bibr">Eraslan <italic toggle="yes">et al.</italic>, 2019</xref>). Inspired by their success, some researchers designed deep-learning models to predict essential proteins. For instance, <xref rid="btac779-B40" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> (2019)</xref> applied deep learning techniques to predict essential proteins by integrating protein–protein interaction (PPI) networks, gene expression profiles and subcellular localization data. <xref rid="btac779-B12" ref-type="bibr">Hasan and Lonardi (2020)</xref> utilized a MLP to develop a deep learning model for essentiality prediction from sequence-derived features. <xref rid="btac779-B43" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> (2020)</xref> proposed DeepHE, a deep learning model to predict human essential genes by integrating features derived from PPI networks and sequences. <xref rid="btac779-B25" ref-type="bibr">Li <italic toggle="yes">et al.</italic> (2021)</xref> developed an ensemble deep learning model, EP-EDL, which applied convolutional neural networks (CNN) to predict human essential proteins from evolutionary information.</p>
    <p>Although a lot of computational methods have been proposed, they still suffer from some limitations. First, accumulated evidence reveals that the protein essentialities are highly related to cellular environments, which means proteins show different essentiality in different cell lines (<xref rid="btac779-B4" ref-type="bibr">Behan <italic toggle="yes">et al.</italic>, 2019</xref>). Most of the existing computational methods do not take cell line-specificity into account. They often merge essential protein data from multiple cell lines with different labels into a single unified dataset to conduct model training, which fails to accurately identify essential proteins in diverse cell lines. Second, most of the existing machine/deep learning-based methods only focus on improving the prediction performance but fail to give an interpretation for their prediction results. The lack of interpretability makes their models become black boxes, which limits the understanding of their models for biologists. Therefore, developing an interpretable model is very important for the practical applications of computational methods.</p>
    <p>To address the above limitations, we proposed DeepCellEss, a cell line-specific deep learning-based essential protein predictor with the attention mechanism. To create a cell line-specific model, we collected extremely large-scale datasets including 16 408 proteins across 323 different cell lines to train and test DeepCellEss. DeepCellEss uses CNN to extract local features from protein sequences, and then applies the multi-head self-attention mechanism to enhance weights from CNN and provide model interpretation. Then, these enhanced signals are fed into a bidirectional long short-term memory (bi-LSTM) to capture long-range dependencies between residues. Finally, a fully connected layer with a sigmoid function performs the classification task.</p>
    <p>We conducted extensive experiments to evaluate the performance of DeepCellEss. In comparison, DeepCellEss shows greater effectiveness in predicting essential proteins than existing sequence-based methods. Compared to network-based centrality measures under cell line-specific networks, the results demonstrate that DeepCellEss effectively compensates for the limitations of network-based centrality measures. Furthermore, we performed some case studies which show the advantages of taking cell line-specificity into consideration. In addition, we carried out ablation studies to demonstrate the benefits of our proposed network architecture. Finally, we built a user-friendly webserver to expand our tool's accessibility.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Data collection</title>
      <p>To construct a practical cell line-specific prediction model, we collected protein essentiality data in extremely large-scale cell lines. <xref rid="btac779-F1" ref-type="fig">Figure 1</xref> shows the collection process of our cell line-specific benchmark datasets, which can be described as follows:</p>
      <fig position="float" id="btac779-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>Data collection process of large-scale cell line-specific protein essentiality datasets</p>
        </caption>
        <graphic xlink:href="btac779f1" position="float"/>
      </fig>
      <list list-type="order">
        <list-item>
          <p>We downloaded the essentiality data generated by the Wellcome Sanger Institutes (Release 1) from the Project Score database (<xref rid="btac779-B7" ref-type="bibr">Dwane <italic toggle="yes">et al.</italic>, 2021</xref>). The data were identified from a large number of systematic genome-scale CRISPR-Cas9 drop-out screens, including varying binary essential scores for 17 485 human protein-coding genes in 323 different human cell lines. The score of 1 refers to essential and 0 refers to non-essential.</p>
        </list-item>
        <list-item>
          <p>We collected sequence information from the Consensus CoDing Sequence (CCDS) database (Release 22) (<xref rid="btac779-B29" ref-type="bibr">Pruitt <italic toggle="yes">et al.</italic>, 2009</xref>) by mapping with unique gene symbols. In previous sequence-based methods, nucleotide-level and protein-level sequences have been used in essentiality prediction task, thus we collected both of them for further comparison and analysis. If one gene could match more than one protein sequence, we chose the sequence of the first annotated protein isoform produced by this gene as its corresponding protein sequence.</p>
        </list-item>
        <list-item>
          <p>We used CD-HIT and CD-HIT-EST (<xref rid="btac779-B24" ref-type="bibr">Li and Godzik, 2006</xref>) to remove the redundant sequences at the protein-level and nucleotide-level datasets, respectively. The sequence identity cutoff is set to 0.8, which means the remaining samples have sequence similarity less than 80% in both nucleotide-level and protein-level sequences.</p>
        </list-item>
      </list>
      <p>Based on the above processes, the resulting benchmark dataset comprises the binary essentiality labels and the sequence information of 16 408 proteins across 323 cell lines, which is the foundation of our sequence-based cell line-specific prediction models.</p>
    </sec>
    <sec>
      <title>2.2 Cell line-specific essentiality data analysis</title>
      <p>We performed some analysis to illustrate the significant differences of essential proteins across cell lines. <xref rid="btac779-F2" ref-type="fig">Figure 2a</xref> shows the distribution of essential protein numbers of 323 cell lines, which indicates a wide span in the essential protein number across different cell lines, with the COLO-678 cell line in the large intestine tissue having the fewest essential proteins (742) and the A2780ADR cell line in the lung tissue having the most (2491). The average number of is 1799, accounting for 10.96% of the total 16 408 proteins. In addition, to investigate the correlation between essential proteins in different cell lines, we took colorectal carcinoma as an example for analysis. In our collected dataset, colorectal carcinoma has 31 different cell lines. We used Pearson correlation coefficients (PCCs) to represent the similarities of essential protein list for a pair of cell lines. The heatmap in <xref rid="btac779-F2" ref-type="fig">Figure 2b</xref> shows the correlation between essential proteins across 31 colorectal carcinoma cell lines (the heatmaps of other cancer types with more than 10 cell lines are shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S1</xref>), where darker color means the two cell lines have more common essential proteins. From it, we can see that although these 31 cell lines belong to the same cancer (colorectal carcinoma), the essential proteins of them are quite different. The differences in essential proteins in different cell lines shown in <xref rid="btac779-F2" ref-type="fig">Figure 2</xref> drove our work on the construction of cell line-specific models.</p>
      <fig position="float" id="btac779-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>Analysis of protein essentiality across different cell lines. (<bold>a</bold>) Distribution of the numbers of essential proteins across 323 cell lines. (<bold>b</bold>) Heatmap of pairwise Pearson correlation coefficients for essential protein data across 31 different colorectal carcinoma cell lines. Darker color means the two cell lines have more common essential proteins</p>
        </caption>
        <graphic xlink:href="btac779f2" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.3 Model architecture</title>
      <p>DeepCellEss is a sequence-based end-to-end deep learning prediction model. The overview of DeepCellEss is presented in <xref rid="btac779-F3" ref-type="fig">Figure 3</xref>, which consists of five modules i.e. sequence representation, CNN, multi-head self-attention, bi-LSTM and prediction. The detailed descriptions of the five modules are as follows.</p>
      <fig position="float" id="btac779-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>DeepCellEss framework. DeepCellEss accepts a protein sequence as input and converts it into a numerical matrix using one-hot encoding. After that, a CNN module is employed to effectively capture sequence local information. The multi-head self-attention is used to produce residue-level attention scores for model interpretability. Additionally, two skip-connection operations are implemented around CNN and the multi-head self-attention to avoid the model degradation problem. After multi-head self-attention, a bi-LSTM module is applied to model sequential data by learning long-range dependencies. Finally, the prediction task is performed after a max-pooling and fully connected layer</p>
        </caption>
        <graphic xlink:href="btac779f3" position="float"/>
      </fig>
      <sec>
        <title>2.3.1 Sequence representation</title>
        <p>The sequence representation module converts the raw protein sequences of variant lengths into fixed-size numeric feature matrices through one-hot encoding method. Formally, given a protein sequence <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mi mathvariant="normal">S</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula>, where <italic toggle="yes">L</italic> means the length of the sequence, <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the residue at position <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mi>i</mml:mi></mml:math></inline-formula>. There are 21 possible <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in a protein sequence i.e. 20 types of standard protein residues and others. By using one-hot encoding, each type of residue is encoded into a 21D binary vector <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. Hence, each protein sequence can be represented numerically as <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> and fed into the next model module.</p>
      </sec>
      <sec>
        <title>2.3.2 Convolutional neural networks</title>
        <p>We applied a CNN module to extract latent local knowledge from the raw protein sequences. CNN is a very popular class of neural networks in the fields of computer vision and natural language processing, and have been successfully applied to many bioinformatics prediction problems (<xref rid="btac779-B17" ref-type="bibr">Kim, 2014</xref>; <xref rid="btac779-B42" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic>, 2020</xref>). Because of parameter sharing and local connectivity, CNN is able to learn dependencies between adjacent residues effectively. Numerous convolution kernels slide along the sequential features and capture important patterns thus delivering features enriched with local knowledge. In DeepCellEss, we employed a 1D-convolution layer after sequence representation and then followed by a rectified linear unit (ReLU) activation function. Thus, we can obtain an output representation with local information.</p>
      </sec>
      <sec>
        <title>2.3.3 Multi-head self-attention</title>
        <p>After CNN, a multi-head self-attention module is utilized in DeepCellEss (<xref rid="btac779-B35" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic>, 2017</xref>). This module has two primary functions. On the one hand, it can enhance the functionality of CNN module by compensating for its limitation of locality. Instead of using a pure CNN module, the combination of CNN and self-attention helps the model to focus on important sequence regions within a larger scope. Specifically, the output of a single-head self-attention is computed as
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="italic">softmax</mml:mi><mml:mfenced separators="|"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal"> </mml:mi><mml:mi>Q</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mfenced><mml:mi>V</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mi>Q</mml:mi></mml:math></inline-formula>, <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mi>K</mml:mi></mml:math></inline-formula> and <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mi>V</mml:mi></mml:math></inline-formula> represent the query, key and value, respectively. <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msqrt><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:mrow></mml:mfrac></mml:math></inline-formula> are the scaling factor of the dot-product attention. The <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mi mathvariant="normal">softmax</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mo>·</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> stands for the softmax operation.</p>
        <p>Additionally, because it is hard to learn the representation from various perspectives by using self-attention with a single head, we introduced the self-attention with multiple heads to capture more informative features. Thus, the output of the multi-head self-attention module is
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="italic">norm</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>concat</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mi>W</mml:mi><mml:mi>O</mml:mi></mml:msup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where h is the number of heads, <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is a learnable parameter matrix, concat<inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mo>·</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> stands for the concatenation operation, norm<inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mo>·</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> stands for the layer normalization operation to maintain the stability of data distribution and better model training.</p>
        <p>On the other hand, the self-attention mechanism enables our model to explain prediction results from interpretable attention score distributions. The details of model interpretability can be found in Section 2.4. Through the multi-head self-attention followed by the CNN module, our module has the ability to learn more information for feature extraction and achieve model interpretability.</p>
      </sec>
      <sec>
        <title>2.3.4 Bidirectional long short-term memory</title>
        <p>To model sequential data and learn long-range dependencies from protein sequences, we applied a bi-LSTM module in DeepCellEss. LSTM is a type of recurrent neural network that can efficiently mitigate vanishing gradient and exploding gradient issues during long sequence training (<xref rid="btac779-B13" ref-type="bibr">Hochreiter and Schmidhuber, 1997</xref>). We used a bi-LSTM that can sequentially update the hidden states <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">lstm</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula> for sequential data from two directions, where <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the dimension of hidden state vectors. More specifically, let <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">lstm</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula> denote the hidden state vector of the <italic toggle="yes">i</italic>th residue, which can be formulated by the following equations:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="italic">sigmoid</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mi mathvariant="italic">attn</mml:mi></mml:msubsup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="italic">lstm</mml:mi></mml:msubsup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mi>I</mml:mi></mml:msup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="italic">sigmoid</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mi mathvariant="italic">attn</mml:mi></mml:msubsup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="italic">lstm</mml:mi></mml:msubsup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mi>F</mml:mi></mml:msup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="E5"><label>(5)</label><mml:math id="M5" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>O</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="italic">sigmoid</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mi mathvariant="italic">attn</mml:mi></mml:msubsup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="italic">lstm</mml:mi></mml:msubsup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mi>O</mml:mi></mml:msup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="E6"><label>(6)</label><mml:math id="M6" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⊙</mml:mo><mml:mi>tanh</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mi mathvariant="italic">attn</mml:mi></mml:msubsup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="italic">lstm</mml:mi></mml:msubsup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mi>G</mml:mi></mml:msup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="E7"><label>(7)</label><mml:math id="M7" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mi mathvariant="italic">lstm</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⊙</mml:mo><mml:mi>tanh</mml:mi><mml:mfenced separators="|"><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula> represents three gates and the cell state at position <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mi>i</mml:mi></mml:math></inline-formula> of input sequence, respectively, <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mo>⊙</mml:mo></mml:math></inline-formula> stands for the Hadamard product operation, sigmoid<inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mo>·</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mi mathvariant="normal">tanh</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mo>·</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> are two types of activation functions. After the bi-LSTM, we obtained the hidden states <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">lstm</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mn>2</mml:mn><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula> as output features by concatenating the hidden states of both directions.</p>
      </sec>
      <sec>
        <title>2.3.5 Prediction</title>
        <p>In the prediction module, we used a max-pooling layer to down-sample the high-level feature representation from bi-LSTM. Then, the outputted features were fed into a fully connected layer, resulting in a prediction score. Finally, we obtained the prediction essential probability for the input sequence using a sigmoid activation function.</p>
      </sec>
    </sec>
    <sec>
      <title>2.4 Model interpretability</title>
      <p>In addition to accurately predicting essential proteins, we would like to explain visually how DeepCellEss makes specific predictions across different cell lines. To achieve model interpretability, we used a residue-level attention score vector from the multi-head self-attention module to represent the contribution of each residue position. Specifically, for the <italic toggle="yes">j</italic>th single-head self-attention, the original attention score matrix <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> can be calculated from the scaled dot-product attention scoring function,
<disp-formula id="E8"><label>(8)</label><mml:math id="M8" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msup><mml:msub><mml:mi>K</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      <p>Then, we obtained an overall attention score matrix <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal"> </mml:mi></mml:math></inline-formula>by averaging all single-head attention score matrices. The attention score matrix reflects the relations between any two components of input sequential vectors. In order to assign a score to each sequence position for assessing their contribution to prediction results, we need to convert the score matrix to a score vector with the same size of sequence length. Therefore, we averaged <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mi>a</mml:mi></mml:math></inline-formula> along the second axis, resulting in an attention score vector <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mi>e</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> for each input sequence. Additionally, because we trained five models for each cell line dataset from 5-fold cross-validation, we averaged the attention score vectors from five trained models to obtain the final attention score vector. Through the residue-level attention score vector, we are able to interpret prediction results by locating crucial regions from the input sequence.</p>
    </sec>
    <sec>
      <title>2.5 Baseline methods</title>
      <p>The primary goal of DeepCellEss is to predict the essentiality of proteins using only sequence information. To demonstrate the effectiveness of DeepCellEss, we compared it with five sequence-based baseline methods on the independent test set of HCT-116 benchmark dataset. The baseline methods are described as follows:
</p>
      <list list-type="order">
        <list-item>
          <p>Seringhaus’s: It is a sequence-based method for essential gene prediction proposed by <xref rid="btac779-B33" ref-type="bibr">Seringhaus <italic toggle="yes">et al.</italic> (2006)</xref>. It extracts 14 features from protein sequences using CodonW, TMHMM v2.0 and PA-SUB v2.5. Then, these sequence-derived features are fed into an ensemble machine-learning model for prediction. We implemented this model and trained it on our benchmark dataset. It should be noted that PA-SUB v2.5 is not available now, so we used Hum-mPLoc 3.0 instead, which is a newly developed protein subcellular localization predictor.</p>
        </list-item>
        <list-item>
          <p>EP-GBDT: It extracts the pseudo amino acid composition features using PseAAC, and then integrates multiple GBDT base classifiers to predict essentiality. We re-trained and tested EP-GBDT based on the source code provided in the original paper.</p>
        </list-item>
        <list-item>
          <p>EP-EDL: It is a deep learning-based model. For a fair comparison of the model structures, we applied the same sequence representation method as DeepCellEss and re-trained EP-EDL based on its source code.</p>
        </list-item>
        <list-item>
          <p>Pheg: It uses λ-interval Z curve method to extract features and SVM classifier to predict essentiality. We directly evaluated Pheg on the independent test set with nucleotide sequence as input using its source code.</p>
        </list-item>
        <list-item>
          <p>DeepCellEss-nc: Both protein-level and nucleotide-level sequences have been applied for essentiality prediction. To investigate which type of sequence feature performs better under the same model structure, we modified the original DeepCellEss with nucleotide sequence as input and named it DeepCellEss-nc. We re-trained it using the same sequence representation method and model structure.</p>
        </list-item>
      </list>
    </sec>
    <sec>
      <title>2.6 Implementation details</title>
      <p>We used the hold-out method to evaluate the model performances on our benchmark datasets. In previous studies, the division of dataset into training and test sets was usually performed by the stratified splitting strategy based on the ratio of positive and negative samples. However, since the datasets are imbalanced i.e. the number of non-essential proteins is larger than the number of essential proteins, the stratified splitting strategy will result in an imbalanced test set. In such an imbalanced test set, it is difficult to measure the prediction performance for essential proteins. Therefore, we randomly chose 20% of essential proteins with the equal number of non-essential proteins as the independent test set, and the rest samples as the training set. To make the most use of training data, we applied a 5-fold cross-validation method for model training on the training set. Specifically, the training set was divided into five folds. Each fold is used once for validation and four times for training. After training and validation, we obtained five trained classifiers. When predicting the essentiality on the test set or new protein sequences, the output values of the five classifiers are averaged as the final prediction score.</p>
      <p>We performed the training procedure with the mini-batch stochastic gradient descent using the Adam optimizer. To take advantage of the mini-batch technique for training, we utilized the truncation and zero-padding techniques to fix the length of sequence features. To avoid overfitting during the training process, an early stop strategy with a patience of 30 epochs was adopted. To alleviate the class-imbalanced training data problem, we adopted weighted binary cross entropy as the loss function. The loss function <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">WBCE</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is defined as
<disp-formula id="E9"><label>(9)</label><mml:math id="M9" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>L</mml:mi><mml:mi mathvariant="italic">WBCE</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>m</mml:mi></mml:mfrac><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mi>w</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mfenced separators="|"><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mi>m</mml:mi></mml:math></inline-formula> is the number of training samples, <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the true label and predictive score of sample <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mi>i</mml:mi></mml:math></inline-formula>. The imbalance parameter <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mi>w</mml:mi></mml:math></inline-formula> is set to the ratio of the number of negative samples to the number of positive samples.</p>
      <p>Our models were implemented in PyTorch and Scikit-learn libraries. All training processes were run with an Intel(R) Xeon(R) Gold 5220 CPU @ 2.20 GHz, 256GB memory and a Nvidia GeForce RTX 2080 Ti GPU. The hyper-parameter settings were determined by grid search techniques.</p>
    </sec>
    <sec>
      <title>2.7 Evaluation metrics</title>
      <p>We evaluated our models on the independent test sets of different cell lines. The model performance was assessed by the area under the receiver-operating characteristic curve (AUROC) and the area under precision-recall curve (AUPRC), which can measure the ranking ability for prediction models. It should be noted that AUPRC is more sensitive to the positive samples i.e. essential proteins and thus can provide more comprehensive evaluation.</p>
    </sec>
  </sec>
  <sec>
    <title>3. Results</title>
    <sec>
      <title>3.1 Prediction performance on large-scale datasets of different cell lines</title>
      <p>To evaluate the performance of DeepCellEss, we trained and tested DeepCellEss on a large collection of benchmark datasets across different cell lines. Specifically, DeepCellEss was trained independently on 323 cell line benchmark datasets using the same model optimization settings. After all training processes are completed, we carried out the tests on the corresponding independent test set of each cell line model. The detailed performance results of all cell line models are listed in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>. <xref rid="btac779-F4" ref-type="fig">Figure 4</xref> shows them in the form of boxplots. Since we had 323 cell lines, we classified them into 28 groups based on their cancer types and assigned different colors to boxplots for the 28 types of cancers. From <xref rid="btac779-F4" ref-type="fig">Figure 4</xref>, we can see that the AUROCs and AUPRCs obtained by DeepCellEss are mainly in the range of 0.72–0.80. Although the performance varies across different cell lines and cancers, the overall prediction performance is robust and promising. In addition, we observed that the best performance is obtained by the SNU-C1 cell line, with an AUROC of 0.825 and an AUPRC of 0.826. The SNU-C1 dataset is a very imbalanced dataset that contains 1298 essential proteins out of a total of 16 408 proteins. The results of SNU-C1 dataset indicate that our model can work well with imbalanced data. Taken together, these results suggest that DeepCellEss is an effective and useful model that can be used for essential protein prediction tasks in various cell lines.</p>
      <fig position="float" id="btac779-F4">
        <label>Fig. 4.</label>
        <caption>
          <p>The AUROCs and AUPRCs on the independent test sets of 323 different cell lines using DeepCellEss. (<bold>a</bold>) Boxplots of AUROC (<bold>b</bold>) Boxplots of AUPRC. Since we had 323 cell lines, we classified them into 28 types of cancers and assigned them various colors to represent the 28 types of cancers. Note that the AUROCs, AUPRCs of 323 cell lines mainly vary from 0.72 to 0.80, demonstrating the promising and robust prediction performances of DeepCellEss</p>
        </caption>
        <graphic xlink:href="btac779f4" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.2 Comparison with baseline methods</title>
      <p>In this section, we carried out comparison experiments to investigate the effectiveness of DeepCellEss for essential protein prediction. We compared the performance of DeepCellEss with five sequence-based baseline methods (described in Section 3.4) on the independent test set of HCT-116 cell line. The comparison results are shown in <xref rid="btac779-T1" ref-type="table">Table 1</xref>, which demonstrates that DeepCellEss outperforms the existing sequence-based methods in terms of AUROC and AUPRC. Specifically, when compared to other baseline methods, DeepCellEss achieves AUROC and AUPRC scores of 0.782 and 0.795, with an increase of 1.8–45.4% and 2.3–76.7%, respectively.</p>
      <table-wrap position="float" id="btac779-T1">
        <label>Table 1.</label>
        <caption>
          <p>Performances of DeepCellEss and existing sequence-based methods on the independent test set of HCT-116</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Method</th>
              <th rowspan="1" colspan="1">AUROC</th>
              <th rowspan="1" colspan="1">AUPRC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Seringhaus's</td>
              <td rowspan="1" colspan="1">0.734</td>
              <td rowspan="1" colspan="1">0.682</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">EP-GBDT</td>
              <td rowspan="1" colspan="1">0.768</td>
              <td rowspan="1" colspan="1">0.777</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">EP-EDL</td>
              <td rowspan="1" colspan="1">0.760</td>
              <td rowspan="1" colspan="1">0.736</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Pheg</td>
              <td rowspan="1" colspan="1">0.427</td>
              <td rowspan="1" colspan="1">0.450</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepCellEss-nc</td>
              <td rowspan="1" colspan="1">0.751</td>
              <td rowspan="1" colspan="1">0.740</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepCellEss</td>
              <td rowspan="1" colspan="1">
                <bold>0.782</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.795</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic toggle="yes">Note:</italic> The best performance values are highlighted in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>In addition, we can see that Pheg gets AUROC and AUPRC scores of 0.427 and 0.450, respectively, which are lower than the other methods. This can be explained by the fact that Pheg web server only provides a general human gene essentiality predictor, and it ignores specific differences in the essentiality of genes and the encoded products across cell lines, resulting in poor prediction performance on cell line-specific test datasets. Such results indicate the difficulty of identifying cell line-specific essential genes and proteins using a general model trained on common essential samples in cell lines. Thus, training cell line-specific models is necessary for discovering specific essential genes and proteins in different cell lines.</p>
      <p>Moreover, we observed that the performance of DeepCellEss is better than DeepCellEss-nc, which means that protein sequence features are more effective than nucleotide sequence features for DeepCellEss model. The results may be thanks to that: (i) protein sequences are composed of 21 types of amino acids while nucleotide sequences are made up of four different types of nucleotides, resulting in protein sequences has a more diverse sequence information; (ii) the encoded protein sequence is much shorter than the nucleotide sequence for a gene, which can reduce computational consumption and processing complexity; and (iii) protein sequence features are more informative for essentiality prediction.</p>
    </sec>
    <sec>
      <title>3.3 Comparison with network-based methods under cell line-specific networks</title>
      <p>Over the past two decades, many studies have reported that the essentiality of proteins is highly related to the topological properties of PPI networks. Extensive network-based centrality measures were developed for discovering new essential proteins. These methods can efficiently mine latent information from network topology and rank essentiality for proteins in PPI networks. However, the network-based methods suffer from several major drawbacks: (i) these methods cannot be directly used for proteins that are not in the PPI networks; (ii) their prediction ability for essential proteins with low degrees is greatly limited. As a sequence-based method, DeepCellEss is able to compensate for the shortcomings of network-based centrality measures.</p>
      <p>In order to investigate whether DeepCellEss can achieve promising prediction performance without interaction information, we designed the following experiments: We first downloaded protein interaction data of HCT-116 cell line from the BioPlex 3.0 database (<xref rid="btac779-B14" ref-type="bibr">Huttlin <italic toggle="yes">et al.</italic>, 2021</xref>) and constructed an HCT-116 cell line-specific PPI network (referred to as the HCT-116 network), which includes 10 115 proteins and 70 966 interactions in total. Then, six classical network-based centrality measures i.e. BC, CC, DC, EC, LAC and NC, are calculated for all 10 115 protein nodes in the HCT-116 network. The scores of BC, CC, DC and EC are calculated using the python library NetworkX (<xref rid="btac779-B11" ref-type="bibr">Hagberg <italic toggle="yes">et al.</italic>, 2008</xref>) and the scores of LAC (<xref rid="btac779-B22" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2011</xref>) and NC (<xref rid="btac779-B37" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2012</xref>) are calculated based on the proposed methods.</p>
      <p>To compare the performance of DeepCellEss with network-based centrality measures based on the same dataset, we screened the 450 intersection proteins of HCT-116 network and HCT-116 test set as a new test set, which includes equal numbers (225) of essential and non-essential proteins. We ranked the result scores predicted by the six network-based methods and DeepCellEss from highest to lowest and compared the cumulative counts of essential proteins in the top 10%, top 20%, 30% and top 40% proteins. The results in <xref rid="btac779-F5" ref-type="fig">Figure 5</xref> show that DeepCellEss is able to identify more essential proteins than centrality measure methods.</p>
      <fig position="float" id="btac779-F5">
        <label>Fig. 5.</label>
        <caption>
          <p>Count of essential proteins detected by network-based methods and DeepCellEss based on the ranked prediction scores on the new HCT-116 test set</p>
        </caption>
        <graphic xlink:href="btac779f5" position="float"/>
      </fig>
      <p>We further explored the prediction performance of DeepCellEss on the proteins with low degree in PPI networks. According to the Centrality-Lethality Rule (<xref rid="btac779-B15" ref-type="bibr">Jeong <italic toggle="yes">et al.</italic>, 2001</xref>), higher centrality measure values indicate higher essentiality of proteins. Therefore, network-based methods usually predict the proteins with low degree to be non-essential, resulting they could barely identify those essential proteins that have few interaction partners or lack of interaction information. To evaluate how well DeepCellEss performs on the low-degree essential proteins, we screened the 147 essential proteins with only one degree in the HCT-116 network. Then, we re-split the HCT-116 dataset with these 147 essential proteins as the new independent test set and the rest as the new training set. After re-training the HCT-116-specific DeepCellEss model, the results show that 69.4% (102) of the 147 essential proteins could be accurately predicted, indicating that our model has practical and effective prediction ability for the essential proteins on the low-degree essential proteins.</p>
      <p>To better illustrate the prediction performance of different types of methods for the proteins with low degree, we gave prediction results of an example essential protein ‘Probable ATP-dependent RNA helicase DDX59’ (DDX59, Uniprot ID: Q5T1V6). DDX59 is a member of the DEAD box helicase family proteins, which involves in all aspects of RNA metabolism and plays an important role in many cellular activities. <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S2</xref> shows the local connectivity information of DDX59. From the ranking results of the HCT-116 network using six classical centrality measure scores [i.e. DC, BC, CC, EC, NC, LAC), DDX59 ranked 9097 (89.9%), 9033 (89.3%), 9671 (95.6%), 8920 (88.2%), 9783 (96.7%) and 9783 (96.7%) out of 10 115 proteins, respectively]. These network-based methods cannot identify the cell line-specific essentiality of DDX59, while DeepCellEss predicts DDX59 correctly with the essentiality score of 0.788 in the HCT-116-specific model.</p>
      <p>Last, we explored the performance of DeepCellEss on the proteins that have no protein interaction information in our HCT-116 benchmark dataset. Based on our statistics, there are 438 essential proteins that do not appear in the HCT-110 network, which means they are not able to be identified by network-based approaches. Then, we tested them using our re-trained HCT-116-specific model. The results show that 63.9% (280) of them are correctly predicted, indicating that our model has practical and effective prediction ability for essential protein prediction without PPI information.</p>
      <p>Overall, these experiment results and comparative analysis of DeepCellEss and network-based methods confirm that DeepCellEss can achieve promising performance for essential proteins with no PPI information or low degree in PPI networks. DeepCellEss effectively compensates for the limitations of network-based methods and offers a more practical approach to essential protein prediction.</p>
    </sec>
    <sec>
      <title>3.4 Case studies</title>
      <p>A major advantage of our proposed model is the capability to learn and predict protein essentialities across different cell lines. To demonstrate the effectiveness of our model for cell line-specific prediction, we used ‘G1/S-specific cyclin-D1’ (CCND1, Uniprot ID: P24385) as an example to compar DeepCellEss with other two available servers (Pheg and EP-GBDT). From pre-existing biological experiments, CCND1 performs obvious differences within various cellular environments in terms of essentiality. For instance, it is identified as essential in CL-11 cell line while non-essential in RPMI-B226 cell line. <xref rid="btac779-F6" ref-type="fig">Figure 6a</xref> presents the prediction results of CCND1 using DeepCellEss, Pheg and EP-GBDT, respectively. With the support of cell line-specific predictions, DeepCellEss gets different essentiality scores with 0.429 of RPMI-B226 and 0.775 of CL-11, yielding accurate predictions of CCND1 under different cell lines. However, Pheg and EP-GBDT can only give overall prediction scores of 0.717 and 1.479 because they cannot support cell line-level prediction. Both Pheg and EP-GBDT predict CCND1 as an essential protein but fail to capture the non-essentiality of CCND1 in cell line of RPMI-B226.</p>
      <fig position="float" id="btac779-F6">
        <label>Fig. 6.</label>
        <caption>
          <p>Case study for cell line-specific predictions and model interpretability by DeepCellEss on CCND1. (<bold>a</bold>) Predictions of CCND1 (Uniprot ID: P24385) by three available online predictors. DeepCellEss enables accurate cell line-specific predictions while Pheg and EP-GBDT only give a unified result for all cell lines. (<bold>b</bold>) Interpretability for the prediction of P24385 in CL-11. In the visual heatmap, the red regions indicate higher attention scores that contribute more to essential, and the blue regions indicate lower attention scores that contribute more to non-essential</p>
        </caption>
        <graphic xlink:href="btac779f6" position="float"/>
      </fig>
      <p>Moreover, DeepCellEss leverages the advantage of the attention mechanism to assign residue-level attention scores for query proteins, and provides the visual heatmap for interpretation. <xref rid="btac779-F6" ref-type="fig">Figure 6b</xref> shows the prediction heatmap of CCND1 in CL-11. The red regions indicate contributions to be essential while the blue regions indicate contributions to be non-essential in the prediction. To further illustrate our interpretable model is possible to detect regions which are important motifs and correlated with protein essential functions, we performed a case study as follows:</p>
      <p>The JAB1/MPN/Mov34 metalloenzyme (JAMM) motif is highly conserved, typically consisting of a canonical sequence of ‘H-[NST]-H-x(7)-S-x(2)-D’. JAMM-containing proteins are metal-dependent proteases and responsible for providing the active site for isopeptidase activity (<xref rid="btac779-B1" ref-type="bibr">Ambroggio <italic toggle="yes">et al.</italic>, 2004</xref>). <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S3a</xref> shows the JAMM motif logo generated from JAMM-containing proteins in UniprotKB database using MEME (<xref rid="btac779-B2" ref-type="bibr">Bailey <italic toggle="yes">et al.</italic>, 2015</xref>). PSMD14/Rpn11/POH1 is a representative JAMM-containing protein. PSMD14 plays a key role within the proteasomes, where it acts as an intrinsic deubiquitinase removing polyubiquitin chains from substrate proteins (<xref rid="btac779-B38" ref-type="bibr">Wauer and Komander, 2014</xref>). Research evidence suggests that the JAMM motif of SMD14 is essential for human cell viability (<xref rid="btac779-B9" ref-type="bibr">Gallery <italic toggle="yes">et al.</italic>, 2007</xref>; <xref rid="btac779-B36" ref-type="bibr">Verma <italic toggle="yes">et al.</italic>, 2002</xref>). We used DeepCellEss to predict PSMD14 (Uniport ID: O00487) under ‘Unknown’ cancer type and ‘Unknown’ cell line options. <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S3b</xref> shows the prediction result (0.687) and the visualization heatmap of SMD14. In the heatmap, the JAMM motif is marked red in the whole sequence. The results suggest that our predictor could identify essential protein and might recognize its important motif.</p>
      <p>Additionally, we analyzed the performance of DeepCellEss on intrinsically disordered proteins (IDPs), which are widely distributed in eukaryotes and closely associated with human diseases. From the cancer-related protein dataset of DisProt database, we found a conditional essential IDP with 100% disorder content, called ‘nuclear factor erythroid 2-related factor 2’ (NFE2L2, Uniprot ID: Q16236). Several studies have revealed that NFE2L2 is highly related to lung cancers (<xref rid="btac779-B5" ref-type="bibr">Binkley <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac779-B32" ref-type="bibr">Sánchez-Ortega <italic toggle="yes">et al.</italic>, 2021</xref>). We used DeepCellEss to predict NFE2L2 under the options of ‘Non-Small Cell Lung Carcinoma’, ‘Squamous Cell Lung Carcinoma’, and ‘No-cancerous’ cancer types, respectively. The results (shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S4</xref>) indicate that DeepCellEss predicts NFE2L2 to be essential in two types of lung cancers but non-essential in non-cancerous, implying that our essentiality predictor is useful for IDPs and has the potential to find some cancer-related essential IDPs.</p>
    </sec>
    <sec>
      <title>3.5 Ablation study</title>
      <p>To measure the contributions of individual components to DeepCellEss structure, we conducted ablation studies by re-training and validating DeepCellEss without different components. Specifically, four main components, including skip connection, CNN module, multi-head self-attention module and bi-LSTM module, were separately removed, and we obtained four variants of DeepCellEss model. Then, we trained and validated these four models. The strategies for dataset splitting and model training remain unchanged as the raw DeepCellEss. <xref rid="btac779-T2" ref-type="table">Table 2</xref> reports the results of DeepCellEss and its variants, which show that the removal of the different components leads to a reduction in the prediction performance of DeepCellEss. Our model yields the best AUROC of 0.782 and the best AUPRC of 0.795, in which the AUPRC is improved by about 4.1%, 2.2%, 4.2% and 3.7% over DeCepCellEss without skip connection, CNN module, multi-head self-attention module and bi-LSTM module, respectively. The ablation studies demonstrate that the model architecture of raw DeepCellEss is optimal for our prediction task.</p>
      <table-wrap position="float" id="btac779-T2">
        <label>Table 2.</label>
        <caption>
          <p>The performances of DeepCellEss and its variant models in the ablation study</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th rowspan="1" colspan="1">AUROC</th>
              <th rowspan="1" colspan="1">AUPRC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Without skip connection</td>
              <td rowspan="1" colspan="1">0.759</td>
              <td rowspan="1" colspan="1">0.764</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Without CNN module</td>
              <td rowspan="1" colspan="1">0.765</td>
              <td rowspan="1" colspan="1">0.776</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Without multi-head self-attention module</td>
              <td rowspan="1" colspan="1">0.766</td>
              <td rowspan="1" colspan="1">0.763</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Without bi-LSTM module</td>
              <td rowspan="1" colspan="1">0.754</td>
              <td rowspan="1" colspan="1">0.767</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepCellEss</td>
              <td rowspan="1" colspan="1">
                <bold>0.782</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.795</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <p><italic toggle="yes">Note:</italic> The best performance values are highlighted in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.6 Web server</title>
      <p>To facilitate the access to DeepCellEss, we developed a user-friendly web server, <ext-link xlink:href="http://csuligroup.com:8000/DeepCellEss" ext-link-type="uri">http://csuligroup.com:8000/DeepCellEss</ext-link>. The DeepCellEss web server provides cell line-specific essential protein prediction and visualization for a large amount of cell lines. <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S5</xref> shows the user interface of DeepCellEss web server. Users can enter an UniProt ID to search for the protein sequence or directly input a single protein sequence with length less than 1000aa in FASTA format, and then choose a cell line from a list of 323 cell lines, to predict and analyze protein essentiality in the certain cell line environment. Besides, if users are unsure which cell line environment the query protein is located in, we offer an option of ‘Unknown’ to enable a unified result for human protein essentiality prediction. The results of this option are the average prediction score under all cell line-specific models. For each submission, the output panel presents two parts, i.e., the result of predicted essentiality and the visualization of residue-level attention scores. The result part gives a five-column table containing the cell line name, the input protein ID, the sequence length, the predicted essentiality score and the final predicted label. The visualization part provides a heatmap and an interactive line plot, which allows users to estimate the contribution of each residue position to the prediction results from various perspectives. To the best of our knowledge, it is the first web server that can predict essential proteins under specific cell lines and provide visualization analysis. We believe that DeepCellEss can serve as a practical and useful tool for human essential protein study.</p>
    </sec>
  </sec>
  <sec>
    <title>4. Conclusion</title>
    <p>The identification of cancer cell line-specific essential proteins is particularly relevant for the discovery of novel precision cancer drug targets. However, existing computational methods have not taken into account the specificity of essential proteins in different cell lines, and lack practical and interpretable tools for human essential protein prediction. In this study, we proposed DeepCellEss, a cell line-specific interpretable deep learning prediction method based on the attention mechanism. The main contributions of DeepCellEss are summarized as follows:
</p>
    <list list-type="bullet">
      <list-item>
        <p>To the best of our knowledge, DeepCellEss is the first computational method that supports cell line-specific essential protein predictions, which makes it possible to predict protein essentialities in different cellular environments;</p>
      </list-item>
      <list-item>
        <p>DeepCellEss implements an interpretable deep-learning model through residue-level attention scores from multi-head self-attention mechanism. The attention scores enable to locate the most important sequence regions for different prediction results, and further make more comprehensive analysis and comparison for cell line-specific essential proteins;</p>
      </list-item>
      <list-item>
        <p>For real practical applications of our cell line-specific model, we constructed extremely large-scale datasets across 323 cell lines. Moreover, we provided a user-friendly web server of cell line-specific essential protein predictions. It is expected to help discover potential diagnostic biomarkers and therapeutic targets for precision cancer therapy.</p>
      </list-item>
    </list>
    <p>Although the extensive results show that DeepCellEss is an effective predictor for cell line-specific essential proteins and outperformers existing sequence-based methods, we would like to point out its limitations. The main limitation is that we do not consider the relations of different cell lines under the same tissue or cancer type. In our reported results, the models of different cell lines under the same cancer type show varying prediction performance. For example, in the cancer type of Colorectal Carcinoma, SNU-C1 model yields the best AUROC (0.825) and AUPRC (0.826), while MDST8 model gets the worst AUROC (0.728) and AUPRC (0.731). Therefore, future efforts could be devoted to improving the poor performance for some cell lines by introducing the relations between different cell lines. One potential solution is to use transfer learning techniques (<xref rid="btac779-B27" ref-type="bibr">Pan and Yang, 2010</xref>; <xref rid="btac779-B40" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic>, 2019</xref>). To be specific, we can first pre-train with multiple cell line datasets that are closely related to the target cell line, and then apply the knowledge to the target cell line dataset to develop a more powerful cell line-specific model.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btac779_Supplementary_Data</label>
      <media xlink:href="btac779_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>We acknowledge for technical support from the High Performance Computing Center of Central South University and thank Dr. Chuan Dong (Wuhan University) for providing the source code of Pheg.</p>
    <sec>
      <title>Funding</title>
      <p>This work was supported by the National Natural Science Foundation of China [62225209]; Hunan Provincial Science and Technology Program [2019CB1007]; The science and technology innovation program of Hunan Province [2021RC4008].</p>
      <p><italic toggle="yes">Conflict of Interest</italic>: The authors declare that they have no conflict of interest.</p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btac779-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ambroggio</surname><given-names>X.I.</given-names></string-name></person-group><etal>et al</etal> (<year>2004</year>) <article-title>JAMM: a metalloprotease-like zinc site in the proteasome and signalosome</article-title>. <source>PLoS Biol</source>., <volume>2</volume>, <fpage>e2</fpage>.<pub-id pub-id-type="pmid">14737182</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bailey</surname><given-names>T.L.</given-names></string-name></person-group><etal>et al</etal> (<year>2015</year>) <article-title>The MEME suite</article-title>. <source>Nucleic Acids Res</source>., <volume>43</volume>, <fpage>W39</fpage>–<lpage>W49</lpage>.<pub-id pub-id-type="pmid">25953851</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bartha</surname><given-names>I.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Human gene essentiality</article-title>. <source>Nat. Rev. Genet</source>., <volume>19</volume>, <fpage>51</fpage>–<lpage>62</lpage>.<pub-id pub-id-type="pmid">29082913</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Behan</surname><given-names>F.M.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Prioritization of cancer therapeutic targets using CRISPR-Cas9 screens</article-title>. <source>Nature</source>, <volume>568</volume>, <fpage>511</fpage>–<lpage>516</lpage>.<pub-id pub-id-type="pmid">30971826</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Binkley</surname><given-names>M.S.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>KEAP1/NFE2L2 mutations predict lung cancer radiation resistance that can be targeted by glutaminase InhibitionKEAP1/NFE2L2 mutations predict lung cancer radio resistance</article-title>. <source>Cancer Discov</source>., <volume>10</volume>, <fpage>1826</fpage>–<lpage>1841</lpage>.<pub-id pub-id-type="pmid">33071215</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deng</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2011</year>) <article-title>Investigating the predictability of essential genes across distantly related organisms using an integrative approach</article-title>. <source>Nucleic Acids Res</source>., <volume>39</volume>, <fpage>795</fpage>–<lpage>807</lpage>.<pub-id pub-id-type="pmid">20870748</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dwane</surname><given-names>L.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Project score database: a resource for investigating cancer cell dependencies and prioritizing therapeutic targets</article-title>. <source>Nucleic Acids Res</source>., <volume>49</volume>, <fpage>D1365</fpage>–<lpage>D1372</lpage>.<pub-id pub-id-type="pmid">33068406</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eraslan</surname><given-names>G.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Deep learning: new computational modelling techniques for genomics</article-title>. <source>Nat. Rev. Genet</source>., <volume>20</volume>, <fpage>389</fpage>–<lpage>403</lpage>.<pub-id pub-id-type="pmid">30971806</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gallery</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2007</year>) <article-title>The JAMM motif of human deubiquitinase Poh1 is essential for cell viability</article-title>. <source>Mol. Cancer Ther</source>., <volume>6</volume>, <fpage>262</fpage>–<lpage>268</lpage>.<pub-id pub-id-type="pmid">17237285</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guo</surname><given-names>F.B.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>Accurate prediction of human essential genes using only nucleotide composition and association information</article-title>. <source>Bioinformatics</source>, <volume>33</volume>, <fpage>1758</fpage>–<lpage>1764</lpage>.<pub-id pub-id-type="pmid">28158612</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B11">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Hagberg</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2008</year>) <source>Exploring Network Structure, Dynamics, and Function Using NetworkX</source>. <publisher-name>Los Alamos National Lab. (LANL</publisher-name>), <publisher-loc>Los Alamos, NM (United States</publisher-loc>).</mixed-citation>
    </ref>
    <ref id="btac779-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hasan</surname><given-names>M.A.</given-names></string-name>, <string-name><surname>Lonardi</surname><given-names>S.</given-names></string-name></person-group> (<year>2020</year>) <article-title>DeeplyEssential: a deep neural network for predicting essential genes in microbes</article-title>. <source>BMC Bioinformatics</source>, <volume>21</volume>, <fpage>1</fpage>–<lpage>19</lpage>.<pub-id pub-id-type="pmid">31898485</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hochreiter</surname><given-names>S.</given-names></string-name>, <string-name><surname>Schmidhuber</surname><given-names>J.</given-names></string-name></person-group> (<year>1997</year>) <article-title>Long short-term memory</article-title>. <source>Neural Comput</source>., <volume>9</volume>, <fpage>1735</fpage>–<lpage>1780</lpage>.<pub-id pub-id-type="pmid">9377276</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huttlin</surname><given-names>E.L.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Dual proteome-scale networks reveal cell-specific remodeling of the human interactome</article-title>. <source>Cell</source>, <volume>184</volume>, <fpage>3022</fpage>–<lpage>3040.e28</lpage>.<pub-id pub-id-type="pmid">33961781</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jeong</surname><given-names>H.</given-names></string-name></person-group><etal>et al</etal> (<year>2001</year>) <article-title>Lethality and centrality in protein networks</article-title>. <source>Nature</source>, <volume>411</volume>, <fpage>41</fpage>–<lpage>42</lpage>.<pub-id pub-id-type="pmid">11333967</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ji</surname><given-names>X.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>The essentiality of drug targets: an analysis of current literature and genomic databases</article-title>. <source>Drug Discov. Today</source>, <volume>24</volume>, <fpage>544</fpage>–<lpage>550</lpage>.<pub-id pub-id-type="pmid">30439449</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B17">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kim</surname><given-names>Y.</given-names></string-name></person-group> (<year>2014</year>) Convolutional neural networks for sentence classification. In: <italic toggle="yes">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</italic>, <italic toggle="yes">Doha, Qatar</italic>, pp. <fpage>1746</fpage>–<lpage>1751</lpage>.</mixed-citation>
    </ref>
    <ref id="btac779-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kuang</surname><given-names>S.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Expression-based prediction of human essential genes and candidate lncRNAs in cancer cells</article-title>. <source>Bioinformatics</source>, <volume>37</volume>, <fpage>396</fpage>–<lpage>403</lpage>.<pub-id pub-id-type="pmid">32790840</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lei</surname><given-names>X.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Predicting essential proteins based on RNA-Seq, subcellular localization and GO annotation datasets</article-title>. <source>Knowl. Based Syst</source>., <volume>151</volume>, <fpage>136</fpage>–<lpage>148</lpage>.</mixed-citation>
    </ref>
    <ref id="btac779-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>G.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) <article-title>Predicting essential proteins based on subcellular localization, orthology and PPI networks</article-title>. <source>BMC Bioinformatics</source>, <volume>17</volume>, <fpage>571</fpage>–<lpage>581</lpage>.</mixed-citation>
    </ref>
    <ref id="btac779-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) <article-title>A reliable neighbor-based method for identifying essential proteins by integrating gene expressions, orthology, and subcellular localization information</article-title>. <source>Tsinghua Sci. Technol</source>., <volume>21</volume>, <fpage>668</fpage>–<lpage>677</lpage>.</mixed-citation>
    </ref>
    <ref id="btac779-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2011</year>) <article-title>A local average connectivity-based method for identifying essential proteins from the network level</article-title>. <source>Comput. Biol. Chem</source>., <volume>35</volume>, <fpage>143</fpage>–<lpage>150</lpage>.<pub-id pub-id-type="pmid">21704260</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2014</year>) <article-title>Effective identification of essential proteins based on priori knowledge, network topology and gene expressions</article-title>. <source>Methods</source>, <volume>67</volume>, <fpage>325</fpage>–<lpage>333</lpage>.<pub-id pub-id-type="pmid">24565748</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>W.</given-names></string-name>, <string-name><surname>Godzik</surname><given-names>A.</given-names></string-name></person-group> (<year>2006</year>) <article-title>Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences</article-title>. <source>Bioinformatics</source>, <volume>22</volume>, <fpage>1658</fpage>–<lpage>1659</lpage>.<pub-id pub-id-type="pmid">16731699</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Accurate prediction of human essential proteins using ensemble deep learning</article-title>. <source>IEEE/ACM Trans. Comput. Biol. Bioinform</source>. doi: <pub-id pub-id-type="doi">10.1109/TCBB.2021.3122294</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac779-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>C.-Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2008</year>) <article-title>Hubba: hub objects analyzer - a framework of interactome hubs identification for network biology</article-title>. <source>Nucleic Acids Res</source>., <volume>36</volume>, <fpage>W438</fpage>–<lpage>W443</lpage>.<pub-id pub-id-type="pmid">18503085</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pan</surname><given-names>S.J.</given-names></string-name>, <string-name><surname>Yang</surname><given-names>Q.</given-names></string-name></person-group> (<year>2010</year>) <article-title>A survey on transfer learning</article-title>. <source>IEEE Trans. Knowl. Data Eng</source>., <volume>22</volume>, <fpage>1345</fpage>–<lpage>1359</lpage>.</mixed-citation>
    </ref>
    <ref id="btac779-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peters</surname><given-names>J.M.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) <article-title>A comprehensive, CRISPR-based functional analysis of essential genes in bacteria</article-title>. <source>Cell</source>, <volume>165</volume>, <fpage>1493</fpage>–<lpage>1506</lpage>.<pub-id pub-id-type="pmid">27238023</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pruitt</surname><given-names>K.D.</given-names></string-name></person-group><etal>et al</etal> (<year>2009</year>) <article-title>The consensus coding sequence (CCDS) project: identifying a common protein-coding gene set for the human and mouse genomes</article-title>. <source>Genome Res</source>., <volume>19</volume>, <fpage>1316</fpage>–<lpage>1323</lpage>.<pub-id pub-id-type="pmid">19498102</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rancati</surname><given-names>G.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Emerging and evolving concepts in gene essentiality</article-title>. <source>Nat. Rev. Genet</source>., <volume>19</volume>, <fpage>34</fpage>–<lpage>49</lpage>.<pub-id pub-id-type="pmid">29033457</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sánchez-Ortega</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Role of NRF2 in lung cancer</article-title>. <source>Cells</source>, <volume>10</volume>, <fpage>1879</fpage>.<pub-id pub-id-type="pmid">34440648</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Seringhaus</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2006</year>) <article-title>Predicting essential genes in fungal genomes</article-title>. <source>Genome Res</source>., <volume>16</volume>, <fpage>1126</fpage>–<lpage>1135</lpage>.<pub-id pub-id-type="pmid">16899653</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tang</surname><given-names>X.</given-names></string-name></person-group><etal>et al</etal> (<year>2014</year>) <article-title>Predicting essential proteins based on weighted degree centrality</article-title>. <source>IEEE/ACM Trans. Comput. Biol. Bioinformatics</source>, <volume>11</volume>, <fpage>407</fpage>–<lpage>418</lpage>.</mixed-citation>
    </ref>
    <ref id="btac779-B35">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Vaswani</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) Attention is all you need. In: <italic toggle="yes">Advances in Neural Information Processing Systems</italic>, <italic toggle="yes">Long Beach, CA, USA</italic>, pp. <fpage>6000</fpage>–<lpage>6010</lpage>.</mixed-citation>
    </ref>
    <ref id="btac779-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Verma</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2002</year>) <article-title>Role of Rpn11 metalloprotease in deubiquitination and degradation by the 26 S proteasome</article-title>. <source>Science</source>, <volume>298</volume>, <fpage>611</fpage>–<lpage>615</lpage>.<pub-id pub-id-type="pmid">12183636</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2012</year>) <article-title>Identification of essential proteins based on edge clustering coefficient</article-title>. <source>IEEE/ACM Trans. Comput. Biol. Bioinformatics</source>, <volume>9</volume>, <fpage>1070</fpage>–<lpage>1080</lpage>.</mixed-citation>
    </ref>
    <ref id="btac779-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wauer</surname><given-names>T.</given-names></string-name>, <string-name><surname>Komander</surname><given-names>D.</given-names></string-name></person-group> (<year>2014</year>) <article-title>The JAMM in the proteasome</article-title>. <source>Nat. Struct. Mol. Biol</source>., <volume>21</volume>, <fpage>346</fpage>–<lpage>348</lpage>.<pub-id pub-id-type="pmid">24699083</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>A deep learning framework for identifying essential proteins by integrating multiple types of biological information</article-title>. <source>IEEE/ACM Trans. Comput. Biol. Bioinformatics</source>, <volume>18</volume>, <fpage>296</fpage>–<lpage>305</lpage>.</mixed-citation>
    </ref>
    <ref id="btac779-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Automatic ICD-9 coding via deep transfer learning</article-title>. <source>Neurocomputing</source>, <volume>324</volume>, <fpage>43</fpage>–<lpage>50</lpage>.</mixed-citation>
    </ref>
    <ref id="btac779-B41">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zeng</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) Improving human essential protein prediction using only protein sequences via ensemble learning. In: <italic toggle="yes">2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</italic>, <italic toggle="yes">Houston, TX, USA</italic>, pp. <fpage>98</fpage>–<lpage>103</lpage>.</mixed-citation>
    </ref>
    <ref id="btac779-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Protein-protein interaction site prediction through combining local and global features with deep neural networks</article-title>. <source>Bioinformatics</source>, <volume>36</volume>, <fpage>1114</fpage>–<lpage>1120</lpage>.<pub-id pub-id-type="pmid">31593229</pub-id></mixed-citation>
    </ref>
    <ref id="btac779-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>X.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>DeepHE: accurately predicting human essential genes based on deep learning</article-title>. <source>PLoS Comput. Biol</source>., <volume>16</volume>, <fpage>e1008229</fpage>.<pub-id pub-id-type="pmid">32936825</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
