<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 2?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Biomed Biotechnol</journal-id>
    <journal-id journal-id-type="iso-abbrev">J. Biomed. Biotechnol</journal-id>
    <journal-id journal-id-type="publisher-id">JBB</journal-id>
    <journal-title-group>
      <journal-title>Journal of Biomedicine and Biotechnology</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1110-7243</issn>
    <issn pub-type="epub">1110-7251</issn>
    <publisher>
      <publisher-name>Hindawi Publishing Corporation</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">3364026</article-id>
    <article-id pub-id-type="pmid">22675248</article-id>
    <article-id pub-id-type="doi">10.1155/2012/324249</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Finger Vein Recognition Based on (2D)<sup>2</sup> PCA and Metric Learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Yang</surname>
          <given-names>Gongping</given-names>
        </name>
        <xref ref-type="aff" rid="I1">
</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Xi</surname>
          <given-names>Xiaoming</given-names>
        </name>
        <xref ref-type="aff" rid="I1">
</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yin</surname>
          <given-names>Yilong</given-names>
        </name>
        <xref ref-type="aff" rid="I1">
</xref>
        <xref ref-type="corresp" rid="cor1">*</xref>
      </contrib>
    </contrib-group>
    <aff id="I1">School of Computer Science and Technology, Shandong University, Jinan 250101, China</aff>
    <author-notes>
      <corresp id="cor1">*Yilong Yin: <email>ylyin@sdu.edu.cn</email></corresp>
      <fn fn-type="other">
        <p>Academic Editor: Sabah Mohammed</p>
      </fn>
    </author-notes>
    <pub-date pub-type="ppub">
      <year>2012</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>20</day>
      <month>5</month>
      <year>2012</year>
    </pub-date>
    <volume>2012</volume>
    <elocation-id>324249</elocation-id>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>2</month>
        <year>2012</year>
      </date>
      <date date-type="accepted">
        <day>19</day>
        <month>3</month>
        <year>2012</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2012 Gongping Yang et al.</copyright-statement>
      <copyright-year>2012</copyright-year>
      <license license-type="open-access">
        <license-p>This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Finger vein recognition is a promising biometric recognition technology, which verifies identities via the vein patterns in the fingers. In this paper, (2D)<sup>2</sup> PCA is applied to extract features of finger veins, based on which a new recognition method is proposed in conjunction with metric learning. It learns a KNN classifier for each individual, which is different from the traditional methods where a fixed threshold is employed for all individuals. Besides, the SMOTE technology is adopted to solve the class-imbalance problem. Our experiments show that the proposed method is effective by achieving a recognition rate of 99.17%.</p>
    </abstract>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <title>1. Introduction</title>
    <p>Finger vein recognition is a promising biometric recognition technology which verifies identities through finger vein patterns. Medical studies have shown that the finger vein pattern is unique and stable. In detail, the finger veins of an individual are different from the others', and even the veins captured from a single individual are quite different from one finger to another. Furthermore, the finger veins are also invariant for healthy adults.</p>
    <p>Compared with fingerprints, finger veins are hard to be forged or stolen as they are hidden inside the fingers. The contactless captures of finger veins also ensure both convenience and cleanliness, and they are user-friendly. Furthermore, Finger veins are less affected by physiology and environment factors such as dry skin and dirt.</p>
    <p>A typical finger vein recognition process is composed of the following four steps. Firstly, the finger vein images are obtained via the finger vein capturing devices. Secondly, the finger vein images are preprocessed. Thirdly, the features are extracted. Finally, the finger vein images are matched based on the extracted features.</p>
    <p>The preprocessing procedure includes image enhancement, normalization, and segmentation. For image enhancement, Yang and Yan incorporated directional decomposition and Frangi filtering to enhance the image quality [<xref ref-type="bibr" rid="B1">1</xref>]. Yu et al. proposed an enhancement algorithm based on multi-threshold combination [<xref ref-type="bibr" rid="B2">2</xref>]. Yang and Yang introduced multi-channel Gabor filter to enhance the images and obtained better performance [<xref ref-type="bibr" rid="B3">3</xref>]. Finger vein segmentation is also a very important step, and there are some typical methods including line tracking [<xref ref-type="bibr" rid="B4">4</xref>], mean curvatures [<xref ref-type="bibr" rid="B5">5</xref>], and region growth-based feature [<xref ref-type="bibr" rid="B6">6</xref>]. A detailed description of these approaches is beyond the scope of this paper. However, a summary of these approaches with the typical references is provided in <xref ref-type="table" rid="tab1">Table 1</xref>.</p>
    <p>PCA is a popular linear dimensionality reduction and feature extraction technology. It has extensive applications in image processing. Wu and Liu extracted the PCA features and then trained a neural network for matching, which results in a high recognition rate [<xref ref-type="bibr" rid="B13">7</xref>]. Since PCA transforms the 2-dimensional image matrix to a 1-dimensional vector, the covariance matrix is always large in size and it is time intensive to obtain the projection matrix which is composed of the covariance matrix's eigenvectors. Yang et al. proposed 2DPCA to reduce the size of the covariance matrix and save time for computing projection matrices [<xref ref-type="bibr" rid="B15">8</xref>]. In order to represent the characteristics of the 2-dimensional images more accurately, Zhang and Zhou introduced (2D)<sup>2</sup> PCA which can reflect the information of the image in row and column directions, respectively, use less time to compute the projection matrix, and get better experimental results on face recognition [<xref ref-type="bibr" rid="B16">9</xref>].</p>
    <p>Recently, more and more researchers apply machine learning methods to finger vein recognition. Liu et al. introduced manifold learning to finger vein recognition [<xref ref-type="bibr" rid="B12">10</xref>]. Wu and Liu used PCA and LDA to extract features and train a SVM model for recognition [<xref ref-type="bibr" rid="B14">11</xref>]. Measuring the distance of the two samples is the premise of machine learning. For example, KNN requires a distance metric to find the neighbors of the target instance and then conducts classification or regression based on the distance metric. Typical distance metrics, such as Euclidean distance, make significant contribution in some application domains. In some conditions, these metrics cannot satisfy the assumption that the distances between instances from the same class are small while those from different classes are large. It limits the utilities of most machine learning methods.</p>
    <p>There are two challenges for finger vein recognition: (1) how to efficiently extract distinguishing features and (2) how to design a strong classifier with high recognition rate and fast recognition speed to make the system more practical in real-world applications.</p>
    <p>To overcome these two challenges, in this paper we apply (2D)<sup>2</sup> PCA to extract the features from finger vein images. In order to address the shortcoming of traditional distance-metric-based classifiers, we build a classifier for each individual based on metric learning. With regard to training samples of each classifier, the number of positive samples is inadequate as compared to the negative samples. Thus, we use SMOTE technology to oversample the positive samples to balance the two classes before training the classifier. The experimental results show that the proposed method has good performance on finger vein recognition.</p>
    <p>The rest of this paper is organized as follows. The technical background is briefly introduced in <xref ref-type="sec" rid="sec2">Section 2</xref>. The proposed method is described in <xref ref-type="sec" rid="sec3">Section 3</xref>. Experimental results are provided in <xref ref-type="sec" rid="sec4">Section 4</xref>. Finally, this paper is concluded in <xref ref-type="sec" rid="sec5">Section 5</xref>.</p>
  </sec>
  <sec id="sec2">
    <title>2. Technical Background</title>
    <sec sec-type="subsection" id="sec2.1">
      <title>2.1. (2D)<sup>2</sup> PCA</title>
      <p>PCA is a typical linear dimensionality reduction and feature extraction method. Due to the transformation from the 2-dimensional image matrix into a 1-dimensional column vector, PCA often makes the size of the corresponding covariance matrix too large, and computing the eigenvectors and eigenvalues becomes time-consuming. In order to solve this problem, Yang et al. proposed 2DPCA to extract the features [<xref ref-type="bibr" rid="B15">8</xref>]. 2DPCA directly uses the image matrix to compute PCA features without transforming the 2-dimensional image matrix into a 1-dimensional column vector. Therefore, it reduces the size of corresponding covariance matrix and obtains the feature projection matrix with less time. However, 2DPCA works only for the row direction of images. To address the problem, Zhang and Zhou proposed (2D)<sup>2</sup> PCA which captures the image information from not only the row direction but also the column direction [<xref ref-type="bibr" rid="B16">9</xref>]. The experimental results show that (2D)<sup>2</sup> PCA outperforms 2DPCA and PCA in terms of both recognition rate and running time. The process of (2D)<sup>2</sup> PCA is described as follows.</p>
      <p>Considering <italic>M</italic> finger vein images, which are denoted by <bold>A</bold><sub>1</sub>,…, <bold>A</bold><sub><italic>M</italic></sub>, we compute the mean image matrix as <inline-formula><mml:math id="M1"><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> = (1/<italic>M</italic>)∑<sub><italic>j</italic></sub><bold>A</bold><sub><italic>j</italic></sub> and the image covariance matrix Gas</p>
      <p>
        <disp-formula id="EEq1">
          <label>(1)</label>
          <mml:math id="M2">
            <mml:mi mathvariant="bold">G</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mi>  </mml:mi>
            <mml:mfrac>
              <mml:mrow>
                <mml:mn>1</mml:mn>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>M</mml:mi>
              </mml:mrow>
            </mml:mfrac>
            <mml:munderover>
              <mml:mstyle displaystyle="true">
                <mml:mo>∑</mml:mo>
              </mml:mstyle>
              <mml:mrow>
                <mml:mi>j</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mn>1</mml:mn>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>M</mml:mi>
              </mml:mrow>
            </mml:munderover>
            <mml:mrow>
              <mml:msup>
                <mml:mrow>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi mathvariant="bold">A</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>j</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>−</mml:mo>
                      <mml:mover>
                        <mml:mrow>
                          <mml:mi mathvariant="bold">A</mml:mi>
                        </mml:mrow>
                        <mml:mo>¯</mml:mo>
                      </mml:mover>
                    </mml:mrow>
                    <mml:mo>)</mml:mo>
                  </mml:mrow>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>T</mml:mi>
                </mml:mrow>
              </mml:msup>
              <mml:mrow>
                <mml:mo>(</mml:mo>
                <mml:mrow>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi mathvariant="bold">A</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>j</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>−</mml:mo>
                  <mml:mover>
                    <mml:mrow>
                      <mml:mi mathvariant="bold">A</mml:mi>
                    </mml:mrow>
                    <mml:mo>¯</mml:mo>
                  </mml:mover>
                </mml:mrow>
                <mml:mo>)</mml:mo>
              </mml:mrow>
            </mml:mrow>
            <mml:mo>.</mml:mo>
          </mml:math>
        </disp-formula>
      </p>
      <p>For a random image matrix <bold>A</bold>, the key of obtaining the new features is to get the projection matrix <bold>X</bold> ∈ <italic>R</italic><sup><italic>n</italic>×<italic>d</italic></sup>, <italic>n</italic>≽<italic>d</italic>. Then the new features are calculated as <bold>Y</bold> = <bold>A</bold><bold>X</bold>. The total scatter of the projected samples is used to determine a good projection matrix <bold>X</bold>, where the total scatter of the projected samples can be characterized by the trace of the covariance matrix of the projected feature vectors. From this point of view, we adopt the following criterion:</p>
      <p>
        <disp-formula id="EEq2">
          <label>(2)</label>
          <mml:math id="M3">
            <mml:mi>  </mml:mi>
            <mml:mtable>
              <mml:mtr>
                <mml:mtd columnalign="right">
                  <mml:mi>J</mml:mi>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mrow>
                      <mml:mi mathvariant="bold">X</mml:mi>
                    </mml:mrow>
                    <mml:mo>)</mml:mo>
                  </mml:mrow>
                </mml:mtd>
                <mml:mtd columnalign="left">
                  <mml:mo>=</mml:mo>
                  <mml:mtext>trace</mml:mtext>
                  <mml:mrow>
                    <mml:mo>{</mml:mo>
                    <mml:mrow>
                      <mml:mi>E</mml:mi>
                      <mml:mrow>
                        <mml:mo>[</mml:mo>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:mo>(</mml:mo>
                            <mml:mrow>
                              <mml:mi mathvariant="bold">Y</mml:mi>
                              <mml:mo>−</mml:mo>
                              <mml:mi>E</mml:mi>
                              <mml:mrow>
                                <mml:mo>(</mml:mo>
                                <mml:mrow>
                                  <mml:mi mathvariant="bold">Y</mml:mi>
                                </mml:mrow>
                                <mml:mo>)</mml:mo>
                              </mml:mrow>
                            </mml:mrow>
                            <mml:mo>)</mml:mo>
                          </mml:mrow>
                          <mml:msup>
                            <mml:mrow>
                              <mml:mrow>
                                <mml:mo>(</mml:mo>
                                <mml:mrow>
                                  <mml:mi mathvariant="bold">Y</mml:mi>
                                  <mml:mo>−</mml:mo>
                                  <mml:mi>E</mml:mi>
                                  <mml:mrow>
                                    <mml:mo>(</mml:mo>
                                    <mml:mrow>
                                      <mml:mi mathvariant="bold">Y</mml:mi>
                                    </mml:mrow>
                                    <mml:mo>)</mml:mo>
                                  </mml:mrow>
                                </mml:mrow>
                                <mml:mo>)</mml:mo>
                              </mml:mrow>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>T</mml:mi>
                            </mml:mrow>
                          </mml:msup>
                        </mml:mrow>
                        <mml:mo>]</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                    <mml:mo>}</mml:mo>
                  </mml:mrow>
                  <mml:mi>  </mml:mi>
                </mml:mtd>
              </mml:mtr>
              <mml:mtr>
                <mml:mtd columnalign="right"/>
                <mml:mtd columnalign="left">
                  <mml:mo>=</mml:mo>
                  <mml:mtext>trace</mml:mtext>
                  <mml:mrow>
                    <mml:mo>{</mml:mo>
                    <mml:mrow>
                      <mml:mi>E</mml:mi>
                      <mml:mrow>
                        <mml:mo>[</mml:mo>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:mo>(</mml:mo>
                            <mml:mrow>
                              <mml:mi mathvariant="bold">A</mml:mi>
                              <mml:mi mathvariant="bold">X</mml:mi>
                              <mml:mi>  </mml:mi>
                              <mml:mo>−</mml:mo>
                              <mml:mi>E</mml:mi>
                              <mml:mrow>
                                <mml:mo>(</mml:mo>
                                <mml:mrow>
                                  <mml:mi mathvariant="bold">A</mml:mi>
                                  <mml:mi mathvariant="bold">X</mml:mi>
                                </mml:mrow>
                                <mml:mo>)</mml:mo>
                              </mml:mrow>
                            </mml:mrow>
                            <mml:mo>)</mml:mo>
                          </mml:mrow>
                          <mml:msup>
                            <mml:mrow>
                              <mml:mrow>
                                <mml:mo>(</mml:mo>
                                <mml:mrow>
                                  <mml:mi mathvariant="bold">A</mml:mi>
                                  <mml:mi mathvariant="bold">X</mml:mi>
                                  <mml:mi>  </mml:mi>
                                  <mml:mo>−</mml:mo>
                                  <mml:mi>E</mml:mi>
                                  <mml:mrow>
                                    <mml:mo>(</mml:mo>
                                    <mml:mrow>
                                      <mml:mi mathvariant="bold">A</mml:mi>
                                      <mml:mi mathvariant="bold">X</mml:mi>
                                    </mml:mrow>
                                    <mml:mo>)</mml:mo>
                                  </mml:mrow>
                                </mml:mrow>
                                <mml:mo>)</mml:mo>
                              </mml:mrow>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>T</mml:mi>
                            </mml:mrow>
                          </mml:msup>
                        </mml:mrow>
                        <mml:mo>]</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                    <mml:mo>}</mml:mo>
                  </mml:mrow>
                  <mml:mi>  </mml:mi>
                </mml:mtd>
              </mml:mtr>
              <mml:mtr>
                <mml:mtd columnalign="right"/>
                <mml:mtd columnalign="left">
                  <mml:mo>=</mml:mo>
                  <mml:mtext>trace</mml:mtext>
                  <mml:mrow>
                    <mml:mo>{</mml:mo>
                    <mml:mrow>
                      <mml:msup>
                        <mml:mrow>
                          <mml:mi mathvariant="bold">X</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>T</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                      <mml:mi>E</mml:mi>
                      <mml:mrow>
                        <mml:mo>[</mml:mo>
                        <mml:mrow>
                          <mml:msup>
                            <mml:mrow>
                              <mml:mrow>
                                <mml:mo>(</mml:mo>
                                <mml:mrow>
                                  <mml:mi mathvariant="bold">A</mml:mi>
                                  <mml:mo>−</mml:mo>
                                  <mml:mi>E</mml:mi>
                                  <mml:mrow>
                                    <mml:mo>(</mml:mo>
                                    <mml:mrow>
                                      <mml:mi mathvariant="bold">A</mml:mi>
                                    </mml:mrow>
                                    <mml:mo>)</mml:mo>
                                  </mml:mrow>
                                </mml:mrow>
                                <mml:mo>)</mml:mo>
                              </mml:mrow>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>T</mml:mi>
                            </mml:mrow>
                          </mml:msup>
                          <mml:mrow>
                            <mml:mo>(</mml:mo>
                            <mml:mrow>
                              <mml:mi mathvariant="bold">A</mml:mi>
                              <mml:mo>−</mml:mo>
                              <mml:mi>E</mml:mi>
                              <mml:mrow>
                                <mml:mo>(</mml:mo>
                                <mml:mrow>
                                  <mml:mi mathvariant="bold">A</mml:mi>
                                </mml:mrow>
                                <mml:mo>)</mml:mo>
                              </mml:mrow>
                            </mml:mrow>
                            <mml:mo>)</mml:mo>
                          </mml:mrow>
                        </mml:mrow>
                        <mml:mo>]</mml:mo>
                      </mml:mrow>
                      <mml:mi mathvariant="bold">X</mml:mi>
                    </mml:mrow>
                    <mml:mo>}</mml:mo>
                  </mml:mrow>
                  <mml:mo>.</mml:mo>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
      </p>
      <p>So,</p>
      <p>
        <disp-formula id="EEq3">
          <label>(3)</label>
          <mml:math id="M4">
            <mml:mi>J</mml:mi>
            <mml:mrow>
              <mml:mo>(</mml:mo>
              <mml:mrow>
                <mml:mi mathvariant="bold">X</mml:mi>
              </mml:mrow>
              <mml:mo>)</mml:mo>
            </mml:mrow>
            <mml:mo>=</mml:mo>
            <mml:mtext>trace</mml:mtext>
            <mml:mrow>
              <mml:mo>{</mml:mo>
              <mml:mrow>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi mathvariant="bold">X</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                  </mml:mrow>
                </mml:msup>
                <mml:mi mathvariant="bold">G</mml:mi>
                <mml:mi mathvariant="bold">X</mml:mi>
              </mml:mrow>
              <mml:mo>}</mml:mo>
            </mml:mrow>
            <mml:mo>.</mml:mo>
          </mml:math>
        </disp-formula>
      </p>
      <p>It has been proven that <italic>J</italic>(<bold>X</bold>) gets the maximum when the projection matrix <bold>X</bold> is composed by the <italic>d</italic> orthonormal eigenvectors coupled to the <italic>d</italic> largest eigenvalues. In so saying, <bold>X</bold> obtains the optional value, and <italic>d</italic> can be controlled by setting a threshold as follows:</p>
      <p><disp-formula id="eq1"><label>(4)</label><mml:math id="M5"><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfrac><mml:mo>≽</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula>
where <italic>θ</italic> is a user-specific threshold and <italic>λ</italic><sub>1</sub>, <italic>λ</italic><sub>2</sub>,…, <italic>λ</italic><sub><italic>n</italic></sub> is the top-<italic>n</italic> largest eigenvalues of <bold>G</bold>.</p>
      <p>Because <bold>X</bold> only reflects the information in the row direction, Zhang and Zhou proposed alternative 2DPCA which reflects the information in the column direction and combines 2DPCA with alternative 2DPCA to obtain a new method called (2D)<sup>2</sup> PCA [<xref ref-type="bibr" rid="B16">9</xref>]. Here is the process of alternative 2DPCA.</p>
      <p>Let the image matrix <bold>A</bold><sub><italic>j</italic></sub> = [(<bold>A</bold><sub><italic>j</italic></sub><sup>(1)</sup>)<sup><italic>T</italic></sup>,…,(<bold>A</bold><sub><italic>j</italic></sub><sup>(<italic>n</italic>)</sup>)<sup><italic>T</italic></sup>]<sup><italic>T</italic></sup>, and the mean image matrix <inline-formula><mml:math id="M6"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>̅</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>̅</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>̅</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, where <bold>A</bold><sub><italic>j</italic></sub><sup>(<italic>i</italic>)</sup> and <inline-formula><mml:math id="M7"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, denote the <italic>i</italic>th row vector of <bold>A</bold><sub><italic>j</italic></sub><sup>(<italic>i</italic>)</sup> and <inline-formula><mml:math id="M8"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> respectively. The image covariance matrix can be rewritten as</p>
      <p>
        <disp-formula id="EEq4">
          <label>(5)</label>
          <mml:math id="M9">
            <mml:msub>
              <mml:mrow>
                <mml:mi mathvariant="bold">G</mml:mi>
              </mml:mrow>
              <mml:mrow>
                <mml:mn>1</mml:mn>
              </mml:mrow>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:mn>1</mml:mn>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>M</mml:mi>
              </mml:mrow>
            </mml:mfrac>
            <mml:munderover>
              <mml:mstyle displaystyle="true">
                <mml:mo>∑</mml:mo>
              </mml:mstyle>
              <mml:mrow>
                <mml:mi>j</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mn>1</mml:mn>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>M</mml:mi>
              </mml:mrow>
            </mml:munderover>
            <mml:mrow>
              <mml:mo> </mml:mo>
              <mml:munderover>
                <mml:mstyle displaystyle="true">
                  <mml:mo>∑</mml:mo>
                </mml:mstyle>
                <mml:mrow>
                  <mml:mi>k</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>m</mml:mi>
                </mml:mrow>
              </mml:munderover>
              <mml:mrow>
                <mml:msup>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mrow>
                        <mml:msubsup>
                          <mml:mrow>
                            <mml:mi mathvariant="bold">A</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>j</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mo minsize="0.75em" maxsize="0.75em">(</mml:mo>
                            <mml:mi>k</mml:mi>
                            <mml:mo minsize="0.75em" maxsize="0.75em">)</mml:mo>
                          </mml:mrow>
                        </mml:msubsup>
                        <mml:mo>−</mml:mo>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mover>
                              <mml:mrow>
                                <mml:mi mathvariant="bold">A</mml:mi>
                              </mml:mrow>
                              <mml:mo>¯</mml:mo>
                            </mml:mover>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mo minsize="0.75em" maxsize="0.75em">(</mml:mo>
                            <mml:mi>k</mml:mi>
                            <mml:mo minsize="0.75em" maxsize="0.75em">)</mml:mo>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                      <mml:mo>)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                  </mml:mrow>
                </mml:msup>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi mathvariant="bold">A</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>j</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo minsize="0.75em" maxsize="0.75em">(</mml:mo>
                        <mml:mi>k</mml:mi>
                        <mml:mo minsize="0.75em" maxsize="0.75em">)</mml:mo>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>−</mml:mo>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mover>
                          <mml:mrow>
                            <mml:mi mathvariant="bold">A</mml:mi>
                          </mml:mrow>
                          <mml:mo>¯</mml:mo>
                        </mml:mover>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo minsize="0.75em" maxsize="0.75em">(</mml:mo>
                        <mml:mrow>
                          <mml:mi>k</mml:mi>
                        </mml:mrow>
                        <mml:mo minsize="0.75em" maxsize="0.75em">)</mml:mo>
                      </mml:mrow>
                    </mml:msup>
                  </mml:mrow>
                  <mml:mo>)</mml:mo>
                </mml:mrow>
              </mml:mrow>
            </mml:mrow>
            <mml:mo>.</mml:mo>
          </mml:math>
        </disp-formula>
      </p>
      <p>Similarly, to achieve the projected matrix <bold>X</bold> in 2DPCA, we can obtain the projection matrix <bold>Z</bold> ∈ <italic>R</italic><sup><italic>m</italic>×<italic>q</italic></sup> from (<xref ref-type="disp-formula" rid="EEq2">2</xref>) and (<xref ref-type="disp-formula" rid="EEq4">5</xref>). We can also compute <italic>q</italic> in the same manner as we compute <italic>d</italic> in 2DPCA.</p>
      <p>Using the projected matrix <bold>X</bold>, <bold>Z</bold> in 2DPCA and alternative 2DPCA, respectively, we can obtain the new feature</p>
      <p>
        <disp-formula id="EEq5">
          <label>(6)</label>
          <mml:math id="M10">
            <mml:mi mathvariant="bold">C</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:msup>
              <mml:mrow>
                <mml:mi mathvariant="bold">Z</mml:mi>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>T</mml:mi>
              </mml:mrow>
            </mml:msup>
            <mml:mi mathvariant="bold">A</mml:mi>
            <mml:mi mathvariant="bold">X</mml:mi>
            <mml:mo>.</mml:mo>
          </mml:math>
        </disp-formula>
      </p>
      <p>We can see from (<xref ref-type="disp-formula" rid="EEq5">6</xref>) that the new feature <bold>C</bold> reflects more information of the image than the features obtained by 2DPCA and alternative 2DPCA. Furthermore, the dimension of <bold>C</bold> is smaller, and thus (2D)<sup>2</sup> PCA costs less time than 2DPCA and alternative 2DPCA for image processing.</p>
    </sec>
    <sec sec-type="subsection" id="sec2.2">
      <title>2.2. Metric Learning</title>
      <p>Most machine learning methods use distance metrics to measure the dissimilarity of instances. Metric learning is able to learn an appropriate distance metric. The main task of the metric learning is to find a better distance metric, based on which the distances between the samples from same class become small while those from different classes become large. This helps to improve the performance of the machine learning methods.</p>
      <p>To overcome the shortage of the KNN classifier using Euclidean distance, Weinberge et al. proposed a metric learning method called LMNN (Large Margin Nearest Neighbor) [<xref ref-type="bibr" rid="B17">12</xref>] which learns a distance metric to improve the performance of KNN classifiers. The metric is obtained by learning a linear transformation matrix <bold>L</bold>. With this distance metric, the distance between the same-class instances becomes smaller, and they are separated from the other instances by a large margin. The details are as follows.</p>
      <p>Let <bold>X</bold><sub><italic>i</italic></sub> ∈ <italic>R</italic><sup><italic>d</italic></sup>  (<italic>i</italic> = 1,…, <italic>n</italic>) denote the feature vector of training instances and let <italic>y</italic><sub><italic>i</italic></sub> be the corresponding label. The essence of LMNN is to obtain a new distance <italic>D</italic>(<italic>x</italic><sub><italic>i</italic></sub>, <italic>x</italic><sub><italic>j</italic></sub>) = ||<bold>L</bold>(<italic>x</italic><sub><italic>i</italic></sub>−<italic>x</italic><sub><italic>j</italic></sub>)||<sup>2</sup> = (<italic>x</italic><sub><italic>i</italic></sub>−<italic>x</italic><sub><italic>j</italic></sub>)<sup><italic>T</italic></sup><bold>L</bold><sup><italic>T</italic></sup><bold>L</bold>(<italic>x</italic><sub><italic>i</italic></sub> − <italic>x</italic><sub><italic>j</italic></sub>) after learning a linear transformation <bold>L</bold> matrix. With this distance metric, the distance between the instance and its <italic>k</italic> nearest neighbors will be minimized and the distance between the instances in different classes will be larger. <xref ref-type="fig" rid="fig1"> Figure 1</xref> shows an example of LMNN.</p>
      <p>In <xref ref-type="fig" rid="fig1">Figure 1</xref>, green circles denote instances from the first class, yellow squares denote instances from the second class, and red squares denote instances from the third class. Consider the instance denoted by the white circle, which is treated as a test instance from the first class, in our following analysis. Based on Euclidean distance, we find 4 nearest neighbors, and this test instance is misclassified into the second class. However, using the LMNN-learned metric; this instance is separated from the second and the third instances. The distance between this instance and its neighbor is small. Now it is correctly classified into the first class.</p>
    </sec>
    <sec sec-type="subsection" id="sec2.3">
      <title>2.3. SMOTE</title>
      <p>The performance of machine learning algorithms is typically evaluated by prediction accuracy. However, this is not applicable when the data is imbalanced. Existing solutions to the class imbalance problem can be divided into two categories. One is to assign distinct costs to training examples. The other is to resample the original dataset, either by oversampling the minority class and/or undersampling the majority class. </p>
      <p>
Chawla et al. proposed an oversampling approach called SMOTE where the minority class is oversampled by creating “synthetic” examples [<xref ref-type="bibr" rid="B18">17</xref>]. The minority class is oversampled by taking each minority class sample and introducing synthetic examples along the line segments joining any/all of the <italic>k</italic> minority class nearest neighbors. Depending upon the amount of oversampling required, neighbors from the <italic>k</italic> nearest neighbors are randomly chosen.</p>
    </sec>
  </sec>
  <sec id="sec3">
    <title>3. The Proposed Method</title>
    <p>The proposed method includes training process and recognition process. As shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>, a classifier is built for each individual, and the samples from a certain individual are treated as positive and others are negative. In the verification mode, we input a test sample to corresponding classifier to verify whether the sample comes from this individual based on the classification result. In the identification mode, we input a test sample to every classifier and identify which individual this sample belongs to.</p>
    <p>In the training process, it is necessary to preprocess the infrared images of the finger veins. Preprocessing includes grayscale, ROI selection, and normalization (e.g., size normalization and gray normalization). After the preprocessing, we apply (2D)<sup>2</sup> PCA to extract the features of the training samples. Then we label the samples as positive and negative class accordingly and oversample the positive samples with SMOTE. We learn a new distance metric, that is, the transformation matrix L, with LMNN. Finally, we build the individual KNN classifier based on this new distance metric.</p>
    <p>The preprocessing and feature extraction in the recognition process are similar to that in the training process. After that, we input the features of the samples to train classifier to verify the individual based on the classification result.</p>
    <sec sec-type="subsection" id="sec3.1">
      <title>3.1. Preprocessing</title>
      <p>The preprocessing includes image grayscale, ROI selection, size normalization, and gray normalization.</p>
      <sec sec-type="subsubsection" id="sec3.1.1">
        <title>3.1.1. Image Grayscale</title>
        <p>The original image (an example is shown in <xref ref-type="fig" rid="fig3">Figure 3(a)</xref>) is a 24-bit color image with a size of 320 × 240. In order to reduce the computational complexity, we transform the original image to an 8-bit image based on the gray-scale equation <italic>Y</italic> = <italic>R</italic>  ∗  0.299 + <italic>G</italic>  ∗  0.588 + <italic>B</italic>  ∗  0.114, where <italic>R</italic>, <italic>G</italic>, and <italic>B</italic> denote the value of red, green, and blue. These three color components are coded by 8 bits. <italic>Y</italic> is the value of pixel after gray-scale transformation.</p>
      </sec>
      <sec sec-type="subsubsection" id="sec3.1.2">
        <title>3.1.2. ROI Selection</title>
        <p>As the background of finger vein region might include noise, we employ an edge-detection method to segment the finger vein region from the gray-scale image. A Sobel operator with a 3 × 3 mask <inline-formula><mml:math id="M11"><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>2</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> is used for detecting the edges of fingers. The width of the finger region can be obtained based on the maximum and minimum abscissa values of the finger profile, and the height of the finger region is similarly detected. A rectangle region can be captured based on the width and height. This rectangle region is called ROI (as shown in <xref ref-type="fig" rid="fig3">Figure 3(b)</xref>).</p>
      </sec>
      <sec sec-type="subsubsection" id="sec3.1.3">
        <title>3.1.3. Size Normalization</title>
        <p>The size of the selected ROI is different from image to image due to personal factors such as different finger size and changing location. Therefore it is necessary to normalize the ROI region to the same size before the feature extraction process by (2D)<sup>2</sup> PCA. We use bilinear interpolation for size normalization in this paper, and the size of the normalized ROI is set to be 96  ∗  64 (as shown in <xref ref-type="fig" rid="fig3">Figure 3(c)</xref>).</p>
      </sec>
      <sec sec-type="subsubsection" id="sec3.1.4">
        <title>3.1.4. Gray Normalization</title>
        <p>In order to extract efficient features, gray normalization is used to obtain a uniform gray distribution (as shown in <xref ref-type="fig" rid="fig3">Figure 3(d)</xref>). Formally, we have</p>
        <p>
          <disp-formula id="eq2">
            <label>(7)</label>
            <mml:math id="M12">
              <mml:mi>p</mml:mi>
              <mml:mrow>
                <mml:mo>(</mml:mo>
                <mml:mrow>
                  <mml:mi>i</mml:mi>
                  <mml:mo>,</mml:mo>
                  <mml:mi>j</mml:mi>
                </mml:mrow>
                <mml:mo>)</mml:mo>
              </mml:mrow>
              <mml:mo>=</mml:mo>
              <mml:mfrac>
                <mml:mrow>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mi>p</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>′</mml:mi>
                    </mml:mrow>
                  </mml:msup>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:mi>j</mml:mi>
                    </mml:mrow>
                    <mml:mo>)</mml:mo>
                  </mml:mrow>
                  <mml:mo>−</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>G</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
                <mml:mrow>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>G</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mn>2</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>−</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>G</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
              </mml:mfrac>
              <mml:mo>,</mml:mo>
            </mml:math>
          </disp-formula>
        </p>
        <p>
where  <italic>p</italic>′(<italic>i</italic>, <italic>j</italic>) is the pixel value of the original image, <italic>G</italic><sub>1</sub> is the min pixel value of original image, <italic>G</italic><sub>2</sub> is the max pixel value of original image, and <italic>p</italic>(<italic>i</italic>, <italic>j</italic>) is the pixel value of image after gray normalization.</p>
      </sec>
    </sec>
    <sec sec-type="subsection" id="sec3.2">
      <title>3.2. Training Process</title>
      <p>After the preprocessing, we extract the features for each image by (2D)<sup>2</sup> PCA and assign labels for them. A classifier is trained for every individual, where the samples belonging to this individual are treated as positive and others are negative. We oversample the positive samples by SMOTE to obtain an augmented training set which achieves class balance in general. LMNN is then used on the augmented training set to obtain a transformation matrix <bold>L</bold>. With this new distance metric, a KNN classifier is built for classification.</p>
    </sec>
    <sec sec-type="subsection" id="sec3.3">
      <title>3.3. Recognition Process</title>
      <p>In the verification mode, we input the feature vector of a test sample to a classifier which represents a certain individual, and then we verify whether the sample belongs to this individual based on the classification result. In the identification mode, we employ all classifiers to classify the test sample. If only a classifier C classifies it as positive class, this sample belongs to the individual which corresponds to the classifier C. If there are many classifiers classifying the sample as positive class, then we use the training accuracy rate for decision making: the sample belongs to the individual that corresponds to the classifier with the best training accuracy.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <title>4. Experimental Result and Analysis</title>
    <sec sec-type="subsection" id="sec4.1">
      <title>4.1. Database</title>
      <p>The experiments were conducted using our finger vein database which is collected from 80 individuals' (including 64 males and 16 females, Asian race) index fingers of right hand, where each index finger contributes 18 finger vein images. Each individual participated in two sessions, separated by two weeks (14 days). The age of the participants was between 19 and 60 years, and their occupations included university students, professors, and workers at our school. The capture device was manufactured by the Joint Lab for Intelligent Computing and Intelligent System of Wuhan University, China, which is illustrated in <xref ref-type="fig" rid="fig4">Figure 4</xref>. </p>
      <p>The original spatial resolution of the data is 320 × 240. After ROI extraction and size normalization, the size of the region used for feature extraction is reduced to 96 × 64. Samples collected from the same finger belong to the same class. Therefore, there are 80 classes, where each class contains 18 samples in our database. Some typical finger vein images are shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p>
    </sec>
    <sec sec-type="subsection" id="sec4.2">
      <title>4.2. Experimental Settings</title>
      <p>All the experiments are implemented with MATLAB and conducted on a machine with 2.4 G CPU and 4 G memory. </p>
      <p>We design three experiments to verify the efficiency of the proposed method. In <xref ref-type="statement" rid="expe1">Experiment 1</xref>, we extract features by (2D)<sup>2</sup> PCA and then compare the classification performance of the metric-learning-based method and the classic Euclidean-distance-based method. In <xref ref-type="statement" rid="expe2">Experiment 2</xref>, we compare the classification performance of KNN classifier combined with LMNN using different number of training samples. In <xref ref-type="statement" rid="expe3">Experiment 3</xref>, we employ the SMOTE technology to further boost the performance.</p>
      <p>
        <statement id="expe1">
          <title>Experiment 1</title>
          <p>In this experiment, we first generate four data sets as follows. We select 480, 720, 960, and 1200 images (i.e., 6, 9, 12, and 15 images for each individual) for training, and the rest of 960, 720, 480, and 240 images (i.e., 12, 9, 6, and 3 images for each individual) are left for testing, respectively. The Euclidean-distance-based recognition method works in the following way. We treat the training samples from each individual as the positive class and construct a center point for each class, where the <italic>i</italic>th feature of the center point is calculated by averaging the corresponding feature values of the training samples. As there are 80 individuals, we then obtain 80 center point <inline-formula><mml:math id="M13"><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>  </mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>80</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>. For any testing sample <bold>C</bold>, we estimate the Euclidean distances from sample <bold>C</bold> to each center point, <inline-formula><mml:math id="M14"><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">C</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">||</mml:mo><mml:mrow><mml:mi mathvariant="bold">C</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">||</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>  </mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>80</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>. If <inline-formula><mml:math id="M15"><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">C</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">C</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>(<italic>i</italic> = 1,2,…, 80), then <bold>C</bold> goes to the <italic>j</italic>th class.</p>
          <p>The metric-learning-based method works similarly as the Euclidean-distance-based method except for the usage of the learned distance metric <inline-formula><mml:math id="M16"><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">C</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">||</mml:mo><mml:mrow><mml:mi mathvariant="bold">L</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">C</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">||</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>  </mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>80</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>. The recognition rates of these two methods are compared in <xref ref-type="table" rid="tab2">Table 2</xref>.</p>
          <p>
It is clearly seen that the recognition rate of the metric-learning-based method is higher than the Euclidean-distance-based method. With distance metric transformation, two samples from different classes with small Euclidean distance are dragged farther. On the other hand, two samples from the same class with large Euclidean distance are pulled closer. Furthermore, the samples from different classes are separated by a large margin. Next we are going to provide an intuitive explanation based on the example shown in Figures <xref ref-type="fig" rid="fig6">6</xref> and <xref ref-type="fig" rid="fig7">7</xref>.</p>
          <p>These two figures show the data distribution of the data set with 480 training samples and 960 testing samples. We obtain 25 features for each sample using (2D)<sup>2</sup> PCA and select 2 features with the largest contribution to Euclidean distance metric. These two features constitute the vertical and horizontal coordinates of <xref ref-type="fig" rid="fig6">Figure 6</xref>. Similarly, these two features are transformed to a new metric space using LMNN, as shown in <xref ref-type="fig" rid="fig7">Figure 7</xref>. The samples of the first individual (including 6 training images and 12 testing images) are treated as positive, and the rest of them are considered as negative. In Figures <xref ref-type="fig" rid="fig6">6</xref> and <xref ref-type="fig" rid="fig7">7</xref>, we use red plus to denote positive training samples, green plus for positive testing samples, blue star for negative training samples, and yellow star for negative testing samples. It is shown from <xref ref-type="fig" rid="fig6">Figure 6</xref> that it is difficult to distinguish the first class from the others because the distances between samples in the first class and the other classes are indiscriminating. This inherent drawback of the Euclidean distance significantly reduces the recognition performance. However, by using LMNN, the samples in the first class are gathered together, as shown in <xref ref-type="fig" rid="fig7">Figure 7</xref>. In detail, the positive samples are located mainly in the area of abscissa value between 0 and 10. On the contrary, most negative samples are scattered out. This makes it easier to discriminate samples in the first class from samples in the other classes.</p>
        </statement>
      </p>
      <p>
        <statement id="expe2">
          <title>Experiment 2</title>
          <p>In this experiment, we select 6, 9, 12, and 15 images from each individual as training samples to build a KNN classifier. The underlying distance metric for each individual is learned by LMNN. Here the number of neighbors, that is, <italic>k</italic>, is empirically set to be 3 in KNN. We obtain different recognition rates with different numbers of training samples, and the experimental results are shown in <xref ref-type="fig" rid="fig8">Figure 8</xref>.</p>
          <p>Overall, the recognition rate increases with the number of training images increases. When the number of the training images goes to 15, the recognition rate reaches 96.67%. It is also worth noting that, as compared to <xref ref-type="table" rid="tab2">Table 2</xref>, the KNN-based method outperforms the above-mentioned metric-learning-based method and the Euclidean-distance-based method, by considering the same number of training images.</p>
        </statement>
      </p>
      <p>
        <statement id="expe3">
          <title>Experiment 3</title>
          <p> This experiment verifies that SMOTE can improve the classification performance. We select 1200 images (15 images for each individual) for training and 240 images (3 images for each individual) for testing. We use SMOTE to oversample the positive samples to be 5, 10, 20, 30, 40, and 50 times as large as the original set. The recognition result is shown in <xref ref-type="table" rid="tab3">Table 3</xref>. We observe that the recognition rate does not improve by only increasing a small number of synthetic positive samples, as shown in SMOTE-5 and SMOTE-10. After that, the recognition rate increases by about 3%, and finally it achieves 99.17% with SMOTE-40 or SMOTE-50. With a sufficiently large set of synthetic positive samples, the recognition performance would not improve any more.</p>
        </statement>
      </p>
    </sec>
  </sec>
  <sec id="sec5">
    <title>5. Conclusion</title>
    <p>This paper proposes a new finger vein recognition method based on (2D)<sup>2</sup> PCA and metric learning. Firstly, we extract features by (2D)<sup>2</sup> PCA and then train a binary classifier for each individual based on metric learning. Furthermore, we address the class imbalance problem by using SMOTE oversampling before the classifier is trained. The experimental results show that the proposed method achieves a recognition rate of 99.17%. The contributions of this paper are as follows. (1) We apply (2D)<sup>2</sup> PCA to extract features of finger vein image, where (2D)<sup>2</sup> PCA reflects the information in both the row direction and the column direction, and it is more efficient for feature extraction as compared to PCA and 2DPCA. (2) We build the KNN classifier based on metric learning using LMNN which changes the sample distribution in the new metric space. LMNN makes the distance between the samples from the same class smaller and the distance between the samples from different classes larger. Furthermore, we also employ a maximum margin framework to improve the recognition performance. This is incorporated with individually trained classifiers which reflect the characteristics of the corresponding individuals. (3) We note the class-imbalance problem; that is, when building the classifier for an individual, the number of the samples from the other individuals is considerably large. We tackle it by oversampling the positive samples with SMOTE, and the experimental results validate the effectiveness. Promising future work includes the exploration of features with better discrimination as well as the processing finger vein images of low quality.</p>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgments</title>
    <p>This work is supported by National Natural Science Foundation of China under Grant nos. 61173069 and 61070097, and the Research Fund for the Doctoral Program of Higher Education under Grant no. 20100131110021. The authors would like to thank Shuaiqiang Wang and Guang-Tong Zhou for their helpful comments and constructive advice on structuring the paper. In addition, the authors would particularly like to thank the anonymous reviewers for their helpful suggestions.</p>
  </ack>
  <ref-list>
    <ref id="B1">
      <label>1</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>An improved method for finger-vein image enhancement</article-title>
        <conf-name>In: Proceedings of the IEEE 10th International Conference on Signal Processing (ICSP '10)</conf-name>
        <conf-date>October 2010</conf-date>
        <conf-loc>Beijing, China</conf-loc>
        <fpage>1706</fpage>
        <lpage>1709</lpage>
      </element-citation>
    </ref>
    <ref id="B2">
      <label>2</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>C-B</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>D-M</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>H-B</given-names>
          </name>
        </person-group>
        <article-title>Finger vein image enhancement based on multi-threshold fuzzy algorithm</article-title>
        <conf-name>In: Proceedings of the 2nd International Congress on Image and Signal Processing (CISP '09)</conf-name>
        <conf-date>October 2009</conf-date>
        <conf-loc>Tianjin, China</conf-loc>
        <fpage>1</fpage>
        <lpage>3</lpage>
      </element-citation>
    </ref>
    <ref id="B3">
      <label>3</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>JF</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>JL</given-names>
          </name>
        </person-group>
        <article-title>Multi-channel gabor filter design for finger vein image enhancement</article-title>
        <conf-name>In: Proceedings of the 5th International Conference on Image and Graphics (ICIG '09)</conf-name>
        <conf-date>September 2009</conf-date>
        <conf-loc>Xi’an, China</conf-loc>
        <fpage>87</fpage>
        <lpage>91</lpage>
      </element-citation>
    </ref>
    <ref id="B4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Miura</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Nagasaka</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Miyatake</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Feature extraction of finger-vein patterns based on repeated line tracking and its application to personal identification</article-title>
        <source>
          <italic>Machine Vision and Applications</italic>
        </source>
        <year>2004</year>
        <volume>15</volume>
        <issue>4</issue>
        <fpage>194</fpage>
        <lpage>203</lpage>
      </element-citation>
    </ref>
    <ref id="B5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Song</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>HC</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>JH</given-names>
          </name>
          <name>
            <surname>Kong</surname>
            <given-names>HJ</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>SR</given-names>
          </name>
        </person-group>
        <article-title>A finger-vein verification system using mean curvature</article-title>
        <source>
          <italic>Pattern Recognition Letters</italic>
        </source>
        <year>2011</year>
        <volume>32</volume>
        <issue>11</issue>
        <fpage>1541</fpage>
        <lpage>1547</lpage>
      </element-citation>
    </ref>
    <ref id="B6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huafeng</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Lan</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Chengbo</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Region growth-based feature extraction method for finger vein recognition</article-title>
        <source>
          <italic>Optical Engineering</italic>
        </source>
        <year>2011</year>
        <volume>50</volume>
        <issue>2</issue>
        <fpage>281</fpage>
        <lpage>307</lpage>
      </element-citation>
    </ref>
    <ref id="B13">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>CT</given-names>
          </name>
        </person-group>
        <article-title>Finger-vein pattern identification using principal component analysis and the neural network technique</article-title>
        <source>
          <italic>Expert Systems with Applications</italic>
        </source>
        <year>2011</year>
        <volume>38</volume>
        <issue>5</issue>
        <fpage>5423</fpage>
        <lpage>5427</lpage>
      </element-citation>
    </ref>
    <ref id="B15">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Frangi</surname>
            <given-names>AF</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>JY</given-names>
          </name>
        </person-group>
        <article-title>Two-dimensional PCA: a new approach to appearance-based face representation and recognition</article-title>
        <source>
          <italic>IEEE Transactions on Pattern Analysis and Machine Intelligence</italic>
        </source>
        <year>2004</year>
        <volume>26</volume>
        <issue>1</issue>
        <fpage>131</fpage>
        <lpage>137</lpage>
        <pub-id pub-id-type="pmid">15382693</pub-id>
      </element-citation>
    </ref>
    <ref id="B16">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>ZH</given-names>
          </name>
        </person-group>
        <article-title>(2D)<sup>2</sup> PCA: two-directional two-dimensional PCA for efficient face representation and recognition</article-title>
        <source>
          <italic>Neurocomputing</italic>
        </source>
        <year>2005</year>
        <volume>69</volume>
        <issue>1–3</issue>
        <fpage>224</fpage>
        <lpage>231</lpage>
      </element-citation>
    </ref>
    <ref id="B12">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>Finger vein recognition with manifold learning</article-title>
        <source>
          <italic>Journal of Network and Computer Applications</italic>
        </source>
        <year>2010</year>
        <volume>33</volume>
        <issue>3</issue>
        <fpage>275</fpage>
        <lpage>282</lpage>
      </element-citation>
    </ref>
    <ref id="B14">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>CT</given-names>
          </name>
        </person-group>
        <article-title>Finger-vein pattern identification using SVM and neural network technique</article-title>
        <source>
          <italic>Expert Systems with Applications</italic>
        </source>
        <year>2011</year>
        <volume>38</volume>
        <issue>11</issue>
        <fpage>14284</fpage>
        <lpage>14289</lpage>
      </element-citation>
    </ref>
    <ref id="B17">
      <label>12</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Weinberge</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Blitzer</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Saul</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Distance metric learning for large margin nearest neighbor classification</article-title>
        <conf-name>In: Proceedings of the Advances in Neural Information Processing Systems (NIPS '06)</conf-name>
        <conf-date>2006</conf-date>
      </element-citation>
    </ref>
    <ref id="B7">
      <label>13</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Dai</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Finger-vein authentication based on wide line detector and pattern normalization</article-title>
        <conf-name>In: Proceedings of the 20th International Conference on Pattern Recognition (ICPR '10)</conf-name>
        <conf-date>August 2010</conf-date>
        <conf-loc>Istanbul, Turkey</conf-loc>
        <fpage>1269</fpage>
        <lpage>1272</lpage>
      </element-citation>
    </ref>
    <ref id="B8">
      <label>14</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Efficient finger vein localization and recognition</article-title>
        <conf-name>In: Proceedings of the 20th International Conference on Pattern Recognition (ICPR '10)</conf-name>
        <conf-date>August 2010</conf-date>
        <conf-loc>Istanbul, Turkey</conf-loc>
        <fpage>1148</fpage>
        <lpage>1151</lpage>
      </element-citation>
    </ref>
    <ref id="B10">
      <label>15</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Qian</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhong</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Shao</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Finger-vein recognition based on the score level moment invariants fusion</article-title>
        <conf-name>In: Proceedings of the International Conference on Computational Intelligence and Software Engineering (CiSE '09)</conf-name>
        <conf-date>December 2009</conf-date>
        <conf-loc>Wuhan, China</conf-loc>
        <fpage>1</fpage>
        <lpage>4</lpage>
      </element-citation>
    </ref>
    <ref id="B11">
      <label>16</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Liukui</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Hong</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Finger vein image recognition based on tri-value template fuzzy matching</article-title>
        <conf-name>In: Proceedings of the 9th WSEAS International Conference on Multimedia Systems and Signal Processing (MUSP '09)</conf-name>
        <conf-date>May 2009</conf-date>
        <conf-loc>Hangzhou, China</conf-loc>
        <fpage>206</fpage>
        <lpage>211</lpage>
      </element-citation>
    </ref>
    <ref id="B18">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chawla</surname>
            <given-names>NV</given-names>
          </name>
          <name>
            <surname>Bowyer</surname>
            <given-names>KW</given-names>
          </name>
          <name>
            <surname>Hall</surname>
            <given-names>LO</given-names>
          </name>
          <name>
            <surname>Kegelmeyer</surname>
            <given-names>WP</given-names>
          </name>
        </person-group>
        <article-title>SMOTE: synthetic minority over-sampling technique</article-title>
        <source>
          <italic>Journal of Artificial Intelligence Research</italic>
        </source>
        <year>2002</year>
        <volume>16</volume>
        <fpage>321</fpage>
        <lpage>357</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="fig1" position="float">
    <label>Figure 1</label>
    <caption>
      <p>An example of LMNN.</p>
    </caption>
    <graphic xlink:href="JBB2012-324249.001"/>
  </fig>
  <fig id="fig2" position="float">
    <label>Figure 2</label>
    <caption>
      <p>The proposed framework for finger vein recognition.</p>
    </caption>
    <graphic xlink:href="JBB2012-324249.002"/>
  </fig>
  <fig id="fig3" position="float">
    <label>Figure 3</label>
    <caption>
      <p>Examples of preprocessing.</p>
    </caption>
    <graphic xlink:href="JBB2012-324249.003"/>
  </fig>
  <fig id="fig4" position="float">
    <label>Figure 4</label>
    <caption>
      <p>The finger vein capture device.</p>
    </caption>
    <graphic xlink:href="JBB2012-324249.004"/>
  </fig>
  <fig id="fig5" position="float">
    <label>Figure 5</label>
    <caption>
      <p>Sample finger vein images.</p>
    </caption>
    <graphic xlink:href="JBB2012-324249.005"/>
  </fig>
  <fig id="fig6" position="float">
    <label>Figure 6</label>
    <caption>
      <p>Samples distribution with Euclidean distance metric.</p>
    </caption>
    <graphic xlink:href="JBB2012-324249.006"/>
  </fig>
  <fig id="fig7" position="float">
    <label>Figure 7</label>
    <caption>
      <p>Samples distribution with new distance metric using LMNN.</p>
    </caption>
    <graphic xlink:href="JBB2012-324249.007"/>
  </fig>
  <fig id="fig8" position="float">
    <label>Figure 8</label>
    <caption>
      <p>The recognition rates with different numbers of training images.</p>
    </caption>
    <graphic xlink:href="JBB2012-324249.008"/>
  </fig>
  <table-wrap id="tab1" position="float">
    <label>Table 1</label>
    <caption>
      <p>Methods for personal authentication using finger vein recognition.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="left" rowspan="1" colspan="1">References</th>
          <th align="center" rowspan="1" colspan="1">Method</th>
          <th align="center" rowspan="1" colspan="1">Database fingers × samples per each</th>
          <th align="left" rowspan="1" colspan="1">Performance</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" rowspan="1" colspan="1">[<xref ref-type="bibr" rid="B4">4</xref>]</td>
          <td align="center" rowspan="1" colspan="1">Linetracking</td>
          <td align="center" rowspan="1" colspan="1">339 × 2 images</td>
          <td align="left" rowspan="1" colspan="1">EER: 0.145%</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">[<xref ref-type="bibr" rid="B5">5</xref>]</td>
          <td align="center" rowspan="1" colspan="1">Mean curvature</td>
          <td align="center" rowspan="1" colspan="1">125 × 9 images</td>
          <td align="left" rowspan="1" colspan="1">EER: 0.25%</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">[<xref ref-type="bibr" rid="B7">13</xref>]</td>
          <td align="center" rowspan="1" colspan="1">Wide line detector</td>
          <td align="center" rowspan="1" colspan="1">10,140 × 5 images</td>
          <td align="left" rowspan="1" colspan="1">EER: 0.87%</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">[<xref ref-type="bibr" rid="B8">14</xref>]</td>
          <td align="center" rowspan="1" colspan="1">Statistical vein energy</td>
          <td align="center" rowspan="1" colspan="1">100 × 10 images</td>
          <td align="left" rowspan="1" colspan="1">CCR: 98.7%</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">[<xref ref-type="bibr" rid="B10">15</xref>]</td>
          <td align="center" rowspan="1" colspan="1">Moment invariants</td>
          <td align="center" rowspan="1" colspan="1">50 × 4 images</td>
          <td align="left" rowspan="1" colspan="1">EER: 8.93%</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">[<xref ref-type="bibr" rid="B11">16</xref>]</td>
          <td align="center" rowspan="1" colspan="1">Sliding window matching</td>
          <td align="center" rowspan="1" colspan="1">76 × 6 images</td>
          <td align="left" rowspan="1" colspan="1">EER: 0.54%</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">[<xref ref-type="bibr" rid="B12">10</xref>]</td>
          <td align="center" rowspan="1" colspan="1">Manifold learning</td>
          <td align="center" rowspan="1" colspan="1">164 × 70 images</td>
          <td align="left" rowspan="1" colspan="1">EER: 0.8%</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">[<xref ref-type="bibr" rid="B13">7</xref>]</td>
          <td align="center" rowspan="1" colspan="1">PCA + BP network</td>
          <td align="center" rowspan="1" colspan="1">10 × 10 images</td>
          <td align="left" rowspan="1" colspan="1">CCR: 99%</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">[<xref ref-type="bibr" rid="B14">11</xref>]</td>
          <td align="center" rowspan="1" colspan="1">PCA + LDA + SVM</td>
          <td align="center" rowspan="1" colspan="1">10 × 10 images</td>
          <td align="left" rowspan="1" colspan="1">CCR: 98%</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="tab2" position="float">
    <label>Table 2</label>
    <caption>
      <p>The recognition rates of the compared methods.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="left" rowspan="1" colspan="1">
</th>
          <th align="center" rowspan="1" colspan="1">Euclidean-distance-based method</th>
          <th align="center" rowspan="1" colspan="1">Metric-learning-based method</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" rowspan="1" colspan="1">480 training, 960 testing</td>
          <td align="center" rowspan="1" colspan="1">78.96%</td>
          <td align="center" rowspan="1" colspan="1">86.46%</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">720 training, 720 testing</td>
          <td align="center" rowspan="1" colspan="1">82.08%</td>
          <td align="center" rowspan="1" colspan="1">91.25%</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">960 training, 480 testing</td>
          <td align="center" rowspan="1" colspan="1">86.25%</td>
          <td align="center" rowspan="1" colspan="1">92.29%</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">1200 training, 240 testing</td>
          <td align="center" rowspan="1" colspan="1">84.58%</td>
          <td align="center" rowspan="1" colspan="1">93.75%</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="tab3" position="float">
    <label>Table 3</label>
    <caption>
      <p>The recognition rate by SMOTE.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <tbody>
        <tr>
          <td align="left" rowspan="1" colspan="1">Without SMOTE</td>
          <td align="center" rowspan="1" colspan="1">96.67%</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">SMOTE-5</td>
          <td align="center" rowspan="1" colspan="1">96.67%</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">SMOTE-10</td>
          <td align="center" rowspan="1" colspan="1">96.67%</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">SMOTE-20</td>
          <td align="center" rowspan="1" colspan="1">98.75%</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">SMOTE-30</td>
          <td align="center" rowspan="1" colspan="1">98.33%</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">SMOTE-40</td>
          <td align="center" rowspan="1" colspan="1">99.17%</td>
        </tr>
        <tr>
          <td align="left" rowspan="1" colspan="1">SMOTE-50</td>
          <td align="center" rowspan="1" colspan="1">99.17%</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
