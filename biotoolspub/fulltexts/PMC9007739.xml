<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Nat Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nat Methods</journal-id>
    <journal-title-group>
      <journal-title>Nature Methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1548-7091</issn>
    <issn pub-type="epub">1548-7105</issn>
    <publisher>
      <publisher-name>Nature Publishing Group US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9007739</article-id>
    <article-id pub-id-type="pmid">35414125</article-id>
    <article-id pub-id-type="publisher-id">1443</article-id>
    <article-id pub-id-type="doi">10.1038/s41592-022-01443-0</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Multi-animal pose estimation, identification and tracking with DeepLabCut</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Lauer</surname>
          <given-names>Jessy</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhou</surname>
          <given-names>Mu</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ye</surname>
          <given-names>Shaokai</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Menegas</surname>
          <given-names>William</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Schneider</surname>
          <given-names>Steffen</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2092-7159</contrib-id>
        <name>
          <surname>Nath</surname>
          <given-names>Tanmay</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Rahman</surname>
          <given-names>Mohammed Mostafizur</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Di Santo</surname>
          <given-names>Valentina</given-names>
        </name>
        <xref ref-type="aff" rid="Aff6">6</xref>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9099-7294</contrib-id>
        <name>
          <surname>Soberanes</surname>
          <given-names>Daniel</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8021-277X</contrib-id>
        <name>
          <surname>Feng</surname>
          <given-names>Guoping</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2443-4252</contrib-id>
        <name>
          <surname>Murthy</surname>
          <given-names>Venkatesh N.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lauder</surname>
          <given-names>George</given-names>
        </name>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dulac</surname>
          <given-names>Catherine</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7368-4456</contrib-id>
        <name>
          <surname>Mathis</surname>
          <given-names>Mackenzie Weygandt</given-names>
        </name>
        <address>
          <email>mackenzie@post.harvard.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3777-2202</contrib-id>
        <name>
          <surname>Mathis</surname>
          <given-names>Alexander</given-names>
        </name>
        <address>
          <email>alexander.mathis@epfl.ch</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.5333.6</institution-id><institution-id institution-id-type="ISNI">0000000121839049</institution-id><institution>Brain Mind Institute, School of Life Sciences, Swiss Federal Institute of Technology (EPFL), </institution></institution-wrap>Lausanne, Switzerland </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.38142.3c</institution-id><institution-id institution-id-type="ISNI">000000041936754X</institution-id><institution>Rowland Institute at Harvard, Harvard University, </institution></institution-wrap>Cambridge, MA USA </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.116068.8</institution-id><institution-id institution-id-type="ISNI">0000 0001 2341 2786</institution-id><institution>Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, </institution><institution>Massachusetts Institute of Technology, </institution></institution-wrap>Cambridge, MA USA </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.38142.3c</institution-id><institution-id institution-id-type="ISNI">000000041936754X</institution-id><institution>Department for Molecular Biology and Center for Brain Science, </institution><institution>Harvard University, </institution></institution-wrap>Cambridge, MA USA </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.413575.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 2167 1581</institution-id><institution>Howard Hughes Medical Institute (HHMI), </institution></institution-wrap>Chevy Chase, MD USA </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.38142.3c</institution-id><institution-id institution-id-type="ISNI">000000041936754X</institution-id><institution>Department of Organismic and Evolutionary Biology, </institution><institution>Harvard University, </institution></institution-wrap>Cambridge, MA USA </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="GRID">grid.10548.38</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 9377</institution-id><institution>Department of Zoology, </institution><institution>Stockholm University, </institution></institution-wrap>Stockholm, Sweden </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>12</day>
      <month>4</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>12</day>
      <month>4</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2022</year>
    </pub-date>
    <volume>19</volume>
    <issue>4</issue>
    <fpage>496</fpage>
    <lpage>504</lpage>
    <history>
      <date date-type="received">
        <day>23</day>
        <month>4</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>4</day>
        <month>3</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Estimating the pose of multiple animals is a challenging computer vision problem: frequent interactions cause occlusions and complicate the association of detected keypoints to the correct individuals, as well as having highly similar looking animals that interact more closely than in typical multi-human scenarios. To take up this challenge, we build on DeepLabCut, an open-source pose estimation toolbox, and provide high-performance animal assembly and tracking—features required for multi-animal scenarios. Furthermore, we integrate the ability to predict an animal’s identity to assist tracking (in case of occlusions). We illustrate the power of this framework with four datasets varying in complexity, which we release to serve as a benchmark for future algorithm development.</p>
    </abstract>
    <abstract id="Abs2" abstract-type="web-summary">
      <p id="Par2">DeepLabCut is extended to enable multi-animal pose estimation, animal identification and tracking, thereby enabling the analysis of social behaviors.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Machine learning</kwd>
      <kwd>Computational neuroscience</kwd>
      <kwd>Zoology</kwd>
      <kwd>Behavioural methods</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/100009835</institution-id>
            <institution>Harvard University | Rowland Institute at Harvard (Rowland Institute)</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/100009152</institution-id>
            <institution>Fondation Bertarelli (Bertarelli Foundation)</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s), under exclusive licence to Springer Nature America, Inc. 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Main</title>
    <p id="Par3">Advances in sensor and transmitter technology, data mining and computational analysis herald a golden age of animal tracking across the globe<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Computer vision is a crucial tool for identifying, counting, as well as annotating animal behavior<sup><xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR4">4</xref></sup>. For the computational analysis of fine-grained behavior, pose estimation is often a crucial step and deep-learning based tools have quickly affected neuroscience, ethology and medicine<sup><xref ref-type="bibr" rid="CR5">5</xref>–<xref ref-type="bibr" rid="CR8">8</xref></sup>.</p>
    <p id="Par4">Many experiments in biology—from parenting mice to fish schooling—require measuring interactions among multiple individuals. Multi-animal pose estimation raises several challenges that can leverage advances in machine vision research, and yet others that need new solutions. In general, the process requires three steps: pose estimation (that is, keypoint localization), assembly (that is, the task of grouping keypoints into distinct animals) and tracking. Each step presents different challenges.</p>
    <p id="Par5">To make pose estimation robust to interacting and occluded animals, one should annotate frames with closely interacting animals. To associate detected keypoints to particular individuals (assembly) several solutions have been proposed, such as part affinity fields (PAFs)<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, associative embeddings<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>, transformers<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> and other mechanisms<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR14">14</xref></sup>. Tracking animals between frames can be difficult because of appearance similarity, nonstationary behaviors and possible occlusions. Building on human pose estimation research, some packages for multi-animal pose estimation have emerged<sup><xref ref-type="bibr" rid="CR15">15</xref>–<xref ref-type="bibr" rid="CR17">17</xref></sup>. Here, we developed top-performing network architectures, a data-driven assembly method, engineered tailored tracking methods and compared the current state-of-the-art networks on COCO (common objects in context)<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> on four animal datasets.</p>
    <p id="Par6">Specifically, we expanded DeepLabCut<sup><xref ref-type="bibr" rid="CR19">19</xref>–<xref ref-type="bibr" rid="CR21">21</xref></sup>, an open-source toolbox for animal pose estimation. Our contributions are as follows:<list list-type="order"><list-item><p id="Par7">Four datasets of varying difficulty for benchmarking multi-animal pose estimation networks.</p></list-item><list-item><p id="Par8">Multi-task architecture that predicts multiple conditional random fields and therefore can predict keypoints, limbs, as well as animal identity.</p></list-item><list-item><p id="Par9">A data-driven method for animal assembly that finds the optimal skeleton without user input, and that is state of the art (compared to top-models from COCO, a standard computer vision benchmark).</p></list-item><list-item><p id="Par10">A module that casts tracking as a network flow optimization problem, which aims to find globally optimal solutions.</p></list-item><list-item><p id="Par11">Unsupervised animal ID tracking: we can predict the identity of animals and reidentify them; this is particularly useful to link animals across time when temporally based tracking fails (due to intermittent occlusions).</p></list-item><list-item><p id="Par12">Graphical user interfaces (GUIs) for keypoint annotation, refinement and semiautomatic trajectory verification.</p></list-item></list></p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <p id="Par13">Multi-animal pose estimation can be cast as a data assignment problem in the spatial and temporal domains. To tackle the generic multi-animal pose-tracking scenario, we designed a practical, almost entirely data-driven solution that breaks down the larger goal into the smaller subtasks of: keypoint estimation, animal assembly (spatially grouping keypoints into individuals), local (temporal) tracking and global ‘tracklet’ stitching (Extended Data Fig. <xref rid="Fig6" ref-type="fig">1)</xref>. We evaluate our pipeline on four new datasets that we release with this paper as a benchmark at <ext-link ext-link-type="uri" xlink:href="https://benchmark.deeplabcut.org/">https://benchmark.deeplabcut.org/</ext-link>.</p>
    <sec id="Sec3">
      <title>Four diverse multi-animal datasets</title>
      <p id="Par14">We considered four multi-animal experiments to broadly validate our approach: three mice in an open field, home-cage parenting in mice, pairs of marmosets housed in a large enclosure and 14 fish in a flow tank. These datasets encompass a wide range of behaviors, presenting difficult and unique computational challenges to pose estimation and tracking (Fig. <xref rid="Fig1" ref-type="fig">1a</xref> and Extended Data Fig. <xref rid="Fig7" ref-type="fig">2)</xref>. The three mice frequently contact and occlude one another. The parenting dataset contained a single adult mouse with unique keypoints in close interaction with two pups hardly distinguishable from the background or the cotton nest, which also leads to occlusions. The marmoset dataset comprises periods of occlusion, close interactions, nonstationary behavior, motion blur and changes in scale. Likewise, the fish school along all dimensions of the tank, hiding each other in cluttered scenes, and occasionally leaving the camera’s field of view. We annotated 5–15 body parts of interest depending on the dataset (Fig. <xref rid="Fig1" ref-type="fig">1a</xref> and Extended Data Fig. <xref rid="Fig6" ref-type="fig">1)</xref>, in multiple frames for cross-validating the pose estimation and assembly performance, as well as semiautomatically annotated several videos for evaluating the tracking performance (Table <xref rid="Tab1" ref-type="table">1</xref>). For analyses, we created a random split of images plus annotations into 70% train and 30% test sets.<fig id="Fig1"><label>Fig. 1</label><caption><title>Multi-animal DeepLabCut architecture and benchmarking datasets.</title><p><bold>a</bold>, Example (cropped) images with (manual) annotations for the four datasets: mice in an open field arena, parenting mice, pairs of marmosets and schooling fish. bpts, body parts. Scale bars, 20 pixels. <bold>b</bold>, A schematic of the general pose estimation module. The architecture is trained to predict the keypoint locations, PAFs and animal identity. Three output layers per keypoint predict the probability that a joint is in a particular pixel (score map) as well as shifts in relation to the discretized output map (location refinement field). Furthermore, PAFs predict vector fields encoding the orientation of a connection between two keypoints. Example predictions are overlaid on the corresponding (cropped) marmoset frame. The PAF for the right limb helps linking the right hand and shoulder keypoints to the correct individual. <bold>c</bold>, Our architecture contains a multi-fusion module and a multi-stage decoder. In the multi-fusion module, we add the high-resolution representation (conv2, conv3) to low-resolution representation (conv5). The features from conv2 and conv3 are downsampled by two and one 3 × 3 convolution layer, respectively to match the resolution of conv5. Before concatenation the features are downsampled by a 1 × 1 convolution layer to reduce computational costs and (spatially) upsampled by two stacked 3 × 3 deconvolution layers with stride 2. The multi-stage decoder predicts score maps and PAFs. At the first stage, the feature map from the multi-fusion module are upsampled by a 3 × 3 deconvolution layer with stride 2, to get the score map, PAF and the upsampled feature. In the latter stages, the predictions from the two branches (score maps and PAFs), along with the upsampled feature are concatenated for the next stage. We applied a shortcut connection between the consecutive stage of the score map. The shown variant of DLCRNet has overall stride 2 (in general, this can be modulated from 2 to 8).</p></caption><graphic xlink:href="41592_2022_1443_Fig1_HTML" id="d32e578"/></fig><table-wrap id="Tab1"><label>Table 1</label><caption><p>Multi-animal pose estimation dataset characteristics</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Feature</th><th>Mouse</th><th>Pups</th><th>Marmosets</th><th>Fish</th></tr></thead><tbody><tr><td>Labeled frames</td><td>161</td><td>542</td><td>7,600</td><td>100</td></tr><tr><td>Keypoints</td><td>12</td><td>5 (+12)</td><td>15</td><td>5</td></tr><tr><td>Individuals</td><td>3</td><td>2 (+1)</td><td>2</td><td>14</td></tr><tr><td>GT identity</td><td>No</td><td>No</td><td>Yes</td><td>No</td></tr><tr><td>Annotated video frames</td><td>11,645</td><td>2,670</td><td>15,000</td><td>1,100</td></tr><tr><td>Total duration (s)</td><td>385</td><td>180</td><td>600</td><td>36</td></tr></tbody></table><table-wrap-foot><p>Number of labeled training frames, keypoints and individuals. Keypoint number in brackets relate to the unique animal in the frame, and unique individual in brackets is noted, that is, one parenting mouse. Animal identity was only annotated for the marmosets. For tracking, separate videos are used and the total number of densely human-annotated video frames (and their combined duration in seconds) is also indicated.</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec4">
      <title>Multi-task convolutional architectures</title>
      <p id="Par15">We developed multi-task convolutional neural networks (CNNs) that perform pose estimation by localizing keypoints in images. This is achieved by predicting score maps, which encode the probability that a keypoint occurs at a particular location, as well as location refinement fields that predict offsets to mitigate quantization errors due to downsampled score maps<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR20">20</xref></sup>. Then, to assemble keypoints into the grouping that defines an animal, we designed the networks to also predict ‘limbs’, that is, PAFs. This task, which is achieved via additional deconvolution layers, is inspired by OpenPose<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. The intuition is that in scenarios where multiple animals are present in the scene, learning to predict the location and orientation of limbs will help group pairs of keypoints belonging to an individual. Moreover, we also introduce an output that allows for animal reidentification (reID) from visual input directly. This is important in the event of animals that are untrackable using temporal information alone, for example, when exiting or re-entering the scene (Fig. <xref rid="Fig1" ref-type="fig">1b</xref>).</p>
      <p id="Par16">Specifically, we adapted ImageNet-pretrained ResNets<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, EfficientNets<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>, as well as developed a multi-scale architecture (which we call DLCRNet_ms5, Fig. <xref rid="Fig1" ref-type="fig">1c</xref>). We then use customized multiple parallel deconvolution layers to predict the location of keypoints as well as what keypoints are connected in a given animal (Fig. <xref rid="Fig1" ref-type="fig">1b</xref>). Ground truth data of annotated keypoints are used to calculate target score maps, location refinement maps, PAFs and to train the network to predict those outputs for a given input image (Fig. <xref rid="Fig1" ref-type="fig">1b,c</xref>) with augmentation.</p>
    </sec>
    <sec id="Sec5">
      <title>Keypoint detection and part affinity performance</title>
      <p id="Par17">After an extensive architecture search (<ext-link ext-link-type="uri" xlink:href="http://maDLCopt.deeplabcut.org">http://maDLCopt.deeplabcut.org</ext-link> and Extended Data Fig. <xref rid="Fig8" ref-type="fig">3)</xref>, we demonstrate that the new DLCRNet performs very well for localizing keypoints (Fig. <xref rid="Fig2" ref-type="fig">2a</xref>). Specifically, we trained independent networks for each dataset, and each split, and evaluated their performance. For each frame and keypoint, we calculated the root-mean squared error (r.m.s.e.) between the detections and their closest ground truth neighbors. All the keypoint detectors performed well (DLCRNet_ms5, median test errors of 2.65, 5.25, 4.59 and 2.72 pixels for the tri-mouse, parenting, marmoset and fish datasets, respectively, Fig. <xref rid="Fig2" ref-type="fig">2a</xref>). The scales of these data are shown in Fig. <xref rid="Fig1" ref-type="fig">1a</xref>). To ease interpretation, errors were also normalized to 33% of the tip–gill distance for the fish dataset and 33% of the left-to-right ear distance for the remaining ones (<xref rid="Sec12" ref-type="sec">Methods</xref>). We found that 93.6 ± 6.9% of the predictions on the test images were within those ranges (Fig. <xref rid="Fig2" ref-type="fig">2a</xref>).<fig id="Fig2"><label>Fig. 2</label><caption><title>Multi-animal DeepLabCut keypoint detection and whole-body assembly performance.</title><p><bold>a</bold>, Distribution of keypoint prediction error for DLCRNet_ms5 with stride 8 (70% train and 30% test split). Violin plots display train (top) and test (bottom) errors. Vertical dotted lines are the first, second and third quartiles. Median test errors were 2.69, 5.62, 4.65 and 2.80 pixels for the illustrated datasets, in order. Gray numbers indicate PCK. Only the first five keypoints of the parenting dataset belong to the pups; the 12 others are keypoints of the adult mouse. <bold>b</bold>, Illustration of our data-driven skeleton selection algorithm. Mouse cartoon adapted with permission from ref. <sup><xref ref-type="bibr" rid="CR29">29</xref></sup> under a Creative Commons licence (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>). <bold>c</bold>, Animal assembly quality as a function of part affinity graph (skeleton) size for baseline (user-defined) versus data-driven skeleton definitions. The top row displays the fraction of keypoints left unconnected after assembly, whereas the bottom row designates the accuracy of their grouping into distinct animals. The colored dots mark statistically significant interactions (two-way, repeated-measures ANOVA; see Supplementary Tables <xref rid="MOESM1" ref-type="media">1</xref>–<xref rid="MOESM1" ref-type="media">4</xref> for full statistics). Light red vertical bars highlight the graph automatically selected. <bold>d</bold>, mAP as a function of graph size. Shown on test data held out from 70% train and 30% test splits. The associative embedding method does not rely on a graph. The performance of MMPose’s implementation of ResNet-AE and HRNet-AE bottom-up variants is shown for comparison against our multi-stage architecture DLCRNet_ms5, here called Baseline. Data-driven is Baseline plus calibration method (one-way ANOVA show significant effects of the model: <italic>P</italic> values, tri-mouse 8.8 × 10<sup>−8</sup>, pups 6.5 × 10<sup>−13</sup>, marmosets 3.8 × 10<sup>−11</sup>, fish 4.0 × 10<sup>−12</sup>). <bold>e</bold>, Marmoset ID–Example test image together with overlaid animal identity prediction accuracy per keypoint averaged over all test images and test splits. With ResNet50_stride8, accuracy peaks at 99.2% for keypoints near the head and drops to only 95.1% for more distal parts. In the lower panel, plus signs denote individual splits, circles show the averages.</p></caption><graphic xlink:href="41592_2022_1443_Fig2_HTML" id="d32e838"/></fig></p>
      <p id="Par18">After detection, keypoints need to be assigned to individuals. We evaluated whether the learned PAFs helped decide whether two body parts belong to the same or different animals. For example, 66 different edges can be formed from the 12 mouse body parts and many provide high discriminability (Extended Data Fig. <xref rid="Fig9" ref-type="fig">4</xref>). We indeed found that predicted limbs were powerful at distinguishing a pair of keypoints belonging to an animal from other (incorrect) pairs linking different mice, as measured by a high auROC (area under the receiver operating characteristic) score (mean ± s.d. 0.99 ± 0.02).</p>
    </sec>
    <sec id="Sec6">
      <title>Data-driven individual assembly performance</title>
      <p id="Par19">Any limb-based assembly approach requires a ‘skeleton’, that is, a list of keypoint connections that allows the algorithm to computationally infer which body parts belong together. Naturally, there has to be a path within this skeleton connecting any two body parts, otherwise the body parts cannot be grouped into one animal. Given the combinatorial nature of skeletons, how should they be designed? We circumvented the need for arbitrary, hand-crafted skeletons by developing a method that is agnostic to an animal’s morphology and does not require any user input.</p>
      <p id="Par20">We devised a data-driven method where the network is first trained to predict all graph edges and the least discriminative edges (for deciding body part ownership) are not used at test time to determine the optimal skeleton. We found that this approach yields skeletons with fewer errors (unconnected body parts and with higher purity, Fig. <xref rid="Fig2" ref-type="fig">2b,c</xref>) and it improves performance. Crucially, it means users do not need to design any skeletons. Our data-driven method (with DLCRNet_ms5) outperforms the naive (baseline) method, enhances ‘purity of the assembly’: that is, the fraction of keypoints that were grouped correctly per individual (Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>), and reduces the number of missing keypoints (Supplementary Table <xref rid="MOESM1" ref-type="media">2</xref>). Comparisons revealed significantly higher assembly purity with automatic skeleton pruning versus a naive skeleton definition at most graph sizes, with respective gains of up to 3.0, 2.0 and 2.4 percentage points in the tri-mouse (two-way repeated measure analyses of variance (ANOVA): graph size 23; <italic>P</italic> &lt; 0.001), marmosets (graph size 34, <italic>P</italic> = 0.002) and fish datasets (graph size 6, <italic>P</italic> &lt; 0.001) (Fig. <xref rid="Fig2" ref-type="fig">2b,c</xref>). Furthermore, to accommodate diverse body plans and annotated keypoints for different animals and experiments, our inference algorithm works for arbitrary graphs. Animal assembly achieves at least 400 frames per second in scenes with 14 animals, and up to 2,000 for small skeletons in two or three animals (Extended Data Fig. <xref rid="Fig10" ref-type="fig">5</xref>).</p>
      <p id="Par21">To additionally benchmark our network and assembly contributions, we compared them to methods that achieve state-of-the art performance on COCO<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, a challenging, large-scale multi-human pose estimation benchmark. Specifically, we considered HRNet-AE and ResNet-AE. Our models performed significantly better than these state-of-the-art methods (one-way ANOVA: <italic>P</italic> values, tri-mouse 8.8 × 10<sup>−08</sup>, pups 6.5 × 10<sup>−13</sup>, marmosets 3.8 × 10<sup>−11</sup>, fish 4.0 × 10<sup>−12</sup>, Fig. <xref rid="Fig2" ref-type="fig">2d</xref>) on all four animal benchmark datasets. Last, while the datasets themselves contain diverse animal behaviors, and only 70% is used to train, as an additional test of generalization we used ten held-out marmoset videos that came from different cages (Extended Data Fig. <xref rid="Fig11" ref-type="fig">6</xref>). We find in this challenging test there is a roughly 0.25 drop in mean average precision (mAP). It is known that simply adding (a fraction of the new) data into the training set alleviates such drops (reviewed in ref. <sup><xref ref-type="bibr" rid="CR7">7</xref></sup>).</p>
      <p id="Par22">We reasoned the strong multi-animal performance is due to the assembly algorithm based on PAFs. Therefore, we tested the performance of the network in a top-down setting with and without PAFs, that is, by considering images that are cropped around each animal (bounding boxes, Extended Data Fig. <xref rid="Fig7" ref-type="fig">7a</xref>). We found that our assembly algorithm significantly improves mAP performance (PAF versus without PAF one-way ANOVA <italic>P</italic>, tri-mouse 4.656 × 10<sup>−11</sup>, pups 3.62 × 10<sup>−12</sup>, marmosets 1.33 × 10<sup>−28</sup>, fish 1.645 × 10<sup>−6</sup>, Extended Data Fig. <xref rid="Fig7" ref-type="fig">7b,c</xref>). Collectively, the direct assembly to tracking (that is, the bottom-up method) is likely the optimal approach for most users as it reasons over the whole image.</p>
    </sec>
    <sec id="Sec7">
      <title>Predicting animal identity from images</title>
      <p id="Par23">Animals sometimes differ visually, for example due to distinct coat patterns, because they are marked, or carry different instruments (such as an integrated microscope<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>). To allow our method to take advantage of such scenarios and improve tracking later on, we developed a network head that learns the identity (ID) of animals with the same CNN backbone. To benchmark the ID output, we focused on the marmoset data, where (for each pair) one marmoset had light blue dye applied to its tufts. ID prediction accuracy on the test images ranged from &gt;0.99 for the keypoints closest to the marmoset’s head to 0.95 for more distal keypoints (Fig. <xref rid="Fig2" ref-type="fig">2e</xref> and Extended Data Fig. <xref rid="Fig8" ref-type="fig">3c</xref>). Thus, DeepLabCut can reID the animal on a per-body-part basis (Fig. <xref rid="Fig2" ref-type="fig">2e</xref>).</p>
    </sec>
    <sec id="Sec8">
      <title>Tracking of individuals</title>
      <p id="Par24">Once keypoints are assembled into individual animals, the next step is to link them temporally. To measure performance in the next steps, entire videos (one from each dataset) are manually refined to form ground truth sequences (Fig. <xref rid="Fig3" ref-type="fig">3a</xref> and Table <xref rid="Tab1" ref-type="table">1</xref>). Reasoning over the whole video for tracking individuals is not only computationally costly, but also unnecessary. For instance, when animals are far apart, it is straightforward to link each one correctly across time. Thus, we devised a divide-and-conquer strategy. We use a simple, online tracking approach to form reliable ‘tracklets’ from detected animals in adjacent frames. Difficult cases (for example, when animals are closely interacting or after occlusion) often interrupt the tracklets, causing ambiguous fragments that cannot be easily temporally linked. We address this crucial issue post hoc by optimally stitching tracklets using multiple spatio-temporal cues.<fig id="Fig3"><label>Fig. 3</label><caption><title>Linking whole-body assemblies across time.</title><p><bold>a</bold>, Ground truth and reconstructed animal tracks (with DLCRNet and ellipse tracking), together with video frames illustrating representative scene challenges. <bold>b</bold>, The identities of animals detected in a frame are propagated across frames using local matching between detections and trackers (with costs, ‘motion’ for all datsets and ‘distance’ for fish). <bold>c</bold>, Tracklets are represented as nodes of a graph, whose edges encode the likelihood that the connected pair of tracklet belongs to the same track. <bold>d</bold>, Four cost functions modeling the affinity between tracklets are implemented: shape similarity using the undirected Hausdorff distance between finite sets of keypoints (i); spatial proximity in Euclidean space (ii); motion affinity using bidirectional prediction of a tracklet’s location (iii); and dynamic similarity via Hankelets and time-delay embedding of a tracklet’s centroid (iv). <bold>e</bold>, Tracklet stitching performance versus box and ellipse tracker baselines (arrows indicate if higher or lower number is better), using MOTA, as well as rates of false negative (FN), false positives (FP) and identity switch expressed in events per animal and per sequence of 100 frames. Inset shows that incorporating appearance/identity prediction in the stitching further reduces the number of switches and improves full track reconstruction. Total number of frames: tri-mouse, 2,330; parenting, 2,670; marmosets, 15,000 and fish, 601.</p></caption><graphic xlink:href="41592_2022_1443_Fig3_HTML" id="d32e985"/></fig></p>
      <p id="Par25">Assembled animals are linked across frames to form tracklets, that is, fragments of continuous trajectories. This task entails the propagation of an animal’s identity in time by finding the optimal association between an animal and its predicted location in the adjacent frame (Fig. <xref rid="Fig3" ref-type="fig">3b–d</xref>). The prediction is made by a lightweight ‘tracker’. In particular, we implemented a box and an ellipse tracker. Whereas the former is standard in the object tracking literature (for example, refs. <sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup>), we recognized the sensitivity of its formulation to outlier detections (as it is mostly used for pedestrian tracking). Thus, the ellipse tracker was developed to provide a finer parametrization of an animal’s geometry. Overall, the ellipse tracker behaves better than the box tracker, reaching near-perfect multi-object tracking accuracy (MOTA) (0.78 versus 0.97) and producing on average 92% less false negatives; no differences in the switch rate was observed (Fig. <xref rid="Fig3" ref-type="fig">3e</xref>).</p>
      <p id="Par26">Because of occlusions, dissimilarity between an animal and its predicted state, or other challenging yet common multi-animal tracking issues, tracklets can be interrupted and therefore rarely form complete tracks across a video. The remaining challenge therefore is to stitch these sparse tracklets so as to guarantee continuity and kinematic consistency. Our approach is to cast this task as a global minimization problem, where connecting two candidate tracklets incurs a cost inversely proportional to the likelihood that they belong to the same track. Advantageously, the problem can now be elegantly solved using optimization techniques on graph and affinity models (Fig. <xref rid="Fig3" ref-type="fig">3c,d</xref>).</p>
      <p id="Par27">Compared to only local tracking, we find that our stitching method reduces switches, even in the challenging fish and marmosets datasets (average reduction compared to local ellipse tracking, 63%; Fig. <xref rid="Fig3" ref-type="fig">3e</xref>). To handle a wide range of scenarios, multiple cost functions are devised to model the affinity between a pair of tracklets based on their shape, proximity, motion, dynamics and/or appearance (below and Supplementary Videos <xref rid="MOESM3" ref-type="media">1</xref>–<xref rid="MOESM6" ref-type="media">4)</xref>. Last, to allow users to understand the error rate and correct errors, we developed a Refine Tracklets GUI. Here, we leverage confidence of the tracking to flag sequences of frames that might need attention, namely when swaps might occur (Extended Data Fig. <xref rid="Fig6" ref-type="fig">1b</xref>).</p>
      <p id="Par28">Other recent methods for tracking animals have been proposed, such as idtracker.ai<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. While this tool does not perform pose estimation, we wanted to specifically compare tracking performance. We attempted to use the easiest (tri-mouse) and marked-animal (marmoset) datasets with idtracker.ai. After an extensive grid search for hyperparameters, only the tri-mouse mice dataset could be reliably tracked, yet the performance of our method was significantly better (one-sided, one-sample <italic>t</italic>-tests indicated that idtracker performed significantly worse than DeepLabCut in both datasets (tri-mouse <italic>t</italic> = −11.03, <italic>P</italic> = 0.0008, <italic>d</italic> = 5.52; marmosets <italic>t</italic> = −8.43, <italic>P</italic> = 0.0018, <italic>d</italic> = 4.22: Supplementary Video <xref rid="MOESM7" ref-type="media">5</xref> and Extended Data Fig. <xref rid="Fig8" ref-type="fig">8)</xref>.</p>
      <p id="Par29">Note, for keypoint selection we remain fully agnostic to the user-defined inputs, giving the user freedom over what keypoints ultimately serve their research, but we do guide the user by showing them how such decisions could affect performance (Extended Data Fig. <xref rid="Fig14" ref-type="fig">9</xref>).</p>
    </sec>
    <sec id="Sec9">
      <title>Leveraging animal ID and reID in tracking</title>
      <p id="Par30">When animals can disappear from the field of view, they cannot be tracked by temporal association alone and appearance cues are necessary. Indeed, for the marmosets, incorporating visual appearance learned in a supervised fashion, further reduced the number of switches by 26% (Fig. <xref rid="Fig3" ref-type="fig">3e</xref>). Additionally, we next considered the case with animals that are not clearly distinguishable to the human annotator, thus no ground truth can be easily provided. To tackle this challenge, we introduce an unsupervised method way based on transformers to learn animal ID via metric learning (Fig. <xref rid="Fig4" ref-type="fig">4a–c</xref> and <xref rid="Sec12" ref-type="sec">Methods</xref>). This provides up to a 10% boost in MOTA performance in the very challenging fish data, particularly in difficult sequences (Fig. <xref rid="Fig4" ref-type="fig">4d</xref>).<fig id="Fig4"><label>Fig. 4</label><caption><title>Unsupervised reID of animals.</title><p><bold>a</bold>, Schematic of the transformer architecture we adapted to take pose-tensor outputs of the DeepLabCut backbone. We trained it with triplets sampled from tracklets and tracks. <bold>b</bold>, Performance of the ReIDTransformer method on unmarked fish, mice and marked marmosets. Triplet accuracy (acc.) is reported for triplets sampled from ground truth (GT) tracks and local tracklets only. We used only the top 5% of the most crowded frames, as those are the most challenging. <bold>c</bold>, Example performance on the challenging fish data. Top: fish-identity-colored tracks. Time is given in frame number. Bottom: example frames (early versus later) from baseline or ReIDTransformer. Arrows highlight performance with ReIDTransformer: pink arrows show misses; orange shows correct ID across frames in ReIDTransformer versus blue to orange in baseline. <bold>d</bold>, Tracking metrics on the most crowded 5% of frames (30 frames for fish, 744 for marmosets, giving 420 fish targets and 1,488 marmoset targets); computed as described in <xref rid="Sec12" ref-type="sec">Methods</xref>. IDF1, ID measure, global min-cost F1 score; IDP, ID measure, global min-cost precision; IDR, ID measures: global min-cost recall; Recall, number of detections over number of objects; Precision, number of detected objects over sum of detected and false positives; GT, number of unique objects; MT, mostly tracked and FM, number of fragmentations.</p></caption><graphic xlink:href="41592_2022_1443_Fig4_HTML" id="d32e1100"/></fig></p>
    </sec>
    <sec id="Sec10">
      <title>Social marmosets</title>
      <p id="Par31">Finally, we demonstrate a use-case of multi-animal DeepLabCut by analyzing 9 h (824,568 frames) of home-cage behavior of pairs of marmosets (Fig. <xref rid="Fig5" ref-type="fig">5a,b</xref>). We tracked by ReID on a frame-by-frame basis versus only using tracklet information. We found that the marmosets display diverse postures that are captured by principal component analysis on egocentrically aligned poses (Fig. <xref rid="Fig5" ref-type="fig">5c,d</xref>). Furthermore, we found that when the animals are close, their bodies tend to be aligned and they tend to look in similar directions (Fig. <xref rid="Fig5" ref-type="fig">5e,f</xref>). Finally, we related the posture and the spatial relationship between the animals and found a nonrandom distribution. For instance, marmosets tended to face the other animal when apart (Fig. <xref rid="Fig5" ref-type="fig">5g,h</xref>). Thus, DeepLabCut can be used to study complex social interactions over long timescales.<fig id="Fig5"><label>Fig. 5</label><caption><title>Application to multi-marmoset social behaviors.</title><p><bold>a</bold>, Schematic of the marmoset recording setup. <bold>b</bold>, Example tracks, 30 min plotted from each marmoset. Scale bars, 0.2 m. <bold>c</bold>, Example egocentric posture data, where the ‘Body2’ point is (0,0) and the angle formed by ‘Body1’ and ‘Body3’ is rotated to 0°. We performed principal component analysis on the pooled data of both marmosets for all data. <bold>d</bold>, Average postures along each principal component; note that only one side of the distribution is represented in the image (that is, 0 to 2 instead of −2 to 2). <bold>e</bold>, Histogram of log-distance between a pair of marmosets normalized to ear-center distance. <bold>f</bold>, Computed body angle versus observation count. <bold>g</bold>, Density plot of where another marmoset is located relative to marmoset 1. <bold>h</bold>, Postural principal components (from <bold>d</bold>) as a function of the relative location of the other marmoset. Thereby, each point represents the average postural component score for marmoset 1 when marmoset 2 is at that point. h.u., head units.</p></caption><graphic xlink:href="41592_2022_1443_Fig5_HTML" id="d32e1155"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec11" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par32">Here we introduced a multi-animal pose estimation and tracking system thereby extending DeepLabCut<sup><xref ref-type="bibr" rid="CR19">19</xref>–<xref ref-type="bibr" rid="CR21">21</xref></sup>. We developed and leveraged more powerful CNNs (DLCRNet) that we show have strong performance for animal pose estimation. Due to the variable body shapes of animals, we developed a data-driven way to automatically find the best skeleton for animal assembly, and we designed fast trackers that also reason over long timescales and are more robust to the body plan, and outperform tailored animal tracking methods such as idTracker.ai. Our method is flexible and not only deals with multiple animals (with the same body plan), but also with one agent dealing with multiple others (as with the parenting mouse, where there are identical looking pups, but a unique adult mouse).</p>
    <p id="Par33">For animal pose and tracking, leveraging identity can be an important tool<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. We introduce ways to allow both marked animals (supervised) and unsupervised animal identity tracking. For marked animals this means if users consistently label a known feature across the dataset (such as the blue tufts of the 40 pairs of marmosets included in labeled data, or consistent marker on a mouse’s tail), they can simply input the identity ‘true’ (or check the box in the GUI) in DeepLabCut and this trains an ID head. If they are not marked, users could use the reID tracking method that performs unsupervised identity estimation. Thereby, our framework integrates various costs related to movement statistics and the learned animal identity.</p>
    <p id="Par34">Open-access benchmark datasets are critical to collectively advance tools<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. Here, we open-source four datasets that pose different challenges. As we show, there is little room for improvement on the tri-mouse data, but the schooling fish is not a solved problem. Thus, while our method is fast, generally robust, and can leverage identity of animals to enhance pose-tracking performance, we believe the benchmark can also spur progress on how to improve performance for occluded, similar looking animals.</p>
    <p id="Par35">We developed both bottom-up and top-down variants for multi-animal DeepLabCut (maDLC), allowing the user an extended selection of methods (Supplementary Note <xref rid="MOESM1" ref-type="media">1)</xref>. While our results suggest that the bottom-up pipeline should be the default, depending on the application, top-down approaches might be better suited. Both classes have limitations and features (reviewed in ref. <sup><xref ref-type="bibr" rid="CR7">7</xref></sup>).</p>
    <p id="Par36">In this work, we strive to develop high-performance code that requires limited user input yet flexibility. In the code, we provide 3D support for multi-animal pose estimation (via multi-camera use), plus this multi-animal variant can be integrated with our real-time software, DeepLabCut-Live!<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Another important user input is at the stage of tracking, where users can input how many animals should be identified in a given video. In this paper, we test up to 14 animals, but this is not a hard upper limit. One ‘upper limit’ is ultimately the camera resolution, as one needs to be able to localize keypoints of animals. Thus, if animals are very small, other tracking tools might be better suited. From pose, to optimal skeleton selection, to tracking: all of the outlined steps can be run in ten lines of code or all from a GUI such that zero programming is required (<ext-link ext-link-type="uri" xlink:href="https://deeplabcut.github.io/DeepLabCut">https://deeplabcut.github.io/DeepLabCut</ext-link>).</p>
  </sec>
  <sec id="Sec12">
    <title>Methods</title>
    <sec id="Sec13">
      <title>Tri-mouse dataset</title>
      <p id="Par37">Three wild-type (C57BL/6J) male mice ran on a paper spool following odor trails<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. These experiments were carried out in the laboratory of V.N. Murthy at Harvard University (temperature of housing was 20–25 °C, humidity 20–50%). Data were recorded at 30 Hz with 640 × 480 pixels resolution acquired with a Point Grey Firefly FMVU-03MTM-CS. One human annotator was instructed to localize the 12 keypoints (snout, left ear, right ear, shoulder, four spine points, tail base and three tail points) across 161 frames sampled from within DeepLabCut using the <italic>k</italic>-means clustering approach (across eight videos). All surgical and experimental procedures for mice were in accordance with the NIH Guide for the Care and Use of Laboratory Animals and approved by the Harvard Institutional Animal Care and Use Committee (IACUC).</p>
    </sec>
    <sec id="Sec14">
      <title>Parenting behavior</title>
      <p id="Par38">Parenting behavior is a pup-directed behavior observed in adult mice involving complex motor actions directed toward the benefit of the offspring<sup><xref ref-type="bibr" rid="CR30">30</xref>,<xref ref-type="bibr" rid="CR31">31</xref></sup>. These experiments were carried out in the laboratory of C. Dulac at Harvard University (temperature of housing was 20–25 °C, humidity 20–50%).</p>
      <p id="Par39">The behavioral assay was performed in the home-cage of single-housed adult (age was less than 180 d old) female C57Bl6/J <italic>Mus musculus</italic> in dark (red light only) conditions. For these videos, the adult mouse was monitored for several minutes in the cage followed by the introduction of pup (4 d old, sex unknown) in one corner of the cage. The behavior of the adult and pup was monitored for a duration of 15min. A video was recorded at 30 Hz using a Microsoft LifeCam camera (part no. 6CH-00001) with a resolution of 1,280 × 720 pixels or a Geovision camera (model no. GV-BX4700-3V) also acquired at 30 frames per second (fps) at a resolution of 704 × 480 pixels. A human annotator labeled on the adult animal the same 12 body points as in the tri-mouse dataset and five body points on the pup along its spine. Initially only the two ends were labeled, and intermediate points were added by interpolation and their positions was manually adjusted if necessary. Frames were generated from across 25 videos.</p>
    </sec>
    <sec id="Sec15">
      <title>Marmoset home-cage behavior</title>
      <p id="Par40">Videos of common marmosets (<italic>Callithrix jacchus</italic>) were made in the laboratory of G. Feng at MIT. Male and female marmoset pairs housed here (<italic>n</italic> = 50, 30 pairs, age range of 2 to 12 years) were recorded using Kinect V2 cameras (Microsoft) with a resolution of 1,080 pixels and frame rate of 30 Hz. After acquisition, images to be used for training the network were manually cropped to 1,000 × 1,000 pixels or smaller. For our analysis, we used 7,600 labeled frames from 40 different marmosets collected from three different colonies (in different facilities, thus over 20 videos were used for dataset generation). Each cage contains a pair of marmosets, where one marmoset had light blue dye applied to its tufts. One human annotator labeled the 15 body points on each animal present in the frame (frames contained either one or two animals). All animal procedures were overseen by veterinary staff of the MIT and Broad Institute Department of Comparative Medicine, in compliance with the National Institutes of Health guide for the care and use of laboratory animals and approved by the MIT and Broad Institute animal care and use committees. As a test of out-of-domain generalization, we additionally labeled 300 frames from ten new cages and animals. See Fig. <xref rid="Fig5" ref-type="fig">5</xref> for example images and results.</p>
      <p id="Par41">We also analyzed two long-term recording sessions from a pairs of marmosets with the DLCRNet_ms5 model, by reidentifying each marmoset in each frame with the ID head. Overall we considered about 9 h (824,568 frames) from two different home cages. We computed the principal components for egocentric postures as well as illustrated their relative head and body orientations (Fig. <xref rid="Fig5" ref-type="fig">5)</xref>. For Fig. <xref rid="Fig5" ref-type="fig">5e</xref>, the distances are normalized based on the running average distance between each ear tuft to center of head, over a second. This measurement does correlate well with depth values in videos recorded with a depth channel (which was not done for the example sessions). To make the postural data egocentric, we first centered the data around ‘Body2’ (<italic>x</italic> = 0, <italic>y</italic> = 0) and then rotated it such that the line formed by ‘Body1’, ‘Body2’ and ‘Body3’ was as close to the line ‘<italic>x</italic> = 0’ as possible.</p>
    </sec>
    <sec id="Sec16">
      <title>Fish schooling behavior</title>
      <p id="Par42">Schools of inland silversides (<italic>Menidia beryllina</italic>, <italic>n</italic> = 14 individuals per school, sex unknown but likely to be equal females and males, aged approximately 9 months) were recorded in the Lauder Laboratory at Harvard University while swimming at 15 speeds (0.5 to 8 body lengths per s at 0.5 body lengths per s intervals) in a flow tank with a total working section of 28 × 28 × 40 cm as described in previous work<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, at a constant temperature (18 ± 1<sup>∘</sup>C) and salinity (33 parts per thousand), at a Reynolds number of approximately 10,000 (based on body length). Dorsal views of steady swimming across these speeds were recorded by high-speed video cameras (FASTCAM Mini AX50, Photron) at 60–125 fps (feeding videos at 60 fps, swimming alone 125 fps). The dorsal view was recorded above the swim tunnel and a floating Plexiglas panel at the water surface prevented surface ripples from interfering with dorsal view videos. Five keypoints were labeled (tip, gill, peduncle, dorsal fin tip, caudal tip) and taken from five videos.</p>
    </sec>
    <sec id="Sec17">
      <title>Dataset properties</title>
      <p id="Par43">All frames were labeled with the annotation GUI; depending on the dataset, between 100 and 7,600 frames were labeled (Table <xref rid="Tab1" ref-type="table">1</xref>). We illustrated the diversity of the postures by clustering (Extended Data Fig. <xref rid="Fig7" ref-type="fig">2</xref>). To assess the level of interactions, we evaluate a Proximity Index (Extended Data Fig. <xref rid="Fig7" ref-type="fig">2m</xref>), whose idea is inspired by ref. <sup><xref ref-type="bibr" rid="CR33">33</xref></sup> but its computation was adapted to keypoints. For each individual, instead of delineating bounding boxes to determine the vicinity of an animal we rather define a disk centered on the individual’s centroid and of sufficiently large radius such that all of that individual’s keypoints are inscribed within the disk; this is a less static description of the immediate space an animal can reach. The index is then taken as the ratio between the number of keypoints within that region that belong to other individuals and the number of keypoints of the individual of interest (Extended Data Fig. <xref rid="Fig7" ref-type="fig">2m</xref>).</p>
      <p id="Par44">For each dataset we created one random split with 70% of the data used for training and the rest for testing (unless otherwise noted). Note that identity prediction accuracy (Fig. <xref rid="Fig2" ref-type="fig">2d</xref>) and tracking performance (Fig. <xref rid="Fig3" ref-type="fig">3e</xref>) are reported on all three splits, and all show little variability. The data are available as a benchmark challenge at <ext-link ext-link-type="uri" xlink:href="https://benchmark.deeplabcut.org/">https://benchmark.deeplabcut.org/</ext-link>.</p>
    </sec>
    <sec id="Sec18">
      <title>Multi-task deep-learning architecture</title>
      <p id="Par45">DeepLabCut consists of keypoint detectors, comprising a deep CNN pretrained on ImageNet as a backbone together with multiple deconvolutional layers<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR21">21</xref></sup>. Here, as backbones we considered Residual Networks (ResNet)<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> and EfficientNets<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>. Other backbones are integrated in the toolbox<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup> such as MobileNetV2 (ref. <sup><xref ref-type="bibr" rid="CR34">34</xref></sup>). We use a stride of 16 for the ResNets (achieved by atrous convolution) and then upsample the filter banks by a factor of two to predict the score maps and location refinement fields with an overall stride of eight. Furthermore, we developed a multi-scale architecture that upsamples from conv5 and fuses those filters with filters learned as 1 × 1 convolutions from conv3. This bank is then upsampled by a factor of two via deconvolution layers. This architecture thus learns from multiple scales with an overall stride of four (including the upsampling in the decoder). We implemented a similar architecture for EfficientNets. These architectures are called ResNet50_strideX and (EfficientNet) bY_strideX for strides four to eight; we used ResNet50 as well as EfficientNets B0 and B7 for experiments (Extended Data Fig. <xref rid="Fig8" ref-type="fig">3</xref>).</p>
      <p id="Par46">We further developed a multi-scale architecture (DLCRNet_ms5) that fuses high-resolution feature map to lower resolution feature map (Fig. <xref rid="Fig1" ref-type="fig">1c</xref>)—we concatenated the feature map from conv5, the feature map learned as a 3 × 3 convolutions followed by a 1 × 1 convolutions from conv3 and the feature map learned as 2 stacked 3 × 3 convolutions and a 1 × 1 convolutions from conv2. This bank is then upsampled via (up to) two deconvolution layers. Depending on how many deconvolution layers are used this architecture learns from multiple scales with an overall stride of 2–8 (including the upsampling in the decoder). Note, during our development phase we used 95% train and 5% test splits of the data; this testing is reported at <ext-link ext-link-type="uri" xlink:href="http://maDLCopt.deeplabcut.org">http://maDLCopt.deeplabcut.org</ext-link> and in our preprint<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>.</p>
      <p id="Par47">DeepLabCut creates three output layers per keypoint that encode an intensity and a vector field. The purpose of the deconvolution layers is to upsample the spatial information (Fig. <xref rid="Fig1" ref-type="fig">1b,c</xref>). Consider an input image <italic>I</italic>(<italic>x</italic>,<italic>y</italic>) with ground truth keypoint (<italic>x</italic><sup><italic>k</italic></sup>,<italic>y</italic><sup><italic>k</italic></sup>) for index <italic>k</italic>. One of the output layers encodes the confidence of a keypoint <italic>k</italic> being in a particular location (<italic>S</italic><sup><italic>k</italic></sup>(<italic>p</italic>,<italic>q</italic>)), and the other two layers encode the (<italic>x</italic>-) and (<italic>y</italic>-) difference (in pixels of the full-sized) image between the original location and the corresponding location in the downsampled (by the overall stride) location (<inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{x}^{k}(p,q)$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{y}^{k}(p,q)$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq2.gif"/></alternatives></inline-formula>). For each training image the architecture is trained end-to-end to predict those outputs. Thereby, the ground truth keypoint is mapped into a target score map, which is 1 for pixels closer to the target (this can be subpixel location) than radius <italic>r</italic> and 0 otherwise. We minimize the cross-entropy loss for the score map (<italic>S</italic><sup><italic>k</italic></sup>) and the location refinement loss was calculated as a Huber loss<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR19">19</xref></sup>.</p>
      <p id="Par48">To link specific keypoints within one animal, we use PAFs, which were proposed by Cao et al.<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. Each (ground truth) PAF <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{x}^{l}(p,q)$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq3.gif"/></alternatives></inline-formula> and <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{y}^{l}(p,q)$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq4.gif"/></alternatives></inline-formula> for limb <italic>l</italic> connecting keypoint <italic>k</italic><sub><italic>i</italic></sub> and <italic>k</italic><sub><italic>j</italic></sub> places a directional unit vector at every pixel vector within a predefined distance from the ideal line connecting two keypoints (modulated by pafwidth). We trained DeepLabCut to also minimize the <italic>L</italic>1-loss between the predicted and true PAFs, which is added to the other losses.</p>
      <p id="Par49">Inspired by Cao et al.<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, we refine the score maps and PAFs in multiple stages. As can be seen from Fig. <xref rid="Fig1" ref-type="fig">1b</xref>, at the first stage, the original image feature from the backbone is fed into the network to predict the score map, PAF and the feature map. The output of each branch, concatenated with the feature map is fed into the subsequent stages. However, unlike Cao et al., we observed that simply adding more stages can cause performance degradation. To overcome that, we introduced shortcut connections between two consequence stages on the score map branch to improve multiple stage prediction.</p>
      <p id="Par50">Examples for score maps, location refinement and PAFs are shown in Fig. <xref rid="Fig1" ref-type="fig">1b</xref>. For training, we used the Adam optimizer<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> with learning schedule (0.0001 for first 7,500 iterations then 5 × 10<sup>−5</sup> until 12,000 iterations and then 1 × 10<sup>−5</sup>) unless otherwise noted. We trained for 60,000–200,000 (for the marmosets) iterations with batch size 8; this was enough to reach good performance (Fig. <xref rid="Fig2" ref-type="fig">2a</xref> and Extended Data Fig. <xref rid="Fig8" ref-type="fig">3)</xref>. During training we also augmented images by using techniques including rotation, covering with random boxes and motion blur. We also developed a keypoint-aware image cropping technique to occasionally augment regions of the image that are dense in keypoints. Crop centers are sampled applying at random one of the following two strategies: uniform sampling over the whole image; or sampling based on keypoint density, where the probability of a point being sampled increases in proportion to its number of neighbors (within a radius equal to 10% of the smallest image side). Crop centers are further shifted along both dimensions by random amounts no greater than 40% of the crop size—the hyperparameters can be changed by the user.</p>
    </sec>
    <sec id="Sec19">
      <title>CNN-based identity prediction</title>
      <p id="Par51">For animal identification we used a classification approach<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, while also considering spatial information. To have a monolithic solution (with just a single CNN), we simply predict in parallel the identity of each animal from the image. For this purpose, <italic>n</italic> deconvolution layers are added for <italic>n</italic> individuals. The network is trained to predict the summed score map for all keypoints of that individual. At test time, we then look up which of the output classes has the highest likelihood (for a given keypoint) and assign that identity to the keypoint. This output is trained jointly in a multi-task configuration. We evaluate the performance for identity prediction on the marmoset dataset (Fig. <xref rid="Fig2" ref-type="fig">2e</xref>).</p>
      <p id="Par52">Identity prediction can be leveraged by DeepLabCut in three different ways: (1) for assembly, by grouping keypoints based on their predicted identity; (2) for local, frame-by-frame tracking, using a soft-voting scheme where body parts are regarded as individual classifiers providing an identity probability and (3) for global stitching, by weighing down the cost of edges connecting two tracklets of similar appearance (as in Figs. <xref rid="Fig3" ref-type="fig">3</xref> and <xref rid="Fig4" ref-type="fig">4</xref>). These three sequential stages can thus be made reliant on visual appearance features alone, as done with the long recordings of marmoset behavior (Fig. <xref rid="Fig5" ref-type="fig">5</xref>).</p>
    </sec>
    <sec id="Sec20">
      <title>Multi-animal inference</title>
      <p id="Par53">Any number of keypoints can be defined and labeled with the toolbox; additional ones can be added later on. Based on our experience and testing, we recommend labeling more keypoints than a subsequent analysis might require, since it improves the part detectors<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> and, more importantly, animal assembly (Extended Data Fig. <xref rid="Fig14" ref-type="fig">9a</xref>).</p>
      <p id="Par54">Before decoding, score maps are smoothed with a Gaussian kernel of spread <italic>σ</italic> = 1 to make peaks more salient<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. For each keypoint one obtains the most likely keypoint location (<italic>x</italic><sup>*</sup>,<italic>y</italic><sup>*</sup>) by taking the maximum: (<italic>p</italic><sup>*</sup>,<italic>q</italic><sup>*</sup>) = argmax<sub>(<italic>p</italic>,<italic>q</italic>)</sub><italic>S</italic><sup><italic>k</italic></sup>(<italic>p</italic>,<italic>q</italic>) and computing:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{l}{x}^{* }={p}^{* }\cdot \lambda +\lambda /2+{L}_{x}^{k}({p}^{* },{q}^{* })\\ {y}^{* }={q}^{* }\cdot \lambda +\lambda /2+{L}_{y}^{k}({p}^{* },{q}^{* })\end{array}$$\end{document}</tex-math><mml:math id="M10"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mi>λ</mml:mi><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mi>λ</mml:mi><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="41592_2022_1443_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>with overall stride <italic>λ</italic>. If there are multiple keypoints <italic>k</italic> present then one can naturally take the local maxima of <italic>S</italic><sup><italic>k</italic></sup> to obtain the corresponding detections. Local maxima are identified via nonmaximum suppression with 2D max pooling of the score maps.</p>
      <p id="Par55">Thus, one obtains putative keypoint proposals from the score maps and location refinement fields. We then use the PAFs to assign the cost for linking two keypoints (within a putative animal). For any pair of keypoint proposals (that are connected via a limb as defined by the part affinity graph) we evaluate the affinity cost by integrating along line <italic>γ</italic> connecting two proposals, normalized by the length of <italic>γ</italic>:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\int \parallel {P}_{x,y}^{l}\parallel {\mathrm{d}}\gamma /\int {\mathrm{d}}\gamma$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mo>∫</mml:mo><mml:mo>∥</mml:mo><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>∥</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mi>γ</mml:mi><mml:mo>/</mml:mo><mml:mo>∫</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mi>γ</mml:mi></mml:mrow></mml:math><graphic xlink:href="41592_2022_1443_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>This integral is computed by sampling. Thus, for a given part affinity graph, one gets a (possibly) large number of detections and costs. The next step is to assemble those detections into animals.</p>
    </sec>
    <sec id="Sec21">
      <title>Data-driven PAF graph selection</title>
      <p id="Par56">To relieve the user from manually defining connections between keypoints, we developed an entirely data-driven procedure. Models are trained on a complete graph to learn all possible body part connections. We tested whether randomly pruning the complete marmoset skeleton (to 25, 50 and 75% of its original size: that is, 26, 52, 78 edges or 52, 104, 156 PAFs) to alleviate memory demands could still yield acceptable results. We found that pruning a large graph before training to a fourth of its original size was harmful (mAP loss of 15–20 points; Extended Data Fig. <xref rid="Fig9" ref-type="fig">9</xref>); at half and 75% of its size, a performance equivalent to that of the full graph was reached at 24 edges, although it remained about 1.5 mAP point under the maximal mAP score observed overall. Consequently, for large skeletons, a random subgraph is expected to yield only slightly inferior performance at a lesser computational cost.</p>
      <p id="Par57">The graph is then pruned based on edge discriminability power on the training set. For this purpose, within- and between-animal part affinity cost distributions (bin width 0.01) are evaluated (see Extended Data Fig. <xref rid="Fig9" ref-type="fig">4</xref> for the mouse dataset). Edges are then ranked in decreasing order of their ability to separate both distributions—evaluated from the auROC curve. The smallest, data-driven graph is taken as the maximum spanning tree (that is, a subgraph covering all keypoints with the minimum possible number of edges that also maximizes part affinity costs). For graph search following a network’s evaluation, up to nine increasingly redundant graphs are formed by extending the minimal skeleton progressively with strongly discriminating edges in the order determined above. By contrast, baseline graphs are grown from a skeleton a user would naively draw, with edges iteratively added in reversed order (that is, from least to most discriminative). The graph jointly maximizing purity and the fraction of connected keypoints is the one retained to carry out the animal assemblies.</p>
    </sec>
    <sec id="Sec22">
      <title>Animal assembly</title>
      <p id="Par58">Animal assembly refers to the problem of assigning keypoints to individuals. Yet, reconstructing the full pose of multiple individuals from a set of detections is NP hard, as it amounts to solving a <italic>k</italic>-dimensional matching problem (a generalization of bipartite matching from 2 to <italic>k</italic> disjoint subsets)<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR38">38</xref></sup>. To make the task more tractable, we break the problem down into smaller matching tasks, in a manner akin to Cao et al.<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>.</p>
      <p id="Par59">For each edge type in the data-driven graph defined earlier, we first pick strong connections based on affinity costs alone. Following the identification of all optimal pairs of keypoints, we seek unambiguous individuals by searching this set of pairs for connected components—in graph theory, these are subsets of keypoints all reachable from one another but that do not share connection with any additional keypoint; consequently, only connectivity, but not spatial information, is taken into account. Breadth-first search runs in linear time complexity, which thus allows the rapid predetermination of unique individuals. Note that, unlike ref. <sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, redundant connections are seamlessly handled and do not require changes in the formulation of the animal assembly. Then, remaining connections are sorted in descending order of their affinity costs (equation (<xref rid="Equ2" ref-type="">2</xref>)) and greedily linked.</p>
      <p id="Par60">To further improve the assembly’s robustness to ambiguous connections (that is, a connection attempting to either link keypoints belonging to two distinct individuals or overwrite existing ones), the assembly procedure can be calibrated by determining the prior probability of an animal’s pose as a multivariate normal distribution over the distances between all pairs of keypoints. Mean and covariance are estimated from the labeled data via density estimation with Gaussian kernel and bandwidth automatically chosen according to Scott’s Rule. A skeleton is then only grown if the candidate connection reduces the Mahalanobis distance between the resulting configuration and the prior (referred to as with calibration in Fig. <xref rid="Fig2" ref-type="fig">2c</xref>). Last, our assembly’s implementation is fully parallelized to benefit greatly from multiple processors (Extended Data Fig. <xref rid="Fig10" ref-type="fig">5)</xref>.</p>
      <p id="Par61">Optionally (and only when analyzing videos), affinity costs between body parts can be weighted so as to prioritize strong connections that were preferentially selected in the past frames. To this end, and inspired by ref. <sup><xref ref-type="bibr" rid="CR39">39</xref></sup>, we compute a temporal coherence cost as follows: <inline-formula id="IEq5"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{1}{j}\mathop{\sum }\nolimits_{i = 1}^{j}{e}^{-\gamma {{\Delta }}t{\left\Vert c-{c}_{n}\right\Vert }^{2}}$$\end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:msup><mml:mrow><mml:mfenced close="∥" open="∥"><mml:mrow><mml:mi>c</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq5.gif"/></alternatives></inline-formula>, where <italic>γ</italic> controls the influence of distant frames (and is set to 0.01 by default), <italic>c</italic> and <italic>c</italic><sub><italic>n</italic></sub> are the current connection and its closest neighbor in the relevant past frame and Δ<italic>t</italic> is the temporal gap separating these frames.</p>
    </sec>
    <sec id="Sec23">
      <title>Top-down pose estimation</title>
      <p id="Par62">In general, top-down pose estimation is characterized by two stages that require an object detector and a single animal pose estimation model<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. This pipeline requires bounding box annotations (which can come from many different algorithms). Here, bounding boxes were determined from ground truth keypoint coordinates. If a box’s aspect ratio was lower than 4:1, its smallest side was extended by 10 pixels. Box bounds were further enlarged by 25 pixels to make sure the bounding boxes covered an animal’s entire body. We pad the cropped images to a square and then resize them to the original size (400 × 400) to keep the aspect ratio constant. Second, we retrain a model (either with or without PAFs) on training images cropped by these bounding boxes. For inference, we retain the best prediction per bounding box, as decided by detection confidence for the model without PAFs and with highest assembly score for the model with PAF. Finally, for evaluation, we map coordinates of our final predictions back to the original images.</p>
    </sec>
    <sec id="Sec24">
      <title>Detection performance and evaluation</title>
      <p id="Par63">To compare the human annotations with the model predictions we used the Euclidean distance to the closest predicted keypoint (r.m.s.e.) calculated per keypoint. Depending on the context, this metric is shown for a specific keypoint, averaged over all keypoints or averaged over a set of train or test images (Fig. <xref rid="Fig2" ref-type="fig">2a</xref> and Extended Data Fig. <xref rid="Fig8" ref-type="fig">3)</xref>. Nonetheless, unnormalized pixel errors may be difficult to interpret in certain scenarios; for example, marmosets dramatically vary in size as they leap from the top to the bottom of the cage. Thus, we also calculated the percentage of correct keypoints (PCK) metric<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR40">40</xref></sup>; that is, the fraction of predicted keypoints that fall within a threshold distance from the location of the ground truth detection. PCK was computed in relation to a third of the tip–gill distance for the fish dataset, and a third of the left-right ear distance for the remaining ones.</p>
      <p id="Par64">Animal assembly quality was evaluated in terms of mAP computed over object keypoint similarity thresholds ranging from 0.50 to 0.95 in steps of 0.05, as is standard in human pose literature and COCO challenges<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. Keypoint standard deviation was set to 0.1. As interpretable metrics, we also computed the number of ground truth keypoints left unconnected (after assembly) and purity—an additional criterion for quality that can be understood as the accuracy of the assignment of all keypoints of a putative subset to the most frequent ground truth animal identity within that subset<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. Since pups are very hard to label consistently (see Extended Data Fig. <xref rid="Fig7" ref-type="fig">7</xref> for examples), we allow flips between symmetric pairs of keypoints (end1 versus end2 or interm1 versus interm3, Extended Data Fig. <xref rid="Fig6" ref-type="fig">1</xref>) to be acceptable detection errors when evaluating keypoint similarity.</p>
    </sec>
    <sec id="Sec25">
      <title>Statistics for assessing data-driven method</title>
      <p id="Par65">Two-way, repeated-measures ANOVA were performed using Pinetwork flow minimizationngouin (v.0.5.0)<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> to test whether graph size and assembling method (naive versus data-driven versus calibrated assembly) had an impact on the fraction of unconnected body parts and assembly purity. Since sphericity was violated, the Greenhouse–Geisser correction was applied. Provided a main effect was found, we conducted multiple post hoc (paired, two-sided) tests adjusted with Benjamini–Hochberg false discovery rate correction to locate pairwise differences. The Hedges’ <italic>g</italic> was calculated to report effect sizes between sets of observations.</p>
    </sec>
    <sec id="Sec26">
      <title>Comparison to state-of-the-art pose estimation models</title>
      <p id="Par66">For benchmarking, we compared our architectures to current state-of-the-art architectures on COCO<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, a challenging, large-scale multi-human pose estimation benchmark. Specifically, we considered HRNet<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR43">43</xref></sup> as well as ResNet backbones<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> with Associative Embedding<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> as implemented in the MMPose toolbox (<ext-link ext-link-type="uri" xlink:href="https://github.com/open-mmlab/mmpose">https://github.com/open-mmlab/mmpose</ext-link>). We chose them as control group for their simplicity (ResNet) and performance (HRNet). We used the bottom-up variants of both models. The bottom-up variants leverage associative embedding as the grouping algorithms<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. In particular, the bottom-up variant of HRNet we used has mAP that is comparable to the state-of-the-art model HigherHRNet<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> in COCO (69.8 versus 70.6) for a multiple scale test and (65.4 versus 67.7) for a single scale test.</p>
      <p id="Par67">To fairly compare, we used the same train and test split. The total training epochs are set such that models from two groups see roughly same number of images. The hyperparameters search was manually performed to find the optimal hyperparameters. For a small dataset such as the tri-mouse and (largest) marmoset, we found that the default settings for excellent performance on COCO gave optimal accuracy except that we needed to modify the total training steps to match DeepLabCut’s. For both the marmoset and tri-mouse datasets, the initial learning rate was 0.0015. For the three mouse dataset, the total epochs is 3,000 epochs and the learning rate decayed by a factor of 10 at 600 and 1,000 epochs. For the marmoset dataset, we trained for 50 epochs and the learning rate decayed after 20 and 40 epochs. The batch size was 32 and 16 for ResNet-AE and HRNet-AE, respectively. For smaller datasets such as tri-mouse, fish and parenting, we found that a smaller learning rate and a smaller batch size gave better results; a total of 3,000 epochs were used. After hyper-parameter search, we set batch size to four and initial learning rate a 0.0001, which then decayed at 1,000 and 2,000 epochs. As within DeepLabCut, multiple scale test and flip test were not performed (which is, however, common for COCO evaluation). For the parenting dataset, MMPose models can only be trained on one dataset (simultaneously), which is why these models are not trained to predict the mouse, and we only compare the performance on the pups. Full results are shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.</p>
    </sec>
    <sec id="Sec27">
      <title>Benchmarking idtacker.ai</title>
      <p id="Par68">We used version idtracker.ai<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> v.3, taken from commit 6b89601b; we tested it on tri-mouse and marmoset data. We report the MOTA results in Extended Data Fig. <xref rid="Fig8" ref-type="fig">8</xref>. For marmoset data, reasonable parameters to segment individual animals with the GUI could not be found (likely due to the complex background), thus we performed a grid search for the valid minimum intensity threshold and maximum intensity threshold, the two critical parameters, by step 2 from range 0 to 255. Even with these efforts, we still could not get reasonable results (Supplementary Video <xref rid="MOESM7" ref-type="media">5)</xref>; that is, MOTA was negative.</p>
    </sec>
    <sec id="Sec28">
      <title>DeepLabCut Tracking modules</title>
      <p id="Par69">Having seen that DeepLabCut provides a strong predictor for individuals and their keypoints, detections are linked across frames using a tracking-by-detection approach (for example, ref. <sup><xref ref-type="bibr" rid="CR44">44</xref></sup>). Thereby, we follow a divide-and-conquer strategy for (local) tracklet generation and tracklet stitching (Extended Data Fig. <xref rid="Fig9" ref-type="fig">4b,c</xref>). Specifically, we build on the Simple Online and Realtime Tracking framework (SORT<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>) to generate tracklets. The inter-frame displacement of assembled individuals is estimated via Kalman filter-based trackers. The task of associating these location estimates to the model detections is then formulated as a bipartite graph matching problem solved with the Hungarian algorithm, therefore guaranteeing a one-to-one correspondence across adjacent frames. Note that the trackers are agnostic to the type of skeleton (animal body plan), which render them robust and computationally efficient.</p>
    </sec>
    <sec id="Sec29">
      <title>Box tracker</title>
      <p id="Par70">Bounding boxes are a common and well-established representation for object tracking. Here they are computed from the keypoint coordinates of each assembled individual, and expanded by a margin optionally set by the user. The state <italic>s</italic> of an individual is parametrized as <inline-formula id="IEq6"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s=[x,y,A,r,\dot{x},\dot{y},\dot{A}]$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq6.gif"/></alternatives></inline-formula>, where <italic>x</italic> and <italic>y</italic> are the 2D coordinates of the center of the bounding box; <italic>A</italic>, its area and <italic>r</italic>, its aspect ratio, together with their first time derivatives. Association between detected animals and tracker hypotheses is based on the intersection-over-union measure of overlap.</p>
    </sec>
    <sec id="Sec30">
      <title>Ellipse tracker</title>
      <p id="Par71">A 2<italic>σ</italic> covariance error ellipse is fitted to an individual’s detected keypoints. The state is modeled as <inline-formula id="IEq7"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s=[x,y,h,w,\theta ,\dot{x},\dot{y},\dot{h},\dot{w},\dot{\theta }]$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq7.gif"/></alternatives></inline-formula>, where <italic>x</italic> and <italic>y</italic> are the 2D coordinates of the center of the ellipse; <italic>h</italic> and <italic>w</italic>, the lengths of its semi-axes and <italic>θ</italic>, its inclination relative to the horizontal. We anticipated that this parametrization would better capture subtle changes in body conformation, most apparent through changes in ellipse width and height and orientation. Moreover, an error ellipse confers robustness to outlier keypoints (for example, a prediction assigned to the wrong individual, which would cause the erroneous delineation of an animal’s boundary under the above-mentioned box tracking). In place of the ellipse overlap, the similarity cost <italic>c</italic> between detected and predicted ellipses is efficiently computed as: <inline-formula id="IEq8"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c=0.8(1-d)+0.2(1-d)(\cos ({\theta }_{d}-{\theta }_{p}))$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>0.2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq8.gif"/></alternatives></inline-formula>, where <italic>d</italic> is the Euclidean distance separating the ellipse centroids normalized by the length of the longest semi-axis.</p>
      <p id="Par72">The existence of untracked individuals in the scene is signaled by assembled detections with a similarity cost lower than iou_threshold (set to 0.6 in our experiments). In other words, the higher the similarity threshold, the more conservative and accurate the frame-by-frame assignment, at the expense of shorter and more numerous tracklets. On creation, a tracker is initialized with the required parameters described above, and all (unobservable) velocities are set to 0. To avoid tracking sporadic, spurious detections, a tracker is required to live for a minimum of min_hits consecutive frames, or is otherwise deleted. Occlusions and reidentification of individuals are handled with the free parameter max_age—the maximal number of consecutive frames tracks can remain undetected before the tracker is considered lost. We set both to 1 to delegate the tasks of tracklet reidentification and false positive filtering to our TrackletStitcher, as we shall see below.</p>
    </sec>
    <sec id="Sec31">
      <title>Tracklet stitching</title>
      <p id="Par73">Greedily linking individuals across frames is locally, but not globally, optimal. An elegant and efficient approach to reconstructing full trajectories (or tracks) from sparse tracklets is to cast the stitching task as a network flow minimization problem<sup><xref ref-type="bibr" rid="CR45">45</xref>,<xref ref-type="bibr" rid="CR46">46</xref></sup>. Each fully reconstructed track is equivalent to finding a flow through the graph from a source to a sink, subject to capacity constraints and whose overall linking cost is minimal (Extended Data Fig. <xref rid="Fig9" ref-type="fig">4c</xref>).</p>
    </sec>
    <sec id="Sec32">
      <title>Formulation</title>
      <p id="Par74">The tracklets collected after animal tracking are denoted as <inline-formula id="IEq9"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\{{{{{\mathcal{T}}}}}_{1},...,{{{{\mathcal{T}}}}}_{n}\}$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq9.gif"/></alternatives></inline-formula>, and each contains a (temporally) ordered sequence of observations and time indices. Thereby, the observations are given as vectors of body part coordinates in pixels and likelihoods. In contrast to most approaches described in the literature, the proposed approach requires solely spatial and temporal information natively, while leveraging visual information (for example, animals’ identities predicted beforehand) is optional (see Fig. <xref rid="Fig3" ref-type="fig">3e</xref> for marmosets). This way, tracklet stitching is agnostic to the framework poses were estimated with, and works readily on previously collected kinematic data.</p>
      <p id="Par75">We construct a directed acyclic graph <italic>G</italic> = (<italic>V</italic>,<italic>E</italic>) using NetworkX<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> to describe the affinity between multiple tracklets, where the <italic>i</italic>th node <italic>V</italic><sub><italic>i</italic></sub> corresponds to the <italic>i</italic>th tracklet <inline-formula id="IEq10"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\mathcal{T}}}}}_{i}$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq10.gif"/></alternatives></inline-formula>, and <italic>E</italic> is the set of edges encoding the cost entailed by linking the two corresponding tracklets (or, in other words, the likelihood that they belong to the same track). In our experiments, tracklets shorter than five frames were flagged as residuals: they do not contribute to the construction of the graph and are incorporated only after stitching. This minimal tracklet length can be changed by a user. To drastically reduce the number of possible associations and make our approach scale efficiently to large videos, edge construction is limited to those tracklets that do not overlap in time (since an animal cannot occupy multiple spatial locations at any one instant) and temporally separated by no more than a certain number of frames. By default, this threshold is automatically taken as 1.5 × <italic>τ</italic>, where <italic>τ</italic> is the smallest temporal gap guaranteeing that all pairs of consecutive tracklets are connected. Alternatively, the maximal gap to consider can be programmatically specified. The source and the sink are two auxiliary nodes that supply and demand an amount of flow <italic>k</italic> equal to the number of tracks to form. Each node is virtually split in half: an input with unit demand and an output with unit supply, connected by a weightless edge. All other edges have unit capacity and a weight <italic>w</italic> calculated from the affinity models described in the next subsection. Altogether, these constraints ensure that all nodes are visited exactly once, which thus amounts to a problem similar to covering <italic>G</italic> with <italic>k</italic> node-disjoint paths at the lowest cost. We considered different affinities for linking tracklets (Fig. <xref rid="Fig4" ref-type="fig">4d</xref>).</p>
    </sec>
    <sec id="Sec33">
      <title>Affinity models</title>
      <sec id="Sec34">
        <title>Motion affinity</title>
        <p id="Par76">Let us consider two nonoverlapping tracklets <inline-formula id="IEq11"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\mathcal{T}}}}}_{1}$$\end{document}</tex-math><mml:math id="M26"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq11.gif"/></alternatives></inline-formula> and <inline-formula id="IEq12"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\mathcal{T}}}}}_{2}$$\end{document}</tex-math><mml:math id="M28"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq12.gif"/></alternatives></inline-formula> consecutive in time. Their motion affinity is measured from the error between the true locations of their centroids (that is, unweighted average keypoint) and predictions made from their linear velocities. Specifically, we calculate a tracklet’s tail and head velocities by averaging instantaneous velocities over its three first and last data points (Fig. <xref rid="Fig4" ref-type="fig">4d</xref>). Assuming uniform, rectilinear motion, the centroid location of <inline-formula id="IEq13"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\mathcal{T}}}}}_{1}$$\end{document}</tex-math><mml:math id="M30"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq13.gif"/></alternatives></inline-formula> at the starting frame of <inline-formula id="IEq14"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\mathcal{T}}}}}_{2}$$\end{document}</tex-math><mml:math id="M32"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq14.gif"/></alternatives></inline-formula> is estimated, and we note <italic>d</italic><sub>f</sub> the distance between the forward prediction and the actual centroid coordinates. The same procedure is repeated backward in time, predicting the centroid location of <inline-formula id="IEq15"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\mathcal{T}}}}}_{2}$$\end{document}</tex-math><mml:math id="M34"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq15.gif"/></alternatives></inline-formula> at the last frame of <inline-formula id="IEq16"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\mathcal{T}}}}}_{1}$$\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq16.gif"/></alternatives></inline-formula> knowing its tail velocity, yielding <italic>d</italic><sub>b</sub>. Motion affinity is then taken as the average error distance.</p>
      </sec>
      <sec id="Sec35">
        <title>Spatial proximity</title>
        <p id="Par77">If a pair of tracklets overlaps in time, we calculate the Euclidean distance between their centroids averaged over their overlapping portion. Otherwise, we evaluate the distance between a tracklet’s tail and the other’s head.</p>
      </sec>
      <sec id="Sec36">
        <title>Shape similarity</title>
        <p id="Par78">Shape similarity between two tracklets is taken as the undirected Hausdorff distance between the two sets of keypoints. Although this measure provides only a crude approximation of the mismatch between two animals’ skeletons, it is defined for finite sets of points of unequal size; for example, it advantageously allows the comparison of skeletons with a different number of visible keypoints.</p>
      </sec>
      <sec id="Sec37">
        <title>Dynamic similarity</title>
        <p id="Par79">To further disambiguate tracklets in the rare event that they are spatially and temporally close, and similar in shape, we propose to use motion dynamics in a manner akin to ref. <sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. The procedure is fully data-driven, and requires no a priori knowledge of the animals’ behavior. In the absence of noise, the rank of the Hankel matrix—a matrix constructed by stacking delayed measurements of a tracklet’s centroid—theoretically determines the dimension of state space models; that is, it is a proxy for the complexity of the underlying dynamics<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>. If two tracklets originate from the same dynamical system, a single, low-order regressor should suffice to approximate them both. On the other hand, tracklets belonging to different tracks would require a higher-order (that is, more complex) model to explain their spatial evolution<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. Low rank approximation of a noisy matrix, however, is a complex problem, as the matrix then tends to be full rank (that is, all its singular values are nonzero). For computational efficiency, we approximate the rank of a large numbers of potentially long tracklets using singular value decomposition via interpolative decomposition. Optimal low rank was chosen as the rank after which eigenvalues drop by less than 1%.</p>
      </sec>
    </sec>
    <sec id="Sec38">
      <title>Problem solution for stitching</title>
      <p id="Par80">The optimal flow solution can be found using a min-cost flow algorithm. We use NetworkX’s capacity scaling variant of the successive shortest augmenting path algorithm, which requires polynomial time for the assignment problem (that is, when all nodes have unit demands and supplies, ref. <sup><xref ref-type="bibr" rid="CR50">50</xref></sup>). Residual tracklets are then greedily added back to the newly stitched tracks at locations that guarantee time continuity and, when there are multiple candidates, minimize the distances to the neighboring tracklets. Note that although residuals are typically very short, making the assignment decisions error-prone. To improve robustness and simultaneously reduce complexity, association hypotheses between temporally close residual tracklets are stored in the form of small directed acyclic graphs during a preliminary forward screening pass. An hypothesis likelihood is then scored based on pairwise tracklet spatial overlap, and weighted longest paths are ultimately kept to locally grow longer, more confident residuals.</p>
      <p id="Par81">This tracklet stitching process is implemented in DeepLabCut and automatically carried out after assembly and tracking. The tracks can then also be manually refined in a dedicated GUI (Extended Data Fig. <xref rid="Fig6" ref-type="fig">1</xref>).</p>
    </sec>
    <sec id="Sec39">
      <title>Transformer for unsupervised ID tracking</title>
      <p id="Par82">To track unidentified animals we turn to metric learning<sup><xref ref-type="bibr" rid="CR4">4</xref></sup> with transformers, which are state-of-the-art for reID of humans and vehicles<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. However, in contrast to ref. <sup><xref ref-type="bibr" rid="CR51">51</xref></sup>, we created a tracking approach and wanted to make use of the task-trained CNNs, and thus require fewer training data.</p>
      <p id="Par83">Specifically, we used the predicted coordinates of each tracklet (individual with temporal continuality) and extract features of 2,048 dimensions from the last layer of our (multi-task-trained) backbone network to form so called ‘keypoint embedding’, which contains embedding of each detected keypoint for every individual (and encode high-level visual features around the keypoint). Then we feed this keypoint embedding to a transformer that processes these embeddings and aggregates information globally. The transformer layers have four heads and four blocks with dimension of 768 and residual connections between blocks. The output of transformer layers are then followed by a multi-layer perceptron that outputs a vector of dimension 128 (more layers, as in ref. <sup><xref ref-type="bibr" rid="CR51">51</xref></sup>, actually gave a worse performance). We then use the output of the multi-layer perceptron to minimize triplet loss where we treat within tracklet embedding as anchor-positive pairs while tracklets from different individuals as anchor-negative pairs. For each test video, we extracted 10,000 triplets from the local-tracking approach (ellipse, to evaluate the capacity based on tracklets) and from the ground truth data (to evaluate the capacity of the approach; as triplets from ground truth tracks already are split into the correct number of animals). We then trained the transformer on 90% of the triplets, and evaluated it on the rest (Fig. <xref rid="Fig4" ref-type="fig">4)</xref>. Thus, the transformer learns to recognize identities of each tracklet and we then use the cosine similarity as an additional cost to our graph. For this purpose, we used the transformer to extract 128 dimensional feature vectors (appearance embeddings) per keypoint embedding, which we then used for tracking (below).</p>
    </sec>
    <sec id="Sec40">
      <title>Tracking performance evaluation</title>
      <p id="Par84">Tracking performance was assessed with the field standard MOTA metrics<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. Namely, we used <ext-link ext-link-type="uri" xlink:href="https://github.com/cheind/py-motmetrics">https://github.com/cheind/py-motmetrics</ext-link> to compute MOTA, which evaluates a tracker’s overall performance at detecting and tracking individuals (all possible sources of errors considered: number of misses, of false positives and of mismatches (switches) respectively) independently of its ability to predict an individual’s location. MOTA is thereby the sum of three errors: the ratio of misses in the sequence, computed over the total number of objects present in all frames, the ratio of false positives and the ratio of mismatches<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. The number of misses counts actual detections for which there are no matching trackers. The number of fragments indicates the number of times tracking was interrupted. The number of switches, occurring most often when two animals pass very close to one another or if tracking resumes with a different ID after an occlusion. In our software, remaining ID swaps are automatically flagged in the Refine Tracklets GUI (Extended Data Fig. <xref rid="Fig6" ref-type="fig">1)</xref> by identifying instants at which the <italic>x</italic> and <italic>y</italic> coordinates of a pair of keypoints simultaneously intersect each other<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>.</p>
    </sec>
    <sec id="Sec41">
      <title>Reporting Summary</title>
      <p id="Par85">Further information on research design is available in the <xref rid="MOESM2" ref-type="media">Nature Research Reporting Summary</xref> linked to this article.</p>
    </sec>
  </sec>
  <sec id="Sec42" sec-type="materials|methods">
    <title>Online content</title>
    <p id="Par86">Any methods, additional references, Nature Research reporting summaries, source data, extended data, supplementary information, acknowledgements, peer review information; details of author contributions and competing interests; and statements of data and code availability are available at 10.1038/s41592-022-01443-0.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <p>
      <supplementary-material content-type="local-data" id="MOESM1">
        <media xlink:href="41592_2022_1443_MOESM1_ESM.pdf">
          <label>Supplementary Information</label>
          <caption>
            <p>Supplementary Note 1 and Tables 1–8.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM2">
        <media xlink:href="41592_2022_1443_MOESM2_ESM.pdf">
          <caption>
            <p>Reporting Summary</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM3">
        <media xlink:href="41592_2022_1443_MOESM3_ESM.mov">
          <label>Supplementary Video 1</label>
          <caption>
            <p>Predictions with DLCRNet MS5_ss-s4 for tri-mouse video clip.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM4">
        <media xlink:href="41592_2022_1443_MOESM4_ESM.mp4">
          <label>Supplementary Video 2</label>
          <caption>
            <p>Predictions with DLCRNet MS5_ss-s4 for parenting video clip.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM5">
        <media xlink:href="41592_2022_1443_MOESM5_ESM.mp4">
          <label>Supplementary Video 3</label>
          <caption>
            <p>Predictions with DLCRNet MS5_ss-s4 for marmoset video clip.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM6">
        <media xlink:href="41592_2022_1443_MOESM6_ESM.m4v">
          <label>Supplementary Video 4</label>
          <caption>
            <p>Predictions with DLCRNet MS5_ss-s4 for fish video clip.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM7">
        <media xlink:href="41592_2022_1443_MOESM7_ESM.mp4">
          <label>Supplementary Video 5</label>
          <caption>
            <p>Predictions with idtracker.ai on tri-mouse and marmoset data.</p>
          </caption>
        </media>
      </supplementary-material>
    </p>
  </sec>
</body>
<back>
  <app-group>
    <app id="App1">
      <sec id="Sec43">
        <title>Extended data</title>
        <p id="Par91">
          <fig id="Fig6">
            <label>Extended Data Fig. 1</label>
            <caption>
              <title>DeepLabCut 2.2 workflow.</title>
              <p>(a) Multi-animal DeepLabCut2.2+ workflow. (b) An example screenshot of the Refine Tracklet GUI. We show the ellipse similarity score (black line), hand-noted GT switches in ID (blue), and additional frames in orange where the selected keypoint requires further examination. (c) Body part keypoint diagrams with names on the animal skeletons (see also Extended Data Figure <xref rid="Fig2" ref-type="fig">2)</xref>.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1443_Fig6_ESM" id="d32e2914"/>
          </fig>
        </p>
        <p id="Par92">
          <fig id="Fig7">
            <label>Extended Data Fig. 2</label>
            <caption>
              <title>Dataset characteristics and statistics.</title>
              <p>For each datasets, normalized animal poses were clustered using K-means adapted for missing elements, and embedded non-linearly in 2D space via Isometric mapping (Tenenbaum et al. 2000). Embeedings as well as representative poses are shown for the tri-mouse dataset (a). Counts of labeled keypoints (b) and distribution of bounding box diagonal lengths (c). (d-l) show the same for the other three datsets. The Proximity Index (m) reflects the crowdedness of the various dataset scenes. Statistics were computed from the ground truth test video annotations. The mice and fish datasets are more cluttered on average than the pups and marmosets.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1443_Fig7_ESM" id="d32e2928"/>
          </fig>
        </p>
        <p id="Par93">
          <fig id="Fig8">
            <label>Extended Data Fig. 3</label>
            <caption>
              <title>Performance of various DeepLabCut network architectures.</title>
              <p>(a) Overall keypoint prediction errors of ResNets-50 and the EfficientNets backbones (B0/B7), DLCRNet at stride 4 and 8. Distribution of train and test errors are displayed as light and dark box plots, respectively. Box plots show median, first and third quartiles, with whiskers extending past the low and high quartiles to ± 1.5 times the interquartile range. All models were trained for 60k iterations. n=independent image samples as follows: for train∣test per dataset: 112∣49 (tri-mouse); 379∣163 (pups); 5316∣2278 (marmosets); 70∣30 (fish). (b): Images on held-out test data, where plus indicates human ground truth, and the circle indicates the model prediction (shown for ResNet50 with stride 8). (c): Marmoset identification train-test accuracy for various backbones.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1443_Fig8_ESM" id="d32e2942"/>
          </fig>
        </p>
        <p id="Par94">
          <fig id="Fig9">
            <label>Extended Data Fig. 4</label>
            <caption>
              <title>Discriminability of part affinity fields.</title>
              <p>Within- (pink) and between-animal (blue) affinity cost distributions for all edges of the mouse skeleton with DLCRNet_ms5. The saturated subplots highlight the 11 edges kept to form the smallest, optimal part affinity graph (see Fig. <xref rid="Fig2" ref-type="fig">2b</xref>). This is based on the separability power of an edge, that is, its ability to discriminate a connection between two keypoints effectively belonging to the same animal from the wrong ones, and reflected by the corresponding AUC scores (listed at the top of the subplots).</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1443_Fig9_ESM" id="d32e2959"/>
          </fig>
        </p>
        <p id="Par95">
          <fig id="Fig10">
            <label>Extended Data Fig. 5</label>
            <caption>
              <title>Average animal assembly speed in frames per second as a function of graph size.</title>
              <p>Assembly rates vs. graphs size for the four datasets. Improving the assembly robustness via calibration with labeled data in large graphs incurs no extra computational cost at best, and a slowdown by 25% at worst; remarkably, it is found to accelerate assembly speed in small graphs. Relying exclusively on keypoint identity prediction results in average speeds of around 5600 frames per second, independent of graph size. Three timing experiments were run per graph size (lighter colored dots) and averages are shown. Note that assembling rates exclude CNN processing times. Speed benchmark was run on a workstation with an Intel(R) Core(TM) i9-10900X CPU 3.70GHz.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1443_Fig10_ESM" id="d32e2973"/>
          </fig>
        </p>
        <p id="Par96">
          <fig id="Fig11">
            <label>Extended Data Fig. 6</label>
            <caption>
              <title>Performance on out of domain marmoset data.</title>
              <p>(a) Example images from original dataset, and example generalization test images. (b) Median RMSE and PCK (gray numbers) for data and network (DLCRNet) as shown in Fig. <xref rid="Fig2" ref-type="fig">2a</xref>. (c) same, but on the generalization test images (n=300) (d) same but per cage as shown in a (n=30 test images per marmoset). Box plots show median, first and third quartiles, with whiskers extending past the low and high quartiles to ± 1.5 times the interquartile range.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1443_Fig11_ESM" id="d32e2990"/>
          </fig>
        </p>
        <p id="Par97">
          <fig id="Fig12">
            <label>Extended Data Fig. 7</label>
            <caption>
              <title>Comparison of top-down methods with and without assembly.</title>
              <p>(a) Schematics of top-down method with example images from the pup dataset, which consists of first detecting individuals and then performing pose prediction on each bounding box (plus is human ground truth, and the circle is bottom-up model (DLCRNet, stride 8, data-driven) predictions. (b) Performance mAP computed for top-down method with and without PAFs and bottom-up method (baseline, data-driven) as also shown in Fig. <xref rid="Fig2" ref-type="fig">2d</xref>. PAF vs. w/o PAF one-way ANOVA p-values, tri-mouse: 4.656e-11, pups: 3.62e-12, marmosets: 1.33e-28, fish: 1.645e-06). There were significant model effects across all datasets: one-way ANOVA p-values– tri-mouse: 4.13e-11, pups: 4.59e-25, marmosets: 3.04e-40, fish: 1.18e-14. (c) Example predictions within the smaller images (that is, bounded crops) from the top-down model (that is, w/PAF), and bottom-up predictions (full images, as noted).</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1443_Fig12_ESM" id="d32e3007"/>
          </fig>
        </p>
        <p id="Par98">
          <fig id="Fig13">
            <label>Extended Data Fig. 8</label>
            <caption>
              <title>Performance of idtracker.ai.</title>
              <p>(a) Segmented regions (red) overlaid on example image in idtracker.ai GUI. Example how idtracker.ai fails to segment only the mice in the full data frame for tri-mouse and (b) in marmoset dataset. (c) Using the ROI selection feature, we could find mostly just mice. However, due to the inhomogeneous lighting, the segmentation is not error-free. (d) Result of a grid search to find optimal parameters for idtracker with MOTA scores on the same videos as shown in Fig. <xref rid="Fig3" ref-type="fig">3a,e</xref>; one-sided, one-sample T-tests indicated that idtracker.ai performed significantly worse than DeepLabCut in both datasets (tri-mouse: T=-11.03, p=0.0008, d=5.52; marmosets: T=-8.43, p=0.0018, d=4.22).</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1443_Fig13_ESM" id="d32e3024"/>
          </fig>
        </p>
        <p id="Par99">
          <fig id="Fig14">
            <label>Extended Data Fig. 9</label>
            <caption>
              <title>Parameter sensitivity: Evaluation of number of body parts, frames, and PAF sizes.</title>
              <p>(a) The number of keypoints affects mAP; evaluated with ResNet50 stride8 on the two datasets with the most keypoints originally labeled by subsampling the keypoints. [Mouse: snout/tailbase (2) + leftear/rightear (4) + shoulder/spine1/spine2/spine3 (8) vs. full (12); Marmoset: Middle/Body2 (2) + FL1/BL1 /FR1/BR1/Left/Right (8) + front/body1/body3 (11) vs. full (15)] (b) Identity prediction is not strongly affected by the number of keypoints used (same experiments as in a, but for identity). (c) Impact of graph size, and randomly dropping edges on performance. (d) Test performance on 30% of the data vs. training set size (as fraction of 70%) for all four datasets.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1443_Fig14_ESM" id="d32e3038"/>
          </fig>
        </p>
      </sec>
    </app>
  </app-group>
  <fn-group>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>These authors jointly supervised this work: Mackenzie Weygandt Mathis, Alexander Mathis.</p>
    </fn>
  </fn-group>
  <sec>
    <sec id="FPar1">
      <title>Extended data</title>
      <p id="Par87">is available for this paper at 10.1038/s41592-022-01443-0.</p>
    </sec>
    <sec id="FPar2" sec-type="supplementary-material">
      <title>Supplementary information</title>
      <p id="Par88">The online version contains supplementary material available at 10.1038/s41592-022-01443-0.</p>
    </sec>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>Funding was provided by the Rowland Institute at Harvard University (M.W.M., T.N., A.M. and J.L.), the Chan Zuckerberg Initiative DAF (M.W.M., A.M. and J.L.), SNSF (M.W.M.) and EPFL (M.W.M., A.M., S.Y., S.S. and M.Z.). Dataset collection was funded by: Office of Naval Research grant nos. N000141410533 and N00014-15-1-2234 (G.L.), HHMI and NIH grant no. 2R01HD082131 (M.M.R. and C.D.); NIH grant no. 1R01NS116593-01 (M.M.R., C.D. and V.N.M.). We are grateful to M. Vidal for converting datasets. We thank the DLC community for feedback and testing. M.W.M. is the Bertarelli Foundation Chair of Integrative Neuroscience. The funders had no role in the conceptualization, design, data collection, analysis, decision to publish or preparation of the manuscript.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>Conceptualization was done by A.M. and M.W.M. Formal analysis and code were done by J.L., A.M. and M.W.M. New deep architectures were designed by M.Z., S.Y. and A.M. GUIs were done by J.L., M.W.M. and T.N. Benchmark was set by S.S., M.W.M., A.M. and J.L. Marmoset data were gathered by W.M. and G.F. Marmoset behavioral analysis was carried out by W.M. Parenting data were gathered by M.M.R., A.M. and C.D. Tri-mouse data were gathered by D.S., A.M. and V.N.M. Fish data were gathered by V.D.S. and G.L. The article was written by A.M., M.W.M. and J.L. with input from all authors. M.W.M. and A.M. co-supervised the project.</p>
  </notes>
  <notes notes-type="peer-review">
    <title>Peer review</title>
    <sec id="FPar3">
      <title>Peer review information</title>
      <p id="Par89"><italic>Nature Methods</italic> thanks the anonymous reviewers for their contribution to the peer review of this work. Nina Vogt was the primary editor on this article and managed its editorial process and peer review in collaboration with the rest of the editorial team.</p>
    </sec>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>For this study, we established four differently challenging multi-animal datasets from ecology and neuroscience. Data collection was institutionally approved: tri-mouse, parenting behavior, fish schooling from Harvard University IACUC and marmosets from MIT and Broad Institute IACUC. They are available to download, minus a small amount (30%) held out as benchmark competition data, at <ext-link ext-link-type="uri" xlink:href="https://benchmark.deeplabcut.org/">https://benchmark.deeplabcut.org/</ext-link>, and on Zenodo<sup><xref ref-type="bibr" rid="CR54">54</xref>–<xref ref-type="bibr" rid="CR57">57</xref></sup>. Findings in this paper can be replicated using the downloadable data and supplied code.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>Code to use this package is found at <ext-link ext-link-type="uri" xlink:href="https://github.com/DeepLabCut/DeepLabCut">https://github.com/DeepLabCut/DeepLabCut</ext-link> with a LGPL-3.0 License. Code to reproduce the figures from this work is found at <ext-link ext-link-type="uri" xlink:href="https://github.com/DeepLabCut/maDLC_NatureMethods2022">https://github.com/DeepLabCut/maDLC_NatureMethods2022</ext-link>.</p>
  </notes>
  <notes id="FPar4" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par90">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kays</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Crofoot</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Jetz</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Wikelski</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Terrestrial animal tracking as an eye on life and planet</article-title>
        <source>Science</source>
        <year>2015</year>
        <volume>348</volume>
        <fpage>aaa2478</fpage>
        <pub-id pub-id-type="doi">10.1126/science.aaa2478</pub-id>
        <?supplied-pmid 26068858?>
        <pub-id pub-id-type="pmid">26068858</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schofield</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Chimpanzee face recognition from videos in the wild using deep learning</article-title>
        <source>Sci. Adv.</source>
        <year>2019</year>
        <volume>5</volume>
        <fpage>eaaw0736</fpage>
        <pub-id pub-id-type="doi">10.1126/sciadv.aaw0736</pub-id>
        <?supplied-pmid 31517043?>
        <pub-id pub-id-type="pmid">31517043</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Norouzzadeh</surname>
            <given-names>MS</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning</article-title>
        <source>Proc. Natl Acad. Sci. USA</source>
        <year>2018</year>
        <volume>115</volume>
        <fpage>E5716</fpage>
        <lpage>E5725</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1719367115</pub-id>
        <?supplied-pmid 29871948?>
        <pub-id pub-id-type="pmid">29871948</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Vidal, M., Wolf, N., Rosenberg, B., Harris, B. P. &amp; Mathis, A. Perspectives on individual animal identification from biology and computer vision. <italic>Integr. Comp. Biol.</italic><bold>61</bold>, 900–916 (2021).</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Datta</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>Anderson</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Branson</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Perona</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Leifer</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Computational neuroethology: a call to action</article-title>
        <source>Neuron</source>
        <year>2019</year>
        <volume>104</volume>
        <fpage>11</fpage>
        <lpage>24</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuron.2019.09.038</pub-id>
        <?supplied-pmid 31600508?>
        <pub-id pub-id-type="pmid">31600508</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mathis</surname>
            <given-names>MW</given-names>
          </name>
          <name>
            <surname>Mathis</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Deep learning tools for the measurement of animal behavior in neuroscience</article-title>
        <source>Curr. Opin. Neurobiol.</source>
        <year>2020</year>
        <volume>60</volume>
        <fpage>1</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="doi">10.1016/j.conb.2019.10.008</pub-id>
        <?supplied-pmid 31791006?>
        <pub-id pub-id-type="pmid">31791006</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mathis</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Schneider</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lauer</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Mathis</surname>
            <given-names>MW</given-names>
          </name>
        </person-group>
        <article-title>A primer on motion capture with deep learning: principles, pitfalls, and perspectives</article-title>
        <source>Neuron</source>
        <year>2020</year>
        <volume>108</volume>
        <fpage>44</fpage>
        <lpage>65</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuron.2020.09.017</pub-id>
        <?supplied-pmid 33058765?>
        <pub-id pub-id-type="pmid">33058765</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Pereira, T. D., Shaevitz, J. W. &amp; Murthy, M. Quantifying behavior to understand the brain. <italic>Nat. Neurosci.</italic><bold>23</bold>, 1537–1549 (2020).</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Cao, Z., Simon, T., Wei, S.-E. &amp; Sheikh, Y. Realtime multi-person 2D pose estimation using part affinity fields. In <italic>Proc. IEEE Conference on Computer Vision and Pattern Recognition</italic> 7291–7299 (IEEE, 2017).</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Newell, A., Huang, Z. &amp; Deng, J. Associative embedding: end-to-end learning for joint detection and grouping. In <italic>Proc. 31st Conference on Neural Information Processing Systems</italic> (eds Guyon, I. et al.) 2277–2287 (NIPS, 2017).</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Cheng, B. et al. Higherhrnet: scale-aware representation learning for bottom-up human pose estimation. In <italic>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> 5386–5395 (IEEE, 2020).</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Stoffl, L., Vidal, M. &amp; Mathis, A. End-to-end trainable multi-instance pose estimation with transformers. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2103.12115">https://arxiv.org/abs/2103.12115</ext-link> (2021).</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Insafutdinov, E., Pishchulin, L., Andres, B., Andriluka, M. &amp; Schiele, B. DeeperCut: a deeper, stronger, and faster multi-person pose estimation model. In <italic>Proc.</italic><italic>European Conference on Computer Vision</italic> 34–50 (Springer, 2016).</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Kreiss, S., Bertoni, L. &amp; Alahi, A. Pifpaf: composite fields for human pose estimation. In <italic>Proc. IEEE Conference on Computer Vision and Pattern Recognition</italic> 11977–11986 (IEEE, 2019).</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Segalin</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The mouse action recognition system (MARS) software pipeline for automated analysis of social behaviors in mice</article-title>
        <source>eLife</source>
        <year>2021</year>
        <volume>10</volume>
        <fpage>e63720</fpage>
        <pub-id pub-id-type="doi">10.7554/eLife.63720</pub-id>
        <?supplied-pmid 34846301?>
        <pub-id pub-id-type="pmid">34846301</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Pereira, T. D. et al. SLEAP: multi-animal pose tracking. Preprint at <italic>bioRxiv</italic>10.1101/2020.08.31.276246 (2020).</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Chen, Z. et al. AlphaTracker: a multi-animal tracking and behavioral analysis tool. Preprint at <italic>bioRxiv</italic>10.1101/2020.12.04.405159 (2020).</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Lin, T.-Y. et al. Microsoft COCO: common objects in context. In <italic>Proc. European Conference on Computer Vision</italic> 740–755 (Springer, 2014).</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mathis</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deeplabcut: markerless pose estimation of user-defined body parts with deep learning</article-title>
        <source>Nat. Neurosci.</source>
        <year>2018</year>
        <volume>21</volume>
        <fpage>1281</fpage>
        <lpage>1289</lpage>
        <pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id>
        <?supplied-pmid 30127430?>
        <pub-id pub-id-type="pmid">30127430</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nath</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Using deeplabcut for 3D markerless pose estimation across species and behaviors</article-title>
        <source>Nat. Protoc.</source>
        <year>2019</year>
        <volume>14</volume>
        <fpage>2152</fpage>
        <lpage>2176</lpage>
        <pub-id pub-id-type="doi">10.1038/s41596-019-0176-0</pub-id>
        <?supplied-pmid 31227823?>
        <pub-id pub-id-type="pmid">31227823</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Mathis, A. et al. Pretraining boosts out-of-domain robustness for pose estimation. In <italic>Proc. IEEE/CVF Winter Conference on Applications of Computer Vision</italic> 1859–1868 (IEEE, 2021).</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &amp; Sun, J. Deep residual learning for image recognition. In <italic>Proc. IEEE Conference on Computer Vision and Pattern Recognition</italic> 770–778 (IEEE, 2016).</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Tan, M. &amp; Le, Q. EfficientNet: rethinking model scaling for convolutional neural networks. In <italic>Proc. International Conference on Machine Learning</italic> 6105–6114 (PMLR, 2019).</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ghosh</surname>
            <given-names>KK</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Miniaturized integration of a fluorescence microscope</article-title>
        <source>Nat. Methods</source>
        <year>2011</year>
        <volume>8</volume>
        <fpage>871</fpage>
        <lpage>878</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.1694</pub-id>
        <?supplied-pmid 21909102?>
        <pub-id pub-id-type="pmid">21909102</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Bewley, A., Ge, Z., Ott, L., Ramos, F. &amp; Upcroft, B. Simple online and realtime tracking. In <italic>Proc. 2016 IEEE International Conference on Image Processing (ICIP)</italic> 3464–3468 (IEEE, 2016).</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Bertozzi, M. et al. Pedestrian localization and tracking system with Kalman filtering. In <italic>Proc.</italic><italic>IEEE Intelligent Vehicles Symposium, 2004</italic> 584–589 (IEEE, 2004).</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Romero-Ferrero</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Bergomi</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Hinz</surname>
            <given-names>RC</given-names>
          </name>
          <name>
            <surname>Heras</surname>
            <given-names>FJ</given-names>
          </name>
          <name>
            <surname>de Polavieja</surname>
            <given-names>GG</given-names>
          </name>
        </person-group>
        <article-title>idtracker.ai: tracking all individuals in small or large collectives of unmarked animals</article-title>
        <source>Nat. Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>179</fpage>
        <lpage>182</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-018-0295-5</pub-id>
        <?supplied-pmid 30643215?>
        <pub-id pub-id-type="pmid">30643215</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kane</surname>
            <given-names>GA</given-names>
          </name>
          <name>
            <surname>Lopes</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Saunders</surname>
            <given-names>JL</given-names>
          </name>
          <name>
            <surname>Mathis</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mathis</surname>
            <given-names>MW</given-names>
          </name>
        </person-group>
        <article-title>Real-time, low-latency closed-loop feedback using markerless posture tracking</article-title>
        <source>eLife</source>
        <year>2020</year>
        <volume>9</volume>
        <fpage>e61909</fpage>
        <pub-id pub-id-type="doi">10.7554/eLife.61909</pub-id>
        <?supplied-pmid 33289631?>
        <pub-id pub-id-type="pmid">33289631</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Claudi, F. Mouse top detailed. <italic>Zenodo</italic>10.5281/zenodo.3925997 (2020).</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Autry</surname>
            <given-names>AE</given-names>
          </name>
          <name>
            <surname>Bergan</surname>
            <given-names>JF</given-names>
          </name>
          <name>
            <surname>Watabe-Uchida</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Dulac</surname>
            <given-names>CG</given-names>
          </name>
        </person-group>
        <article-title>Galanin neurons in the medial preoptic area govern parental behaviour</article-title>
        <source>Nature</source>
        <year>2014</year>
        <volume>509</volume>
        <fpage>325</fpage>
        <lpage>330</lpage>
        <pub-id pub-id-type="doi">10.1038/nature13307</pub-id>
        <?supplied-pmid 24828191?>
        <pub-id pub-id-type="pmid">24828191</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kohl</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Functional circuit architecture underlying parental behaviour</article-title>
        <source>Nature</source>
        <year>2018</year>
        <volume>556</volume>
        <fpage>326</fpage>
        <lpage>331</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-018-0027-0</pub-id>
        <?supplied-pmid 29643503?>
        <pub-id pub-id-type="pmid">29643503</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Di Santo</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Blevins</surname>
            <given-names>EL</given-names>
          </name>
          <name>
            <surname>Lauder</surname>
            <given-names>GV</given-names>
          </name>
        </person-group>
        <article-title>Batoid locomotion: effects of speed on pectoral fin deformation in the little skate, <italic>Eucoraja erinacea</italic></article-title>
        <source>J. Exp. Biol.</source>
        <year>2017</year>
        <volume>220</volume>
        <fpage>705</fpage>
        <lpage>712</lpage>
        <pub-id pub-id-type="doi">10.1242/jeb.148767</pub-id>
        <?supplied-pmid 27965272?>
        <pub-id pub-id-type="pmid">27965272</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Li, J. et al. CrowdPose: efficient crowded scenes pose estimation and a new benchmark. In <italic>Proc. IEEE Conference on Computer Vision and Pattern Recognition</italic> 10863–10872 (IEEE, 2019).</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Sandler, M., Howard, A., Zhu, M., Zhmoginov, A. &amp; Chen, L.-C. MobileNetV2: inverted residuals and linear bottlenecks. In <italic>Proc. IEEE Conference on Computer Vision and Pattern Recognition</italic> 4510–4520 (IEEE, 2018).</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Lauer, J. et al. Multi-animal pose estimation and tracking with DeepLabCut. Preprint at <italic>bioRxiv</italic>10.1101/2021.04.30.442096 (2021).</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Kingma, D. P. &amp; Ba, J. Adam: a method for stochastic optimization. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link> (2014).</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Huang, J., Zhu, Z., Guo, F. &amp; Huang, G. The devil is in the details: delving into unbiased data processing for human pose estimation. In <italic>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> 5700–5709 (IEEE, 2020).</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Insafutdinov, E. et al. ArtTrack: articulated multi-person tracking in the wild. In <italic>Proc. IEEE Conference on Computer Vision and Pattern Recognition</italic> 6457–6465 (IEEE, 2017).</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Biggs, B., Roddick, T., Fitzgibbon, A. &amp; Cipolla, R. Creatures great and small: recovering the shape and motion of animals from video. In <italic>Proc. Asian Conference on Computer Vision</italic> 3–19 (Springer, 2018).</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ramanan</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Articulated human detection with flexible mixtures of parts</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2012</year>
        <volume>35</volume>
        <fpage>2878</fpage>
        <lpage>2890</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2012.261</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Huang, A. Similarity measures for text document clustering. In <italic>Proc. Sixth New Zealand Computer Science Research Student Conference (NZCSRSC2008)</italic> Vol. 4, 9–56 (2008).</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vallat</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Pingouin: statistics in Python</article-title>
        <source>J. Open Source Softw.</source>
        <year>2018</year>
        <volume>3</volume>
        <fpage>1026</fpage>
        <pub-id pub-id-type="doi">10.21105/joss.01026</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Sun, K., Xiao, B., Liu, D. &amp; Wang, J. Deep high-resolution representation learning for human pose estimation. In <italic>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> 5693–5703 (IEEE, 2019).</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Girdhar, R., Gkioxari, G., Torresani, L., Paluri, M. &amp; Tran, D. Detect-and-track: efficient pose estimation in videos. In <italic>Proc. IEEE Conference on Computer Vision and Pattern Recognition</italic> 350–359 (IEEE, 2018).</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Emami, P., Pardalos, P. M., Elefteriadou, L. &amp; Ranka, S. Machine learning methods for data association in multi-object tracking. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1802.06897">https://arxiv.org/abs/1802.06897</ext-link> (2018).</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Zhang, L., Li, Y. &amp; Nevatia, R. Global data association for multi-object tracking using network flows. In <italic>Proc.</italic><italic>2008 IEEE Conference on Computer Vision and Pattern Recognition</italic> 1–8 (IEEE, 2008).</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">Hagberg, A. A., Schult, D. A. &amp; Swart, P. J. Exploring network structure, dynamics, and function using NetworkX. In <italic>Proc. 7th Python in Science Conference</italic> (eds Varoquaux, G. et al.) 11–15 (2008).</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <mixed-citation publication-type="other">Dicle, C., Camps, O. I. &amp; Sznaier, M. The way they move: tracking multiple targets with similar appearance. In <italic>Proc. IEEE International Conference on Computer Vision</italic> 2304–2311 (IEEE, 2013).</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yin</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Model order determination using the Hankel matrix of impulse responses</article-title>
        <source>Appl. Math. Lett.</source>
        <year>2011</year>
        <volume>24</volume>
        <fpage>797</fpage>
        <lpage>802</lpage>
        <pub-id pub-id-type="doi">10.1016/j.aml.2010.12.046</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Ahuja, R. K., Magnanti, T. L. &amp; Orlin, J. B. <italic>Network Flows: Theory, Algorithms, and Applications</italic> (Prentice-Hall, 1993).</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">He, S. et al. TransReID: transformer-based object re-identification. In <italic>Proc. IEEE/CVF International Conference on Computer Vision (ICCV)</italic> 15013–15022 (IEEE, 2021).</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bernardin</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Stiefelhagen</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Evaluating multiple object tracking performance: the clear mot metrics</article-title>
        <source>EURASIP J. Image Video Proc.</source>
        <year>2008</year>
        <volume>2008</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1155/2008/246309</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tenenbaum</surname>
            <given-names>JB</given-names>
          </name>
          <name>
            <surname>De Silva</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Langford</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>A global geometric framework for nonlinear dimensionality reduction</article-title>
        <source>Science</source>
        <year>2000</year>
        <volume>290</volume>
        <fpage>2319</fpage>
        <lpage>2323</lpage>
        <pub-id pub-id-type="doi">10.1126/science.290.5500.2319</pub-id>
        <?supplied-pmid 11125149?>
        <pub-id pub-id-type="pmid">11125149</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <mixed-citation publication-type="other">Lauer, J. et al. madlc marmoset benchmark dataset—training. <italic>Zenodo</italic>10.5281/zenodo.5849371 (2022).</mixed-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <mixed-citation publication-type="other">Lauer, J. et al. madlc fish benchmark dataset—training. <italic>Zenodo</italic>10.5281/zenodo.5849286 (2022).</mixed-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <mixed-citation publication-type="other">Lauer, J. et al. madlc parenting benchmark dataset—training. <italic>Zenodo</italic>10.5281/zenodo.5851109 (2022).</mixed-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <mixed-citation publication-type="other">Lauer, J. et al. madlc tri-mouse benchmark dataset—training. <italic>Zenodo</italic>10.5281/zenodo.5851157 (2022).</mixed-citation>
    </ref>
  </ref-list>
</back>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Nat Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nat Methods</journal-id>
    <journal-title-group>
      <journal-title>Nature Methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1548-7091</issn>
    <issn pub-type="epub">1548-7105</issn>
    <publisher>
      <publisher-name>Nature Publishing Group US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9007739</article-id>
    <article-id pub-id-type="pmid">35414125</article-id>
    <article-id pub-id-type="publisher-id">1443</article-id>
    <article-id pub-id-type="doi">10.1038/s41592-022-01443-0</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Multi-animal pose estimation, identification and tracking with DeepLabCut</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Lauer</surname>
          <given-names>Jessy</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhou</surname>
          <given-names>Mu</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ye</surname>
          <given-names>Shaokai</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Menegas</surname>
          <given-names>William</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Schneider</surname>
          <given-names>Steffen</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2092-7159</contrib-id>
        <name>
          <surname>Nath</surname>
          <given-names>Tanmay</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Rahman</surname>
          <given-names>Mohammed Mostafizur</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Di Santo</surname>
          <given-names>Valentina</given-names>
        </name>
        <xref ref-type="aff" rid="Aff6">6</xref>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9099-7294</contrib-id>
        <name>
          <surname>Soberanes</surname>
          <given-names>Daniel</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8021-277X</contrib-id>
        <name>
          <surname>Feng</surname>
          <given-names>Guoping</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2443-4252</contrib-id>
        <name>
          <surname>Murthy</surname>
          <given-names>Venkatesh N.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lauder</surname>
          <given-names>George</given-names>
        </name>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dulac</surname>
          <given-names>Catherine</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7368-4456</contrib-id>
        <name>
          <surname>Mathis</surname>
          <given-names>Mackenzie Weygandt</given-names>
        </name>
        <address>
          <email>mackenzie@post.harvard.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3777-2202</contrib-id>
        <name>
          <surname>Mathis</surname>
          <given-names>Alexander</given-names>
        </name>
        <address>
          <email>alexander.mathis@epfl.ch</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.5333.6</institution-id><institution-id institution-id-type="ISNI">0000000121839049</institution-id><institution>Brain Mind Institute, School of Life Sciences, Swiss Federal Institute of Technology (EPFL), </institution></institution-wrap>Lausanne, Switzerland </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.38142.3c</institution-id><institution-id institution-id-type="ISNI">000000041936754X</institution-id><institution>Rowland Institute at Harvard, Harvard University, </institution></institution-wrap>Cambridge, MA USA </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.116068.8</institution-id><institution-id institution-id-type="ISNI">0000 0001 2341 2786</institution-id><institution>Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, </institution><institution>Massachusetts Institute of Technology, </institution></institution-wrap>Cambridge, MA USA </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.38142.3c</institution-id><institution-id institution-id-type="ISNI">000000041936754X</institution-id><institution>Department for Molecular Biology and Center for Brain Science, </institution><institution>Harvard University, </institution></institution-wrap>Cambridge, MA USA </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.413575.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 2167 1581</institution-id><institution>Howard Hughes Medical Institute (HHMI), </institution></institution-wrap>Chevy Chase, MD USA </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.38142.3c</institution-id><institution-id institution-id-type="ISNI">000000041936754X</institution-id><institution>Department of Organismic and Evolutionary Biology, </institution><institution>Harvard University, </institution></institution-wrap>Cambridge, MA USA </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="GRID">grid.10548.38</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 9377</institution-id><institution>Department of Zoology, </institution><institution>Stockholm University, </institution></institution-wrap>Stockholm, Sweden </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>12</day>
      <month>4</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>12</day>
      <month>4</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2022</year>
    </pub-date>
    <volume>19</volume>
    <issue>4</issue>
    <fpage>496</fpage>
    <lpage>504</lpage>
    <history>
      <date date-type="received">
        <day>23</day>
        <month>4</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>4</day>
        <month>3</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Estimating the pose of multiple animals is a challenging computer vision problem: frequent interactions cause occlusions and complicate the association of detected keypoints to the correct individuals, as well as having highly similar looking animals that interact more closely than in typical multi-human scenarios. To take up this challenge, we build on DeepLabCut, an open-source pose estimation toolbox, and provide high-performance animal assembly and tracking—features required for multi-animal scenarios. Furthermore, we integrate the ability to predict an animal’s identity to assist tracking (in case of occlusions). We illustrate the power of this framework with four datasets varying in complexity, which we release to serve as a benchmark for future algorithm development.</p>
    </abstract>
    <abstract id="Abs2" abstract-type="web-summary">
      <p id="Par2">DeepLabCut is extended to enable multi-animal pose estimation, animal identification and tracking, thereby enabling the analysis of social behaviors.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Machine learning</kwd>
      <kwd>Computational neuroscience</kwd>
      <kwd>Zoology</kwd>
      <kwd>Behavioural methods</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/100009835</institution-id>
            <institution>Harvard University | Rowland Institute at Harvard (Rowland Institute)</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/100009152</institution-id>
            <institution>Fondation Bertarelli (Bertarelli Foundation)</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s), under exclusive licence to Springer Nature America, Inc. 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Main</title>
    <p id="Par3">Advances in sensor and transmitter technology, data mining and computational analysis herald a golden age of animal tracking across the globe<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Computer vision is a crucial tool for identifying, counting, as well as annotating animal behavior<sup><xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR4">4</xref></sup>. For the computational analysis of fine-grained behavior, pose estimation is often a crucial step and deep-learning based tools have quickly affected neuroscience, ethology and medicine<sup><xref ref-type="bibr" rid="CR5">5</xref>–<xref ref-type="bibr" rid="CR8">8</xref></sup>.</p>
    <p id="Par4">Many experiments in biology—from parenting mice to fish schooling—require measuring interactions among multiple individuals. Multi-animal pose estimation raises several challenges that can leverage advances in machine vision research, and yet others that need new solutions. In general, the process requires three steps: pose estimation (that is, keypoint localization), assembly (that is, the task of grouping keypoints into distinct animals) and tracking. Each step presents different challenges.</p>
    <p id="Par5">To make pose estimation robust to interacting and occluded animals, one should annotate frames with closely interacting animals. To associate detected keypoints to particular individuals (assembly) several solutions have been proposed, such as part affinity fields (PAFs)<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, associative embeddings<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>, transformers<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> and other mechanisms<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR14">14</xref></sup>. Tracking animals between frames can be difficult because of appearance similarity, nonstationary behaviors and possible occlusions. Building on human pose estimation research, some packages for multi-animal pose estimation have emerged<sup><xref ref-type="bibr" rid="CR15">15</xref>–<xref ref-type="bibr" rid="CR17">17</xref></sup>. Here, we developed top-performing network architectures, a data-driven assembly method, engineered tailored tracking methods and compared the current state-of-the-art networks on COCO (common objects in context)<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> on four animal datasets.</p>
    <p id="Par6">Specifically, we expanded DeepLabCut<sup><xref ref-type="bibr" rid="CR19">19</xref>–<xref ref-type="bibr" rid="CR21">21</xref></sup>, an open-source toolbox for animal pose estimation. Our contributions are as follows:<list list-type="order"><list-item><p id="Par7">Four datasets of varying difficulty for benchmarking multi-animal pose estimation networks.</p></list-item><list-item><p id="Par8">Multi-task architecture that predicts multiple conditional random fields and therefore can predict keypoints, limbs, as well as animal identity.</p></list-item><list-item><p id="Par9">A data-driven method for animal assembly that finds the optimal skeleton without user input, and that is state of the art (compared to top-models from COCO, a standard computer vision benchmark).</p></list-item><list-item><p id="Par10">A module that casts tracking as a network flow optimization problem, which aims to find globally optimal solutions.</p></list-item><list-item><p id="Par11">Unsupervised animal ID tracking: we can predict the identity of animals and reidentify them; this is particularly useful to link animals across time when temporally based tracking fails (due to intermittent occlusions).</p></list-item><list-item><p id="Par12">Graphical user interfaces (GUIs) for keypoint annotation, refinement and semiautomatic trajectory verification.</p></list-item></list></p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <p id="Par13">Multi-animal pose estimation can be cast as a data assignment problem in the spatial and temporal domains. To tackle the generic multi-animal pose-tracking scenario, we designed a practical, almost entirely data-driven solution that breaks down the larger goal into the smaller subtasks of: keypoint estimation, animal assembly (spatially grouping keypoints into individuals), local (temporal) tracking and global ‘tracklet’ stitching (Extended Data Fig. <xref rid="Fig6" ref-type="fig">1)</xref>. We evaluate our pipeline on four new datasets that we release with this paper as a benchmark at <ext-link ext-link-type="uri" xlink:href="https://benchmark.deeplabcut.org/">https://benchmark.deeplabcut.org/</ext-link>.</p>
    <sec id="Sec3">
      <title>Four diverse multi-animal datasets</title>
      <p id="Par14">We considered four multi-animal experiments to broadly validate our approach: three mice in an open field, home-cage parenting in mice, pairs of marmosets housed in a large enclosure and 14 fish in a flow tank. These datasets encompass a wide range of behaviors, presenting difficult and unique computational challenges to pose estimation and tracking (Fig. <xref rid="Fig1" ref-type="fig">1a</xref> and Extended Data Fig. <xref rid="Fig7" ref-type="fig">2)</xref>. The three mice frequently contact and occlude one another. The parenting dataset contained a single adult mouse with unique keypoints in close interaction with two pups hardly distinguishable from the background or the cotton nest, which also leads to occlusions. The marmoset dataset comprises periods of occlusion, close interactions, nonstationary behavior, motion blur and changes in scale. Likewise, the fish school along all dimensions of the tank, hiding each other in cluttered scenes, and occasionally leaving the camera’s field of view. We annotated 5–15 body parts of interest depending on the dataset (Fig. <xref rid="Fig1" ref-type="fig">1a</xref> and Extended Data Fig. <xref rid="Fig6" ref-type="fig">1)</xref>, in multiple frames for cross-validating the pose estimation and assembly performance, as well as semiautomatically annotated several videos for evaluating the tracking performance (Table <xref rid="Tab1" ref-type="table">1</xref>). For analyses, we created a random split of images plus annotations into 70% train and 30% test sets.<fig id="Fig1"><label>Fig. 1</label><caption><title>Multi-animal DeepLabCut architecture and benchmarking datasets.</title><p><bold>a</bold>, Example (cropped) images with (manual) annotations for the four datasets: mice in an open field arena, parenting mice, pairs of marmosets and schooling fish. bpts, body parts. Scale bars, 20 pixels. <bold>b</bold>, A schematic of the general pose estimation module. The architecture is trained to predict the keypoint locations, PAFs and animal identity. Three output layers per keypoint predict the probability that a joint is in a particular pixel (score map) as well as shifts in relation to the discretized output map (location refinement field). Furthermore, PAFs predict vector fields encoding the orientation of a connection between two keypoints. Example predictions are overlaid on the corresponding (cropped) marmoset frame. The PAF for the right limb helps linking the right hand and shoulder keypoints to the correct individual. <bold>c</bold>, Our architecture contains a multi-fusion module and a multi-stage decoder. In the multi-fusion module, we add the high-resolution representation (conv2, conv3) to low-resolution representation (conv5). The features from conv2 and conv3 are downsampled by two and one 3 × 3 convolution layer, respectively to match the resolution of conv5. Before concatenation the features are downsampled by a 1 × 1 convolution layer to reduce computational costs and (spatially) upsampled by two stacked 3 × 3 deconvolution layers with stride 2. The multi-stage decoder predicts score maps and PAFs. At the first stage, the feature map from the multi-fusion module are upsampled by a 3 × 3 deconvolution layer with stride 2, to get the score map, PAF and the upsampled feature. In the latter stages, the predictions from the two branches (score maps and PAFs), along with the upsampled feature are concatenated for the next stage. We applied a shortcut connection between the consecutive stage of the score map. The shown variant of DLCRNet has overall stride 2 (in general, this can be modulated from 2 to 8).</p></caption><graphic xlink:href="41592_2022_1443_Fig1_HTML" id="d32e578"/></fig><table-wrap id="Tab1"><label>Table 1</label><caption><p>Multi-animal pose estimation dataset characteristics</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Feature</th><th>Mouse</th><th>Pups</th><th>Marmosets</th><th>Fish</th></tr></thead><tbody><tr><td>Labeled frames</td><td>161</td><td>542</td><td>7,600</td><td>100</td></tr><tr><td>Keypoints</td><td>12</td><td>5 (+12)</td><td>15</td><td>5</td></tr><tr><td>Individuals</td><td>3</td><td>2 (+1)</td><td>2</td><td>14</td></tr><tr><td>GT identity</td><td>No</td><td>No</td><td>Yes</td><td>No</td></tr><tr><td>Annotated video frames</td><td>11,645</td><td>2,670</td><td>15,000</td><td>1,100</td></tr><tr><td>Total duration (s)</td><td>385</td><td>180</td><td>600</td><td>36</td></tr></tbody></table><table-wrap-foot><p>Number of labeled training frames, keypoints and individuals. Keypoint number in brackets relate to the unique animal in the frame, and unique individual in brackets is noted, that is, one parenting mouse. Animal identity was only annotated for the marmosets. For tracking, separate videos are used and the total number of densely human-annotated video frames (and their combined duration in seconds) is also indicated.</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec4">
      <title>Multi-task convolutional architectures</title>
      <p id="Par15">We developed multi-task convolutional neural networks (CNNs) that perform pose estimation by localizing keypoints in images. This is achieved by predicting score maps, which encode the probability that a keypoint occurs at a particular location, as well as location refinement fields that predict offsets to mitigate quantization errors due to downsampled score maps<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR20">20</xref></sup>. Then, to assemble keypoints into the grouping that defines an animal, we designed the networks to also predict ‘limbs’, that is, PAFs. This task, which is achieved via additional deconvolution layers, is inspired by OpenPose<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. The intuition is that in scenarios where multiple animals are present in the scene, learning to predict the location and orientation of limbs will help group pairs of keypoints belonging to an individual. Moreover, we also introduce an output that allows for animal reidentification (reID) from visual input directly. This is important in the event of animals that are untrackable using temporal information alone, for example, when exiting or re-entering the scene (Fig. <xref rid="Fig1" ref-type="fig">1b</xref>).</p>
      <p id="Par16">Specifically, we adapted ImageNet-pretrained ResNets<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, EfficientNets<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>, as well as developed a multi-scale architecture (which we call DLCRNet_ms5, Fig. <xref rid="Fig1" ref-type="fig">1c</xref>). We then use customized multiple parallel deconvolution layers to predict the location of keypoints as well as what keypoints are connected in a given animal (Fig. <xref rid="Fig1" ref-type="fig">1b</xref>). Ground truth data of annotated keypoints are used to calculate target score maps, location refinement maps, PAFs and to train the network to predict those outputs for a given input image (Fig. <xref rid="Fig1" ref-type="fig">1b,c</xref>) with augmentation.</p>
    </sec>
    <sec id="Sec5">
      <title>Keypoint detection and part affinity performance</title>
      <p id="Par17">After an extensive architecture search (<ext-link ext-link-type="uri" xlink:href="http://maDLCopt.deeplabcut.org">http://maDLCopt.deeplabcut.org</ext-link> and Extended Data Fig. <xref rid="Fig8" ref-type="fig">3)</xref>, we demonstrate that the new DLCRNet performs very well for localizing keypoints (Fig. <xref rid="Fig2" ref-type="fig">2a</xref>). Specifically, we trained independent networks for each dataset, and each split, and evaluated their performance. For each frame and keypoint, we calculated the root-mean squared error (r.m.s.e.) between the detections and their closest ground truth neighbors. All the keypoint detectors performed well (DLCRNet_ms5, median test errors of 2.65, 5.25, 4.59 and 2.72 pixels for the tri-mouse, parenting, marmoset and fish datasets, respectively, Fig. <xref rid="Fig2" ref-type="fig">2a</xref>). The scales of these data are shown in Fig. <xref rid="Fig1" ref-type="fig">1a</xref>). To ease interpretation, errors were also normalized to 33% of the tip–gill distance for the fish dataset and 33% of the left-to-right ear distance for the remaining ones (<xref rid="Sec12" ref-type="sec">Methods</xref>). We found that 93.6 ± 6.9% of the predictions on the test images were within those ranges (Fig. <xref rid="Fig2" ref-type="fig">2a</xref>).<fig id="Fig2"><label>Fig. 2</label><caption><title>Multi-animal DeepLabCut keypoint detection and whole-body assembly performance.</title><p><bold>a</bold>, Distribution of keypoint prediction error for DLCRNet_ms5 with stride 8 (70% train and 30% test split). Violin plots display train (top) and test (bottom) errors. Vertical dotted lines are the first, second and third quartiles. Median test errors were 2.69, 5.62, 4.65 and 2.80 pixels for the illustrated datasets, in order. Gray numbers indicate PCK. Only the first five keypoints of the parenting dataset belong to the pups; the 12 others are keypoints of the adult mouse. <bold>b</bold>, Illustration of our data-driven skeleton selection algorithm. Mouse cartoon adapted with permission from ref. <sup><xref ref-type="bibr" rid="CR29">29</xref></sup> under a Creative Commons licence (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>). <bold>c</bold>, Animal assembly quality as a function of part affinity graph (skeleton) size for baseline (user-defined) versus data-driven skeleton definitions. The top row displays the fraction of keypoints left unconnected after assembly, whereas the bottom row designates the accuracy of their grouping into distinct animals. The colored dots mark statistically significant interactions (two-way, repeated-measures ANOVA; see Supplementary Tables <xref rid="MOESM1" ref-type="media">1</xref>–<xref rid="MOESM1" ref-type="media">4</xref> for full statistics). Light red vertical bars highlight the graph automatically selected. <bold>d</bold>, mAP as a function of graph size. Shown on test data held out from 70% train and 30% test splits. The associative embedding method does not rely on a graph. The performance of MMPose’s implementation of ResNet-AE and HRNet-AE bottom-up variants is shown for comparison against our multi-stage architecture DLCRNet_ms5, here called Baseline. Data-driven is Baseline plus calibration method (one-way ANOVA show significant effects of the model: <italic>P</italic> values, tri-mouse 8.8 × 10<sup>−8</sup>, pups 6.5 × 10<sup>−13</sup>, marmosets 3.8 × 10<sup>−11</sup>, fish 4.0 × 10<sup>−12</sup>). <bold>e</bold>, Marmoset ID–Example test image together with overlaid animal identity prediction accuracy per keypoint averaged over all test images and test splits. With ResNet50_stride8, accuracy peaks at 99.2% for keypoints near the head and drops to only 95.1% for more distal parts. In the lower panel, plus signs denote individual splits, circles show the averages.</p></caption><graphic xlink:href="41592_2022_1443_Fig2_HTML" id="d32e838"/></fig></p>
      <p id="Par18">After detection, keypoints need to be assigned to individuals. We evaluated whether the learned PAFs helped decide whether two body parts belong to the same or different animals. For example, 66 different edges can be formed from the 12 mouse body parts and many provide high discriminability (Extended Data Fig. <xref rid="Fig9" ref-type="fig">4</xref>). We indeed found that predicted limbs were powerful at distinguishing a pair of keypoints belonging to an animal from other (incorrect) pairs linking different mice, as measured by a high auROC (area under the receiver operating characteristic) score (mean ± s.d. 0.99 ± 0.02).</p>
    </sec>
    <sec id="Sec6">
      <title>Data-driven individual assembly performance</title>
      <p id="Par19">Any limb-based assembly approach requires a ‘skeleton’, that is, a list of keypoint connections that allows the algorithm to computationally infer which body parts belong together. Naturally, there has to be a path within this skeleton connecting any two body parts, otherwise the body parts cannot be grouped into one animal. Given the combinatorial nature of skeletons, how should they be designed? We circumvented the need for arbitrary, hand-crafted skeletons by developing a method that is agnostic to an animal’s morphology and does not require any user input.</p>
      <p id="Par20">We devised a data-driven method where the network is first trained to predict all graph edges and the least discriminative edges (for deciding body part ownership) are not used at test time to determine the optimal skeleton. We found that this approach yields skeletons with fewer errors (unconnected body parts and with higher purity, Fig. <xref rid="Fig2" ref-type="fig">2b,c</xref>) and it improves performance. Crucially, it means users do not need to design any skeletons. Our data-driven method (with DLCRNet_ms5) outperforms the naive (baseline) method, enhances ‘purity of the assembly’: that is, the fraction of keypoints that were grouped correctly per individual (Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>), and reduces the number of missing keypoints (Supplementary Table <xref rid="MOESM1" ref-type="media">2</xref>). Comparisons revealed significantly higher assembly purity with automatic skeleton pruning versus a naive skeleton definition at most graph sizes, with respective gains of up to 3.0, 2.0 and 2.4 percentage points in the tri-mouse (two-way repeated measure analyses of variance (ANOVA): graph size 23; <italic>P</italic> &lt; 0.001), marmosets (graph size 34, <italic>P</italic> = 0.002) and fish datasets (graph size 6, <italic>P</italic> &lt; 0.001) (Fig. <xref rid="Fig2" ref-type="fig">2b,c</xref>). Furthermore, to accommodate diverse body plans and annotated keypoints for different animals and experiments, our inference algorithm works for arbitrary graphs. Animal assembly achieves at least 400 frames per second in scenes with 14 animals, and up to 2,000 for small skeletons in two or three animals (Extended Data Fig. <xref rid="Fig10" ref-type="fig">5</xref>).</p>
      <p id="Par21">To additionally benchmark our network and assembly contributions, we compared them to methods that achieve state-of-the art performance on COCO<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, a challenging, large-scale multi-human pose estimation benchmark. Specifically, we considered HRNet-AE and ResNet-AE. Our models performed significantly better than these state-of-the-art methods (one-way ANOVA: <italic>P</italic> values, tri-mouse 8.8 × 10<sup>−08</sup>, pups 6.5 × 10<sup>−13</sup>, marmosets 3.8 × 10<sup>−11</sup>, fish 4.0 × 10<sup>−12</sup>, Fig. <xref rid="Fig2" ref-type="fig">2d</xref>) on all four animal benchmark datasets. Last, while the datasets themselves contain diverse animal behaviors, and only 70% is used to train, as an additional test of generalization we used ten held-out marmoset videos that came from different cages (Extended Data Fig. <xref rid="Fig11" ref-type="fig">6</xref>). We find in this challenging test there is a roughly 0.25 drop in mean average precision (mAP). It is known that simply adding (a fraction of the new) data into the training set alleviates such drops (reviewed in ref. <sup><xref ref-type="bibr" rid="CR7">7</xref></sup>).</p>
      <p id="Par22">We reasoned the strong multi-animal performance is due to the assembly algorithm based on PAFs. Therefore, we tested the performance of the network in a top-down setting with and without PAFs, that is, by considering images that are cropped around each animal (bounding boxes, Extended Data Fig. <xref rid="Fig7" ref-type="fig">7a</xref>). We found that our assembly algorithm significantly improves mAP performance (PAF versus without PAF one-way ANOVA <italic>P</italic>, tri-mouse 4.656 × 10<sup>−11</sup>, pups 3.62 × 10<sup>−12</sup>, marmosets 1.33 × 10<sup>−28</sup>, fish 1.645 × 10<sup>−6</sup>, Extended Data Fig. <xref rid="Fig7" ref-type="fig">7b,c</xref>). Collectively, the direct assembly to tracking (that is, the bottom-up method) is likely the optimal approach for most users as it reasons over the whole image.</p>
    </sec>
    <sec id="Sec7">
      <title>Predicting animal identity from images</title>
      <p id="Par23">Animals sometimes differ visually, for example due to distinct coat patterns, because they are marked, or carry different instruments (such as an integrated microscope<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>). To allow our method to take advantage of such scenarios and improve tracking later on, we developed a network head that learns the identity (ID) of animals with the same CNN backbone. To benchmark the ID output, we focused on the marmoset data, where (for each pair) one marmoset had light blue dye applied to its tufts. ID prediction accuracy on the test images ranged from &gt;0.99 for the keypoints closest to the marmoset’s head to 0.95 for more distal keypoints (Fig. <xref rid="Fig2" ref-type="fig">2e</xref> and Extended Data Fig. <xref rid="Fig8" ref-type="fig">3c</xref>). Thus, DeepLabCut can reID the animal on a per-body-part basis (Fig. <xref rid="Fig2" ref-type="fig">2e</xref>).</p>
    </sec>
    <sec id="Sec8">
      <title>Tracking of individuals</title>
      <p id="Par24">Once keypoints are assembled into individual animals, the next step is to link them temporally. To measure performance in the next steps, entire videos (one from each dataset) are manually refined to form ground truth sequences (Fig. <xref rid="Fig3" ref-type="fig">3a</xref> and Table <xref rid="Tab1" ref-type="table">1</xref>). Reasoning over the whole video for tracking individuals is not only computationally costly, but also unnecessary. For instance, when animals are far apart, it is straightforward to link each one correctly across time. Thus, we devised a divide-and-conquer strategy. We use a simple, online tracking approach to form reliable ‘tracklets’ from detected animals in adjacent frames. Difficult cases (for example, when animals are closely interacting or after occlusion) often interrupt the tracklets, causing ambiguous fragments that cannot be easily temporally linked. We address this crucial issue post hoc by optimally stitching tracklets using multiple spatio-temporal cues.<fig id="Fig3"><label>Fig. 3</label><caption><title>Linking whole-body assemblies across time.</title><p><bold>a</bold>, Ground truth and reconstructed animal tracks (with DLCRNet and ellipse tracking), together with video frames illustrating representative scene challenges. <bold>b</bold>, The identities of animals detected in a frame are propagated across frames using local matching between detections and trackers (with costs, ‘motion’ for all datsets and ‘distance’ for fish). <bold>c</bold>, Tracklets are represented as nodes of a graph, whose edges encode the likelihood that the connected pair of tracklet belongs to the same track. <bold>d</bold>, Four cost functions modeling the affinity between tracklets are implemented: shape similarity using the undirected Hausdorff distance between finite sets of keypoints (i); spatial proximity in Euclidean space (ii); motion affinity using bidirectional prediction of a tracklet’s location (iii); and dynamic similarity via Hankelets and time-delay embedding of a tracklet’s centroid (iv). <bold>e</bold>, Tracklet stitching performance versus box and ellipse tracker baselines (arrows indicate if higher or lower number is better), using MOTA, as well as rates of false negative (FN), false positives (FP) and identity switch expressed in events per animal and per sequence of 100 frames. Inset shows that incorporating appearance/identity prediction in the stitching further reduces the number of switches and improves full track reconstruction. Total number of frames: tri-mouse, 2,330; parenting, 2,670; marmosets, 15,000 and fish, 601.</p></caption><graphic xlink:href="41592_2022_1443_Fig3_HTML" id="d32e985"/></fig></p>
      <p id="Par25">Assembled animals are linked across frames to form tracklets, that is, fragments of continuous trajectories. This task entails the propagation of an animal’s identity in time by finding the optimal association between an animal and its predicted location in the adjacent frame (Fig. <xref rid="Fig3" ref-type="fig">3b–d</xref>). The prediction is made by a lightweight ‘tracker’. In particular, we implemented a box and an ellipse tracker. Whereas the former is standard in the object tracking literature (for example, refs. <sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup>), we recognized the sensitivity of its formulation to outlier detections (as it is mostly used for pedestrian tracking). Thus, the ellipse tracker was developed to provide a finer parametrization of an animal’s geometry. Overall, the ellipse tracker behaves better than the box tracker, reaching near-perfect multi-object tracking accuracy (MOTA) (0.78 versus 0.97) and producing on average 92% less false negatives; no differences in the switch rate was observed (Fig. <xref rid="Fig3" ref-type="fig">3e</xref>).</p>
      <p id="Par26">Because of occlusions, dissimilarity between an animal and its predicted state, or other challenging yet common multi-animal tracking issues, tracklets can be interrupted and therefore rarely form complete tracks across a video. The remaining challenge therefore is to stitch these sparse tracklets so as to guarantee continuity and kinematic consistency. Our approach is to cast this task as a global minimization problem, where connecting two candidate tracklets incurs a cost inversely proportional to the likelihood that they belong to the same track. Advantageously, the problem can now be elegantly solved using optimization techniques on graph and affinity models (Fig. <xref rid="Fig3" ref-type="fig">3c,d</xref>).</p>
      <p id="Par27">Compared to only local tracking, we find that our stitching method reduces switches, even in the challenging fish and marmosets datasets (average reduction compared to local ellipse tracking, 63%; Fig. <xref rid="Fig3" ref-type="fig">3e</xref>). To handle a wide range of scenarios, multiple cost functions are devised to model the affinity between a pair of tracklets based on their shape, proximity, motion, dynamics and/or appearance (below and Supplementary Videos <xref rid="MOESM3" ref-type="media">1</xref>–<xref rid="MOESM6" ref-type="media">4)</xref>. Last, to allow users to understand the error rate and correct errors, we developed a Refine Tracklets GUI. Here, we leverage confidence of the tracking to flag sequences of frames that might need attention, namely when swaps might occur (Extended Data Fig. <xref rid="Fig6" ref-type="fig">1b</xref>).</p>
      <p id="Par28">Other recent methods for tracking animals have been proposed, such as idtracker.ai<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. While this tool does not perform pose estimation, we wanted to specifically compare tracking performance. We attempted to use the easiest (tri-mouse) and marked-animal (marmoset) datasets with idtracker.ai. After an extensive grid search for hyperparameters, only the tri-mouse mice dataset could be reliably tracked, yet the performance of our method was significantly better (one-sided, one-sample <italic>t</italic>-tests indicated that idtracker performed significantly worse than DeepLabCut in both datasets (tri-mouse <italic>t</italic> = −11.03, <italic>P</italic> = 0.0008, <italic>d</italic> = 5.52; marmosets <italic>t</italic> = −8.43, <italic>P</italic> = 0.0018, <italic>d</italic> = 4.22: Supplementary Video <xref rid="MOESM7" ref-type="media">5</xref> and Extended Data Fig. <xref rid="Fig8" ref-type="fig">8)</xref>.</p>
      <p id="Par29">Note, for keypoint selection we remain fully agnostic to the user-defined inputs, giving the user freedom over what keypoints ultimately serve their research, but we do guide the user by showing them how such decisions could affect performance (Extended Data Fig. <xref rid="Fig14" ref-type="fig">9</xref>).</p>
    </sec>
    <sec id="Sec9">
      <title>Leveraging animal ID and reID in tracking</title>
      <p id="Par30">When animals can disappear from the field of view, they cannot be tracked by temporal association alone and appearance cues are necessary. Indeed, for the marmosets, incorporating visual appearance learned in a supervised fashion, further reduced the number of switches by 26% (Fig. <xref rid="Fig3" ref-type="fig">3e</xref>). Additionally, we next considered the case with animals that are not clearly distinguishable to the human annotator, thus no ground truth can be easily provided. To tackle this challenge, we introduce an unsupervised method way based on transformers to learn animal ID via metric learning (Fig. <xref rid="Fig4" ref-type="fig">4a–c</xref> and <xref rid="Sec12" ref-type="sec">Methods</xref>). This provides up to a 10% boost in MOTA performance in the very challenging fish data, particularly in difficult sequences (Fig. <xref rid="Fig4" ref-type="fig">4d</xref>).<fig id="Fig4"><label>Fig. 4</label><caption><title>Unsupervised reID of animals.</title><p><bold>a</bold>, Schematic of the transformer architecture we adapted to take pose-tensor outputs of the DeepLabCut backbone. We trained it with triplets sampled from tracklets and tracks. <bold>b</bold>, Performance of the ReIDTransformer method on unmarked fish, mice and marked marmosets. Triplet accuracy (acc.) is reported for triplets sampled from ground truth (GT) tracks and local tracklets only. We used only the top 5% of the most crowded frames, as those are the most challenging. <bold>c</bold>, Example performance on the challenging fish data. Top: fish-identity-colored tracks. Time is given in frame number. Bottom: example frames (early versus later) from baseline or ReIDTransformer. Arrows highlight performance with ReIDTransformer: pink arrows show misses; orange shows correct ID across frames in ReIDTransformer versus blue to orange in baseline. <bold>d</bold>, Tracking metrics on the most crowded 5% of frames (30 frames for fish, 744 for marmosets, giving 420 fish targets and 1,488 marmoset targets); computed as described in <xref rid="Sec12" ref-type="sec">Methods</xref>. IDF1, ID measure, global min-cost F1 score; IDP, ID measure, global min-cost precision; IDR, ID measures: global min-cost recall; Recall, number of detections over number of objects; Precision, number of detected objects over sum of detected and false positives; GT, number of unique objects; MT, mostly tracked and FM, number of fragmentations.</p></caption><graphic xlink:href="41592_2022_1443_Fig4_HTML" id="d32e1100"/></fig></p>
    </sec>
    <sec id="Sec10">
      <title>Social marmosets</title>
      <p id="Par31">Finally, we demonstrate a use-case of multi-animal DeepLabCut by analyzing 9 h (824,568 frames) of home-cage behavior of pairs of marmosets (Fig. <xref rid="Fig5" ref-type="fig">5a,b</xref>). We tracked by ReID on a frame-by-frame basis versus only using tracklet information. We found that the marmosets display diverse postures that are captured by principal component analysis on egocentrically aligned poses (Fig. <xref rid="Fig5" ref-type="fig">5c,d</xref>). Furthermore, we found that when the animals are close, their bodies tend to be aligned and they tend to look in similar directions (Fig. <xref rid="Fig5" ref-type="fig">5e,f</xref>). Finally, we related the posture and the spatial relationship between the animals and found a nonrandom distribution. For instance, marmosets tended to face the other animal when apart (Fig. <xref rid="Fig5" ref-type="fig">5g,h</xref>). Thus, DeepLabCut can be used to study complex social interactions over long timescales.<fig id="Fig5"><label>Fig. 5</label><caption><title>Application to multi-marmoset social behaviors.</title><p><bold>a</bold>, Schematic of the marmoset recording setup. <bold>b</bold>, Example tracks, 30 min plotted from each marmoset. Scale bars, 0.2 m. <bold>c</bold>, Example egocentric posture data, where the ‘Body2’ point is (0,0) and the angle formed by ‘Body1’ and ‘Body3’ is rotated to 0°. We performed principal component analysis on the pooled data of both marmosets for all data. <bold>d</bold>, Average postures along each principal component; note that only one side of the distribution is represented in the image (that is, 0 to 2 instead of −2 to 2). <bold>e</bold>, Histogram of log-distance between a pair of marmosets normalized to ear-center distance. <bold>f</bold>, Computed body angle versus observation count. <bold>g</bold>, Density plot of where another marmoset is located relative to marmoset 1. <bold>h</bold>, Postural principal components (from <bold>d</bold>) as a function of the relative location of the other marmoset. Thereby, each point represents the average postural component score for marmoset 1 when marmoset 2 is at that point. h.u., head units.</p></caption><graphic xlink:href="41592_2022_1443_Fig5_HTML" id="d32e1155"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec11" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par32">Here we introduced a multi-animal pose estimation and tracking system thereby extending DeepLabCut<sup><xref ref-type="bibr" rid="CR19">19</xref>–<xref ref-type="bibr" rid="CR21">21</xref></sup>. We developed and leveraged more powerful CNNs (DLCRNet) that we show have strong performance for animal pose estimation. Due to the variable body shapes of animals, we developed a data-driven way to automatically find the best skeleton for animal assembly, and we designed fast trackers that also reason over long timescales and are more robust to the body plan, and outperform tailored animal tracking methods such as idTracker.ai. Our method is flexible and not only deals with multiple animals (with the same body plan), but also with one agent dealing with multiple others (as with the parenting mouse, where there are identical looking pups, but a unique adult mouse).</p>
    <p id="Par33">For animal pose and tracking, leveraging identity can be an important tool<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. We introduce ways to allow both marked animals (supervised) and unsupervised animal identity tracking. For marked animals this means if users consistently label a known feature across the dataset (such as the blue tufts of the 40 pairs of marmosets included in labeled data, or consistent marker on a mouse’s tail), they can simply input the identity ‘true’ (or check the box in the GUI) in DeepLabCut and this trains an ID head. If they are not marked, users could use the reID tracking method that performs unsupervised identity estimation. Thereby, our framework integrates various costs related to movement statistics and the learned animal identity.</p>
    <p id="Par34">Open-access benchmark datasets are critical to collectively advance tools<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. Here, we open-source four datasets that pose different challenges. As we show, there is little room for improvement on the tri-mouse data, but the schooling fish is not a solved problem. Thus, while our method is fast, generally robust, and can leverage identity of animals to enhance pose-tracking performance, we believe the benchmark can also spur progress on how to improve performance for occluded, similar looking animals.</p>
    <p id="Par35">We developed both bottom-up and top-down variants for multi-animal DeepLabCut (maDLC), allowing the user an extended selection of methods (Supplementary Note <xref rid="MOESM1" ref-type="media">1)</xref>. While our results suggest that the bottom-up pipeline should be the default, depending on the application, top-down approaches might be better suited. Both classes have limitations and features (reviewed in ref. <sup><xref ref-type="bibr" rid="CR7">7</xref></sup>).</p>
    <p id="Par36">In this work, we strive to develop high-performance code that requires limited user input yet flexibility. In the code, we provide 3D support for multi-animal pose estimation (via multi-camera use), plus this multi-animal variant can be integrated with our real-time software, DeepLabCut-Live!<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Another important user input is at the stage of tracking, where users can input how many animals should be identified in a given video. In this paper, we test up to 14 animals, but this is not a hard upper limit. One ‘upper limit’ is ultimately the camera resolution, as one needs to be able to localize keypoints of animals. Thus, if animals are very small, other tracking tools might be better suited. From pose, to optimal skeleton selection, to tracking: all of the outlined steps can be run in ten lines of code or all from a GUI such that zero programming is required (<ext-link ext-link-type="uri" xlink:href="https://deeplabcut.github.io/DeepLabCut">https://deeplabcut.github.io/DeepLabCut</ext-link>).</p>
  </sec>
  <sec id="Sec12">
    <title>Methods</title>
    <sec id="Sec13">
      <title>Tri-mouse dataset</title>
      <p id="Par37">Three wild-type (C57BL/6J) male mice ran on a paper spool following odor trails<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. These experiments were carried out in the laboratory of V.N. Murthy at Harvard University (temperature of housing was 20–25 °C, humidity 20–50%). Data were recorded at 30 Hz with 640 × 480 pixels resolution acquired with a Point Grey Firefly FMVU-03MTM-CS. One human annotator was instructed to localize the 12 keypoints (snout, left ear, right ear, shoulder, four spine points, tail base and three tail points) across 161 frames sampled from within DeepLabCut using the <italic>k</italic>-means clustering approach (across eight videos). All surgical and experimental procedures for mice were in accordance with the NIH Guide for the Care and Use of Laboratory Animals and approved by the Harvard Institutional Animal Care and Use Committee (IACUC).</p>
    </sec>
    <sec id="Sec14">
      <title>Parenting behavior</title>
      <p id="Par38">Parenting behavior is a pup-directed behavior observed in adult mice involving complex motor actions directed toward the benefit of the offspring<sup><xref ref-type="bibr" rid="CR30">30</xref>,<xref ref-type="bibr" rid="CR31">31</xref></sup>. These experiments were carried out in the laboratory of C. Dulac at Harvard University (temperature of housing was 20–25 °C, humidity 20–50%).</p>
      <p id="Par39">The behavioral assay was performed in the home-cage of single-housed adult (age was less than 180 d old) female C57Bl6/J <italic>Mus musculus</italic> in dark (red light only) conditions. For these videos, the adult mouse was monitored for several minutes in the cage followed by the introduction of pup (4 d old, sex unknown) in one corner of the cage. The behavior of the adult and pup was monitored for a duration of 15min. A video was recorded at 30 Hz using a Microsoft LifeCam camera (part no. 6CH-00001) with a resolution of 1,280 × 720 pixels or a Geovision camera (model no. GV-BX4700-3V) also acquired at 30 frames per second (fps) at a resolution of 704 × 480 pixels. A human annotator labeled on the adult animal the same 12 body points as in the tri-mouse dataset and five body points on the pup along its spine. Initially only the two ends were labeled, and intermediate points were added by interpolation and their positions was manually adjusted if necessary. Frames were generated from across 25 videos.</p>
    </sec>
    <sec id="Sec15">
      <title>Marmoset home-cage behavior</title>
      <p id="Par40">Videos of common marmosets (<italic>Callithrix jacchus</italic>) were made in the laboratory of G. Feng at MIT. Male and female marmoset pairs housed here (<italic>n</italic> = 50, 30 pairs, age range of 2 to 12 years) were recorded using Kinect V2 cameras (Microsoft) with a resolution of 1,080 pixels and frame rate of 30 Hz. After acquisition, images to be used for training the network were manually cropped to 1,000 × 1,000 pixels or smaller. For our analysis, we used 7,600 labeled frames from 40 different marmosets collected from three different colonies (in different facilities, thus over 20 videos were used for dataset generation). Each cage contains a pair of marmosets, where one marmoset had light blue dye applied to its tufts. One human annotator labeled the 15 body points on each animal present in the frame (frames contained either one or two animals). All animal procedures were overseen by veterinary staff of the MIT and Broad Institute Department of Comparative Medicine, in compliance with the National Institutes of Health guide for the care and use of laboratory animals and approved by the MIT and Broad Institute animal care and use committees. As a test of out-of-domain generalization, we additionally labeled 300 frames from ten new cages and animals. See Fig. <xref rid="Fig5" ref-type="fig">5</xref> for example images and results.</p>
      <p id="Par41">We also analyzed two long-term recording sessions from a pairs of marmosets with the DLCRNet_ms5 model, by reidentifying each marmoset in each frame with the ID head. Overall we considered about 9 h (824,568 frames) from two different home cages. We computed the principal components for egocentric postures as well as illustrated their relative head and body orientations (Fig. <xref rid="Fig5" ref-type="fig">5)</xref>. For Fig. <xref rid="Fig5" ref-type="fig">5e</xref>, the distances are normalized based on the running average distance between each ear tuft to center of head, over a second. This measurement does correlate well with depth values in videos recorded with a depth channel (which was not done for the example sessions). To make the postural data egocentric, we first centered the data around ‘Body2’ (<italic>x</italic> = 0, <italic>y</italic> = 0) and then rotated it such that the line formed by ‘Body1’, ‘Body2’ and ‘Body3’ was as close to the line ‘<italic>x</italic> = 0’ as possible.</p>
    </sec>
    <sec id="Sec16">
      <title>Fish schooling behavior</title>
      <p id="Par42">Schools of inland silversides (<italic>Menidia beryllina</italic>, <italic>n</italic> = 14 individuals per school, sex unknown but likely to be equal females and males, aged approximately 9 months) were recorded in the Lauder Laboratory at Harvard University while swimming at 15 speeds (0.5 to 8 body lengths per s at 0.5 body lengths per s intervals) in a flow tank with a total working section of 28 × 28 × 40 cm as described in previous work<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, at a constant temperature (18 ± 1<sup>∘</sup>C) and salinity (33 parts per thousand), at a Reynolds number of approximately 10,000 (based on body length). Dorsal views of steady swimming across these speeds were recorded by high-speed video cameras (FASTCAM Mini AX50, Photron) at 60–125 fps (feeding videos at 60 fps, swimming alone 125 fps). The dorsal view was recorded above the swim tunnel and a floating Plexiglas panel at the water surface prevented surface ripples from interfering with dorsal view videos. Five keypoints were labeled (tip, gill, peduncle, dorsal fin tip, caudal tip) and taken from five videos.</p>
    </sec>
    <sec id="Sec17">
      <title>Dataset properties</title>
      <p id="Par43">All frames were labeled with the annotation GUI; depending on the dataset, between 100 and 7,600 frames were labeled (Table <xref rid="Tab1" ref-type="table">1</xref>). We illustrated the diversity of the postures by clustering (Extended Data Fig. <xref rid="Fig7" ref-type="fig">2</xref>). To assess the level of interactions, we evaluate a Proximity Index (Extended Data Fig. <xref rid="Fig7" ref-type="fig">2m</xref>), whose idea is inspired by ref. <sup><xref ref-type="bibr" rid="CR33">33</xref></sup> but its computation was adapted to keypoints. For each individual, instead of delineating bounding boxes to determine the vicinity of an animal we rather define a disk centered on the individual’s centroid and of sufficiently large radius such that all of that individual’s keypoints are inscribed within the disk; this is a less static description of the immediate space an animal can reach. The index is then taken as the ratio between the number of keypoints within that region that belong to other individuals and the number of keypoints of the individual of interest (Extended Data Fig. <xref rid="Fig7" ref-type="fig">2m</xref>).</p>
      <p id="Par44">For each dataset we created one random split with 70% of the data used for training and the rest for testing (unless otherwise noted). Note that identity prediction accuracy (Fig. <xref rid="Fig2" ref-type="fig">2d</xref>) and tracking performance (Fig. <xref rid="Fig3" ref-type="fig">3e</xref>) are reported on all three splits, and all show little variability. The data are available as a benchmark challenge at <ext-link ext-link-type="uri" xlink:href="https://benchmark.deeplabcut.org/">https://benchmark.deeplabcut.org/</ext-link>.</p>
    </sec>
    <sec id="Sec18">
      <title>Multi-task deep-learning architecture</title>
      <p id="Par45">DeepLabCut consists of keypoint detectors, comprising a deep CNN pretrained on ImageNet as a backbone together with multiple deconvolutional layers<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR21">21</xref></sup>. Here, as backbones we considered Residual Networks (ResNet)<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> and EfficientNets<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>. Other backbones are integrated in the toolbox<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup> such as MobileNetV2 (ref. <sup><xref ref-type="bibr" rid="CR34">34</xref></sup>). We use a stride of 16 for the ResNets (achieved by atrous convolution) and then upsample the filter banks by a factor of two to predict the score maps and location refinement fields with an overall stride of eight. Furthermore, we developed a multi-scale architecture that upsamples from conv5 and fuses those filters with filters learned as 1 × 1 convolutions from conv3. This bank is then upsampled by a factor of two via deconvolution layers. This architecture thus learns from multiple scales with an overall stride of four (including the upsampling in the decoder). We implemented a similar architecture for EfficientNets. These architectures are called ResNet50_strideX and (EfficientNet) bY_strideX for strides four to eight; we used ResNet50 as well as EfficientNets B0 and B7 for experiments (Extended Data Fig. <xref rid="Fig8" ref-type="fig">3</xref>).</p>
      <p id="Par46">We further developed a multi-scale architecture (DLCRNet_ms5) that fuses high-resolution feature map to lower resolution feature map (Fig. <xref rid="Fig1" ref-type="fig">1c</xref>)—we concatenated the feature map from conv5, the feature map learned as a 3 × 3 convolutions followed by a 1 × 1 convolutions from conv3 and the feature map learned as 2 stacked 3 × 3 convolutions and a 1 × 1 convolutions from conv2. This bank is then upsampled via (up to) two deconvolution layers. Depending on how many deconvolution layers are used this architecture learns from multiple scales with an overall stride of 2–8 (including the upsampling in the decoder). Note, during our development phase we used 95% train and 5% test splits of the data; this testing is reported at <ext-link ext-link-type="uri" xlink:href="http://maDLCopt.deeplabcut.org">http://maDLCopt.deeplabcut.org</ext-link> and in our preprint<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>.</p>
      <p id="Par47">DeepLabCut creates three output layers per keypoint that encode an intensity and a vector field. The purpose of the deconvolution layers is to upsample the spatial information (Fig. <xref rid="Fig1" ref-type="fig">1b,c</xref>). Consider an input image <italic>I</italic>(<italic>x</italic>,<italic>y</italic>) with ground truth keypoint (<italic>x</italic><sup><italic>k</italic></sup>,<italic>y</italic><sup><italic>k</italic></sup>) for index <italic>k</italic>. One of the output layers encodes the confidence of a keypoint <italic>k</italic> being in a particular location (<italic>S</italic><sup><italic>k</italic></sup>(<italic>p</italic>,<italic>q</italic>)), and the other two layers encode the (<italic>x</italic>-) and (<italic>y</italic>-) difference (in pixels of the full-sized) image between the original location and the corresponding location in the downsampled (by the overall stride) location (<inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{x}^{k}(p,q)$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{y}^{k}(p,q)$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq2.gif"/></alternatives></inline-formula>). For each training image the architecture is trained end-to-end to predict those outputs. Thereby, the ground truth keypoint is mapped into a target score map, which is 1 for pixels closer to the target (this can be subpixel location) than radius <italic>r</italic> and 0 otherwise. We minimize the cross-entropy loss for the score map (<italic>S</italic><sup><italic>k</italic></sup>) and the location refinement loss was calculated as a Huber loss<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR19">19</xref></sup>.</p>
      <p id="Par48">To link specific keypoints within one animal, we use PAFs, which were proposed by Cao et al.<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. Each (ground truth) PAF <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{x}^{l}(p,q)$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq3.gif"/></alternatives></inline-formula> and <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{y}^{l}(p,q)$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq4.gif"/></alternatives></inline-formula> for limb <italic>l</italic> connecting keypoint <italic>k</italic><sub><italic>i</italic></sub> and <italic>k</italic><sub><italic>j</italic></sub> places a directional unit vector at every pixel vector within a predefined distance from the ideal line connecting two keypoints (modulated by pafwidth). We trained DeepLabCut to also minimize the <italic>L</italic>1-loss between the predicted and true PAFs, which is added to the other losses.</p>
      <p id="Par49">Inspired by Cao et al.<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, we refine the score maps and PAFs in multiple stages. As can be seen from Fig. <xref rid="Fig1" ref-type="fig">1b</xref>, at the first stage, the original image feature from the backbone is fed into the network to predict the score map, PAF and the feature map. The output of each branch, concatenated with the feature map is fed into the subsequent stages. However, unlike Cao et al., we observed that simply adding more stages can cause performance degradation. To overcome that, we introduced shortcut connections between two consequence stages on the score map branch to improve multiple stage prediction.</p>
      <p id="Par50">Examples for score maps, location refinement and PAFs are shown in Fig. <xref rid="Fig1" ref-type="fig">1b</xref>. For training, we used the Adam optimizer<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> with learning schedule (0.0001 for first 7,500 iterations then 5 × 10<sup>−5</sup> until 12,000 iterations and then 1 × 10<sup>−5</sup>) unless otherwise noted. We trained for 60,000–200,000 (for the marmosets) iterations with batch size 8; this was enough to reach good performance (Fig. <xref rid="Fig2" ref-type="fig">2a</xref> and Extended Data Fig. <xref rid="Fig8" ref-type="fig">3)</xref>. During training we also augmented images by using techniques including rotation, covering with random boxes and motion blur. We also developed a keypoint-aware image cropping technique to occasionally augment regions of the image that are dense in keypoints. Crop centers are sampled applying at random one of the following two strategies: uniform sampling over the whole image; or sampling based on keypoint density, where the probability of a point being sampled increases in proportion to its number of neighbors (within a radius equal to 10% of the smallest image side). Crop centers are further shifted along both dimensions by random amounts no greater than 40% of the crop size—the hyperparameters can be changed by the user.</p>
    </sec>
    <sec id="Sec19">
      <title>CNN-based identity prediction</title>
      <p id="Par51">For animal identification we used a classification approach<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, while also considering spatial information. To have a monolithic solution (with just a single CNN), we simply predict in parallel the identity of each animal from the image. For this purpose, <italic>n</italic> deconvolution layers are added for <italic>n</italic> individuals. The network is trained to predict the summed score map for all keypoints of that individual. At test time, we then look up which of the output classes has the highest likelihood (for a given keypoint) and assign that identity to the keypoint. This output is trained jointly in a multi-task configuration. We evaluate the performance for identity prediction on the marmoset dataset (Fig. <xref rid="Fig2" ref-type="fig">2e</xref>).</p>
      <p id="Par52">Identity prediction can be leveraged by DeepLabCut in three different ways: (1) for assembly, by grouping keypoints based on their predicted identity; (2) for local, frame-by-frame tracking, using a soft-voting scheme where body parts are regarded as individual classifiers providing an identity probability and (3) for global stitching, by weighing down the cost of edges connecting two tracklets of similar appearance (as in Figs. <xref rid="Fig3" ref-type="fig">3</xref> and <xref rid="Fig4" ref-type="fig">4</xref>). These three sequential stages can thus be made reliant on visual appearance features alone, as done with the long recordings of marmoset behavior (Fig. <xref rid="Fig5" ref-type="fig">5</xref>).</p>
    </sec>
    <sec id="Sec20">
      <title>Multi-animal inference</title>
      <p id="Par53">Any number of keypoints can be defined and labeled with the toolbox; additional ones can be added later on. Based on our experience and testing, we recommend labeling more keypoints than a subsequent analysis might require, since it improves the part detectors<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> and, more importantly, animal assembly (Extended Data Fig. <xref rid="Fig14" ref-type="fig">9a</xref>).</p>
      <p id="Par54">Before decoding, score maps are smoothed with a Gaussian kernel of spread <italic>σ</italic> = 1 to make peaks more salient<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. For each keypoint one obtains the most likely keypoint location (<italic>x</italic><sup>*</sup>,<italic>y</italic><sup>*</sup>) by taking the maximum: (<italic>p</italic><sup>*</sup>,<italic>q</italic><sup>*</sup>) = argmax<sub>(<italic>p</italic>,<italic>q</italic>)</sub><italic>S</italic><sup><italic>k</italic></sup>(<italic>p</italic>,<italic>q</italic>) and computing:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{l}{x}^{* }={p}^{* }\cdot \lambda +\lambda /2+{L}_{x}^{k}({p}^{* },{q}^{* })\\ {y}^{* }={q}^{* }\cdot \lambda +\lambda /2+{L}_{y}^{k}({p}^{* },{q}^{* })\end{array}$$\end{document}</tex-math><mml:math id="M10"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mi>λ</mml:mi><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mi>λ</mml:mi><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="41592_2022_1443_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>with overall stride <italic>λ</italic>. If there are multiple keypoints <italic>k</italic> present then one can naturally take the local maxima of <italic>S</italic><sup><italic>k</italic></sup> to obtain the corresponding detections. Local maxima are identified via nonmaximum suppression with 2D max pooling of the score maps.</p>
      <p id="Par55">Thus, one obtains putative keypoint proposals from the score maps and location refinement fields. We then use the PAFs to assign the cost for linking two keypoints (within a putative animal). For any pair of keypoint proposals (that are connected via a limb as defined by the part affinity graph) we evaluate the affinity cost by integrating along line <italic>γ</italic> connecting two proposals, normalized by the length of <italic>γ</italic>:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\int \parallel {P}_{x,y}^{l}\parallel {\mathrm{d}}\gamma /\int {\mathrm{d}}\gamma$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mo>∫</mml:mo><mml:mo>∥</mml:mo><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>∥</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mi>γ</mml:mi><mml:mo>/</mml:mo><mml:mo>∫</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mi>γ</mml:mi></mml:mrow></mml:math><graphic xlink:href="41592_2022_1443_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>This integral is computed by sampling. Thus, for a given part affinity graph, one gets a (possibly) large number of detections and costs. The next step is to assemble those detections into animals.</p>
    </sec>
    <sec id="Sec21">
      <title>Data-driven PAF graph selection</title>
      <p id="Par56">To relieve the user from manually defining connections between keypoints, we developed an entirely data-driven procedure. Models are trained on a complete graph to learn all possible body part connections. We tested whether randomly pruning the complete marmoset skeleton (to 25, 50 and 75% of its original size: that is, 26, 52, 78 edges or 52, 104, 156 PAFs) to alleviate memory demands could still yield acceptable results. We found that pruning a large graph before training to a fourth of its original size was harmful (mAP loss of 15–20 points; Extended Data Fig. <xref rid="Fig9" ref-type="fig">9</xref>); at half and 75% of its size, a performance equivalent to that of the full graph was reached at 24 edges, although it remained about 1.5 mAP point under the maximal mAP score observed overall. Consequently, for large skeletons, a random subgraph is expected to yield only slightly inferior performance at a lesser computational cost.</p>
      <p id="Par57">The graph is then pruned based on edge discriminability power on the training set. For this purpose, within- and between-animal part affinity cost distributions (bin width 0.01) are evaluated (see Extended Data Fig. <xref rid="Fig9" ref-type="fig">4</xref> for the mouse dataset). Edges are then ranked in decreasing order of their ability to separate both distributions—evaluated from the auROC curve. The smallest, data-driven graph is taken as the maximum spanning tree (that is, a subgraph covering all keypoints with the minimum possible number of edges that also maximizes part affinity costs). For graph search following a network’s evaluation, up to nine increasingly redundant graphs are formed by extending the minimal skeleton progressively with strongly discriminating edges in the order determined above. By contrast, baseline graphs are grown from a skeleton a user would naively draw, with edges iteratively added in reversed order (that is, from least to most discriminative). The graph jointly maximizing purity and the fraction of connected keypoints is the one retained to carry out the animal assemblies.</p>
    </sec>
    <sec id="Sec22">
      <title>Animal assembly</title>
      <p id="Par58">Animal assembly refers to the problem of assigning keypoints to individuals. Yet, reconstructing the full pose of multiple individuals from a set of detections is NP hard, as it amounts to solving a <italic>k</italic>-dimensional matching problem (a generalization of bipartite matching from 2 to <italic>k</italic> disjoint subsets)<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR38">38</xref></sup>. To make the task more tractable, we break the problem down into smaller matching tasks, in a manner akin to Cao et al.<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>.</p>
      <p id="Par59">For each edge type in the data-driven graph defined earlier, we first pick strong connections based on affinity costs alone. Following the identification of all optimal pairs of keypoints, we seek unambiguous individuals by searching this set of pairs for connected components—in graph theory, these are subsets of keypoints all reachable from one another but that do not share connection with any additional keypoint; consequently, only connectivity, but not spatial information, is taken into account. Breadth-first search runs in linear time complexity, which thus allows the rapid predetermination of unique individuals. Note that, unlike ref. <sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, redundant connections are seamlessly handled and do not require changes in the formulation of the animal assembly. Then, remaining connections are sorted in descending order of their affinity costs (equation (<xref rid="Equ2" ref-type="">2</xref>)) and greedily linked.</p>
      <p id="Par60">To further improve the assembly’s robustness to ambiguous connections (that is, a connection attempting to either link keypoints belonging to two distinct individuals or overwrite existing ones), the assembly procedure can be calibrated by determining the prior probability of an animal’s pose as a multivariate normal distribution over the distances between all pairs of keypoints. Mean and covariance are estimated from the labeled data via density estimation with Gaussian kernel and bandwidth automatically chosen according to Scott’s Rule. A skeleton is then only grown if the candidate connection reduces the Mahalanobis distance between the resulting configuration and the prior (referred to as with calibration in Fig. <xref rid="Fig2" ref-type="fig">2c</xref>). Last, our assembly’s implementation is fully parallelized to benefit greatly from multiple processors (Extended Data Fig. <xref rid="Fig10" ref-type="fig">5)</xref>.</p>
      <p id="Par61">Optionally (and only when analyzing videos), affinity costs between body parts can be weighted so as to prioritize strong connections that were preferentially selected in the past frames. To this end, and inspired by ref. <sup><xref ref-type="bibr" rid="CR39">39</xref></sup>, we compute a temporal coherence cost as follows: <inline-formula id="IEq5"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{1}{j}\mathop{\sum }\nolimits_{i = 1}^{j}{e}^{-\gamma {{\Delta }}t{\left\Vert c-{c}_{n}\right\Vert }^{2}}$$\end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:msup><mml:mrow><mml:mfenced close="∥" open="∥"><mml:mrow><mml:mi>c</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq5.gif"/></alternatives></inline-formula>, where <italic>γ</italic> controls the influence of distant frames (and is set to 0.01 by default), <italic>c</italic> and <italic>c</italic><sub><italic>n</italic></sub> are the current connection and its closest neighbor in the relevant past frame and Δ<italic>t</italic> is the temporal gap separating these frames.</p>
    </sec>
    <sec id="Sec23">
      <title>Top-down pose estimation</title>
      <p id="Par62">In general, top-down pose estimation is characterized by two stages that require an object detector and a single animal pose estimation model<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. This pipeline requires bounding box annotations (which can come from many different algorithms). Here, bounding boxes were determined from ground truth keypoint coordinates. If a box’s aspect ratio was lower than 4:1, its smallest side was extended by 10 pixels. Box bounds were further enlarged by 25 pixels to make sure the bounding boxes covered an animal’s entire body. We pad the cropped images to a square and then resize them to the original size (400 × 400) to keep the aspect ratio constant. Second, we retrain a model (either with or without PAFs) on training images cropped by these bounding boxes. For inference, we retain the best prediction per bounding box, as decided by detection confidence for the model without PAFs and with highest assembly score for the model with PAF. Finally, for evaluation, we map coordinates of our final predictions back to the original images.</p>
    </sec>
    <sec id="Sec24">
      <title>Detection performance and evaluation</title>
      <p id="Par63">To compare the human annotations with the model predictions we used the Euclidean distance to the closest predicted keypoint (r.m.s.e.) calculated per keypoint. Depending on the context, this metric is shown for a specific keypoint, averaged over all keypoints or averaged over a set of train or test images (Fig. <xref rid="Fig2" ref-type="fig">2a</xref> and Extended Data Fig. <xref rid="Fig8" ref-type="fig">3)</xref>. Nonetheless, unnormalized pixel errors may be difficult to interpret in certain scenarios; for example, marmosets dramatically vary in size as they leap from the top to the bottom of the cage. Thus, we also calculated the percentage of correct keypoints (PCK) metric<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR40">40</xref></sup>; that is, the fraction of predicted keypoints that fall within a threshold distance from the location of the ground truth detection. PCK was computed in relation to a third of the tip–gill distance for the fish dataset, and a third of the left-right ear distance for the remaining ones.</p>
      <p id="Par64">Animal assembly quality was evaluated in terms of mAP computed over object keypoint similarity thresholds ranging from 0.50 to 0.95 in steps of 0.05, as is standard in human pose literature and COCO challenges<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. Keypoint standard deviation was set to 0.1. As interpretable metrics, we also computed the number of ground truth keypoints left unconnected (after assembly) and purity—an additional criterion for quality that can be understood as the accuracy of the assignment of all keypoints of a putative subset to the most frequent ground truth animal identity within that subset<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. Since pups are very hard to label consistently (see Extended Data Fig. <xref rid="Fig7" ref-type="fig">7</xref> for examples), we allow flips between symmetric pairs of keypoints (end1 versus end2 or interm1 versus interm3, Extended Data Fig. <xref rid="Fig6" ref-type="fig">1</xref>) to be acceptable detection errors when evaluating keypoint similarity.</p>
    </sec>
    <sec id="Sec25">
      <title>Statistics for assessing data-driven method</title>
      <p id="Par65">Two-way, repeated-measures ANOVA were performed using Pinetwork flow minimizationngouin (v.0.5.0)<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> to test whether graph size and assembling method (naive versus data-driven versus calibrated assembly) had an impact on the fraction of unconnected body parts and assembly purity. Since sphericity was violated, the Greenhouse–Geisser correction was applied. Provided a main effect was found, we conducted multiple post hoc (paired, two-sided) tests adjusted with Benjamini–Hochberg false discovery rate correction to locate pairwise differences. The Hedges’ <italic>g</italic> was calculated to report effect sizes between sets of observations.</p>
    </sec>
    <sec id="Sec26">
      <title>Comparison to state-of-the-art pose estimation models</title>
      <p id="Par66">For benchmarking, we compared our architectures to current state-of-the-art architectures on COCO<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, a challenging, large-scale multi-human pose estimation benchmark. Specifically, we considered HRNet<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR43">43</xref></sup> as well as ResNet backbones<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> with Associative Embedding<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> as implemented in the MMPose toolbox (<ext-link ext-link-type="uri" xlink:href="https://github.com/open-mmlab/mmpose">https://github.com/open-mmlab/mmpose</ext-link>). We chose them as control group for their simplicity (ResNet) and performance (HRNet). We used the bottom-up variants of both models. The bottom-up variants leverage associative embedding as the grouping algorithms<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. In particular, the bottom-up variant of HRNet we used has mAP that is comparable to the state-of-the-art model HigherHRNet<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> in COCO (69.8 versus 70.6) for a multiple scale test and (65.4 versus 67.7) for a single scale test.</p>
      <p id="Par67">To fairly compare, we used the same train and test split. The total training epochs are set such that models from two groups see roughly same number of images. The hyperparameters search was manually performed to find the optimal hyperparameters. For a small dataset such as the tri-mouse and (largest) marmoset, we found that the default settings for excellent performance on COCO gave optimal accuracy except that we needed to modify the total training steps to match DeepLabCut’s. For both the marmoset and tri-mouse datasets, the initial learning rate was 0.0015. For the three mouse dataset, the total epochs is 3,000 epochs and the learning rate decayed by a factor of 10 at 600 and 1,000 epochs. For the marmoset dataset, we trained for 50 epochs and the learning rate decayed after 20 and 40 epochs. The batch size was 32 and 16 for ResNet-AE and HRNet-AE, respectively. For smaller datasets such as tri-mouse, fish and parenting, we found that a smaller learning rate and a smaller batch size gave better results; a total of 3,000 epochs were used. After hyper-parameter search, we set batch size to four and initial learning rate a 0.0001, which then decayed at 1,000 and 2,000 epochs. As within DeepLabCut, multiple scale test and flip test were not performed (which is, however, common for COCO evaluation). For the parenting dataset, MMPose models can only be trained on one dataset (simultaneously), which is why these models are not trained to predict the mouse, and we only compare the performance on the pups. Full results are shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.</p>
    </sec>
    <sec id="Sec27">
      <title>Benchmarking idtacker.ai</title>
      <p id="Par68">We used version idtracker.ai<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> v.3, taken from commit 6b89601b; we tested it on tri-mouse and marmoset data. We report the MOTA results in Extended Data Fig. <xref rid="Fig8" ref-type="fig">8</xref>. For marmoset data, reasonable parameters to segment individual animals with the GUI could not be found (likely due to the complex background), thus we performed a grid search for the valid minimum intensity threshold and maximum intensity threshold, the two critical parameters, by step 2 from range 0 to 255. Even with these efforts, we still could not get reasonable results (Supplementary Video <xref rid="MOESM7" ref-type="media">5)</xref>; that is, MOTA was negative.</p>
    </sec>
    <sec id="Sec28">
      <title>DeepLabCut Tracking modules</title>
      <p id="Par69">Having seen that DeepLabCut provides a strong predictor for individuals and their keypoints, detections are linked across frames using a tracking-by-detection approach (for example, ref. <sup><xref ref-type="bibr" rid="CR44">44</xref></sup>). Thereby, we follow a divide-and-conquer strategy for (local) tracklet generation and tracklet stitching (Extended Data Fig. <xref rid="Fig9" ref-type="fig">4b,c</xref>). Specifically, we build on the Simple Online and Realtime Tracking framework (SORT<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>) to generate tracklets. The inter-frame displacement of assembled individuals is estimated via Kalman filter-based trackers. The task of associating these location estimates to the model detections is then formulated as a bipartite graph matching problem solved with the Hungarian algorithm, therefore guaranteeing a one-to-one correspondence across adjacent frames. Note that the trackers are agnostic to the type of skeleton (animal body plan), which render them robust and computationally efficient.</p>
    </sec>
    <sec id="Sec29">
      <title>Box tracker</title>
      <p id="Par70">Bounding boxes are a common and well-established representation for object tracking. Here they are computed from the keypoint coordinates of each assembled individual, and expanded by a margin optionally set by the user. The state <italic>s</italic> of an individual is parametrized as <inline-formula id="IEq6"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s=[x,y,A,r,\dot{x},\dot{y},\dot{A}]$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq6.gif"/></alternatives></inline-formula>, where <italic>x</italic> and <italic>y</italic> are the 2D coordinates of the center of the bounding box; <italic>A</italic>, its area and <italic>r</italic>, its aspect ratio, together with their first time derivatives. Association between detected animals and tracker hypotheses is based on the intersection-over-union measure of overlap.</p>
    </sec>
    <sec id="Sec30">
      <title>Ellipse tracker</title>
      <p id="Par71">A 2<italic>σ</italic> covariance error ellipse is fitted to an individual’s detected keypoints. The state is modeled as <inline-formula id="IEq7"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s=[x,y,h,w,\theta ,\dot{x},\dot{y},\dot{h},\dot{w},\dot{\theta }]$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq7.gif"/></alternatives></inline-formula>, where <italic>x</italic> and <italic>y</italic> are the 2D coordinates of the center of the ellipse; <italic>h</italic> and <italic>w</italic>, the lengths of its semi-axes and <italic>θ</italic>, its inclination relative to the horizontal. We anticipated that this parametrization would better capture subtle changes in body conformation, most apparent through changes in ellipse width and height and orientation. Moreover, an error ellipse confers robustness to outlier keypoints (for example, a prediction assigned to the wrong individual, which would cause the erroneous delineation of an animal’s boundary under the above-mentioned box tracking). In place of the ellipse overlap, the similarity cost <italic>c</italic> between detected and predicted ellipses is efficiently computed as: <inline-formula id="IEq8"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c=0.8(1-d)+0.2(1-d)(\cos ({\theta }_{d}-{\theta }_{p}))$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>0.2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq8.gif"/></alternatives></inline-formula>, where <italic>d</italic> is the Euclidean distance separating the ellipse centroids normalized by the length of the longest semi-axis.</p>
      <p id="Par72">The existence of untracked individuals in the scene is signaled by assembled detections with a similarity cost lower than iou_threshold (set to 0.6 in our experiments). In other words, the higher the similarity threshold, the more conservative and accurate the frame-by-frame assignment, at the expense of shorter and more numerous tracklets. On creation, a tracker is initialized with the required parameters described above, and all (unobservable) velocities are set to 0. To avoid tracking sporadic, spurious detections, a tracker is required to live for a minimum of min_hits consecutive frames, or is otherwise deleted. Occlusions and reidentification of individuals are handled with the free parameter max_age—the maximal number of consecutive frames tracks can remain undetected before the tracker is considered lost. We set both to 1 to delegate the tasks of tracklet reidentification and false positive filtering to our TrackletStitcher, as we shall see below.</p>
    </sec>
    <sec id="Sec31">
      <title>Tracklet stitching</title>
      <p id="Par73">Greedily linking individuals across frames is locally, but not globally, optimal. An elegant and efficient approach to reconstructing full trajectories (or tracks) from sparse tracklets is to cast the stitching task as a network flow minimization problem<sup><xref ref-type="bibr" rid="CR45">45</xref>,<xref ref-type="bibr" rid="CR46">46</xref></sup>. Each fully reconstructed track is equivalent to finding a flow through the graph from a source to a sink, subject to capacity constraints and whose overall linking cost is minimal (Extended Data Fig. <xref rid="Fig9" ref-type="fig">4c</xref>).</p>
    </sec>
    <sec id="Sec32">
      <title>Formulation</title>
      <p id="Par74">The tracklets collected after animal tracking are denoted as <inline-formula id="IEq9"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\{{{{{\mathcal{T}}}}}_{1},...,{{{{\mathcal{T}}}}}_{n}\}$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq9.gif"/></alternatives></inline-formula>, and each contains a (temporally) ordered sequence of observations and time indices. Thereby, the observations are given as vectors of body part coordinates in pixels and likelihoods. In contrast to most approaches described in the literature, the proposed approach requires solely spatial and temporal information natively, while leveraging visual information (for example, animals’ identities predicted beforehand) is optional (see Fig. <xref rid="Fig3" ref-type="fig">3e</xref> for marmosets). This way, tracklet stitching is agnostic to the framework poses were estimated with, and works readily on previously collected kinematic data.</p>
      <p id="Par75">We construct a directed acyclic graph <italic>G</italic> = (<italic>V</italic>,<italic>E</italic>) using NetworkX<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> to describe the affinity between multiple tracklets, where the <italic>i</italic>th node <italic>V</italic><sub><italic>i</italic></sub> corresponds to the <italic>i</italic>th tracklet <inline-formula id="IEq10"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\mathcal{T}}}}}_{i}$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq10.gif"/></alternatives></inline-formula>, and <italic>E</italic> is the set of edges encoding the cost entailed by linking the two corresponding tracklets (or, in other words, the likelihood that they belong to the same track). In our experiments, tracklets shorter than five frames were flagged as residuals: they do not contribute to the construction of the graph and are incorporated only after stitching. This minimal tracklet length can be changed by a user. To drastically reduce the number of possible associations and make our approach scale efficiently to large videos, edge construction is limited to those tracklets that do not overlap in time (since an animal cannot occupy multiple spatial locations at any one instant) and temporally separated by no more than a certain number of frames. By default, this threshold is automatically taken as 1.5 × <italic>τ</italic>, where <italic>τ</italic> is the smallest temporal gap guaranteeing that all pairs of consecutive tracklets are connected. Alternatively, the maximal gap to consider can be programmatically specified. The source and the sink are two auxiliary nodes that supply and demand an amount of flow <italic>k</italic> equal to the number of tracks to form. Each node is virtually split in half: an input with unit demand and an output with unit supply, connected by a weightless edge. All other edges have unit capacity and a weight <italic>w</italic> calculated from the affinity models described in the next subsection. Altogether, these constraints ensure that all nodes are visited exactly once, which thus amounts to a problem similar to covering <italic>G</italic> with <italic>k</italic> node-disjoint paths at the lowest cost. We considered different affinities for linking tracklets (Fig. <xref rid="Fig4" ref-type="fig">4d</xref>).</p>
    </sec>
    <sec id="Sec33">
      <title>Affinity models</title>
      <sec id="Sec34">
        <title>Motion affinity</title>
        <p id="Par76">Let us consider two nonoverlapping tracklets <inline-formula id="IEq11"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\mathcal{T}}}}}_{1}$$\end{document}</tex-math><mml:math id="M26"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq11.gif"/></alternatives></inline-formula> and <inline-formula id="IEq12"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\mathcal{T}}}}}_{2}$$\end{document}</tex-math><mml:math id="M28"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq12.gif"/></alternatives></inline-formula> consecutive in time. Their motion affinity is measured from the error between the true locations of their centroids (that is, unweighted average keypoint) and predictions made from their linear velocities. Specifically, we calculate a tracklet’s tail and head velocities by averaging instantaneous velocities over its three first and last data points (Fig. <xref rid="Fig4" ref-type="fig">4d</xref>). Assuming uniform, rectilinear motion, the centroid location of <inline-formula id="IEq13"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\mathcal{T}}}}}_{1}$$\end{document}</tex-math><mml:math id="M30"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq13.gif"/></alternatives></inline-formula> at the starting frame of <inline-formula id="IEq14"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\mathcal{T}}}}}_{2}$$\end{document}</tex-math><mml:math id="M32"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq14.gif"/></alternatives></inline-formula> is estimated, and we note <italic>d</italic><sub>f</sub> the distance between the forward prediction and the actual centroid coordinates. The same procedure is repeated backward in time, predicting the centroid location of <inline-formula id="IEq15"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\mathcal{T}}}}}_{2}$$\end{document}</tex-math><mml:math id="M34"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq15.gif"/></alternatives></inline-formula> at the last frame of <inline-formula id="IEq16"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\mathcal{T}}}}}_{1}$$\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41592_2022_1443_Article_IEq16.gif"/></alternatives></inline-formula> knowing its tail velocity, yielding <italic>d</italic><sub>b</sub>. Motion affinity is then taken as the average error distance.</p>
      </sec>
      <sec id="Sec35">
        <title>Spatial proximity</title>
        <p id="Par77">If a pair of tracklets overlaps in time, we calculate the Euclidean distance between their centroids averaged over their overlapping portion. Otherwise, we evaluate the distance between a tracklet’s tail and the other’s head.</p>
      </sec>
      <sec id="Sec36">
        <title>Shape similarity</title>
        <p id="Par78">Shape similarity between two tracklets is taken as the undirected Hausdorff distance between the two sets of keypoints. Although this measure provides only a crude approximation of the mismatch between two animals’ skeletons, it is defined for finite sets of points of unequal size; for example, it advantageously allows the comparison of skeletons with a different number of visible keypoints.</p>
      </sec>
      <sec id="Sec37">
        <title>Dynamic similarity</title>
        <p id="Par79">To further disambiguate tracklets in the rare event that they are spatially and temporally close, and similar in shape, we propose to use motion dynamics in a manner akin to ref. <sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. The procedure is fully data-driven, and requires no a priori knowledge of the animals’ behavior. In the absence of noise, the rank of the Hankel matrix—a matrix constructed by stacking delayed measurements of a tracklet’s centroid—theoretically determines the dimension of state space models; that is, it is a proxy for the complexity of the underlying dynamics<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>. If two tracklets originate from the same dynamical system, a single, low-order regressor should suffice to approximate them both. On the other hand, tracklets belonging to different tracks would require a higher-order (that is, more complex) model to explain their spatial evolution<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. Low rank approximation of a noisy matrix, however, is a complex problem, as the matrix then tends to be full rank (that is, all its singular values are nonzero). For computational efficiency, we approximate the rank of a large numbers of potentially long tracklets using singular value decomposition via interpolative decomposition. Optimal low rank was chosen as the rank after which eigenvalues drop by less than 1%.</p>
      </sec>
    </sec>
    <sec id="Sec38">
      <title>Problem solution for stitching</title>
      <p id="Par80">The optimal flow solution can be found using a min-cost flow algorithm. We use NetworkX’s capacity scaling variant of the successive shortest augmenting path algorithm, which requires polynomial time for the assignment problem (that is, when all nodes have unit demands and supplies, ref. <sup><xref ref-type="bibr" rid="CR50">50</xref></sup>). Residual tracklets are then greedily added back to the newly stitched tracks at locations that guarantee time continuity and, when there are multiple candidates, minimize the distances to the neighboring tracklets. Note that although residuals are typically very short, making the assignment decisions error-prone. To improve robustness and simultaneously reduce complexity, association hypotheses between temporally close residual tracklets are stored in the form of small directed acyclic graphs during a preliminary forward screening pass. An hypothesis likelihood is then scored based on pairwise tracklet spatial overlap, and weighted longest paths are ultimately kept to locally grow longer, more confident residuals.</p>
      <p id="Par81">This tracklet stitching process is implemented in DeepLabCut and automatically carried out after assembly and tracking. The tracks can then also be manually refined in a dedicated GUI (Extended Data Fig. <xref rid="Fig6" ref-type="fig">1</xref>).</p>
    </sec>
    <sec id="Sec39">
      <title>Transformer for unsupervised ID tracking</title>
      <p id="Par82">To track unidentified animals we turn to metric learning<sup><xref ref-type="bibr" rid="CR4">4</xref></sup> with transformers, which are state-of-the-art for reID of humans and vehicles<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. However, in contrast to ref. <sup><xref ref-type="bibr" rid="CR51">51</xref></sup>, we created a tracking approach and wanted to make use of the task-trained CNNs, and thus require fewer training data.</p>
      <p id="Par83">Specifically, we used the predicted coordinates of each tracklet (individual with temporal continuality) and extract features of 2,048 dimensions from the last layer of our (multi-task-trained) backbone network to form so called ‘keypoint embedding’, which contains embedding of each detected keypoint for every individual (and encode high-level visual features around the keypoint). Then we feed this keypoint embedding to a transformer that processes these embeddings and aggregates information globally. The transformer layers have four heads and four blocks with dimension of 768 and residual connections between blocks. The output of transformer layers are then followed by a multi-layer perceptron that outputs a vector of dimension 128 (more layers, as in ref. <sup><xref ref-type="bibr" rid="CR51">51</xref></sup>, actually gave a worse performance). We then use the output of the multi-layer perceptron to minimize triplet loss where we treat within tracklet embedding as anchor-positive pairs while tracklets from different individuals as anchor-negative pairs. For each test video, we extracted 10,000 triplets from the local-tracking approach (ellipse, to evaluate the capacity based on tracklets) and from the ground truth data (to evaluate the capacity of the approach; as triplets from ground truth tracks already are split into the correct number of animals). We then trained the transformer on 90% of the triplets, and evaluated it on the rest (Fig. <xref rid="Fig4" ref-type="fig">4)</xref>. Thus, the transformer learns to recognize identities of each tracklet and we then use the cosine similarity as an additional cost to our graph. For this purpose, we used the transformer to extract 128 dimensional feature vectors (appearance embeddings) per keypoint embedding, which we then used for tracking (below).</p>
    </sec>
    <sec id="Sec40">
      <title>Tracking performance evaluation</title>
      <p id="Par84">Tracking performance was assessed with the field standard MOTA metrics<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. Namely, we used <ext-link ext-link-type="uri" xlink:href="https://github.com/cheind/py-motmetrics">https://github.com/cheind/py-motmetrics</ext-link> to compute MOTA, which evaluates a tracker’s overall performance at detecting and tracking individuals (all possible sources of errors considered: number of misses, of false positives and of mismatches (switches) respectively) independently of its ability to predict an individual’s location. MOTA is thereby the sum of three errors: the ratio of misses in the sequence, computed over the total number of objects present in all frames, the ratio of false positives and the ratio of mismatches<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. The number of misses counts actual detections for which there are no matching trackers. The number of fragments indicates the number of times tracking was interrupted. The number of switches, occurring most often when two animals pass very close to one another or if tracking resumes with a different ID after an occlusion. In our software, remaining ID swaps are automatically flagged in the Refine Tracklets GUI (Extended Data Fig. <xref rid="Fig6" ref-type="fig">1)</xref> by identifying instants at which the <italic>x</italic> and <italic>y</italic> coordinates of a pair of keypoints simultaneously intersect each other<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>.</p>
    </sec>
    <sec id="Sec41">
      <title>Reporting Summary</title>
      <p id="Par85">Further information on research design is available in the <xref rid="MOESM2" ref-type="media">Nature Research Reporting Summary</xref> linked to this article.</p>
    </sec>
  </sec>
  <sec id="Sec42" sec-type="materials|methods">
    <title>Online content</title>
    <p id="Par86">Any methods, additional references, Nature Research reporting summaries, source data, extended data, supplementary information, acknowledgements, peer review information; details of author contributions and competing interests; and statements of data and code availability are available at 10.1038/s41592-022-01443-0.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <p>
      <supplementary-material content-type="local-data" id="MOESM1">
        <media xlink:href="41592_2022_1443_MOESM1_ESM.pdf">
          <label>Supplementary Information</label>
          <caption>
            <p>Supplementary Note 1 and Tables 1–8.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM2">
        <media xlink:href="41592_2022_1443_MOESM2_ESM.pdf">
          <caption>
            <p>Reporting Summary</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM3">
        <media xlink:href="41592_2022_1443_MOESM3_ESM.mov">
          <label>Supplementary Video 1</label>
          <caption>
            <p>Predictions with DLCRNet MS5_ss-s4 for tri-mouse video clip.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM4">
        <media xlink:href="41592_2022_1443_MOESM4_ESM.mp4">
          <label>Supplementary Video 2</label>
          <caption>
            <p>Predictions with DLCRNet MS5_ss-s4 for parenting video clip.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM5">
        <media xlink:href="41592_2022_1443_MOESM5_ESM.mp4">
          <label>Supplementary Video 3</label>
          <caption>
            <p>Predictions with DLCRNet MS5_ss-s4 for marmoset video clip.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM6">
        <media xlink:href="41592_2022_1443_MOESM6_ESM.m4v">
          <label>Supplementary Video 4</label>
          <caption>
            <p>Predictions with DLCRNet MS5_ss-s4 for fish video clip.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM7">
        <media xlink:href="41592_2022_1443_MOESM7_ESM.mp4">
          <label>Supplementary Video 5</label>
          <caption>
            <p>Predictions with idtracker.ai on tri-mouse and marmoset data.</p>
          </caption>
        </media>
      </supplementary-material>
    </p>
  </sec>
</body>
<back>
  <app-group>
    <app id="App1">
      <sec id="Sec43">
        <title>Extended data</title>
        <p id="Par91">
          <fig id="Fig6">
            <label>Extended Data Fig. 1</label>
            <caption>
              <title>DeepLabCut 2.2 workflow.</title>
              <p>(a) Multi-animal DeepLabCut2.2+ workflow. (b) An example screenshot of the Refine Tracklet GUI. We show the ellipse similarity score (black line), hand-noted GT switches in ID (blue), and additional frames in orange where the selected keypoint requires further examination. (c) Body part keypoint diagrams with names on the animal skeletons (see also Extended Data Figure <xref rid="Fig2" ref-type="fig">2)</xref>.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1443_Fig6_ESM" id="d32e2914"/>
          </fig>
        </p>
        <p id="Par92">
          <fig id="Fig7">
            <label>Extended Data Fig. 2</label>
            <caption>
              <title>Dataset characteristics and statistics.</title>
              <p>For each datasets, normalized animal poses were clustered using K-means adapted for missing elements, and embedded non-linearly in 2D space via Isometric mapping (Tenenbaum et al. 2000). Embeedings as well as representative poses are shown for the tri-mouse dataset (a). Counts of labeled keypoints (b) and distribution of bounding box diagonal lengths (c). (d-l) show the same for the other three datsets. The Proximity Index (m) reflects the crowdedness of the various dataset scenes. Statistics were computed from the ground truth test video annotations. The mice and fish datasets are more cluttered on average than the pups and marmosets.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1443_Fig7_ESM" id="d32e2928"/>
          </fig>
        </p>
        <p id="Par93">
          <fig id="Fig8">
            <label>Extended Data Fig. 3</label>
            <caption>
              <title>Performance of various DeepLabCut network architectures.</title>
              <p>(a) Overall keypoint prediction errors of ResNets-50 and the EfficientNets backbones (B0/B7), DLCRNet at stride 4 and 8. Distribution of train and test errors are displayed as light and dark box plots, respectively. Box plots show median, first and third quartiles, with whiskers extending past the low and high quartiles to ± 1.5 times the interquartile range. All models were trained for 60k iterations. n=independent image samples as follows: for train∣test per dataset: 112∣49 (tri-mouse); 379∣163 (pups); 5316∣2278 (marmosets); 70∣30 (fish). (b): Images on held-out test data, where plus indicates human ground truth, and the circle indicates the model prediction (shown for ResNet50 with stride 8). (c): Marmoset identification train-test accuracy for various backbones.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1443_Fig8_ESM" id="d32e2942"/>
          </fig>
        </p>
        <p id="Par94">
          <fig id="Fig9">
            <label>Extended Data Fig. 4</label>
            <caption>
              <title>Discriminability of part affinity fields.</title>
              <p>Within- (pink) and between-animal (blue) affinity cost distributions for all edges of the mouse skeleton with DLCRNet_ms5. The saturated subplots highlight the 11 edges kept to form the smallest, optimal part affinity graph (see Fig. <xref rid="Fig2" ref-type="fig">2b</xref>). This is based on the separability power of an edge, that is, its ability to discriminate a connection between two keypoints effectively belonging to the same animal from the wrong ones, and reflected by the corresponding AUC scores (listed at the top of the subplots).</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1443_Fig9_ESM" id="d32e2959"/>
          </fig>
        </p>
        <p id="Par95">
          <fig id="Fig10">
            <label>Extended Data Fig. 5</label>
            <caption>
              <title>Average animal assembly speed in frames per second as a function of graph size.</title>
              <p>Assembly rates vs. graphs size for the four datasets. Improving the assembly robustness via calibration with labeled data in large graphs incurs no extra computational cost at best, and a slowdown by 25% at worst; remarkably, it is found to accelerate assembly speed in small graphs. Relying exclusively on keypoint identity prediction results in average speeds of around 5600 frames per second, independent of graph size. Three timing experiments were run per graph size (lighter colored dots) and averages are shown. Note that assembling rates exclude CNN processing times. Speed benchmark was run on a workstation with an Intel(R) Core(TM) i9-10900X CPU 3.70GHz.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1443_Fig10_ESM" id="d32e2973"/>
          </fig>
        </p>
        <p id="Par96">
          <fig id="Fig11">
            <label>Extended Data Fig. 6</label>
            <caption>
              <title>Performance on out of domain marmoset data.</title>
              <p>(a) Example images from original dataset, and example generalization test images. (b) Median RMSE and PCK (gray numbers) for data and network (DLCRNet) as shown in Fig. <xref rid="Fig2" ref-type="fig">2a</xref>. (c) same, but on the generalization test images (n=300) (d) same but per cage as shown in a (n=30 test images per marmoset). Box plots show median, first and third quartiles, with whiskers extending past the low and high quartiles to ± 1.5 times the interquartile range.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1443_Fig11_ESM" id="d32e2990"/>
          </fig>
        </p>
        <p id="Par97">
          <fig id="Fig12">
            <label>Extended Data Fig. 7</label>
            <caption>
              <title>Comparison of top-down methods with and without assembly.</title>
              <p>(a) Schematics of top-down method with example images from the pup dataset, which consists of first detecting individuals and then performing pose prediction on each bounding box (plus is human ground truth, and the circle is bottom-up model (DLCRNet, stride 8, data-driven) predictions. (b) Performance mAP computed for top-down method with and without PAFs and bottom-up method (baseline, data-driven) as also shown in Fig. <xref rid="Fig2" ref-type="fig">2d</xref>. PAF vs. w/o PAF one-way ANOVA p-values, tri-mouse: 4.656e-11, pups: 3.62e-12, marmosets: 1.33e-28, fish: 1.645e-06). There were significant model effects across all datasets: one-way ANOVA p-values– tri-mouse: 4.13e-11, pups: 4.59e-25, marmosets: 3.04e-40, fish: 1.18e-14. (c) Example predictions within the smaller images (that is, bounded crops) from the top-down model (that is, w/PAF), and bottom-up predictions (full images, as noted).</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1443_Fig12_ESM" id="d32e3007"/>
          </fig>
        </p>
        <p id="Par98">
          <fig id="Fig13">
            <label>Extended Data Fig. 8</label>
            <caption>
              <title>Performance of idtracker.ai.</title>
              <p>(a) Segmented regions (red) overlaid on example image in idtracker.ai GUI. Example how idtracker.ai fails to segment only the mice in the full data frame for tri-mouse and (b) in marmoset dataset. (c) Using the ROI selection feature, we could find mostly just mice. However, due to the inhomogeneous lighting, the segmentation is not error-free. (d) Result of a grid search to find optimal parameters for idtracker with MOTA scores on the same videos as shown in Fig. <xref rid="Fig3" ref-type="fig">3a,e</xref>; one-sided, one-sample T-tests indicated that idtracker.ai performed significantly worse than DeepLabCut in both datasets (tri-mouse: T=-11.03, p=0.0008, d=5.52; marmosets: T=-8.43, p=0.0018, d=4.22).</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1443_Fig13_ESM" id="d32e3024"/>
          </fig>
        </p>
        <p id="Par99">
          <fig id="Fig14">
            <label>Extended Data Fig. 9</label>
            <caption>
              <title>Parameter sensitivity: Evaluation of number of body parts, frames, and PAF sizes.</title>
              <p>(a) The number of keypoints affects mAP; evaluated with ResNet50 stride8 on the two datasets with the most keypoints originally labeled by subsampling the keypoints. [Mouse: snout/tailbase (2) + leftear/rightear (4) + shoulder/spine1/spine2/spine3 (8) vs. full (12); Marmoset: Middle/Body2 (2) + FL1/BL1 /FR1/BR1/Left/Right (8) + front/body1/body3 (11) vs. full (15)] (b) Identity prediction is not strongly affected by the number of keypoints used (same experiments as in a, but for identity). (c) Impact of graph size, and randomly dropping edges on performance. (d) Test performance on 30% of the data vs. training set size (as fraction of 70%) for all four datasets.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1443_Fig14_ESM" id="d32e3038"/>
          </fig>
        </p>
      </sec>
    </app>
  </app-group>
  <fn-group>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>These authors jointly supervised this work: Mackenzie Weygandt Mathis, Alexander Mathis.</p>
    </fn>
  </fn-group>
  <sec>
    <sec id="FPar1">
      <title>Extended data</title>
      <p id="Par87">is available for this paper at 10.1038/s41592-022-01443-0.</p>
    </sec>
    <sec id="FPar2" sec-type="supplementary-material">
      <title>Supplementary information</title>
      <p id="Par88">The online version contains supplementary material available at 10.1038/s41592-022-01443-0.</p>
    </sec>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>Funding was provided by the Rowland Institute at Harvard University (M.W.M., T.N., A.M. and J.L.), the Chan Zuckerberg Initiative DAF (M.W.M., A.M. and J.L.), SNSF (M.W.M.) and EPFL (M.W.M., A.M., S.Y., S.S. and M.Z.). Dataset collection was funded by: Office of Naval Research grant nos. N000141410533 and N00014-15-1-2234 (G.L.), HHMI and NIH grant no. 2R01HD082131 (M.M.R. and C.D.); NIH grant no. 1R01NS116593-01 (M.M.R., C.D. and V.N.M.). We are grateful to M. Vidal for converting datasets. We thank the DLC community for feedback and testing. M.W.M. is the Bertarelli Foundation Chair of Integrative Neuroscience. The funders had no role in the conceptualization, design, data collection, analysis, decision to publish or preparation of the manuscript.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>Conceptualization was done by A.M. and M.W.M. Formal analysis and code were done by J.L., A.M. and M.W.M. New deep architectures were designed by M.Z., S.Y. and A.M. GUIs were done by J.L., M.W.M. and T.N. Benchmark was set by S.S., M.W.M., A.M. and J.L. Marmoset data were gathered by W.M. and G.F. Marmoset behavioral analysis was carried out by W.M. Parenting data were gathered by M.M.R., A.M. and C.D. Tri-mouse data were gathered by D.S., A.M. and V.N.M. Fish data were gathered by V.D.S. and G.L. The article was written by A.M., M.W.M. and J.L. with input from all authors. M.W.M. and A.M. co-supervised the project.</p>
  </notes>
  <notes notes-type="peer-review">
    <title>Peer review</title>
    <sec id="FPar3">
      <title>Peer review information</title>
      <p id="Par89"><italic>Nature Methods</italic> thanks the anonymous reviewers for their contribution to the peer review of this work. Nina Vogt was the primary editor on this article and managed its editorial process and peer review in collaboration with the rest of the editorial team.</p>
    </sec>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>For this study, we established four differently challenging multi-animal datasets from ecology and neuroscience. Data collection was institutionally approved: tri-mouse, parenting behavior, fish schooling from Harvard University IACUC and marmosets from MIT and Broad Institute IACUC. They are available to download, minus a small amount (30%) held out as benchmark competition data, at <ext-link ext-link-type="uri" xlink:href="https://benchmark.deeplabcut.org/">https://benchmark.deeplabcut.org/</ext-link>, and on Zenodo<sup><xref ref-type="bibr" rid="CR54">54</xref>–<xref ref-type="bibr" rid="CR57">57</xref></sup>. Findings in this paper can be replicated using the downloadable data and supplied code.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>Code to use this package is found at <ext-link ext-link-type="uri" xlink:href="https://github.com/DeepLabCut/DeepLabCut">https://github.com/DeepLabCut/DeepLabCut</ext-link> with a LGPL-3.0 License. Code to reproduce the figures from this work is found at <ext-link ext-link-type="uri" xlink:href="https://github.com/DeepLabCut/maDLC_NatureMethods2022">https://github.com/DeepLabCut/maDLC_NatureMethods2022</ext-link>.</p>
  </notes>
  <notes id="FPar4" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par90">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kays</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Crofoot</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Jetz</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Wikelski</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Terrestrial animal tracking as an eye on life and planet</article-title>
        <source>Science</source>
        <year>2015</year>
        <volume>348</volume>
        <fpage>aaa2478</fpage>
        <pub-id pub-id-type="doi">10.1126/science.aaa2478</pub-id>
        <?supplied-pmid 26068858?>
        <pub-id pub-id-type="pmid">26068858</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schofield</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Chimpanzee face recognition from videos in the wild using deep learning</article-title>
        <source>Sci. Adv.</source>
        <year>2019</year>
        <volume>5</volume>
        <fpage>eaaw0736</fpage>
        <pub-id pub-id-type="doi">10.1126/sciadv.aaw0736</pub-id>
        <?supplied-pmid 31517043?>
        <pub-id pub-id-type="pmid">31517043</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Norouzzadeh</surname>
            <given-names>MS</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning</article-title>
        <source>Proc. Natl Acad. Sci. USA</source>
        <year>2018</year>
        <volume>115</volume>
        <fpage>E5716</fpage>
        <lpage>E5725</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1719367115</pub-id>
        <?supplied-pmid 29871948?>
        <pub-id pub-id-type="pmid">29871948</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Vidal, M., Wolf, N., Rosenberg, B., Harris, B. P. &amp; Mathis, A. Perspectives on individual animal identification from biology and computer vision. <italic>Integr. Comp. Biol.</italic><bold>61</bold>, 900–916 (2021).</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Datta</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>Anderson</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Branson</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Perona</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Leifer</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Computational neuroethology: a call to action</article-title>
        <source>Neuron</source>
        <year>2019</year>
        <volume>104</volume>
        <fpage>11</fpage>
        <lpage>24</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuron.2019.09.038</pub-id>
        <?supplied-pmid 31600508?>
        <pub-id pub-id-type="pmid">31600508</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mathis</surname>
            <given-names>MW</given-names>
          </name>
          <name>
            <surname>Mathis</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Deep learning tools for the measurement of animal behavior in neuroscience</article-title>
        <source>Curr. Opin. Neurobiol.</source>
        <year>2020</year>
        <volume>60</volume>
        <fpage>1</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="doi">10.1016/j.conb.2019.10.008</pub-id>
        <?supplied-pmid 31791006?>
        <pub-id pub-id-type="pmid">31791006</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mathis</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Schneider</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lauer</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Mathis</surname>
            <given-names>MW</given-names>
          </name>
        </person-group>
        <article-title>A primer on motion capture with deep learning: principles, pitfalls, and perspectives</article-title>
        <source>Neuron</source>
        <year>2020</year>
        <volume>108</volume>
        <fpage>44</fpage>
        <lpage>65</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuron.2020.09.017</pub-id>
        <?supplied-pmid 33058765?>
        <pub-id pub-id-type="pmid">33058765</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Pereira, T. D., Shaevitz, J. W. &amp; Murthy, M. Quantifying behavior to understand the brain. <italic>Nat. Neurosci.</italic><bold>23</bold>, 1537–1549 (2020).</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Cao, Z., Simon, T., Wei, S.-E. &amp; Sheikh, Y. Realtime multi-person 2D pose estimation using part affinity fields. In <italic>Proc. IEEE Conference on Computer Vision and Pattern Recognition</italic> 7291–7299 (IEEE, 2017).</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Newell, A., Huang, Z. &amp; Deng, J. Associative embedding: end-to-end learning for joint detection and grouping. In <italic>Proc. 31st Conference on Neural Information Processing Systems</italic> (eds Guyon, I. et al.) 2277–2287 (NIPS, 2017).</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Cheng, B. et al. Higherhrnet: scale-aware representation learning for bottom-up human pose estimation. In <italic>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> 5386–5395 (IEEE, 2020).</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Stoffl, L., Vidal, M. &amp; Mathis, A. End-to-end trainable multi-instance pose estimation with transformers. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2103.12115">https://arxiv.org/abs/2103.12115</ext-link> (2021).</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Insafutdinov, E., Pishchulin, L., Andres, B., Andriluka, M. &amp; Schiele, B. DeeperCut: a deeper, stronger, and faster multi-person pose estimation model. In <italic>Proc.</italic><italic>European Conference on Computer Vision</italic> 34–50 (Springer, 2016).</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Kreiss, S., Bertoni, L. &amp; Alahi, A. Pifpaf: composite fields for human pose estimation. In <italic>Proc. IEEE Conference on Computer Vision and Pattern Recognition</italic> 11977–11986 (IEEE, 2019).</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Segalin</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The mouse action recognition system (MARS) software pipeline for automated analysis of social behaviors in mice</article-title>
        <source>eLife</source>
        <year>2021</year>
        <volume>10</volume>
        <fpage>e63720</fpage>
        <pub-id pub-id-type="doi">10.7554/eLife.63720</pub-id>
        <?supplied-pmid 34846301?>
        <pub-id pub-id-type="pmid">34846301</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Pereira, T. D. et al. SLEAP: multi-animal pose tracking. Preprint at <italic>bioRxiv</italic>10.1101/2020.08.31.276246 (2020).</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Chen, Z. et al. AlphaTracker: a multi-animal tracking and behavioral analysis tool. Preprint at <italic>bioRxiv</italic>10.1101/2020.12.04.405159 (2020).</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Lin, T.-Y. et al. Microsoft COCO: common objects in context. In <italic>Proc. European Conference on Computer Vision</italic> 740–755 (Springer, 2014).</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mathis</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deeplabcut: markerless pose estimation of user-defined body parts with deep learning</article-title>
        <source>Nat. Neurosci.</source>
        <year>2018</year>
        <volume>21</volume>
        <fpage>1281</fpage>
        <lpage>1289</lpage>
        <pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id>
        <?supplied-pmid 30127430?>
        <pub-id pub-id-type="pmid">30127430</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nath</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Using deeplabcut for 3D markerless pose estimation across species and behaviors</article-title>
        <source>Nat. Protoc.</source>
        <year>2019</year>
        <volume>14</volume>
        <fpage>2152</fpage>
        <lpage>2176</lpage>
        <pub-id pub-id-type="doi">10.1038/s41596-019-0176-0</pub-id>
        <?supplied-pmid 31227823?>
        <pub-id pub-id-type="pmid">31227823</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Mathis, A. et al. Pretraining boosts out-of-domain robustness for pose estimation. In <italic>Proc. IEEE/CVF Winter Conference on Applications of Computer Vision</italic> 1859–1868 (IEEE, 2021).</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &amp; Sun, J. Deep residual learning for image recognition. In <italic>Proc. IEEE Conference on Computer Vision and Pattern Recognition</italic> 770–778 (IEEE, 2016).</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Tan, M. &amp; Le, Q. EfficientNet: rethinking model scaling for convolutional neural networks. In <italic>Proc. International Conference on Machine Learning</italic> 6105–6114 (PMLR, 2019).</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ghosh</surname>
            <given-names>KK</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Miniaturized integration of a fluorescence microscope</article-title>
        <source>Nat. Methods</source>
        <year>2011</year>
        <volume>8</volume>
        <fpage>871</fpage>
        <lpage>878</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.1694</pub-id>
        <?supplied-pmid 21909102?>
        <pub-id pub-id-type="pmid">21909102</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Bewley, A., Ge, Z., Ott, L., Ramos, F. &amp; Upcroft, B. Simple online and realtime tracking. In <italic>Proc. 2016 IEEE International Conference on Image Processing (ICIP)</italic> 3464–3468 (IEEE, 2016).</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Bertozzi, M. et al. Pedestrian localization and tracking system with Kalman filtering. In <italic>Proc.</italic><italic>IEEE Intelligent Vehicles Symposium, 2004</italic> 584–589 (IEEE, 2004).</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Romero-Ferrero</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Bergomi</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Hinz</surname>
            <given-names>RC</given-names>
          </name>
          <name>
            <surname>Heras</surname>
            <given-names>FJ</given-names>
          </name>
          <name>
            <surname>de Polavieja</surname>
            <given-names>GG</given-names>
          </name>
        </person-group>
        <article-title>idtracker.ai: tracking all individuals in small or large collectives of unmarked animals</article-title>
        <source>Nat. Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>179</fpage>
        <lpage>182</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-018-0295-5</pub-id>
        <?supplied-pmid 30643215?>
        <pub-id pub-id-type="pmid">30643215</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kane</surname>
            <given-names>GA</given-names>
          </name>
          <name>
            <surname>Lopes</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Saunders</surname>
            <given-names>JL</given-names>
          </name>
          <name>
            <surname>Mathis</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mathis</surname>
            <given-names>MW</given-names>
          </name>
        </person-group>
        <article-title>Real-time, low-latency closed-loop feedback using markerless posture tracking</article-title>
        <source>eLife</source>
        <year>2020</year>
        <volume>9</volume>
        <fpage>e61909</fpage>
        <pub-id pub-id-type="doi">10.7554/eLife.61909</pub-id>
        <?supplied-pmid 33289631?>
        <pub-id pub-id-type="pmid">33289631</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Claudi, F. Mouse top detailed. <italic>Zenodo</italic>10.5281/zenodo.3925997 (2020).</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Autry</surname>
            <given-names>AE</given-names>
          </name>
          <name>
            <surname>Bergan</surname>
            <given-names>JF</given-names>
          </name>
          <name>
            <surname>Watabe-Uchida</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Dulac</surname>
            <given-names>CG</given-names>
          </name>
        </person-group>
        <article-title>Galanin neurons in the medial preoptic area govern parental behaviour</article-title>
        <source>Nature</source>
        <year>2014</year>
        <volume>509</volume>
        <fpage>325</fpage>
        <lpage>330</lpage>
        <pub-id pub-id-type="doi">10.1038/nature13307</pub-id>
        <?supplied-pmid 24828191?>
        <pub-id pub-id-type="pmid">24828191</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kohl</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Functional circuit architecture underlying parental behaviour</article-title>
        <source>Nature</source>
        <year>2018</year>
        <volume>556</volume>
        <fpage>326</fpage>
        <lpage>331</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-018-0027-0</pub-id>
        <?supplied-pmid 29643503?>
        <pub-id pub-id-type="pmid">29643503</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Di Santo</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Blevins</surname>
            <given-names>EL</given-names>
          </name>
          <name>
            <surname>Lauder</surname>
            <given-names>GV</given-names>
          </name>
        </person-group>
        <article-title>Batoid locomotion: effects of speed on pectoral fin deformation in the little skate, <italic>Eucoraja erinacea</italic></article-title>
        <source>J. Exp. Biol.</source>
        <year>2017</year>
        <volume>220</volume>
        <fpage>705</fpage>
        <lpage>712</lpage>
        <pub-id pub-id-type="doi">10.1242/jeb.148767</pub-id>
        <?supplied-pmid 27965272?>
        <pub-id pub-id-type="pmid">27965272</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Li, J. et al. CrowdPose: efficient crowded scenes pose estimation and a new benchmark. In <italic>Proc. IEEE Conference on Computer Vision and Pattern Recognition</italic> 10863–10872 (IEEE, 2019).</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Sandler, M., Howard, A., Zhu, M., Zhmoginov, A. &amp; Chen, L.-C. MobileNetV2: inverted residuals and linear bottlenecks. In <italic>Proc. IEEE Conference on Computer Vision and Pattern Recognition</italic> 4510–4520 (IEEE, 2018).</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Lauer, J. et al. Multi-animal pose estimation and tracking with DeepLabCut. Preprint at <italic>bioRxiv</italic>10.1101/2021.04.30.442096 (2021).</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Kingma, D. P. &amp; Ba, J. Adam: a method for stochastic optimization. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link> (2014).</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Huang, J., Zhu, Z., Guo, F. &amp; Huang, G. The devil is in the details: delving into unbiased data processing for human pose estimation. In <italic>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> 5700–5709 (IEEE, 2020).</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Insafutdinov, E. et al. ArtTrack: articulated multi-person tracking in the wild. In <italic>Proc. IEEE Conference on Computer Vision and Pattern Recognition</italic> 6457–6465 (IEEE, 2017).</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Biggs, B., Roddick, T., Fitzgibbon, A. &amp; Cipolla, R. Creatures great and small: recovering the shape and motion of animals from video. In <italic>Proc. Asian Conference on Computer Vision</italic> 3–19 (Springer, 2018).</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ramanan</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Articulated human detection with flexible mixtures of parts</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2012</year>
        <volume>35</volume>
        <fpage>2878</fpage>
        <lpage>2890</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2012.261</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Huang, A. Similarity measures for text document clustering. In <italic>Proc. Sixth New Zealand Computer Science Research Student Conference (NZCSRSC2008)</italic> Vol. 4, 9–56 (2008).</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vallat</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Pingouin: statistics in Python</article-title>
        <source>J. Open Source Softw.</source>
        <year>2018</year>
        <volume>3</volume>
        <fpage>1026</fpage>
        <pub-id pub-id-type="doi">10.21105/joss.01026</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Sun, K., Xiao, B., Liu, D. &amp; Wang, J. Deep high-resolution representation learning for human pose estimation. In <italic>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> 5693–5703 (IEEE, 2019).</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Girdhar, R., Gkioxari, G., Torresani, L., Paluri, M. &amp; Tran, D. Detect-and-track: efficient pose estimation in videos. In <italic>Proc. IEEE Conference on Computer Vision and Pattern Recognition</italic> 350–359 (IEEE, 2018).</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Emami, P., Pardalos, P. M., Elefteriadou, L. &amp; Ranka, S. Machine learning methods for data association in multi-object tracking. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1802.06897">https://arxiv.org/abs/1802.06897</ext-link> (2018).</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Zhang, L., Li, Y. &amp; Nevatia, R. Global data association for multi-object tracking using network flows. In <italic>Proc.</italic><italic>2008 IEEE Conference on Computer Vision and Pattern Recognition</italic> 1–8 (IEEE, 2008).</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">Hagberg, A. A., Schult, D. A. &amp; Swart, P. J. Exploring network structure, dynamics, and function using NetworkX. In <italic>Proc. 7th Python in Science Conference</italic> (eds Varoquaux, G. et al.) 11–15 (2008).</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <mixed-citation publication-type="other">Dicle, C., Camps, O. I. &amp; Sznaier, M. The way they move: tracking multiple targets with similar appearance. In <italic>Proc. IEEE International Conference on Computer Vision</italic> 2304–2311 (IEEE, 2013).</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yin</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Model order determination using the Hankel matrix of impulse responses</article-title>
        <source>Appl. Math. Lett.</source>
        <year>2011</year>
        <volume>24</volume>
        <fpage>797</fpage>
        <lpage>802</lpage>
        <pub-id pub-id-type="doi">10.1016/j.aml.2010.12.046</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Ahuja, R. K., Magnanti, T. L. &amp; Orlin, J. B. <italic>Network Flows: Theory, Algorithms, and Applications</italic> (Prentice-Hall, 1993).</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">He, S. et al. TransReID: transformer-based object re-identification. In <italic>Proc. IEEE/CVF International Conference on Computer Vision (ICCV)</italic> 15013–15022 (IEEE, 2021).</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bernardin</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Stiefelhagen</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Evaluating multiple object tracking performance: the clear mot metrics</article-title>
        <source>EURASIP J. Image Video Proc.</source>
        <year>2008</year>
        <volume>2008</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1155/2008/246309</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tenenbaum</surname>
            <given-names>JB</given-names>
          </name>
          <name>
            <surname>De Silva</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Langford</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>A global geometric framework for nonlinear dimensionality reduction</article-title>
        <source>Science</source>
        <year>2000</year>
        <volume>290</volume>
        <fpage>2319</fpage>
        <lpage>2323</lpage>
        <pub-id pub-id-type="doi">10.1126/science.290.5500.2319</pub-id>
        <?supplied-pmid 11125149?>
        <pub-id pub-id-type="pmid">11125149</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <mixed-citation publication-type="other">Lauer, J. et al. madlc marmoset benchmark dataset—training. <italic>Zenodo</italic>10.5281/zenodo.5849371 (2022).</mixed-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <mixed-citation publication-type="other">Lauer, J. et al. madlc fish benchmark dataset—training. <italic>Zenodo</italic>10.5281/zenodo.5849286 (2022).</mixed-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <mixed-citation publication-type="other">Lauer, J. et al. madlc parenting benchmark dataset—training. <italic>Zenodo</italic>10.5281/zenodo.5851109 (2022).</mixed-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <mixed-citation publication-type="other">Lauer, J. et al. madlc tri-mouse benchmark dataset—training. <italic>Zenodo</italic>10.5281/zenodo.5851157 (2022).</mixed-citation>
    </ref>
  </ref-list>
</back>
