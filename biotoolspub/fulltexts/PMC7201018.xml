<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName archivearticle.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Bioeng Biotechnol</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Bioeng Biotechnol</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Bioeng. Biotechnol.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Bioengineering and Biotechnology</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2296-4185</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7201018</article-id>
    <article-id pub-id-type="doi">10.3389/fbioe.2020.00391</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Bioengineering and Biotechnology</subject>
        <subj-group>
          <subject>Methods</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SDN2GO: An Integrated Deep Learning Model for Protein Function Prediction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Cai</surname>
          <given-names>Yideng</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/930058/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Jiacheng</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Deng</surname>
          <given-names>Lei</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/586767/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>School of Computer Science and Engineering, Central South University</institution>, <addr-line>Changsha</addr-line>, <country>China</country></aff>
    <aff id="aff2"><sup>2</sup><institution>School of Software, Xinjiang University</institution>, <addr-line>Urumqi</addr-line>, <country>China</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Zhibin Lv, University of Electronic Science and Technology of China, China</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Qingting Wei, Nanchang University, China; Yanglan Gan, Donghua University, China</p>
      </fn>
      <corresp id="c001">*Correspondence: Lei Deng <email>leideng@csu.edu.cn</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Synthetic Biology, a section of the journal Frontiers in Bioengineering and Biotechnology</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>29</day>
      <month>4</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>8</volume>
    <elocation-id>391</elocation-id>
    <history>
      <date date-type="received">
        <day>13</day>
        <month>3</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>07</day>
        <month>4</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2020 Cai, Wang and Deng.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Cai, Wang and Deng</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>The assignment of function to proteins at a large scale is essential for understanding the molecular mechanism of life. However, only a very small percentage of the more than 179 million proteins in UniProtKB have Gene Ontology (GO) annotations supported by experimental evidence. In this paper, we proposed an integrated deep-learning-based classification model, named SDN2GO, to predict protein functions. SDN2GO applies convolutional neural networks to learn and extract features from sequences, protein domains, and known PPI networks, and then utilizes a weight classifier to integrate these features and achieve accurate predictions of GO terms. We constructed the training set and the independent test set according to the time-delayed principle of the Critical Assessment of Function Annotation (CAFA) and compared it with two highly competitive methods and the classic BLAST method on the independent test set. The results show that our method outperforms others on each sub-ontology of GO. We also investigated the performance of using protein domain information. We learned from the Natural Language Processing (NLP) to process domain information and pre-trained a deep learning sub-model to extract the comprehensive features of domains. The experimental results demonstrate that the domain features we obtained are much improved the performance of our model. Our deep learning models together with the data pre-processing scripts are publicly available as an open source software at <ext-link ext-link-type="uri" xlink:href="https://github.com/Charrick/SDN2GO">https://github.com/Charrick/SDN2GO</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>protein function</kwd>
      <kwd>word embedding</kwd>
      <kwd>convolutional neural network</kwd>
      <kwd>deep multi-label classification</kwd>
      <kwd>deep learning</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">National Natural Science Foundation of China<named-content content-type="fundref-id">10.13039/501100001809</named-content></funding-source>
        <award-id rid="cn001">No.61672541</award-id>
        <award-id rid="cn001">No.61972422</award-id>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="2"/>
      <equation-count count="8"/>
      <ref-count count="58"/>
      <page-count count="11"/>
      <word-count count="8404"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>As an essential structural molecule, protein is a vital component of all biological tissues and cells and is also the primary bearer of life activities (Weaver, <xref rid="B50" ref-type="bibr">2011</xref>). Understanding protein function is important both for biology and medicine and pharmacy. For example, clarifying the function of a protein can provide a target for genetic manipulation, and provide a reliable basis for designing a new protein or transform an existing protein, etc. So that, accurate annotation of protein functions is a significant and crucial task. Traditional experimental methods require a lot of resources and time to determine protein function, despite there are high accuracy and reliability. With the continuous development of high-throughput sequencing technology and genomics, the sequence of proteins has been exploded, but just a small percentage of the total known and predicted protein sequences have been extensively annotated regarding their functions. Currently, only &lt;0.1% of the more than 179 million proteins in UniProtKB have been experimentally annotated (Consortium, <xref rid="B9" ref-type="bibr">2019</xref>). However, it isn't straightforward to scale up the experimental method to accommodate such a large amount of protein sequence data, which urgently requires the development of computational methods to assist to annotate protein functions (Radivojac et al., <xref rid="B39" ref-type="bibr">2013</xref>).</p>
    <p>Gene Ontology, launched in 1998, is widely used in the field of Bioinformatics, and the original intention of GO was to provide a representative platform for terminology description or interpretation of words of genes and gene product characteristics. It enables Bioinformatics researchers to summarize, process, interpret, and share the data of genes and gene products (Ashburner et al., <xref rid="B3" ref-type="bibr">2000</xref>). Gene Ontology is a Directed Acyclic Graph (DAG) type ontology. At present, GO contains more than 45,000 biological concepts include functions and cell locations, and is divided into three categories, covering three aspects of biology: Biological Process, Molecular Function, and Cellular Component. A protein generally has multiple GO annotations; therefore, protein function prediction is a very large-scale multi-label classification problem (Zhang and Zhou, <xref rid="B56" ref-type="bibr">2013</xref>), and accurately assigning GO terms to proteins is a challenging task.</p>
    <p>In recent years, some organizations and teams have developed algorithms, tools, and systems for protein function prediction using advanced computer technologies, such as machine learning and deep neural networks (Kulmanov et al., <xref rid="B28" ref-type="bibr">2018</xref>; You et al., <xref rid="B54" ref-type="bibr">2018</xref>, <xref rid="B53" ref-type="bibr">2019</xref>; Hakala et al., <xref rid="B15" ref-type="bibr">2019</xref>; Lv et al., <xref rid="B33" ref-type="bibr">2019b</xref>; Piovesan and Tosatto, <xref rid="B38" ref-type="bibr">2019</xref>; Rifaioglu et al., <xref rid="B41" ref-type="bibr">2019</xref>; Kulmanov and Hoehndorf, <xref rid="B27" ref-type="bibr">2020</xref>). Researchers predict protein functions from one or more of the followings: protein sequences (Kulmanov et al., <xref rid="B28" ref-type="bibr">2018</xref>; You et al., <xref rid="B54" ref-type="bibr">2018</xref>, <xref rid="B53" ref-type="bibr">2019</xref>; Hakala et al., <xref rid="B15" ref-type="bibr">2019</xref>; Piovesan and Tosatto, <xref rid="B38" ref-type="bibr">2019</xref>; Kulmanov and Hoehndorf, <xref rid="B27" ref-type="bibr">2020</xref>), protein structures (Yang et al., <xref rid="B52" ref-type="bibr">2015</xref>; Zhang et al., <xref rid="B55" ref-type="bibr">2018</xref>), protein protein interactions (PPI) network (Kulmanov et al., <xref rid="B28" ref-type="bibr">2018</xref>; Zhang et al., <xref rid="B55" ref-type="bibr">2018</xref>; You et al., <xref rid="B53" ref-type="bibr">2019</xref>), and others (Kahanda and Ben-Hur, <xref rid="B22" ref-type="bibr">2017</xref>; Hakala et al., <xref rid="B15" ref-type="bibr">2019</xref>; Piovesan and Tosatto, <xref rid="B38" ref-type="bibr">2019</xref>; Rifaioglu et al., <xref rid="B41" ref-type="bibr">2019</xref>). For example specifically, GOLabeler (You et al., <xref rid="B54" ref-type="bibr">2018</xref>) integrated five different types of sequence-based information and learned from the idea of web page ranking to train an LTR (learning to rank) regression model to receive these five types of information to achieve accurate annotation of GO terms. As a result, this model got the best overall performance among all submissions of the 3rd Critical Assessment of Function Annotation (CAFA3). NetGO (You et al., <xref rid="B53" ref-type="bibr">2019</xref>), proposed by the GOLabeler team, is based on GOLabeler and incorporates massive amounts of protein-protein interaction (PPI) network information into the LTR framework. Compared with GOLabler, it has achieved a significant improvement in protein function prediction performance. Hakala et al. (<xref rid="B15" ref-type="bibr">2019</xref>) developed an integrated system, which obtain features from several different tools or methods: BLASTP, InterproScan, NCBI Taxonomy, NucPred, NetAcet, PredGPI, and Amino Acid Index (Kawashima and Kanehisa, <xref rid="B23" ref-type="bibr">2000</xref>; Heddad et al., <xref rid="B18" ref-type="bibr">2004</xref>; Kiemer et al., <xref rid="B24" ref-type="bibr">2005</xref>; Pierleoni et al., <xref rid="B37" ref-type="bibr">2008</xref>; Camacho et al., <xref rid="B6" ref-type="bibr">2009</xref>; Federhen, <xref rid="B11" ref-type="bibr">2012</xref>; Jones et al., <xref rid="B21" ref-type="bibr">2014</xref>), and then respectively feed all the features to two classifiers based on neural network and random forest and finally combined the NN classifier and the RF classifier to achieve the best prediction performance. DeepGO (Kulmanov et al., <xref rid="B28" ref-type="bibr">2018</xref>) encodes the amino acid sequence of the protein by trigrams and maps the trigrams to vector by one-hot encoding and dense embedding, and then feed it to a convolutional neural network (CNN) to extract the feature map. Next, a combined feature vector consisting of CNN features and PPI Network embedding features entered into the hierarchically structured classification layers for classification of GO terms. INGA2.0 (Piovesan and Tosatto, <xref rid="B38" ref-type="bibr">2019</xref>) uses four components, Homology which inferred from sequence similarity, Domain architecture, protein-protein interaction networks, and integrated information from the “dark proteome” which include disordered and transmembrane regions, to predict protein function. This method has better capabilities to predict some extremely rare GO terms compared with others. Overall, these highly competitive models and systems have proven their outstanding performance in protein function prediction and are continually being optimized.</p>
    <p>The amino acid sequence is crucial for understanding and analyzing proteins of various species. Some studies have shown that sequence homology-based BLAST methods are highly competitive in protein function prediction (Altshul, <xref rid="B1" ref-type="bibr">1997</xref>; Gillis and Pavlidis, <xref rid="B13" ref-type="bibr">2013</xref>; Hamp et al., <xref rid="B16" ref-type="bibr">2013</xref>). Besides, there are several high-level physiological functions, such as apoptosis or rhythm regulation, which are often the result of the interaction of multiple proteins (Kulmanov et al., <xref rid="B28" ref-type="bibr">2018</xref>), and according to the so-called “guilt-by-association” principle, interacting proteins should have some similar functions (Oliver, <xref rid="B36" ref-type="bibr">2000</xref>; Schwikowski et al., <xref rid="B42" ref-type="bibr">2000</xref>). Those shows that protein sequence information and PPI network information are essential to predict protein function. We have also noticed the critical position of the protein domain in protein-related features. The domain is a structural motif that exists independently in different combinations, and orders in the protein (Forslund and Sonnhammer, <xref rid="B12" ref-type="bibr">2008</xref>) and is a higher-level protein component than the amino acid sequence (Richardson, <xref rid="B40" ref-type="bibr">1981</xref>). Therefore, it makes sense to analyze and examine the effect of Domain content on protein function and try to use it to predict protein function. Besides, Machine Learning (ML) is currently popular and efficient for bioinformatics problems (You et al., <xref rid="B54" ref-type="bibr">2018</xref>, <xref rid="B53" ref-type="bibr">2019</xref>; Lai et al., <xref rid="B29" ref-type="bibr">2019</xref>; Tan et al., <xref rid="B47" ref-type="bibr">2019</xref>; Wang et al., <xref rid="B48" ref-type="bibr">2019a</xref>; Zhu et al., <xref rid="B58" ref-type="bibr">2019</xref>; Dao et al., <xref rid="B10" ref-type="bibr">2020</xref>), especially, due to its strong ability to fit high-dimensional, sparse, and highly collinear complex data, deep learning technology has been widely used in bioinformatics fields, such as protein structure and function (Sønderby and Winther, <xref rid="B44" ref-type="bibr">2014</xref>; Spencer et al., <xref rid="B45" ref-type="bibr">2014</xref>; Wei et al., <xref rid="B51" ref-type="bibr">2018</xref>; Kulmanov and Hoehndorf, <xref rid="B27" ref-type="bibr">2020</xref>), gene expression regulation (Chen et al., <xref rid="B7" ref-type="bibr">2016</xref>; Lanchantin et al., <xref rid="B30" ref-type="bibr">2016</xref>), protein classification (Asgari and Mofrad, <xref rid="B2" ref-type="bibr">2015</xref>; Sønderby et al., <xref rid="B43" ref-type="bibr">2015</xref>), and structure and functions of nucleic acid (Zhang et al., <xref rid="B57" ref-type="bibr">2016</xref>; Lv et al., <xref rid="B32" ref-type="bibr">2019a</xref>; Wang et al., <xref rid="B48" ref-type="bibr">2019a</xref>,<xref rid="B49" ref-type="bibr">b</xref>). For these considerations, here we proposed an integrated deep learning model based on protein sequences, protein domain content, and known protein-protein interaction networks to predict protein function. We first built three different neural network modules to learn features from protein sequences, domain content, and PPI Net separately, and then combined the features from these three different sources and inputted them to the neural network classifier to predict the probability of each GO term. The experimental results show that our method of adding domain content to predict protein function is successful, and our model achieved better performance than BLAST and two other recent high-performance methods on an independent dataset constructed using time-delay rules.</p>
  </sec>
  <sec sec-type="materials and methods" id="s2">
    <title>2. Materials and Methods</title>
    <sec>
      <title>2.1. Data Source</title>
      <sec>
        <title>2.1.1. Training Data</title>
        <list list-type="bullet">
          <list-item>
            <p>Sequence Data</p>
            <p>For our experiments, we downloaded the sequence information of the proteins needed for the research from the UniProt database as FASTA-format files (<ext-link ext-link-type="uri" xlink:href="http://www.uniprot.org/downloads">http://www.uniprot.org/downloads</ext-link>) (Consortium, <xref rid="B8" ref-type="bibr">2015</xref>). Then a CD-hit tool was used to de-redundant the downloaded protein sequence data. We grouped proteins with a sequence similarity &gt;60% into one cluster, and only one protein per cluster was retained. Finally, we obtained a benchmark for humans contains 13,704 proteins, and a benchmark for Yeast contains 6,623 proteins.</p>
          </list-item>
          <list-item>
            <p>Annotation Data</p>
            <p>We downloaded GO annotation data for proteins from GOA (<ext-link ext-link-type="uri" xlink:href="http://www.ebi.ac.uk/GOA">http://www.ebi.ac.uk/GOA</ext-link>) (Barrell et al., <xref rid="B4" ref-type="bibr">2009</xref>) published in December 2013. Please note that the GO annotation data here is for training only, and all data are annotated in 2013 or earlier. Finally, the annotation data contains 13,882 categories (9,221 in BP, 3,483 in MF, and 1,178 in CC) for Human and 4,796 categories (2,439 in BP, 1,733 in MF, and 624 in CC) for Yeast.</p>
          </list-item>
          <list-item>
            <p>Protein-Protein interaction (PPI) Network Data</p>
            <p>We have added protein-protein interaction (PPI) network data, which is derived from the STRING database v10 (<ext-link ext-link-type="uri" xlink:href="https://string-db.org/">https://string-db.org/</ext-link>) (Szklarczyk et al., <xref rid="B46" ref-type="bibr">2015</xref>), to improve the performance of the experiment. Among them, human PPI data contains 11,759,455 scored links of 19,257 proteins, and Yeast's PPI data contains 1,845,966 scored links of 6,507 proteins.</p>
          </list-item>
          <list-item>
            <p>Protein Domain Data</p>
            <p>We downloaded protein domain data from the public database interpro (Hunter et al., <xref rid="B19" ref-type="bibr">2009</xref>) (<ext-link ext-link-type="uri" xlink:href="http://www.ebi.ac.uk/interpro/download/">http://www.ebi.ac.uk/interpro/download/</ext-link>), which contains the all UniProtKB proteins and the InterPro entries and individual signatures they match. For a specific protein, we can obtain the types, quantity, and locations of all the domains it contains, and the start and the end positions in the protein sequence of a domain are indicated. We searched by the protein's UniProt ID to obtain the domain data of all the proteins we needed. Next, we performed de-redundancy; for the same domain information supported by contradictory evidence, we kept only one of them. In the end, our domain data contains 113,972 pieces of information of 14,242 domains for Human, and 23,326 pieces of information of 6,707 domains for Yeast.</p>
          </list-item>
        </list>
      </sec>
      <sec>
        <title>2.1.2. Independent Testing Data</title>
        <p>The independent test data set is used for comparison with the competing methods. The collection of data generally follows the time-delayed rule of the CAFA challenge. We downloaded GO annotation data for proteins from GOA published in January 2016 and then obtained protein GO annotations added after 2013 (2014 and 2015). Specifically, we removed the annotation data published in December 2013 from the annotation data published in January 2016 and only retained the newly added protein annotation data. Next, we constructed an independent test benchmark based on the newly added annotation data; please note that all proteins contained in this benchmark do not have any GO annotations before 2014. Similarly, we filtered those proteins that were only annotated by GO terms that are extremely infrequent. The filtered independent test set contains 68 proteins for BP, 136 proteins for MF, and 106 proteins for CC.</p>
      </sec>
    </sec>
    <sec>
      <title>2.2. Data Representation</title>
      <sec>
        <title>2.2.1. Protein Sequence Data</title>
        <p>Protein sequence information is one of the inputs to our model. The sequence of each protein is a string composed of 20 specific amino acid codes with different lengths. In this experiment, we only selected proteins with a sequence length not exceeding 1,500. If the sequence length is &lt;1,500, we padded zero at the end of the sequence to ensure that the length of each input protein sequence information is fixed. To fully extract the context and semantic knowledge of the sequence, we utilized the ProtVec of BioVec (Asgari and Mofrad, <xref rid="B2" ref-type="bibr">2015</xref>), which is a biological sequence representation and feature extraction method, to map the sequence information. This method borrows the ideas of “word embedding” from Natural Language Processing (NLP) and obtains vector representations of biological sequences through training, and ProtVec is used for protein sequences. We followed ProtVec and used 3-grams encoding for protein sequences, that is, using a window of length 3 with a step size of 1 to slide the protein sequence to obtain a 3-grams sequence with a length of 1498 for each protein.</p>
        <p>In order to convert 3-grams sequences information into vectors that can be received by the computing model, we used the ProtVec-100d-3grams table released by BioVec. We Downloaded this data from Harvard Dataverse (<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.7910/DVN/JMFHTN">http://dx.doi.org/10.7910/DVN/JMFHTN</ext-link>). In this table, the protein vector is a distributed representation of proteins, and a 100-D vector presents each 3-gram. For our experiment, according to ProtVec, each protein will be represented as a 1,498 * 100 vector matrix, and then used as input to the model. In particular, according to the way we treat proteins &lt;1,500 in length, if a 3-gram word contains one or more zeros we have padded, then the 3-gram will be represented as a 100D zero-vector.</p>
      </sec>
      <sec>
        <title>2.2.2. Protein Network Data</title>
        <p>The protein network data we downloaded is scored links between proteins. The higher the score, the greater the probability of interactions between proteins. We filtered all scored links with 400 points, leaving only scored links whose score higher than 400, and then integrated the filtered protein network data into a PPI scored matrix. Each row of this matrix is a vector that represents the interaction of a protein with other proteins. If protein A interacts with another protein B in selected data, we set the value at the corresponding position in the vector to the fraction of these two proteins; otherwise, we set it to 0.</p>
      </sec>
      <sec>
        <title>2.2.3. Protein Domain Data</title>
        <p>In proteins, the types and number of domains and the relative positions of different domains will affect the functions of the protein. To fully discover and extract the comprehensive information of the type, number, and position of domains in proteins to improve the performance of the model, we first need to sort the domains contained in each protein according to the information of positions in the domain data, so that we can obtain the information relative positions of different domains. However, the position information given by the database is only a possible range of domains in the protein sequence. For example, if the database provides the position of domain D in the sequence of protein P is 60–200, this only indicates that a domain D exists in the area of 60–200 in protein P, but we cannot obtain the actual length and location of this domain D. This is the result of technical limitations, which cause the existence of different domains to overlap, even a region completely contains another region, in a protein, and makes it challenging to sort domains.</p>
        <p>In our experiments, we proposed a simple sorting method based on regional center points to solve this problem. Specifically, in a specific protein, there are three possibilities for the geographical relationship between any two different domains: detached, crossing, and containing. If the relationship is detached, we can quickly sort the two domains. If it is a cross-relationship or a containing-relationship, we calculated the center points of the two regions separately, and then put the domain with a forward center point in front of another one. After this, the information on the type, quantity, and relative position of the domain in the protein are obtained. Next, we learned from the idea of Natural Language Processing and treat each domain as a biological word, so the information of domains describing a specific protein is a biological sentence composed of some domain words in a particular order, while the functions of a protein are what the biological sentence means. The purpose of the domain module is to receive the biological sentence of protein and then abstract the features that represent the meaning of the sentence. Because the number of domains contained in different proteins is inconsistent, here we also need to solve the problem of the inconsistent size of model input. We obtained the maximum number of domains of proteins and used this maximum number (357 for Human and 41 for Yeast) as a standard and proteins with fewer domains than the maximum number were padded with 0. We encoded domains by word Embedding to input it into the model. Specifically, we utilized PyTorch's Sparse layer, which can initialize a simple lookup table to map sparse vectors to dense vectors, to generate a fixed lookup table for the domains. In this lookup table, each domain is represented by a 128-dimensional vector. In principle, the Sparse layer automatically maps high-dimensional one-hot vectors to low-dimensional dense vectors and provides the index of the dense vectors. The dimensions of both the one-hot vectors and the dense vectors are manually set by the user as needed, and we could get the required dense vector by entering the index. Therefore, the domains sentence of Human is represented by a 357*128 two-dimensional matrix, while the domains sentence of Yeast is represented by a 41*128 two-dimensional matrix. The Sparse layer will be integrated into the model and trained together, that is, as the model is continuously optimized, the representation vectors of domains in the lookup table will become increasingly accurate.</p>
      </sec>
      <sec>
        <title>2.2.4. Protein GO Terms</title>
        <p>Given that a large number of specific GO terms often only exist in the annotation sets of a small number of proteins (You et al., <xref rid="B54" ref-type="bibr">2018</xref>), and considering the calculation limit, we ranked the GO terms according to the number of annotations in proteins, and then use a set of thresholds (40 for BP, 20 for MF and 20 for CC) to select the GO terms, which contains 491 BP terms, 321 MF terms, and 240 CC terms, for Human, and a set of thresholds (10 for BP, 10 for MF and 10 for CC) to select the GO terms, which contains 373 BP terms, 171 MF terms, and 151 CC terms, for Yeast. We created three binary vectors for each protein to represent the labels of three sub-ontologies of GO: BP Ontology, MF Ontology, and CC Ontology. If a protein is annotated by a GO term, the value at the corresponding position of the label vector is set as 1, and otherwise is set as zero. Please note that all GO categories in the label vectors are selected.</p>
      </sec>
    </sec>
    <sec>
      <title>2.3. Deep Model</title>
      <p>We trained three models for the three sub-ontologies of GO. We randomly extracted 80% of the training data for iterative training of the model, and used the remaining 20% to verify the performance of the model after each iteration, and retained the model with the best generalization performance. Given that our model needs to receive input from three aspects of sequence, domain content, and PPI network information, as shown in <xref ref-type="fig" rid="F1">Figure 1</xref>, we divided the model into four components: Sequence sub-model, Domain sub-model, PPI-Net sub-model, and Weighted Classifier.</p>
      <fig id="F1" position="float">
        <label>Figure 1</label>
        <caption>
          <p>The integrated deep learning model architecture. (1) The Sequence sub-model utilizes 1-Dimensional convolutional neural networks to extract features from sequence input, which was encoded as 3-grams and then mapped to 3-grams-vector-matrix. (2) The PPI Net sub-model is generated to dense the features from PPI Network using classical neural networks. (3) The Domain sub-model initializes a Sparse layer, which is integrated into the sub-model to optimize, to generate a lookup table for domains, and the sorted domains sentence processed by the Sparse layer is entered into 1-Dimensional convolutional neural networks to extract features. (4) All the output features of the three sub-models are combined and entered into the Weighted Classifier, and the output vector represents the probability of GO terms.</p>
        </caption>
        <graphic xlink:href="fbioe-08-00391-g0001"/>
      </fig>
      <sec>
        <title>2.3.1. Sequence Sub-model</title>
        <p>The input of this sub-model is a two-dimensional 3-grams-vector-matrix that represents protein sequence information. To extract in-depth high-dimensional features of protein biological sequences, we design and implement a model based on convolutional neural networks (CNN). The neural network is a mathematical algorithm model that mimics the behavioral characteristics of biological neural networks for distributed and parallel information processing (Haykin, <xref rid="B17" ref-type="bibr">1994</xref>). In CNN, there is depth structure, and the input is convolved to obtain the output (LeCun et al., <xref rid="B31" ref-type="bibr">1998</xref>), the convolution layer contains multiple convolution kernels, which can make the model extract more features in different aspects. In our experiment, we used a 1-Dimensional convolutional neural network, which uses a one-dimensional convolution kernel to perform convolution operations on the input data. After the sequence input is convolved to extract features, the output feature map is passed to the pooling layer for feature selection and information filtering; this is because the feature map still contains redundancy. Here, we use the max-pooling layer to treat the feature map. After processing, the selected feature map will be passed to the next layer as input. Specifically, three convolutional layers were set for the sequence sub-model, which were connected end to end. The feature map obtained after the convolution operation of each convolutional layer uses a maximum pooling layer to filter information to remove redundancy. The in-channels of the first convolutional layer are the same width as the input sequence information matrix and are set to 100. The in-channels of the other two convolutional layers are the same as the out-channels of the previous layer, and the out-channels of the three convolutional layers are set as 64, 32, and 16, respectively. For each convolution layer, a convolution kernel with a size of 16 is used for the convolution operation with a step size of 1. In order to completely extract the input features, padding was performed on the input with 0 before each convolution. Each maximum pooling layer is filtered using a kernel of size 2 with a step size of 2. The output feature map of the last pooling layer will be tiled into one dimension and input to the fully connected (FC) layers for dimensionality reduction. Finally, a feature vector representing the protein sequence information was obtained. The number of nodes in the output layer of the fully connected layer is set according to the number of three GO sub-ontology. Specifically, for Human, it was set as 491 for BP, 321 for MF, and 240 for CC, and for Yeast, it was set as 373 for BP, 171 for MF, and 151 for CC.</p>
      </sec>
      <sec>
        <title>2.3.2. PPI-Net Sub-model</title>
        <p>In the PPI scored matrix, the feature vectors that characterize the interaction between proteins and other proteins have large dimensions, which are 18,901 for Human and 6,054 for Yeast, respectively, so we built a three-layer trapezoidal neural network module to dense the PPI features. In this module, the number of nodes in the input layer is the same as the dimension of the input feature vector, which is 18,901 for Human and 6,054 for Yeast. The number of nodes in the hidden layer is set to an intermediate value according to the number of nodes in the input layer and the output layer, which are 4,096 for Human and 2,048 for Yeast. And the size of the output layer is based on different species and GO sub-ontology, and is the same as the output layer of the Sequence sub-model.</p>
      </sec>
      <sec>
        <title>2.3.3. Domain Sub-model</title>
        <p>The input of the Domain sub-model is the sorted protein domain content information. According to the input data, the first structure of the module is the integrated Sparse layer, the number of embedding is 14,243 for Human, and 6,708 for Yeast, and embedding dim are set as 128. For a specific protein, the output of the Sparse layer of the domain sentence input is a two-dimensional matrix. Therefore, similar to the sequence sub-model, we constructed a convolutional neural networks module containing two 1-D convolutional layers and two max-pooling layers. The in-channels of the first convolutional layer are set to 357 for Human, and 41 for Yeast, the in-channels of the second convolutional layer are consistent with the out-channels of the previous layer, and the out-channels of the two convolutional layers are set to 128 and 64. Besides, each convolutional layer used a convolution kernel of size 2 to perform a convolution operation with a step size of 2. In order to completely extract the input features, we padded the input with 0 before each convolution. The setting of the two maximum pooling layers is the same as the setting of the maximum pooling layer in the Sequence sub-model. The feature map output by the last pooling layer is tiled into one dimension and then input to the fully connected layers to reduce the dimension and the output layer of the fully connected layer. The size of the output layer is based on different species and GO sub-ontology, and is the same as the output layer of the Sequence sub-model.</p>
      </sec>
      <sec>
        <title>2.3.4. Weighted Classifier</title>
        <p>Weighted Classifier accepts output vectors from three sub-models: Sequence sub-model, Domain sub-model, PPI-Net sub-model. Through training, each GO classifier learns and optimal the weights that receive the features from three sub-models to achieve the best effect of multi-label classification. Note that the output vectors of the three modules have the same dimensions. As a whole, our Weight Classifier is a three-layer non-fully connected network model. The number of nodes in the input layer is the sum of the number of output nodes of the three sub-models, and both the nodes of hidden layer and the nodes of out layer are the same as nodes of the output layer of the three sub-models, which are set according to different species and GO sub-ontology. From the perspective of a single GO classifier, the structure is shown in <xref ref-type="fig" rid="F2">Figure 2</xref>. For a specific GO classifier, the hidden node only accepts three features, which are from the corresponding position of the output vector of three sub-model, respectively, corresponding to the GO category, and to extract the corresponding area, we used a binary mask matrix to implement this connection control. The output node of the Classifier also only receives the output of the corresponding hidden node, and we also used a binary mask matrix to implement connection control. In general, let the entire Weight Classifier as a whole again, each node in the hidden layer is only connected to the three corresponding nodes in the output layer, and each node in the output layer is connected to only one corresponding hidden layer node. Therefore, the weights between the hidden layer nodes and the input layer nodes represent the preference of the Classifier for features from three sub-models, and the weights between the output layer nodes and hidden layer nodes globally balance the output values of the Classifier to the same level.</p>
        <fig id="F2" position="float">
          <label>Figure 2</label>
          <caption>
            <p>The architecture of one single GO classifier in the weighted classifier.</p>
          </caption>
          <graphic xlink:href="fbioe-08-00391-g0002"/>
        </fig>
        <p>For all components of the model, we used the Rectified-linear-unit (ReLU) (Glorot et al., <xref rid="B14" ref-type="bibr">2011</xref>), which could improve the computational efficiency and retain gradient (Nair and Hinton, <xref rid="B34" ref-type="bibr">2010</xref>), as the activation function. Besides, by running specific optimization algorithms to minimize the loss function, the DNN model can be iteratively optimized by updating the weights and biases. Especially, the model is trained using an adaptive optimizer, Adam (Kingma and Ba, <xref rid="B25" ref-type="bibr">2014</xref>).</p>
      </sec>
    </sec>
    <sec>
      <title>2.4. Evaluate Methods</title>
      <p>We evaluate the performance of the model through three measures, which are F-max, AUPR (area under the precision-recall curve), and AUC (area under the receiver operator characteristics curve), where F-max and AUC are used in the CAFA challenge (Radivojac et al., <xref rid="B39" ref-type="bibr">2013</xref>). We use the standard provided by CAFA to calculate F-max and the formulas as follows:</p>
      <disp-formula id="E1">
        <label>(1)</label>
        <mml:math id="M1">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>F</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>m</mml:mi>
                    <mml:mi>a</mml:mi>
                    <mml:mi>x</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munder class="msub">
                    <mml:mrow>
                      <mml:mo class="qopname">max</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                    </mml:mrow>
                  </mml:munder>
                </mml:mstyle>
                <mml:mrow>
                  <mml:mo stretchy="false">{</mml:mo>
                  <mml:mrow>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mn>2</mml:mn>
                        <mml:mo>·</mml:mo>
                        <mml:mi>p</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mrow>
                          <mml:mo stretchy="false">(</mml:mo>
                          <mml:mrow>
                            <mml:mi>t</mml:mi>
                          </mml:mrow>
                          <mml:mo stretchy="false">)</mml:mo>
                        </mml:mrow>
                        <mml:mo>·</mml:mo>
                        <mml:mi>r</mml:mi>
                        <mml:mi>c</mml:mi>
                        <mml:mrow>
                          <mml:mo stretchy="false">(</mml:mo>
                          <mml:mrow>
                            <mml:mi>t</mml:mi>
                          </mml:mrow>
                          <mml:mo stretchy="false">)</mml:mo>
                        </mml:mrow>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>p</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mrow>
                          <mml:mo stretchy="false">(</mml:mo>
                          <mml:mrow>
                            <mml:mi>t</mml:mi>
                          </mml:mrow>
                          <mml:mo stretchy="false">)</mml:mo>
                        </mml:mrow>
                        <mml:mo>+</mml:mo>
                        <mml:mi>r</mml:mi>
                        <mml:mi>c</mml:mi>
                        <mml:mrow>
                          <mml:mo stretchy="false">(</mml:mo>
                          <mml:mrow>
                            <mml:mi>t</mml:mi>
                          </mml:mrow>
                          <mml:mo stretchy="false">)</mml:mo>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:mrow>
                  <mml:mo stretchy="false">}</mml:mo>
                </mml:mrow>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where <italic>pr</italic>(<italic>t</italic>) and <italic>rc</italic>(<italic>t</italic>), respectively represent precision and recall of the threshold <italic>t</italic> ∈ [0, 1], and can be calculated by the following formulas:</p>
      <disp-formula id="E2">
        <label>(2)</label>
        <mml:math id="M2">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>p</mml:mi>
                <mml:mi>r</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>t</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>m</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mo>·</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munderover accentunder="false" accent="false">
                    <mml:mrow>
                      <mml:mo>∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>m</mml:mi>
                      <mml:mrow>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mrow>
                          <mml:mi>t</mml:mi>
                        </mml:mrow>
                        <mml:mo stretchy="false">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:mi>p</mml:mi>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>r</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>t</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>and</p>
      <disp-formula id="E3">
        <label>(3)</label>
        <mml:math id="M3">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>r</mml:mi>
                <mml:mi>c</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>t</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>n</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mo>·</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munderover accentunder="false" accent="false">
                    <mml:mrow>
                      <mml:mo>∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>n</mml:mi>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:mi>r</mml:mi>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>t</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where <italic>m</italic>(<italic>t</italic>) is the number of proteins that annotated with at least one GO term using a threshold <italic>t</italic>, <italic>n</italic> is the total number of proteins in the target data set. <italic>pr</italic><sub><italic>i</italic></sub>(<italic>t</italic>) and <italic>rc</italic><sub><italic>i</italic></sub>(<italic>t</italic>) represent the precision and recall of a specific protein <italic>i</italic> using a threshold <italic>t</italic>, and are calculated by the following formulas:</p>
      <disp-formula id="E4">
        <label>(4)</label>
        <mml:math id="M4">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>p</mml:mi>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>r</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>t</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mo>∑</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>f</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mi>I</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:mi>f</mml:mi>
                        <mml:mo>∈</mml:mo>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>P</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mrow>
                          <mml:mo stretchy="false">(</mml:mo>
                          <mml:mrow>
                            <mml:mi>t</mml:mi>
                          </mml:mrow>
                          <mml:mo stretchy="false">)</mml:mo>
                        </mml:mrow>
                        <mml:mo>∧</mml:mo>
                        <mml:mi>f</mml:mi>
                        <mml:mo>∈</mml:mo>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>T</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mo>∑</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>f</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mi>I</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:mi>f</mml:mi>
                        <mml:mo>∈</mml:mo>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>P</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mrow>
                          <mml:mo stretchy="false">(</mml:mo>
                          <mml:mrow>
                            <mml:mi>t</mml:mi>
                          </mml:mrow>
                          <mml:mo stretchy="false">)</mml:mo>
                        </mml:mrow>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:mfrac>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>and</p>
      <disp-formula id="E5">
        <label>(5)</label>
        <mml:math id="M5">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>r</mml:mi>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>t</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mo>∑</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>f</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mi>I</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:mi>f</mml:mi>
                        <mml:mo>∈</mml:mo>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>P</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mrow>
                          <mml:mo stretchy="false">(</mml:mo>
                          <mml:mrow>
                            <mml:mi>t</mml:mi>
                          </mml:mrow>
                          <mml:mo stretchy="false">)</mml:mo>
                        </mml:mrow>
                        <mml:mo>∧</mml:mo>
                        <mml:mi>f</mml:mi>
                        <mml:mo>∈</mml:mo>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>T</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mo>∑</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>f</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mi>I</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:mi>f</mml:mi>
                        <mml:mo>∈</mml:mo>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>T</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:mfrac>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where <italic>f</italic> is a functional term in the ontology, Function <italic>I</italic>(·) is the standard indicator function. <italic>T</italic><sub><italic>i</italic></sub> is the set of true labels for protein <italic>i</italic>, and <italic>P</italic><sub><italic>i</italic></sub>(<italic>t</italic>) is the set of predicted labels for protein <italic>i</italic> using a threshold <italic>t</italic>. Once the precision and recall that calculated by different values of t for a particular functional term were determined overall proteins, we could then calculate the AUPR using the trapezoid rule. Compared with AUC, AUPR has a greater penalty for false positives[6].</p>
      <p>We also calculate the AUC value for each model of the GO sub-ontology, and the calculation formulas are as follows:</p>
      <disp-formula id="E6">
        <label>(6)</label>
        <mml:math id="M6">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>A</mml:mi>
                <mml:mi>U</mml:mi>
                <mml:mi>C</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:msubsup>
                  <mml:mrow>
                    <mml:mstyle displaystyle="true">
                      <mml:mo>∫</mml:mo>
                    </mml:mstyle>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo>-</mml:mo>
                    <mml:mi>∞</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>∞</mml:mi>
                  </mml:mrow>
                </mml:msubsup>
                <mml:mi>T</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mi>R</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>t</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mo>-</mml:mo>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>R</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mi>d</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mo>,</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <disp-formula id="E7">
        <label>(7)</label>
        <mml:math id="M7">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>T</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mi>R</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>t</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                    <mml:mo>+</mml:mo>
                    <mml:mi>F</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:mfrac>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>and</p>
      <disp-formula id="E8">
        <label>(8)</label>
        <mml:math id="M8">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>F</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mi>R</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>t</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                    <mml:mo>+</mml:mo>
                    <mml:mi>T</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:mfrac>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where <italic>TP</italic> is the number of true positives, <italic>FP</italic> is the number of false positives, and <italic>TN</italic> is the number of true negatives, <italic>FN</italic> is the number of false negatives.</p>
    </sec>
    <sec>
      <title>2.5. Model Implementation and Computing Environment</title>
      <p>We used PyTorch, a Python-based deep learning framework, to implement our model. To speed up the training process, we used a <italic>RHEL</italic> server with four <italic>NVIDIACorporationGM</italic>107<italic>GL</italic> graphics cards installed and total video memory of 32 GB. Under a set of parameters, the whole training time for the most computationally-intensive BP model is &lt;10 h. In terms of prediction, in the case where the sequence, domain, and PPI input information of the predicted protein has been processed in advance, using an optimized model to predict 1,000 proteins takes about 6 min.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>3. Results</title>
    <sec>
      <title>3.1. Experiment</title>
      <p>Owing to the complexity of our model composition and the requirement to determine a large number of hyperparameters, we first pre-trained the three-component sub-models of Sequence, Domain, and PPI Net. We used the GO annotations of proteins as a label and calculated the binary cross-entropy between the predicted values and the actual values, and use this as the loss to back-propagate to update the weights and biases between the nodes connected in the model. We manually adjusted the hyper-parameters, such as the learning rate and batch-size of each module, and selected the optimal model based on the validation loss value using the training set. After adjusting the parameters of the three sub-modules, we used the output of these three fine-tuned models as input to manually adjusted the hyperparameters of the Weighted Classifier, and also select the optimal model based on the validation loss value using the training set. <xref ref-type="supplementary-material" rid="SM1">Tables S1</xref>–<xref ref-type="supplementary-material" rid="SM1">S4</xref> shows the details of the training of different hyperparameters.</p>
      <p>We used 5-fold cross-validation on the training set to test the performance of the model, and the results are shown in <xref rid="T1" ref-type="table">Table 1</xref>. It is clear that the model has achieved a favorable F-max value for each sub-ontology of GO, which indicates that our method is an effective protein function prediction method.</p>
      <table-wrap id="T1" position="float">
        <label>Table 1</label>
        <caption>
          <p>The 5-fold cross validation results of training data.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Method</bold>
              </th>
              <th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>BP</bold>
              </th>
              <th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>MF</bold>
              </th>
              <th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>CC</bold>
              </th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>
                  <italic>F</italic>
                  <sub>max</sub>
                </bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AUPR</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AUC</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>
                  <italic>F</italic>
                  <sub>max</sub>
                </bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AUPR</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AUC</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>
                  <italic>F</italic>
                  <sub>max</sub>
                </bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AUPR</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AUC</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">SN2GO (human)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.473</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.441</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.908</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.546</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.527</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.938</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.587</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.600</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.949</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">SDN2GO (human)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.507</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.487</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.921</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.653</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.655</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.957</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.601</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.617</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.952</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">SN2GO (yeast)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.414</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.289</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.810</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.548</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.435</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.870</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.520</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.395</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.881</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">SDN2GO (yeast)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.415</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.304</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.839</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.611</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.530</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.903</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.528</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.424</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.878</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>The bold values indicate the best values</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.2. Evaluating the Performance of Using Domain Content</title>
      <p>Using the comprehensive information of types, quantities, and positions of protein domain content for prediction of protein function is the crucial component and emphasis of this research. In order to explore and explain the critical role of comprehensive domain information on protein function prediction, the deep models without the domain module were constructed for three sub-ontology of GO, and each model contained only the Sequence sub-model, PPI-Net sub-model, and Weighted Classifier, and we named it SN2GO. Among SN2GO, since the Sequence sub-model and PPI-Net sub-model in the SDN2GO model are pre-trained separately, the structure and hyperparameter settings of the Sequence sub-model and the PPI-Net sub-model are the same as those of the corresponding modules in the SDN2GO model, and the Weighted Classifier removes the relevant part of the domain from the input layer, the settings of the hidden layer and output layer are still the same as those of the SDN2GO Weighted Classifier. To ensure fairness of comparison, we also manually readjusted the learning rate and batch size hyperparameters and selected the optimal Weighted Classifier model for SN2GO.</p>
      <p>We observed the performance of SN2GO on the training set and compared it with SDN2GO. As the same, we used SN2GO to perform a 5-fold cross-validation experiment on the training set. <xref rid="T1" ref-type="table">Table 1</xref> shows the cross-validation results of SN2GO. We find that compared with SN2GO, the performance of the SDN2GO that uses domain information has been significantly improved on all the sub-ontology of GO, especially in the MF Ontology of humans, the F-measure value of SDN2GO has been enhanced by nearly 20% (0.65 vs. 0.55) compared to SN2GO. As shown in <xref ref-type="fig" rid="F3">Figure 3</xref>, the PR curves of SDN2GO and SN2GO on validation data of humans, it is clear that the red PR curve surrounds the other one on each sub-ontology. This result shows that domain information plays an essential role in protein function prediction, and proves that our coding and processing methods for protein domain information and the sub deep learning models for domains are useful and meaningful.</p>
      <fig id="F3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Precision-recall (P-R) curves of SDN2GO and SN2GO. The performances of the two methods were evaluated on the validation data of human in each sub-ontology of GO (gene ontology).</p>
        </caption>
        <graphic xlink:href="fbioe-08-00391-g0003"/>
      </fig>
    </sec>
    <sec>
      <title>3.3. Comparison With Competing Methods</title>
      <p>In order to further verify the performance of SDN2GO, we compared the two novel methods, NetGO and DeepGO, on the independent test set. Both of these two methods are competitive and excellent in protein function prediction and have achieved outstanding results on some datasets. As a state-of-the-art machine learning method for protein function prediction, NetGO provides constructive ideas on how to integrate features based on different sources. At the same time, DeepGO is quite representative of using deep learning technology for protein function prediction. Specifically, NetGO integrates five different types of sequence-based evidence and massive network information into the learning to rank (LTR) framework to predict protein function. We uploaded the protein sequence of the independent test set in Fasta format to the AFP (automated function prediction) webserver (<ext-link ext-link-type="uri" xlink:href="http://issubmission.sjtu.edu.cn/netgo/">http://issubmission.sjtu.edu.cn/netgo/</ext-link>) released by NetGO and then downloaded the prediction result of NetGO in txt format after a while. DeepGO uses convolutional neural networks to extract protein sequence features and combines known PPI network information as combined features to predict protein functions. We downloaded all source code of DeepGO from GitHub and downloaded the required data, and the fine turned neural network models saved in PKL format from the provided webserver (<ext-link ext-link-type="uri" xlink:href="http://deepgo.bio2vec.net/data/deepgo/">http://deepgo.bio2vec.net/data/deepgo/</ext-link>), and then entered the test protein sequence in Fasta format to this open-source tool, and obtained the prediction results of DeepGO. Besides, the BLAST was also used in comparative experiments.</p>
      <p>The comparison results are shown in <xref rid="T2" ref-type="table">Table 2</xref>. We have observed that BLAST performs well on every GO sub-ontology, which illustrates again that the sequence homology-based BLAST method is still quite competitive. NetGO and DeepGO performed well on MFO and BPO, respectively, but did not achieve their claimed effects on other sub-ontology. We further analyzed the prediction results of these two methods, and we found that the false-positive rates of both of them are relatively high, which leads to their inability to obtain high precision values. <xref ref-type="fig" rid="F4">Figure 4</xref>, which shows the PR curves of MFO on independent test sets for various methods, demonstrates our analysis results from one aspect. The PR curves of BPO and CCO and other specific details can be seen in <xref ref-type="supplementary-material" rid="SM1">Figures S1</xref>, <xref ref-type="supplementary-material" rid="SM1">S2</xref>. Obviously, SDN2GO outperformed other methods on all sub-ontologies, especially on MFO. Those shows that our model has excellent generalization performance and is a currently competitive method for protein function prediction. In particular, we paid attention to the performance of SN2GO, which lacks the domain sub-model on the test set. The results show that its performance on BPO and MFO is far worse than that of SDN2GO, and prove that extracting features from protein domains for protein function prediction is feasible, and will improve the accuracy of GO term labeling for proteins, especially on BPO and MFO.</p>
      <table-wrap id="T2" position="float">
        <label>Table 2</label>
        <caption>
          <p>The comparison results of the competing method on the independent testing set.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Method</bold>
              </th>
              <th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>BP</bold>
              </th>
              <th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>MF</bold>
              </th>
              <th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>CC</bold>
              </th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>
                  <italic>F</italic>
                  <sub>max</sub>
                </bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AUPR</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AUC</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>
                  <italic>F</italic>
                  <sub>max</sub>
                </bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AUPR</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AUC</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>
                  <italic>F</italic>
                  <sub>max</sub>
                </bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AUPR</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AUC</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">BLAST</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.347</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.192</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.771</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.381</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.292</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.873</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.386</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.245</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.860</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DeepGO</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.321</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.095</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.729</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.291</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.117</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.784</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.210</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.080</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.687</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">NetGO</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.173</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.048</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.594</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.386</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.243</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.919</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.217</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.092</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.669</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">SN2GO</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.132</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.044</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.893</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.423</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.306</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.953</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.384</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.264</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.948</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">SDN2GO</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.361</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.203</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.917</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.561</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.471</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.964</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.432</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.290</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.947</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>The bold values indicate the best values</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <fig id="F4" position="float">
        <label>Figure 4</label>
        <caption>
          <p>Precision-recall (P-R) curves of BLAST, DeepGO, NetGO, SN2GO, and SDN2GO. The performances of the five methods were evaluated on the independent testing set in MFO (molecular function ontology).</p>
        </caption>
        <graphic xlink:href="fbioe-08-00391-g0004"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>4. Discussion</title>
    <p>SDN2GO, an integrated deep learning-based weight model we have proposed, combines three aspects of information: protein sequence, protein domain content, and known protein-protein interaction networks. We constructed three sub-models for these three aspects of information, and then learned and extracted three components of features through pre-training the sub-models. Each GO term of the protein was finally scored and annotated through the integrated deep learning weight classifier. The 5-fold cross-validation results show that SDN2GO is a stable and reliable method for protein function prediction. In order to further verify the generalization performance and competitiveness of SDN2GO, we constructed an independent test set based on the principle of time-delay for comparison with the novel method and the classic BLAST method. The comparison results show that our method has achieved the maximum F-max value for each sub-ontology of GO.</p>
    <p>Many studies illustrated that protein sequence and PPI network are valid for protein function (Kirac and Ozsoyoglu, <xref rid="B26" ref-type="bibr">2008</xref>; Jiang and McQuay, <xref rid="B20" ref-type="bibr">2011</xref>; Nguyen et al., <xref rid="B35" ref-type="bibr">2011</xref>; Baryshnikova, <xref rid="B5" ref-type="bibr">2016</xref>; Kulmanov et al., <xref rid="B28" ref-type="bibr">2018</xref>). Besides, some researchers have used protein domain information to predict protein function (Altshul, <xref rid="B1" ref-type="bibr">1997</xref>; Forslund and Sonnhammer, <xref rid="B12" ref-type="bibr">2008</xref>), but they only focused on a single aspect of type or structure of the domain and failed to fully mine the general characteristics of various aspects of the domain. We considered this and drowned lessons from the principle of NLP to encode domains to integrate the type, quantity, and position information of the protein domains, and utilized the convolutional neural network to extract the general characteristics of the domains, which is the advantage of our model. We built a comparison model SN2GO based on SDN2GO without domain sub-model and conducted comparative experiments on both the training data and the independent test set. The results show that the domain information has significantly improved the prediction effect of the model, especially in BPO On MFO; this might be because the domain information, as a higher-level protein feature than sequence, is more intuitive in expression and closer to the functions of the protein. And to a certain extent, the comparison results illustrated the correctness and generalizability of our methods of protein domain information processing and feature extraction.</p>
    <p>In the future, we will continue to improve our model, such as adding more GO annotation categories to expand the scale of multi-label classification. Besides, we will also try to integrate more aspects of protein-related features, such as protein structure information and co-expression information, into our model to explore the role of different information on protein function prediction.</p>
  </sec>
  <sec sec-type="data-availability" id="s5">
    <title>Data Availability Statement</title>
    <p>Publicly available datasets were analyzed in this study. This data can be found here: <ext-link ext-link-type="uri" xlink:href="https://github.com/Charrick/SDN2GO/tree/master/data">https://github.com/Charrick/SDN2GO/tree/master/data</ext-link>.</p>
  </sec>
  <sec id="s6">
    <title>Author Contributions</title>
    <p>YC and LD conceived this work and designed the experiments. YC and JW built the experimental environment. YC carried out the experiments. YC, LD, and JW collected the data and analyzed the results. YC and LD wrote, revised, and approved the manuscript.</p>
  </sec>
  <sec id="s7">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
</body>
<back>
  <ack>
    <p>We would like to thank the Experimental Center of School of Computer Science of Central South University, for providing computing resources.</p>
  </ack>
  <fn-group>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding.</bold> This research was funded by National Natural Science Foundation of China under grant Nos. 61972422 and 61672541.</p>
    </fn>
  </fn-group>
  <sec sec-type="supplementary-material" id="s8">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fbioe.2020.00391/full#supplementary-material">https://www.frontiersin.org/articles/10.3389/fbioe.2020.00391/full#supplementary-material</ext-link></p>
    <supplementary-material content-type="local-data" id="SM1">
      <media xlink:href="Image_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Altshul</surname><given-names>S. F.</given-names></name></person-group> (<year>1997</year>). <article-title>Gapped blast and psi-blast: a new generation of protein database search programs</article-title>. <source>Nucleic Acids Res</source>. <volume>25</volume>, <fpage>3389</fpage>–<lpage>3402</lpage>. <pub-id pub-id-type="doi">10.1093/nar/25.17.3389</pub-id><pub-id pub-id-type="pmid">9254694</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asgari</surname><given-names>E.</given-names></name><name><surname>Mofrad</surname><given-names>M. R.</given-names></name></person-group> (<year>2015</year>). <article-title>Continuous distributed representation of biological sequences for deep proteomics and genomics</article-title>. <source>PLoS ONE</source>
<volume>10</volume>:<fpage>e0141287</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0141287</pub-id><?supplied-pmid 26555596?><pub-id pub-id-type="pmid">26555596</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashburner</surname><given-names>M.</given-names></name><name><surname>Ball</surname><given-names>C. A.</given-names></name><name><surname>Blake</surname><given-names>J. A.</given-names></name><name><surname>Botstein</surname><given-names>D.</given-names></name><name><surname>Butler</surname><given-names>H.</given-names></name><name><surname>Cherry</surname><given-names>J. M.</given-names></name><etal/></person-group>. (<year>2000</year>). <article-title>Gene ontology: tool for the unification of biology</article-title>. <source>Nat. Genet</source>. <volume>25</volume>, <fpage>25</fpage>–<lpage>29</lpage>. <pub-id pub-id-type="doi">10.1038/75556</pub-id><?supplied-pmid 10802651?><pub-id pub-id-type="pmid">10802651</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrell</surname><given-names>D.</given-names></name><name><surname>Dimmer</surname><given-names>E.</given-names></name><name><surname>Huntley</surname><given-names>R. P.</given-names></name><name><surname>Binns</surname><given-names>D.</given-names></name><name><surname>O'Donovan</surname><given-names>C.</given-names></name><name><surname>Apweiler</surname><given-names>R.</given-names></name></person-group> (<year>2009</year>). <article-title>The goa database in 2009-an integrated gene ontology annotation resource</article-title>. <source>Nucleic Acids Res</source>. <volume>37</volume>, <fpage>D396</fpage>–<lpage>D403</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkn803</pub-id><?supplied-pmid 18957448?><pub-id pub-id-type="pmid">18957448</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baryshnikova</surname><given-names>A.</given-names></name></person-group> (<year>2016</year>). <article-title>Systematic functional annotation and visualization of biological networks</article-title>. <source>Cell Syst</source>. <volume>2</volume>, <fpage>412</fpage>–<lpage>421</lpage>. <pub-id pub-id-type="doi">10.1016/j.cels.2016.04.014</pub-id><?supplied-pmid 27237738?><pub-id pub-id-type="pmid">27237738</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Camacho</surname><given-names>C.</given-names></name><name><surname>Coulouris</surname><given-names>G.</given-names></name><name><surname>Avagyan</surname><given-names>V.</given-names></name><name><surname>Ma</surname><given-names>N.</given-names></name><name><surname>Papadopoulos</surname><given-names>J.</given-names></name><name><surname>Bealer</surname><given-names>K.</given-names></name><etal/></person-group>. (<year>2009</year>). <article-title>Blast+: architecture and applications</article-title>. <source>BMC Bioinformatics</source><volume>10</volume>:<fpage>421</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2105-10-421</pub-id><?supplied-pmid 20003500?><pub-id pub-id-type="pmid">20003500</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Narayan</surname><given-names>R.</given-names></name><name><surname>Subramanian</surname><given-names>A.</given-names></name><name><surname>Xie</surname><given-names>X.</given-names></name></person-group> (<year>2016</year>). <article-title>Gene expression inference with deep learning</article-title>. <source>Bioinformatics</source>
<volume>32</volume>, <fpage>1832</fpage>–<lpage>1839</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btw074</pub-id><?supplied-pmid 26873929?><pub-id pub-id-type="pmid">26873929</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Consortium</surname><given-names>U.</given-names></name></person-group> (<year>2015</year>). <article-title>Uniprot: a hub for protein information</article-title>. <source>Nucleic Acids Res</source>. <volume>43</volume>, <fpage>D204</fpage>–<lpage>D212</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gku989</pub-id><pub-id pub-id-type="pmid">25348405</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Consortium</surname><given-names>U.</given-names></name></person-group> (<year>2019</year>). <article-title>Uniprot: a worldwide hub of protein knowledge</article-title>. <source>Nucleic Acids Res</source>. <volume>47</volume>, <fpage>D506</fpage>–<lpage>D515</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gky1049</pub-id><pub-id pub-id-type="pmid">30395287</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dao</surname><given-names>F.-Y.</given-names></name><name><surname>Lv</surname><given-names>H.</given-names></name><name><surname>Zulfiqar</surname><given-names>H.</given-names></name><name><surname>Yang</surname><given-names>H.</given-names></name><name><surname>Su</surname><given-names>W.</given-names></name><name><surname>Gao</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>A computational platform to identify origins of replication sites in eukaryotes</article-title>. <source>Brief. Bioinform. [Preprint]</source><fpage>bbaa017</fpage>. <pub-id pub-id-type="doi">10.1093/bib/bbaa017</pub-id><?supplied-pmid 32065211?><pub-id pub-id-type="pmid">32065211</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Federhen</surname><given-names>S.</given-names></name></person-group> (<year>2012</year>). <article-title>The ncbi taxonomy database</article-title>. <source>Nucleic Acids Res</source>. <volume>40</volume>, <fpage>D136</fpage>–<lpage>D143</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkr1178</pub-id><?supplied-pmid 22139910?><pub-id pub-id-type="pmid">22139910</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Forslund</surname><given-names>K.</given-names></name><name><surname>Sonnhammer</surname><given-names>E. L.</given-names></name></person-group> (<year>2008</year>). <article-title>Predicting protein function from domain content</article-title>. <source>Bioinformatics</source>
<volume>24</volume>, <fpage>1681</fpage>–<lpage>1687</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btn312</pub-id><?supplied-pmid 18591194?><pub-id pub-id-type="pmid">18591194</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillis</surname><given-names>J.</given-names></name><name><surname>Pavlidis</surname><given-names>P.</given-names></name></person-group> (<year>2013</year>). <article-title>Characterizing the state of the art in the computational assignment of gene function: lessons from the first critical assessment of functional annotation (cafa)</article-title>. <source>BMC Bioinformatics</source>
<volume>14</volume>:<fpage>S15</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2105-14-S3-S15</pub-id><?supplied-pmid 23630983?><pub-id pub-id-type="pmid">23630983</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Glorot</surname><given-names>X.</given-names></name><name><surname>Bordes</surname><given-names>A.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name></person-group> (<year>2011</year>). <article-title>“Deep sparse rectifier neural networks,”</article-title> in <source>Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</source> (<publisher-loc>Fort Lauderdale, FL</publisher-loc>), <fpage>315</fpage>–<lpage>323</lpage>.</mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hakala</surname><given-names>K.</given-names></name><name><surname>Kaewphan</surname><given-names>S.</given-names></name><name><surname>Björne</surname><given-names>J.</given-names></name><name><surname>Mehryary</surname><given-names>F.</given-names></name><name><surname>Moen</surname><given-names>H.</given-names></name><name><surname>Tolvanen</surname><given-names>M.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Neural network and random forest models in protein function prediction</article-title>. <source>BioRxiv</source>
<fpage>690271</fpage>
<pub-id pub-id-type="doi">10.1101/690271</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamp</surname><given-names>T.</given-names></name><name><surname>Kassner</surname><given-names>R.</given-names></name><name><surname>Seemayer</surname><given-names>S.</given-names></name><name><surname>Vicedo</surname><given-names>E.</given-names></name><name><surname>Schaefer</surname><given-names>C.</given-names></name><name><surname>Achten</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2013</year>). <article-title>Homology-based inference sets the bar high for protein function prediction</article-title>. <source>BMC Bioinformatics</source><volume>14</volume>:<fpage>S7</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2105-14-S3-S7</pub-id><?supplied-pmid 23514582?><pub-id pub-id-type="pmid">23514582</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Haykin</surname><given-names>S.</given-names></name></person-group> (<year>1994</year>). <source>Neural Networks: A Comprehensive Foundation</source>. <publisher-loc>Upper Saddle River, NJ</publisher-loc>: <publisher-name>Prentice Hall PTR</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Heddad</surname><given-names>A.</given-names></name><name><surname>Brameier</surname><given-names>M.</given-names></name><name><surname>MacCallum</surname><given-names>R. M.</given-names></name></person-group> (<year>2004</year>). <article-title>“Evolving regular expression-based sequence classifiers for protein nuclear localisation,”</article-title> in <source>Workshops on Applications of Evolutionary Computation</source> (<publisher-loc>Berlin; Heidelberg</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>31</fpage>–<lpage>40</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-540-24653-4_4</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>S.</given-names></name><name><surname>Apweiler</surname><given-names>R.</given-names></name><name><surname>Attwood</surname><given-names>T. K.</given-names></name><name><surname>Bairoch</surname><given-names>A.</given-names></name><name><surname>Bateman</surname><given-names>A.</given-names></name><name><surname>Binns</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2009</year>). <article-title>Interpro: the integrative protein signature database</article-title>. <source>Nucleic Acids Res</source>. <volume>37</volume>, <fpage>D211</fpage>–<lpage>D215</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkn785</pub-id><?supplied-pmid 18940856?><pub-id pub-id-type="pmid">18940856</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>J. Q.</given-names></name><name><surname>McQuay</surname><given-names>L. J.</given-names></name></person-group> (<year>2011</year>). <article-title>Predicting protein function by multi-label correlated semi-supervised learning</article-title>. <source>IEEE/ACM Trans. Comput. Biol. Bioinform</source>. <volume>9</volume>, <fpage>1059</fpage>–<lpage>1069</lpage>. <pub-id pub-id-type="doi">10.1109/TCBB.2011.156</pub-id><?supplied-pmid 22595236?><pub-id pub-id-type="pmid">22595236</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>P.</given-names></name><name><surname>Binns</surname><given-names>D.</given-names></name><name><surname>Chang</surname><given-names>H.-Y.</given-names></name><name><surname>Fraser</surname><given-names>M.</given-names></name><name><surname>Li</surname><given-names>W.</given-names></name><name><surname>McAnulla</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>Interproscan 5: genome-scale protein function classification</article-title>. <source>Bioinformatics</source><volume>30</volume>, <fpage>1236</fpage>–<lpage>1240</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btu031</pub-id><?supplied-pmid 24451626?><pub-id pub-id-type="pmid">24451626</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kahanda</surname><given-names>I.</given-names></name><name><surname>Ben-Hur</surname><given-names>A.</given-names></name></person-group> (<year>2017</year>). <article-title>“Gostruct 2.0: Automated protein function prediction for annotated proteins,”</article-title> in <source>Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics</source> (<publisher-loc>New York, NY</publisher-loc>), <fpage>60</fpage>–<lpage>66</lpage>. <pub-id pub-id-type="doi">10.1145/3107411.3107417</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kawashima</surname><given-names>S.</given-names></name><name><surname>Kanehisa</surname><given-names>M.</given-names></name></person-group> (<year>2000</year>). <article-title>Aaindex: amino acid index database</article-title>. <source>Nucleic Acids Res</source>. <volume>28</volume>, <fpage>374</fpage>–<lpage>374</lpage>. <pub-id pub-id-type="doi">10.1093/nar/28.1.374</pub-id><?supplied-pmid 10592278?><pub-id pub-id-type="pmid">10592278</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiemer</surname><given-names>L.</given-names></name><name><surname>Bendtsen</surname><given-names>J. D.</given-names></name><name><surname>Blom</surname><given-names>N.</given-names></name></person-group> (<year>2005</year>). <article-title>Netacet: prediction of n-terminal acetylation sites</article-title>. <source>Bioinformatics</source>
<volume>21</volume>, <fpage>1269</fpage>–<lpage>1270</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bti130</pub-id><?supplied-pmid 15539450?><pub-id pub-id-type="pmid">15539450</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D. P.</given-names></name><name><surname>Ba</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>). <article-title>Adam: A method for stochastic optimization</article-title>. <source>arXiv [Preprint] arxiv</source>:1412.6980.</mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kirac</surname><given-names>M.</given-names></name><name><surname>Ozsoyoglu</surname><given-names>G.</given-names></name></person-group> (<year>2008</year>). <article-title>“Protein function prediction based on patterns in biological networks,”</article-title> in <source>Annual International Conference on Research in Computational Molecular Biology</source> (<publisher-loc>Berlin</publisher-loc>: <publisher-name>Heidelberg: Springer</publisher-name>), <fpage>197</fpage>–<lpage>213</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-540-78839-3_18</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kulmanov</surname><given-names>M.</given-names></name><name><surname>Hoehndorf</surname><given-names>R.</given-names></name></person-group> (<year>2020</year>). <article-title>Deepgoplus: improved protein function prediction from sequence</article-title>. <source>Bioinformatics</source>
<volume>36</volume>, <fpage>422</fpage>–<lpage>429</lpage>. <pub-id pub-id-type="doi">10.1101/615260</pub-id><?supplied-pmid 31350877?><pub-id pub-id-type="pmid">31350877</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kulmanov</surname><given-names>M.</given-names></name><name><surname>Khan</surname><given-names>M. A.</given-names></name><name><surname>Hoehndorf</surname><given-names>R.</given-names></name></person-group> (<year>2018</year>). <article-title>Deepgo: predicting protein functions from sequence and interactions using a deep ontology-aware classifier</article-title>. <source>Bioinformatics</source>
<volume>34</volume>, <fpage>660</fpage>–<lpage>668</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btx624</pub-id><?supplied-pmid 29028931?><pub-id pub-id-type="pmid">29028931</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lai</surname><given-names>H.-Y.</given-names></name><name><surname>Zhang</surname><given-names>Z.-Y.</given-names></name><name><surname>Su</surname><given-names>Z.-D.</given-names></name><name><surname>Su</surname><given-names>W.</given-names></name><name><surname>Ding</surname><given-names>H.</given-names></name><name><surname>Chen</surname><given-names>W.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>iproep: a computational predictor for predicting promoter</article-title>. <source>Mol. Ther. Nucleic Acids</source><volume>17</volume>, <fpage>337</fpage>–<lpage>346</lpage>. <pub-id pub-id-type="doi">10.1016/j.omtn.2019.05.028</pub-id><?supplied-pmid 31299595?><pub-id pub-id-type="pmid">31299595</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lanchantin</surname><given-names>J.</given-names></name><name><surname>Singh</surname><given-names>R.</given-names></name><name><surname>Lin</surname><given-names>Z.</given-names></name><name><surname>Qi</surname><given-names>Y.</given-names></name></person-group> (<year>2016</year>). <article-title>Deep motif: Visualizing genomic sequence classifications</article-title>. <source>arXiv [Preprint] arxiv</source>:1605.01133.</mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y.</given-names></name><name><surname>Bottou</surname><given-names>L.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Haffner</surname><given-names>P.</given-names></name></person-group> (<year>1998</year>). <article-title>Gradient-based learning applied to document recognition</article-title>. <source>Proc. IEEE</source>
<volume>86</volume>, <fpage>2278</fpage>–<lpage>2324</lpage>. <pub-id pub-id-type="doi">10.1109/5.726791</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lv</surname><given-names>H.</given-names></name><name><surname>Zhang</surname><given-names>Z.-M.</given-names></name><name><surname>Li</surname><given-names>S.-H.</given-names></name><name><surname>Tan</surname><given-names>J.-X.</given-names></name><name><surname>Chen</surname><given-names>W.</given-names></name><name><surname>Lin</surname><given-names>H.</given-names></name></person-group> (<year>2019a</year>). <article-title>Evaluation of different computational methods on 5-methylcytosine sites identification</article-title>. <source>Brief. Bioinform</source>. <pub-id pub-id-type="doi">10.1093/bib/bbz048</pub-id><?supplied-pmid 31157855?><pub-id pub-id-type="pmid">31157855</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lv</surname><given-names>Z.</given-names></name><name><surname>Ao</surname><given-names>C.</given-names></name><name><surname>Zou</surname><given-names>Q.</given-names></name></person-group> (<year>2019b</year>). <article-title>Protein function prediction: from traditional classifier to deep learning</article-title>. <source>Proteomics</source>
<volume>19</volume>:<fpage>1900119</fpage>. <pub-id pub-id-type="doi">10.1002/pmic.201900119</pub-id><?supplied-pmid 31187588?><pub-id pub-id-type="pmid">31187588</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nair</surname><given-names>V.</given-names></name><name><surname>Hinton</surname><given-names>G. E.</given-names></name></person-group> (<year>2010</year>). <article-title>“Rectified linear units improve restricted boltzmann machines,”</article-title> in <source>Proceedings of the 27th International Conference on Machine Learning (ICML-10)</source> (<publisher-loc> Haifa</publisher-loc>), <fpage>807</fpage>–<lpage>814</lpage>.</mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>C. D.</given-names></name><name><surname>Gardiner</surname><given-names>K. J.</given-names></name><name><surname>Cios</surname><given-names>K. J.</given-names></name></person-group> (<year>2011</year>). <article-title>Protein annotation from protein interaction networks and gene ontology</article-title>. <source>J. Biomed. Inform</source>. <volume>44</volume>, <fpage>824</fpage>–<lpage>829</lpage>. <pub-id pub-id-type="doi">10.1016/j.jbi.2011.04.010</pub-id><?supplied-pmid 21571095?><pub-id pub-id-type="pmid">21571095</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oliver</surname><given-names>S.</given-names></name></person-group> (<year>2000</year>). <article-title>Guilt-by-association goes global</article-title>. <source>Nature</source>
<volume>403</volume>, <fpage>601</fpage>–<lpage>602</lpage>. <pub-id pub-id-type="doi">10.1038/35001165</pub-id><?supplied-pmid 10688178?><pub-id pub-id-type="pmid">10688178</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pierleoni</surname><given-names>A.</given-names></name><name><surname>Martelli</surname><given-names>P. L.</given-names></name><name><surname>Casadio</surname><given-names>R.</given-names></name></person-group> (<year>2008</year>). <article-title>PredGPI: a GPI-anchor predictor</article-title>. <source>BMC Bioinformatics</source>
<volume>9</volume>:<fpage>392</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2105-9-392</pub-id><?supplied-pmid 18811934?><pub-id pub-id-type="pmid">18811934</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piovesan</surname><given-names>D.</given-names></name><name><surname>Tosatto</surname><given-names>S. C.</given-names></name></person-group> (<year>2019</year>). <article-title>INGA 2.0: improving protein function prediction for the dark proteome</article-title>. <source>Nucleic Acids Res</source>. <volume>47</volume>, <fpage>W373</fpage>–<lpage>W378</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkz375</pub-id><?supplied-pmid 31073595?><pub-id pub-id-type="pmid">31073595</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radivojac</surname><given-names>P.</given-names></name><name><surname>Clark</surname><given-names>W. T.</given-names></name><name><surname>Oron</surname><given-names>T. R.</given-names></name><name><surname>Schnoes</surname><given-names>A. M.</given-names></name><name><surname>Wittkop</surname><given-names>T.</given-names></name><name><surname>Sokolov</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>2013</year>). <article-title>A large-scale evaluation of computational protein function prediction</article-title>. <source>Nat. Methods</source><volume>10</volume>, <fpage>221</fpage>–<lpage>227</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.2340</pub-id><?supplied-pmid 23353650?><pub-id pub-id-type="pmid">23353650</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richardson</surname><given-names>J. S.</given-names></name></person-group> (<year>1981</year>). <article-title>The anatomy and taxonomy of protein structure</article-title>. <source>Adv. Prot. Chem</source>. <volume>34</volume>, <fpage>167</fpage>–<lpage>339</lpage>. <pub-id pub-id-type="doi">10.1016/S0065-3233(08)60520-3</pub-id><?supplied-pmid 7020376?><pub-id pub-id-type="pmid">7020376</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rifaioglu</surname><given-names>A. S.</given-names></name><name><surname>Doğan</surname><given-names>T.</given-names></name><name><surname>Martin</surname><given-names>M. J.</given-names></name><name><surname>Cetin-Atalay</surname><given-names>R.</given-names></name><name><surname>Atalay</surname><given-names>V.</given-names></name></person-group> (<year>2019</year>). <article-title>Deepred: automated protein function prediction with multi-task feed-forward deep neural networks</article-title>. <source>Sci. Rep</source>. <volume>9</volume>, <fpage>1</fpage>–<lpage>16</lpage>. <pub-id pub-id-type="doi">10.1038/s41598-019-43708-3</pub-id><pub-id pub-id-type="pmid">30626917</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwikowski</surname><given-names>B.</given-names></name><name><surname>Uetz</surname><given-names>P.</given-names></name><name><surname>Fields</surname><given-names>S.</given-names></name></person-group> (<year>2000</year>). <article-title>A network of protein-protein interactions in yeast</article-title>. <source>Nat. Biotechnol</source>. <volume>18</volume>, <fpage>1257</fpage>–<lpage>1261</lpage>. <pub-id pub-id-type="doi">10.1038/82360</pub-id><?supplied-pmid 11101803?><pub-id pub-id-type="pmid">11101803</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sønderby</surname><given-names>S. K.</given-names></name><name><surname>Sønderby</surname><given-names>C. K.</given-names></name><name><surname>Nielsen</surname><given-names>H.</given-names></name><name><surname>Winther</surname><given-names>O.</given-names></name></person-group> (<year>2015</year>). <article-title>“Convolutional LSTM networks for subcellular localization of proteins,”</article-title> in <source>International Conference on Algorithms for Computational Biology</source> (<publisher-loc>Springer</publisher-loc>), <fpage>68</fpage>–<lpage>80</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-21233-3_6</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sønderby</surname><given-names>S. K.</given-names></name><name><surname>Winther</surname><given-names>O.</given-names></name></person-group> (<year>2014</year>). <article-title>Protein secondary structure prediction with long short term memory networks</article-title>. <source>arXiv [Preprint] arxiv</source>:1412.7828.</mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spencer</surname><given-names>M.</given-names></name><name><surname>Eickholt</surname><given-names>J.</given-names></name><name><surname>Cheng</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>). <article-title>A deep learning network approach to ab initio protein secondary structure prediction</article-title>. <source>IEEE/ACM Trans. Comput. Biol. Bioinform</source>. <volume>12</volume>, <fpage>103</fpage>–<lpage>112</lpage>. <pub-id pub-id-type="doi">10.1109/TCBB.2014.2343960</pub-id><?supplied-pmid 25750595?><pub-id pub-id-type="pmid">25750595</pub-id></mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szklarczyk</surname><given-names>D.</given-names></name><name><surname>Franceschini</surname><given-names>A.</given-names></name><name><surname>Wyder</surname><given-names>S.</given-names></name><name><surname>Forslund</surname><given-names>K.</given-names></name><name><surname>Heller</surname><given-names>D.</given-names></name><name><surname>Huerta-Cepas</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>String v10: protein-protein interaction networks, integrated over the tree of life</article-title>. <source>Nucleic Acids Res</source>. <volume>43</volume>, <fpage>D447</fpage>–<lpage>D452</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gku1003</pub-id><?supplied-pmid 25352553?><pub-id pub-id-type="pmid">25352553</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>J.-X.</given-names></name><name><surname>Li</surname><given-names>S.-H.</given-names></name><name><surname>Zhang</surname><given-names>Z.-M.</given-names></name><name><surname>Chen</surname><given-names>C.-X.</given-names></name><name><surname>Chen</surname><given-names>W.</given-names></name><name><surname>Tang</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Identification of hormone binding proteins based on machine learning methods</article-title>. <source>Math. Biosci. Eng</source>. <volume>16</volume>, <fpage>2466</fpage>–<lpage>2480</lpage>. <pub-id pub-id-type="doi">10.3934/mbe.2019123</pub-id><?supplied-pmid 31137222?><pub-id pub-id-type="pmid">31137222</pub-id></mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Cai</surname><given-names>Y.</given-names></name><name><surname>Deng</surname><given-names>L.</given-names></name></person-group> (<year>2019a</year>). <article-title>Deepmir2go: Inferring functions of human micrornas using a deep multi-label classification model</article-title>. <source>Int. J. Mol. Sci</source>. <volume>20</volume>:<fpage>6046</fpage>. <pub-id pub-id-type="doi">10.3390/ijms20236046</pub-id><?supplied-pmid 31801264?><pub-id pub-id-type="pmid">31801264</pub-id></mixed-citation>
    </ref>
    <ref id="B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Zhong</surname><given-names>X.</given-names></name><name><surname>Liu</surname><given-names>H.</given-names></name><name><surname>Lu</surname><given-names>C.</given-names></name><name><surname>Li</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2019b</year>). <article-title>Dmfold: A novel method to predict rna secondary structure with pseudoknots based on deep learning and improved base pair maximization principle</article-title>. <source>Front. Genet</source>. <volume>10</volume>:<fpage>143</fpage>. <pub-id pub-id-type="doi">10.3389/fgene.2019.00143</pub-id><?supplied-pmid 30886627?><pub-id pub-id-type="pmid">30886627</pub-id></mixed-citation>
    </ref>
    <ref id="B50">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Weaver</surname><given-names>R.</given-names></name></person-group> (<year>2011</year>). <source>Molecular Biology (WCB Cell</source> &amp; <italic>Molecular Biology)</italic>
<publisher-loc>New York, NY</publisher-loc>: <publisher-name>McGraw-hill Education</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>L.</given-names></name><name><surname>Ding</surname><given-names>Y.</given-names></name><name><surname>Su</surname><given-names>R.</given-names></name><name><surname>Tang</surname><given-names>J.</given-names></name><name><surname>Zou</surname><given-names>Q.</given-names></name></person-group> (<year>2018</year>). <article-title>Prediction of human protein subcellular localization using deep learning</article-title>. <source>J. Parallel Distrib. Comput</source>. <volume>117</volume>, <fpage>212</fpage>–<lpage>217</lpage>. <pub-id pub-id-type="doi">10.1016/j.jpdc.2017.08.009</pub-id></mixed-citation>
    </ref>
    <ref id="B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>J.</given-names></name><name><surname>Yan</surname><given-names>R.</given-names></name><name><surname>Roy</surname><given-names>A.</given-names></name><name><surname>Xu</surname><given-names>D.</given-names></name><name><surname>Poisson</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name></person-group> (<year>2015</year>). <article-title>The i-tasser suite: protein structure and function prediction</article-title>. <source>Nat. Methods</source>
<volume>12</volume>:<fpage>7</fpage>. <pub-id pub-id-type="doi">10.1038/nmeth.3213</pub-id><?supplied-pmid 25549265?><pub-id pub-id-type="pmid">25549265</pub-id></mixed-citation>
    </ref>
    <ref id="B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>You</surname><given-names>R.</given-names></name><name><surname>Yao</surname><given-names>S.</given-names></name><name><surname>Xiong</surname><given-names>Y.</given-names></name><name><surname>Huang</surname><given-names>X.</given-names></name><name><surname>Sun</surname><given-names>F.</given-names></name><name><surname>Mamitsuka</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Netgo: improving large-scale protein function prediction with massive network information</article-title>. <source>Nucleic Acids Res</source>. <volume>47</volume>, <fpage>W379</fpage>–<lpage>W387</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkz388</pub-id><?supplied-pmid 31106361?><pub-id pub-id-type="pmid">31106361</pub-id></mixed-citation>
    </ref>
    <ref id="B54">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>You</surname><given-names>R.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Xiong</surname><given-names>Y.</given-names></name><name><surname>Sun</surname><given-names>F.</given-names></name><name><surname>Mamitsuka</surname><given-names>H.</given-names></name><name><surname>Zhu</surname><given-names>S.</given-names></name></person-group> (<year>2018</year>). <article-title>Golabeler: improving sequence-based large-scale protein function prediction by learning to rank</article-title>. <source>Bioinformatics</source>
<volume>34</volume>, <fpage>2465</fpage>–<lpage>2473</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bty130</pub-id><?supplied-pmid 29522145?><pub-id pub-id-type="pmid">29522145</pub-id></mixed-citation>
    </ref>
    <ref id="B55">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>C.</given-names></name><name><surname>Zheng</surname><given-names>W.</given-names></name><name><surname>Freddolino</surname><given-names>P. L.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name></person-group> (<year>2018</year>). <article-title>Metago: Predicting gene ontology of non-homologous proteins through low-resolution protein structure prediction and protein-protein network mapping</article-title>. <source>J. Mol. Biol</source>. <volume>430</volume>, <fpage>2256</fpage>–<lpage>2265</lpage>. <pub-id pub-id-type="doi">10.1016/j.jmb.2018.03.004</pub-id><?supplied-pmid 29534977?><pub-id pub-id-type="pmid">29534977</pub-id></mixed-citation>
    </ref>
    <ref id="B56">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>M.-L.</given-names></name><name><surname>Zhou</surname><given-names>Z.-H.</given-names></name></person-group> (<year>2013</year>). <article-title>A review on multi-label learning algorithms</article-title>. <source>IEEE Trans. Knowl. Data Eng</source>. <volume>26</volume>, <fpage>1819</fpage>–<lpage>1837</lpage>. <pub-id pub-id-type="doi">10.1109/TKDE.2013.39</pub-id></mixed-citation>
    </ref>
    <ref id="B57">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>S.</given-names></name><name><surname>Zhou</surname><given-names>J.</given-names></name><name><surname>Hu</surname><given-names>H.</given-names></name><name><surname>Gong</surname><given-names>H.</given-names></name><name><surname>Chen</surname><given-names>L.</given-names></name><name><surname>Cheng</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>A deep learning framework for modeling structural features of RNA-binding protein targets</article-title>. <source>Nucleic Acids Res</source>. <volume>44</volume>:<fpage>e32</fpage>. <pub-id pub-id-type="doi">10.1093/nar/gkv1025</pub-id><?supplied-pmid 26467480?><pub-id pub-id-type="pmid">26467480</pub-id></mixed-citation>
    </ref>
    <ref id="B58">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>X.-J.</given-names></name><name><surname>Feng</surname><given-names>C.-Q.</given-names></name><name><surname>Lai</surname><given-names>H.-Y.</given-names></name><name><surname>Chen</surname><given-names>W.</given-names></name><name><surname>Hao</surname><given-names>L.</given-names></name></person-group> (<year>2019</year>). <article-title>Predicting protein structural classes for low-similarity sequences by evaluating different features</article-title>. <source>Knowl. Based Syst</source>. <volume>163</volume>, <fpage>787</fpage>–<lpage>793</lpage>. <pub-id pub-id-type="doi">10.1016/j.knosys.2018.10.007</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
