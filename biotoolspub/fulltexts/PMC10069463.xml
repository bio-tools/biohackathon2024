<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.0 20120330//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Genome Res</journal-id>
    <journal-id journal-id-type="iso-abbrev">Genome Res</journal-id>
    <journal-id journal-id-type="hwp">genome</journal-id>
    <journal-id journal-id-type="publisher-id">GENOME</journal-id>
    <journal-title-group>
      <journal-title>Genome Research</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1088-9051</issn>
    <issn pub-type="epub">1549-5469</issn>
    <publisher>
      <publisher-name>Cold Spring Harbor Laboratory Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10069463</article-id>
    <article-id pub-id-type="pmid">36849204</article-id>
    <article-id pub-id-type="medline">9509184</article-id>
    <article-id pub-id-type="doi">10.1101/gr.277068.122</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methods</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Complex hierarchical structures in single-cell genomics data unveiled by deep hyperbolic manifold learning</article-title>
      <alt-title alt-title-type="left-running">Tian et al.</alt-title>
      <alt-title alt-title-type="right-running">Hyperbolic embedding of single-cell genomics data</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Tian</surname>
          <given-names>Tian</given-names>
        </name>
        <xref rid="af1" ref-type="aff">1</xref>
        <xref rid="FN1" ref-type="author-notes">4</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Zhong</surname>
          <given-names>Cheng</given-names>
        </name>
        <xref rid="af2" ref-type="aff">2</xref>
        <xref rid="FN1" ref-type="author-notes">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lin</surname>
          <given-names>Xiang</given-names>
        </name>
        <xref rid="af2" ref-type="aff">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Wei</surname>
          <given-names>Zhi</given-names>
        </name>
        <xref rid="af2" ref-type="aff">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hakonarson</surname>
          <given-names>Hakon</given-names>
        </name>
        <xref rid="af1" ref-type="aff">1</xref>
        <xref rid="af3" ref-type="aff">3</xref>
      </contrib>
    </contrib-group>
    <aff id="af1"><label>1</label>Center for Applied Genomics, Children's Hospital of Philadelphia, Philadelphia, Pennsylvania 19104, USA;</aff>
    <aff id="af2"><label>2</label>Department of Computer Science, Ying Wu College of Computing, New Jersey Institute of Technology, Newark, New Jersey 07102, USA;</aff>
    <aff id="af3"><label>3</label>Division of Human Genetics, Department of Pediatrics, The Perelman School of Medicine, University of Pennsylvania, Philadelphia, Pennsylvania 19104, USA</aff>
    <author-notes>
      <fn id="FN1" fn-type="equal">
        <label>4</label>
        <p>These authors contributed equally to this work.</p>
      </fn>
      <corresp>Corresponding author: <email>zhiwei@njit.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <volume>33</volume>
    <issue>2</issue>
    <fpage>232</fpage>
    <lpage>246</lpage>
    <history>
      <date date-type="received">
        <day>28</day>
        <month>6</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>24</day>
        <month>1</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>
        <ext-link xlink:href="http://genome.cshlp.org/site/misc/terms.xhtml" ext-link-type="uri">© 2023 Tian et al.; Published by Cold Spring Harbor Laboratory Press</ext-link>
      </copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This article is distributed exclusively by Cold Spring Harbor Laboratory Press for the first six months after the full-issue publication date (see <ext-link ext-link-type="uri" xlink:href="https://genome.cshlp.org/site/misc/terms.xhtml">https://genome.cshlp.org/site/misc/terms.xhtml</ext-link>). After six months, it is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), as described at <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="232.pdf"/>
    <abstract>
      <p>With the advances in single-cell sequencing techniques, numerous analytical methods have been developed for delineating cell development. However, most are based on Euclidean space, which would distort the complex hierarchical structure of cell differentiation. Recently, methods acting on hyperbolic space have been proposed to visualize hierarchical structures in single-cell RNA-seq (scRNA-seq) data and have been proven to be superior to methods acting on Euclidean space. However, these methods have fundamental limitations and are not optimized for the highly sparse single-cell count data. To address these limitations, we propose scDHMap, a model-based deep learning approach to visualize the complex hierarchical structures of scRNA-seq data in low-dimensional hyperbolic space. The evaluations on extensive simulation and real experiments show that scDHMap outperforms existing dimensionality-reduction methods in various common analytical tasks as needed for scRNA-seq data, including revealing trajectory branches, batch correction, and denoising the count matrix with high dropout rates. In addition, we extend scDHMap to visualize single-cell ATAC-seq data.</p>
    </abstract>
    <funding-group>
      <award-group id="funding-1">
        <funding-source>New Jersey Institute of Technology</funding-source>
      </award-group>
      <award-group id="funding-2">
        <funding-source>Tianjin University of Finance and Economics</funding-source>
      </award-group>
      <award-group id="funding-3">
        <funding-source>Extreme Science and Engineering Discovery Environment (XSEDE)</funding-source>
        <award-id>CIE170034</award-id>
      </award-group>
      <award-group id="funding-4">
        <funding-source>
          <institution-wrap>
            <institution>National Science Foundation </institution>
            <institution-id institution-id-type="doi">10.13039/100000001</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>ACI-1548562</award-id>
        <award-id>R15HG012087</award-id>
      </award-group>
      <award-group id="funding-5">
        <funding-source>
          <institution-wrap>
            <institution>National Institutes of Health (NIH) </institution>
            <institution-id institution-id-type="doi">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group id="funding-6">
        <funding-source>National Center for Advancing Translational Sciences (NCATS)</funding-source>
      </award-group>
      <award-group id="funding-7">
        <funding-source>
          <institution-wrap>
            <institution>NIH </institution>
            <institution-id institution-id-type="doi">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>UL1TR003017</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="15"/>
    </counts>
  </article-meta>
</front>
<body>
  <p>Single-cell RNA-seq (scRNA-seq) has provided plenty of new opportunities for exploring cell development and differentiation (<xref rid="GR277068TIAC56" ref-type="bibr">Tanay and Regev 2017</xref>). Computation methods to accurately reveal and display the cell development process from large single-cell data have grown tremendously in recent years (<xref rid="GR277068TIAC50" ref-type="bibr">Saelens et al. 2019</xref>). The progression of cells in continuous trajectories is like a hierarchical tree, with multiple branches typically, such as in Waddington's classic epigenetic landscape (<xref rid="GR277068TIAC12" ref-type="bibr">Goldberg et al. 2007</xref>). Methods for analyzing these complex structures in the single-cell data have been published, including visualization (<xref rid="GR277068TIAC16" ref-type="bibr">Haghverdi et al. 2015</xref>; <xref rid="GR277068TIAC9" ref-type="bibr">Ding et al. 2018</xref>; <xref rid="GR277068TIAC35" ref-type="bibr">Lopez et al. 2018</xref>; <xref rid="GR277068TIAC2" ref-type="bibr">Amodio et al. 2019</xref>; <xref rid="GR277068TIAC38" ref-type="bibr">Moon et al. 2019</xref>; <xref rid="GR277068TIAC65" ref-type="bibr">Wolf et al. 2019</xref>), clustering (<xref rid="GR277068TIAC33" ref-type="bibr">Levine et al. 2015</xref>; <xref rid="GR277068TIAC62" ref-type="bibr">Wang et al. 2017</xref>; <xref rid="GR277068TIAC57" ref-type="bibr">Tian et al. 2019</xref>), and pseudotime inference (<xref rid="GR277068TIAC17" ref-type="bibr">Haghverdi et al. 2016</xref>; <xref rid="GR277068TIAC47" ref-type="bibr">Qiu et al. 2017</xref>). Visualizing large-scale single-cell data in low dimensions will effectively reveal high-level structural information, which often provides interesting insights for downstream analyses. Despite the compelling potential of scRNA-seq, we note that scRNA-seq data are highly noisy, full of zeros (the dropout phenomenon), and highly dimensional, which makes dimensionality reduction a daunting task. An ideal dimensionality-reduction method is desired to address all these challenges to effectively reveal biological structural patterns in the data.</p>
  <p>Numerous embedding methods have been proposed to reduce the high-dimensional scRNA-seq data for downstream analysis. Most of these methods act on Euclidean space, including t-SNE (<xref rid="GR277068TIAC61" ref-type="bibr">van der Maaten and Hinton 2008</xref>), UMAP (<xref rid="GR277068TIAC37" ref-type="bibr">McInnes et al. 2018</xref>), PaCMap (<xref rid="GR277068TIAC63" ref-type="bibr">Wang et al. 2021</xref>), diffusion map (<xref rid="GR277068TIAC16" ref-type="bibr">Haghverdi et al. 2015</xref>), PAGA (<xref rid="GR277068TIAC65" ref-type="bibr">Wolf et al. 2019</xref>), PHATE (<xref rid="GR277068TIAC38" ref-type="bibr">Moon et al. 2019</xref>), Monocle (<xref rid="GR277068TIAC47" ref-type="bibr">Qiu et al. 2017</xref>), scVI (<xref rid="GR277068TIAC35" ref-type="bibr">Lopez et al. 2018</xref>), SAUCIE (<xref rid="GR277068TIAC2" ref-type="bibr">Amodio et al. 2019</xref>), and scvis (<xref rid="GR277068TIAC9" ref-type="bibr">Ding et al. 2018</xref>), among others. Although these methods of Euclidean space have different features (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Table 1</ext-link>; <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Note 1</ext-link>), they share a common limitation; namely, they would distort the high-dimensional pairwise distance and would result in suboptimal downstream analysis, such as clustering and trajectory inference.</p>
  <p>Compared with Euclidean space, hyperbolic space can help to reduce distorting the high-dimensional pairwise distance. Hyperbolic space is a non-Euclidean space with a constant negative curvature. It can be considered as a continuous version of hierarchical trees and has the advantage of low distortion in low-dimensional embedding, even in two-dimensional space (<xref rid="GR277068TIAC15" ref-type="bibr">Gromov 2007</xref>). Hyperbolic embedding has been successfully applied to various data types for representing complex hierarchical structures, including word embedding (<xref rid="GR277068TIAC41" ref-type="bibr">Nickel and Kiela 2017</xref>), image embedding (<xref rid="GR277068TIAC36" ref-type="bibr">Mathieu et al. 2019</xref>; <xref rid="GR277068TIAC43" ref-type="bibr">Ovinnikov 2019</xref>), and graph embedding (<xref rid="GR277068TIAC5" ref-type="bibr">Chami et al. 2019</xref>). Recently, two main methods, PoincaréMap (<xref rid="GR277068TIAC27" ref-type="bibr">Klimovskaia et al. 2020</xref>) and scPhere (<xref rid="GR277068TIAC8" ref-type="bibr">Ding and Regev 2021</xref>), acting on hyperbolic space, have been proposed for visualizing single-cell trajectory. In the cell-differentiation process, the number of cells can grow exponentially. The volume of balls also grows exponentially in the hyperbolic space with respect to the radius, which is a helpful property for the visualization of single-cell lineage. However, in the Euclidean space, the volume of balls only increases polynomially, which results in insufficient space for the exponentially growing number of cells and would distort the high-dimensional distances in the embedding. PoincaréMap is a hyperbolic version of t-SNE, which reduces scRNA-seq data to a 2D Poincaré ball. The scPhere is a deep variational autoencoder (<xref rid="GR277068TIAC25" ref-type="bibr">Kingma and Welling 2014</xref>) that represents scRNA-seq data in a low-dimensional hyperbolic embedding. To characterize scRNA-seq count data, scPhere uses negative binomial (NB) reconstruction loss and integrates data from different sources via a conditional autoencoder (<xref rid="GR277068TIAC53" ref-type="bibr">Sohn et al. 2015</xref>). Both of these two hyperbolic embedding methods have been proven to outperform Euclidean embedding methods empirically.</p>
  <p>Despite the superiority of these existing hyperbolic embedding methods, they are not optimized for computational challenges of single-cell data. Dropout events cause high proportions of zeros in scRNA-seq data, making the structural pattern vague; integrating data of different sources to a unified embedding with the batch effect eliminated is a common task in the single-cell analysis, but PoincaréMap fails to tackle these problems. Furthermore, PoincaréMap relies on a graph Laplacian of the pairwise distance matrix and a symmetric Kullback–Leibler (KL) divergence for local and global structure proximities. Calculating the Laplacian matrix is very time- and memory-consuming (<xref rid="GR277068TIAC23" ref-type="bibr">Jianbo and Malik 2000</xref>), and the symmetric KL divergence requires the memory to store the whole similarity matrix, which makes PoincaréMap infeasible for large data sets. As a workaround, down-sampling has to be used to run PoincaréMap analysis on large data sets (<xref rid="GR277068TIAC27" ref-type="bibr">Klimovskaia et al. 2020</xref>). For scPhere, which is an autoencoder-based model, it does not guarantee similarity preservation during the dimensionality reduction, making it not applicable for trajectory inference at single-cell resolution.</p>
  <p>To address these issues, we propose a model-based deep hyperbolic manifold learning approach: single-cell deep hierarchical map (scDHMap) (<xref rid="GR277068TIAF1" ref-type="fig">Fig. 1</xref>). To characterize the overdispersed and zero-inflated count matrix of the scRNA-seq data, we apply a zero-inflated negative binomial (ZINB) model-based loss function. The ZINB model has been successfully applied to various single-cell analyses, including imputation, dimensionality reduction, and clustering (<xref rid="GR277068TIAC35" ref-type="bibr">Lopez et al. 2018</xref>; <xref rid="GR277068TIAC49" ref-type="bibr">Risso et al. 2018</xref>; <xref rid="GR277068TIAC11" ref-type="bibr">Eraslan et al. 2019</xref>; <xref rid="GR277068TIAC57" ref-type="bibr">Tian et al. 2019</xref>). The scDHMap model can be used for versatile types of single-cell analysis. To represent the continuous hierarchical structures, we use the ZINB model–based variational autoencoder to map the high-dimensional-count data into a 2D hyperbolic space. Like in scvis, the structure of high-dimensional data is preserved by t-SNE regularization in our model but using the hyperbolic distance metric. The architecture of scDHMap can be considered as a model-based parametric t-SNE (<xref rid="GR277068TIAC60" ref-type="bibr">van der Maaten 2009</xref>) in the hyperbolic space, which combines the strength of local structure preservation from t-SNE and global structure preservation from autoencoder (<xref rid="GR277068TIAC9" ref-type="bibr">Ding et al. 2018</xref>; <xref rid="GR277068TIAC14" ref-type="bibr">Graving and Couzin 2020</xref>), thus making it a strong candidate for representing complex hierarchical structures. We regularize the latent embedding by following a standard wrapped normal distribution via the variational inference (<xref rid="GR277068TIAC25" ref-type="bibr">Kingma and Welling 2014</xref>), which makes the embedding normally distributed for better visualization. The deep generative model has recently emerged as a powerful method for representation learning of single-cell genomics data (<xref rid="GR277068TIAC9" ref-type="bibr">Ding et al. 2018</xref>; <xref rid="GR277068TIAC35" ref-type="bibr">Lopez et al. 2018</xref>; <xref rid="GR277068TIAC8" ref-type="bibr">Ding and Regev 2021</xref>; <xref rid="GR277068TIAC34" ref-type="bibr">Liu et al. 2021</xref>). To integrate data sets from different batches to a joint embedding, we combine the strength of Harmony (<xref rid="GR277068TIAC29" ref-type="bibr">Korsunsky et al. 2019</xref>) with a conditional autoencoder (<xref rid="GR277068TIAC53" ref-type="bibr">Sohn et al. 2015</xref>) responding to batch IDs. Following the previous studies (<xref rid="GR277068TIAC35" ref-type="bibr">Lopez et al. 2018</xref>; <xref rid="GR277068TIAC11" ref-type="bibr">Eraslan et al. 2019</xref>), estimated mean parameters in the ZINB model can be used as the denoised counts. Our model can be optimized per mini-batch on the graphic processing unit (GPU), which can be easily scaled to large data sets. Using both simulated and real data sets, we illustrate that scDHMap outperforms competing methods in various embedding tasks in terms of embedding quality metrics and visualizing developmental trajectories. Finally, we extend scDHMap to visualize the differential trajectories of single-cell ATAC-seq (scATAC-seq) data.</p>
  <fig position="float" id="GR277068TIAF1">
    <label>Figure 1.</label>
    <caption>
      <p>Network architecture of scDHMap. The encoder and decoder are fully connected neural networks. The latent embedding is in a 2D hyperbolic space for visualization and with the t-SNE regularization for structural preservations. KL divergence loss minimizes the divergence between the posterior and prior distributions of the latent embedding. Batch IDs can be incorporated to align different batches. Zero-inflated negative binomial (ZINB) reconstruction loss characterizes single-cell count data.</p>
    </caption>
    <graphic xlink:href="232f01" position="float"/>
  </fig>
  <sec sec-type="results" id="s1">
    <title>Results</title>
    <sec id="s1a">
      <title>Simulation evaluation of dimensionality reduction</title>
      <p>Dropout events are pervasive in the scRNA-seq data and cause the main computational challenge in the single-cell analysis. To evaluate the dimensionality-reduction performance, we generated simulated data sets with various dropout rates. Each setting is repeated 10 times with different random seeds. Two embedding quality metrics are used to quantify the embedding performance: Q local and Q global, which reflect the local and global structural preservations, respectively (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Note 2</ext-link>). Larger Q scores mean better preservations of local and global high-dimensional distances. In the simulated data sets, we know the true counts, which are counts without the interference of dropout events. We built the ground-truth high-dimensional similarities by the 50 principal components (PCs) of analytic Pearson residuals (<xref rid="GR277068TIAC31" ref-type="bibr">Lause et al. 2021</xref>) normalized true counts. All embedding methods received the 50 PCs of normalized raw counts as the input (except scDHMap and scPhere; scDHMap used both normalized raw counts and 50 PCs, and scPhere used raw counts), which are counts after adding dropout events. We simulated data sets having a hierarchical tree structure with various branches. We compared the embedding performance of scDHMap with various methods, including PoincaréMap (<xref rid="GR277068TIAC27" ref-type="bibr">Klimovskaia et al. 2020</xref>), scPhere (<xref rid="GR277068TIAC8" ref-type="bibr">Ding and Regev 2021</xref>), scvis (<xref rid="GR277068TIAC9" ref-type="bibr">Ding et al. 2018</xref>), principal component analysis (PCA), PaCMap (<xref rid="GR277068TIAC63" ref-type="bibr">Wang et al. 2021</xref>), t-SNE (<xref rid="GR277068TIAC61" ref-type="bibr">van der Maaten and Hinton 2008</xref>), UMAP (<xref rid="GR277068TIAC37" ref-type="bibr">McInnes et al. 2018</xref>), and PHATE (<xref rid="GR277068TIAC38" ref-type="bibr">Moon et al. 2019</xref>). All methods reduce the high-dimensional count data to 2D representations. Simulation results are summarized in <xref rid="GR277068TIAF2" ref-type="fig">Figure 2</xref>, A and B. As we observed, scDHMap outperformed all competing methods, especially in terms of Q global. Although PoincaréMap and scvis had comparable Q local scores with scDHMap, they performed worse than scDHMap in terms of Q global. Hyperbolic embedding methods, including scDHMap and PoincaréMap, outperformed Euclidean embedding methods such as PaCMap, t-SNE, UMAP, and PHATE in all settings. ScPhere is a hyperbolic variational autoencoder but is not designed for similarity preservation, and it performed poorly on all simulated data sets. We also visualized embeddings of all the methods in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Figures S1 and S2</ext-link>. We noted that hyperbolic methods, including scDHMap and PoincaréMap, can accurately reveal the continuous trajectory paths. Methods of Euclidean space, including scvis, PaCMap, and UMAP, had some breaking points in trajectory paths, making a continuous path into multiple parts. With the increase of dropout rates, we observed that embeddings of competing methods become noisier, such as scPhere, scvis, and PHATE, whereas scDHMap's performance was unaffected. These results illustrated that scDHMap could reveal complex hierarchical structures better than the competing methods, even with high dropout rates. To make a further comparison, we conducted an experiment reducing the simulated data into 3D representations. We found that scDHMap showed similar superiority in the 3D embeddings, with the best Q scores (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S3A,B</ext-link>).</p>
      <fig position="float" id="GR277068TIAF2">
        <label>Figure 2.</label>
        <caption>
          <p>Embedding quality metrics of different methods on simulated data sets with various dropout rates. Q values measure the local (<italic>A</italic>) and global (<italic>B</italic>) structure preservations. Larger values mean better preservations. Each setting generated 10 data sets.</p>
        </caption>
        <graphic xlink:href="232f02" position="float"/>
      </fig>
      <p>Next, we conducted an ablation study with two variant models: one with NB reconstruction loss and another one discarding the ZINB model–based decoder of scDHMap. The evaluation of the embedding qualities is summarized in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Figure S4, A and B</ext-link>. We observed that scDHMap performed better than the model with NB loss and the model without decoder in terms of Q local (paired one-sided <italic>t</italic>-test <italic>P</italic>-value &lt; 0.01 for the dropout rates is 50.4%, 68.1%, 75.6%, 81.7%, and 86.6%) (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S4C</ext-link>). The improvements became more significant with the increase of dropout rates, which reflected the contribution of the ZINB model–based decoder. We also tested the performance of scDHMap with different network architectures. We found that scDHMap is quite robust against different numbers of hidden layers in the encoder and decoder (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S5A,B</ext-link>). The contribution of pretraining scDHMap (the ZINB model–based autoencoder without the t-SNE loss) was shown in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Figure S6, A and B</ext-link>, and we found that pretraining can slightly improve the embedding quality, especially in terms of Q local (paired one-sided <italic>t</italic>-test <italic>P</italic>-value &lt; 0.05 for dropout rates is 50.4%, 59.6%, and 68.1%). Perplexity is an important parameter in the t-SNE algorithm, which controls the number of neighbors that the model focuses on during dimensionality reduction. We reported the performance of scDHMap with different perplexity values in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Figure S7, A and B</ext-link>. Notably, scDHMap was robust against a large range of perplexity values from 10–50, and perplexity value 30 was the best choice based on the performance of both local and global structural preservations. So, we suggest perplexity = 30 as the default setting for scDHMap. Because scDHMap calculates the t-SNE regularization per mini-batch, if the perplexity is 30, then the effective perplexity of the whole data will be 30/512 · <italic>n</italic> ≈ 5.8% · <italic>n</italic>, where <italic>n</italic> is the number of total cells and 512 is the mini-batch size. This setting is similar to the suggestion of the previous study using t-SNE to scRNA-seq data (<xref rid="GR277068TIAC28" ref-type="bibr">Kobak and Berens 2019</xref>).</p>
      <p>With the accumulation of scRNA-seq data, it is a common request to include new samples into the existing latent representations. Traditional nonparametric methods such as PoincaréMap, t-SNE, and UMAP are not possible for this out-of-sample support, but scDHMap can use the learned neural network to include new samples. To evaluate the out-of-sample performance, we randomly split the simulated data sets into training and testing sets with the proportions of 90% and 10%. We trained scDHMap on training sets and then mapped testing sets to the embeddings. Q scores were calculated for both training and testing sets, respectively. As shown in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Figure S8, A and B</ext-link>, scDHMap could preserve global structures in testing sets well but was not so good at preserving local structures. We further plotted the embeddings of training and testing sets in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Figure S9</ext-link> and confirmed that testing sets were mapped to the expected positions. These results show that scDHMap can be used for embedding new samples after model training.</p>
      <p>We summarized the running time of scDHMap on simulated data sets of various cell numbers in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Figure S10</ext-link>. Each data set had been repeated three times. Although there are different numbers of iterations before the early stop, we noted that the running time scaled roughly linearly with the number of cells. This result is consistent with the computational complexity analysis in the scvis paper (<xref rid="GR277068TIAC9" ref-type="bibr">Ding et al. 2018</xref>). For a mini-batch with a given size, the computational complexity of scDHMap is constant, which is quadratic to the mini-batch size. Large data sets need more iterations to converge, and the number of iterations scales linearly with the cell number when the mini-batch size is given (with some variances owing to different numbers of iterations before the early stop). As a result, the total running time of scDHMap scales linearly with the number of cells in the data set. This property makes it useful for analyzing large scRNA-seq data sets.</p>
    </sec>
    <sec id="s1b">
      <title>scDHMap for batch correction</title>
      <p>Integrating data sets from different batches is a common task in single-cell analysis. Integrating means eliminating the technical batch effects in separate experiments and revealing real biological signals. Methods for correcting batch effects have been proposed, such as Harmony (<xref rid="GR277068TIAC29" ref-type="bibr">Korsunsky et al. 2019</xref>), Seurat (<xref rid="GR277068TIAC3" ref-type="bibr">Butler et al. 2018</xref>), and MNN (<xref rid="GR277068TIAC18" ref-type="bibr">Haghverdi et al. 2018</xref>). Most embedding methods do not consider batch effects, but these methods can be used as upstream tools before the dimensionality reduction. In the scDHMap, we propose an integrated pipeline to correct batch effects. Conditional autoencoder is applied to improve embedding quality for the autoencoder part, and for the t-SNE regularization, Harmony (a method that iteratively removes batch effects in PCs) is used for correcting batches. To evaluate the performance, we simulated data sets with six different batches, and batches had varied sizes from 5% to 25% of the total number of cells. This simulation represents a setting of complex batch effects. We tested methods including scDHMap, PCA + PoincaréMap (50 PCs for PoincaréMap), Harmony + PoincaréMap (Harmony-corrected 50 PCs for PoincaréMap), scPhere, and a batch-aware version of t-SNE–BC-t-SNE (<xref rid="GR277068TIAC1" ref-type="bibr">Aliverti et al. 2020</xref>) on these simulated data sets.</p>
      <p>The Harmony-corrected 50 PCs of analytic Pearson residual normalized true counts were used for ground-truth high-dimensional similarities. We display the embedding results in <xref rid="GR277068TIAF3" ref-type="fig">Figure 3</xref>. We found that scDHMap had the best Q scores among the different methods (<xref rid="GR277068TIAF3" ref-type="fig">Fig. 3</xref>A,B), which meant that scDHMap performed best in local and global structural preservations. Next, we quantified the alignment between different batches by the silhouette coefficient (SIL) (<xref rid="GR277068TIAF3" ref-type="fig">Fig. 3</xref>C). Although BC-t-SNE had the best SIL, it failed to display the continuous hierarchical trajectories (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S11</ext-link>). We observed that scDHMap had a better SIL than Harmony + PoincaréMap's (paired one-sided <italic>t</italic>-test <italic>P</italic>-value = 0.01), indicating that the cooperation of Harmony and the conditional autoencoder could improve the batch alignment and the embedding quality simultaneously. It is not surprising that scPhere had a fair result in batch correction but a poor performance in the Q values, because there is no guarantee of distance preservation. Plotting the 2D embedding confirmed that scDHMap could effectively eliminate batch effects (<xref rid="GR277068TIAF3" ref-type="fig">Fig. 3</xref>D), and if using PCA + PoincaréMap, the batch effect made the embedding meaningless (<xref rid="GR277068TIAF3" ref-type="fig">Fig. 3</xref>E). We conducted an ablation study of the two components (Harmony and conditional autoencoder) in scDHMap to integrate batches. We observed that although the batch effect was corrected primarily by Harmony, the conditional autoencoder could improve the quality of embedding further (Q locals of scDHMap vs. scDHMap without conditional autoencoder, paired one-sided <italic>t</italic>-test <italic>P</italic>-value = 0.01) (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S12A–G</ext-link>).</p>
      <fig position="float" id="GR277068TIAF3">
        <label>Figure 3.</label>
        <caption>
          <p>Evaluation of embeddings for batch alignment. (<italic>A</italic>,<italic>B</italic>) Embedding quality metrics of methods on simulated data sets with six batches. Ten data sets had been generated. (<italic>C</italic>) Silhouette coefficient (SIL) for quantifying the batch alignments. Larger values mean better alignments. (<italic>D</italic>) Embedding of scDHMap. (<italic>E</italic>) Embedding of PCA + PoincaréMap. Colors represent branches, and dot shapes represent batches (<italic>D</italic>,<italic>E</italic>).</p>
        </caption>
        <graphic xlink:href="232f03" position="float"/>
      </fig>
    </sec>
    <sec id="s1c">
      <title>scDHMap for trajectory interpretation and denoising counts</title>
      <p>The cell ordering among the trajectory path or trajectory pseudotime inference is the essential analysis in trajectory inference. For this experiment, we generate simulated data sets having three branches with high dropout rates (∼75%). We performed embedding of scDHMap and PoincaréMap on these simulated data sets and quantified the quality by Q scores (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S13A,B</ext-link>). We observed that scDHMap outperformed PoincaréMap in both Q local scores and Q global scores. The embedding of scDHMap had placed points following the true trajectory order correctly, but the embedding of PoincaréMap was chaotic (<xref rid="GR277068TIAF4" ref-type="fig">Fig. 4</xref>A; <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S13C</ext-link>). The Poincaré norm is the geodesic distance between a point and the origin and can be used as the trajectory pseudotime to order cells in the trajectory path (<xref rid="GR277068TIAC27" ref-type="bibr">Klimovskaia et al. 2020</xref>). In the simulated data sets, for each branch, we transformed the origin to the starting points (in the simulation, one branch could have multiple points with starting state; we used the hyperbolic centroid of these points as the starting point) and summarized the trajectory pseudotime of points in the three branches. As plotted in <xref rid="GR277068TIAF4" ref-type="fig">Figure 4</xref>B, the Spearman's correlations between the pseudotime inferred by scDHMap and ground-truth pseudotime were significantly better than PoincaréMap's correlations. These results concluded that scDHMap could order cells better, matching the true trajectory order, than PoincaréMap even in a challenging situation with high dropout rates.</p>
      <fig position="float" id="GR277068TIAF4">
        <label>Figure 4.</label>
        <caption>
          <p>scDHMap can be used for trajectory interpretation and denoising counts. (<italic>A</italic>) Embedding of scDHMap on the simulated data set with three branches (M1–M3, M3–M2, and M3–M4). Dot shapes represent branches; red, blue, and green colors from shallow to deep represent ground-truth pseudotime. Ten data sets had been generated, and one example is displayed. (<italic>B</italic>) Spearman's correlation coefficient between the Poincaré pseudotime inferred by scDHMap, PoincaréMap, and the ground-truth pseudotime in the three branches. (<italic>C</italic>) Imputation errors of scDHMap pretraining, scDHMap final training, and deep count autoencoder (DCA) on the simulated data sets. (<italic>D</italic>) Area under the curve (AUC) plots of trajectory differential expression (DE) of raw counts and scDHMap-denoised counts (final training). (<italic>E</italic>) One DE gene in the branch M3–M2; plots display raw counts and scDHMap-denoised counts against the ground-truth pseudotime. Trend lines are smoothed by the LOESS regression.</p>
        </caption>
        <graphic xlink:href="232f04" position="float"/>
      </fig>
      <p>The ZINB model–based decoder of scDHMap can be used for denoising the count matrix of scRNA-seq data. We calculated the imputation errors of scDHMap pretrained, scDHMap trained with the t-SNE part, and a ZINB autoencoder model, DCA (<xref rid="GR277068TIAC11" ref-type="bibr">Eraslan et al. 2019</xref>), that measures the relative difference between denoised and true counts at the zero measures of the count matrix. We found that scDHMap-denoised counts were significantly better than DCA's, and adding t-SNE regularization could improve the imputation accuracy of scDHMap significantly (scDHMap vs. scDHMap pretrain, paired <italic>t</italic>-test <italic>P</italic>-value = 3.3 × 10<sup>−5</sup>) (<xref rid="GR277068TIAF4" ref-type="fig">Fig. 4</xref>C). This observation is expected because autoencoder imputes dropout counts by borrowing information from neighbors, and the t-SNE regularization could pull neighbors together, thus improving the imputation accuracy effectively. Next, we conducted a trajectory differential expression (DE) test by tradeSeq, which identifies the DE gene among the trajectory path by the NB spline regression (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Note 3</ext-link>; <xref rid="GR277068TIAC59" ref-type="bibr">Van den Berge et al. 2020</xref>). Ground-truth pseudotime was used for tradeSeq analysis. As displayed in <xref rid="GR277068TIAF4" ref-type="fig">Figure 4</xref>D and <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Figure S14, A–C</ext-link>, after denoising, the areas under the curve (AUCs) of DE analysis were significantly improved in the branches M3–M2 and M3–M4 (paired <italic>t</italic>-test <italic>P</italic>-values &lt; 0.05). We picked one gene as an example and showed that dropout made the trend among the trajectory path very obscure, but after scDHMap denoising, the trend became much clearer (<xref rid="GR277068TIAF4" ref-type="fig">Fig. 4</xref>E).</p>
    </sec>
    <sec id="s1d">
      <title>Application to real scRNA-seq data</title>
      <p>We applied scDHMap to three scRNA-seq data to illustrate the performance of different embedding tasks. Real data sets do not have so-called “true counts,” so we used 50 PCs of analytic Pearson residual normalized counts for evaluating structural preservations. We first evaluated the embedding qualities of Q scores, as shown in <xref rid="GR277068TIAF5" ref-type="fig">Figure 5</xref>A, scDHMap was the best in the three data sets compared with the hyperbolic embedding methods and Euclidean embedding methods. In most cases, the hyperbolic embedding methods, including scDHMap and PoincaréMap, outperformed the Euclidean methods, including PaCMap, t-SNE, UMAP, and PHATE, which indicated the low distortion of representing hierarchical structures in the hyperbolic space.</p>
      <fig position="float" id="GR277068TIAF5">
        <label>Figure 5.</label>
        <caption>
          <p>Embeddings of scDHMap on three real scRNA-seq data sets. (<italic>A</italic>) Embedding quality metrics of methods on real scRNA-seq data sets. (<italic>B</italic>) scDHMap embedding of the Paul cells. Colors represent cell types. (<italic>C</italic>) scDHMap embedding of the colon epithelial cells. Colors represent cell types. Arrowed lines indicate the suggested trajectory paths (<italic>B</italic>,<italic>C</italic>). (<italic>D</italic>,<italic>E</italic>) scDHMap embedding of the <italic>C. elegans</italic> embryonic cells. Colors represent cell types (<italic>D</italic>), and colors represent embryonic time bins (<italic>E</italic>).</p>
        </caption>
        <graphic xlink:href="232f05" position="float"/>
      </fig>
      <p>In the Paul data (<xref rid="GR277068TIAC46" ref-type="bibr">Paul et al. 2015</xref>), there are about 2000 cells profiled from murine bone marrow. The investigators identified 19 clusters in the data. We projected the Paul data to 2D Poincaré space by scDHMap in <xref rid="GR277068TIAF5" ref-type="fig">Figure 5</xref>B. As we can see, the data contained two main branches, and the cell types were posed in the expected orders. This result is consistent with the previous trajectory analysis (<xref rid="GR277068TIAC17" ref-type="bibr">Haghverdi et al. 2016</xref>). Visualizations of other methods had similar two main branches (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Figs. S15, S16A,B</ext-link>). The data have a predefined root cell. In the embedding of scDHMap, we transformed the origin of the Poincaré ball to the root and ordered cells by their geodesic distance to the new origin as the Poincaré pseudotime (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S17A–C</ext-link>). Using a similar branching method to that of <xref rid="GR277068TIAC17" ref-type="bibr">Haghverdi et al. (2016)</xref>, we divided the data set into three branches: two long branches and one short trunk based on the Poincaré pseudotime (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S17D</ext-link>; <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Note 4</ext-link>). In branch 2, most cells were basophils, monocytes, and neutrophils. We selected marker genes in these cell types and plotted the raw and scDHMap-denoised counts along the Poincaré pseudotime in branch 2 (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S17E</ext-link>). In branch 3, most cells were erythroids, and we also plotted the marker genes (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S17F</ext-link>). As the plots show, dropouts were pervasive in the raw counts; however, after scDHMap denoising, the changing trend through the pseudotime became much clearer, including the marker genes <italic>Cebpe</italic>, <italic>Csf1r</italic>, <italic>Gfi1</italic>, <italic>Irf8</italic>, <italic>Epor</italic>, and <italic>Gypa</italic>.</p>
      <p>The colon epithelial cells (<xref rid="GR277068TIAC52" ref-type="bibr">Smillie et al. 2019</xref>) were collected from various people and profiled by different sequencing platforms. We selected healthy individuals for the analysis. The batch effects were first corrected by Harmony and then used as input for embedding methods. We found two clear trajectories in the embedding of scDHMap (<xref rid="GR277068TIAF5" ref-type="fig">Fig. 5</xref>C), which were stems → cycling TA → secretory TA → immature goblet → goblet and stems → TA2 → immature enterocytes → enterocytes. PoincaréMap and scPhere can also reveal the two trajectories but with some noise (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S18A</ext-link>). For example, goblets should be adjacent to immature goblets, but goblet cells were close to enterocytes in the embedding of Harmony + PoincaréMap. The methods of Euclidean space had some distortions of the cell developmental order (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S19A,C</ext-link>), especially in the embeddings of UMAP and PHATE. In the embedding of PaCMap, it pushed Best4+ enterocytes far away from other enterocytes. Next, we plotted the embeddings of different methods against patient IDs in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Figures S18B and S19D</ext-link>. The embedding of PCA + PoincaréMap confirmed that different individuals had undesired technical variances, and batches were not aligned. After Harmony corrected the 50 PCs, embeddings from different subjects were merged in most methods. However, some batches were still unaligned, such as points from patient 4 in the embeddings of PoincaréMap (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S18B</ext-link>), PaCMap, t-SNE, UMAP, and PHATE (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S19D</ext-link>). Meanwhile, scDHMap could merge points from different individuals well (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S18B</ext-link>), indicating the combining of conditional autoencoder and Harmony in scDHMap could improve batch integration and embedding quality together. We have observed similar results in the simulation experiment already. The quantitative SIL coefficient of batch alignment further confirmed that scDHMap had the best SIL score (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Figs. S18C, S19B</ext-link>).</p>
      <p>Finally, we applied scDHMap to <italic>Caenorhabditis elegans</italic> embryonic cells (<xref rid="GR277068TIAC44" ref-type="bibr">Packer et al. 2019</xref>), collected along with a series from &lt;100 min to &gt;650 min of embryonic time. Cells were profiled in different batches, and we first corrected 50 PCs by Harmony. In the embedding of scDHMap, we observed clear developmental paths showing that various main cell types originated from the same root and then differentiated to different places (<xref rid="GR277068TIAF5" ref-type="fig">Fig. 5</xref>D) and that cells were ordered by embryonic time (<xref rid="GR277068TIAF5" ref-type="fig">Fig. 5</xref>E). We displayed the embedding per embryonic time bin and confirmed that within the same cell type, cells were also ordered by embryonic time (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S20</ext-link>). For example, body wall muscle (BWM) cells first appeared in the time bin 130–170 and then moved to the boundary of the Poincaré ball along the embryonic time. Other hyperbolic embedding methods, including PoincaréMap and scPhere, had similar properties (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S21A,B</ext-link>). The embedding of PoincaréMap had some isolated small clusters, which made the trajectory paths not as smooth as scDHMap's. The embedding of scPhere had ordered cell types according to embryonic time well, but high-dimensional distance preservation was not as good as scDHMap's (<xref rid="GR277068TIAF5" ref-type="fig">Fig. 5</xref>A). Our scDHMap could combine the advantages of the two hyperbolic methods. Same as with the previous studies (<xref rid="GR277068TIAC44" ref-type="bibr">Packer et al. 2019</xref>; <xref rid="GR277068TIAC27" ref-type="bibr">Klimovskaia et al. 2020</xref>; <xref rid="GR277068TIAC8" ref-type="bibr">Ding and Regev 2021</xref>), all the Euclidean embedding methods failed to display trajectory paths correctly (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S22A–C</ext-link>). Only PaCMap's embedding had placed main cell types among continuous hierarchical paths, but with some incorrect groups. The embeddings of t-SNE and UMAP clustered cells into many isolated small groups, which were unfavorable for trajectory interpretation. These results illustrate that only hyperbolic methods can learn a smooth and interpretable embedding of <italic>C. elegans</italic> embryonic cells and that scDHMap captures the preferred features of both PoincaréMap and scPhere. To explore the trajectory branches in the data set further, we provide an optional parameter, γ, to control the intensity of the repulsive force between nonneighboring points. As we can see in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Figure S23, A and B</ext-link>, a larger value of γ brings greater repulsive force and could separate different branches more clearly. This feature is useful when users want to visualize the branches more closely.</p>
    </sec>
    <sec id="s1e">
      <title>Application to scATAC-seq data</title>
      <p>We analyzed Satpathy's scATAC-seq data (<xref rid="GR277068TIAC51" ref-type="bibr">Satpathy et al. 2019</xref>) by scDHMap. scATAC-seq profiles genome-wide chromatin accessibility. The data matrix of scATAC-seq is close to binary and extremely sparse, and the features in scATAC-seq are not genes but peaks. To make it fit the scDHMap model, we first aggregated peaks into genes and obtained the gene activity scores by Signac (<xref rid="GR277068TIAC55" ref-type="bibr">Stuart et al. 2021</xref>), which quantifies the activity of each gene by assessing the chromatin accessibility associated with each gene. The gene activity matrix is cell by gene, and we used it as the input for embedding methods. <xref rid="GR277068TIAF6" ref-type="fig">Figure 6</xref>A displays the embedding qualities of different methods. Again, we found scDHMap had the best Q values. <xref rid="GR277068TIAF6" ref-type="fig">Figure 6</xref>B plotted the scDHMap embedding and different cell types. Satpathy's scATAC-seq data contain about 58,000 human bone marrow and blood cells and can be divided into several major groups, from progenitor cells (including hematopoietic stem cell [HSC], lymphoid-primed multipotent progenitor [LMPP], common lymphoid progenitor [CLP], megakaryocyte–erythroid progenitor [MEP], basophil–mast cell progenitor [BMP], Pro-B, and Pre-B) to end-stage cell types, such as myeloid cells, B cells, CD4<sup>+</sup> T cells, CD8<sup>+</sup> T cells, basophils, and NK cells. We selected progenitors and some groups in the scDHMap embedding for plotting (<xref rid="GR277068TIAF6" ref-type="fig">Fig. 6</xref>C–F). In <xref rid="GR277068TIAF6" ref-type="fig">Figure 6</xref>C, we observed the trajectory path from HSC to myeloid cells (HSC, CMP → GMP → MDP → pDC, cDC, monocytes); in <xref rid="GR277068TIAF6" ref-type="fig">Figure 6</xref>D, we observed the trajectory path from HSC to B cells (HSC, LMPP → CLP → Pro-B → Pre-B → naïve and memory B cells); in <xref rid="GR277068TIAF6" ref-type="fig">Figure 6</xref>, E and F, we observed the trajectory path from HSC to T cells (HSC, LMPP → CLP → naïve CD4<sup>+</sup> and CD8<sup>+</sup> T cells → memory CD4<sup>+</sup> T cells and CD8<sup>+</sup> T cells). These differentiation trajectories were consistent with known bone marrow differentiation orders (<xref rid="GR277068TIAC51" ref-type="bibr">Satpathy et al. 2019</xref>). Next, we compared the embedding of scDHMap with embeddings of other methods (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S24A,B</ext-link>) and further focused on the subsets of the specified cell types (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S25A–D</ext-link>). Competing methods did not recover differentiation orders correctly in some cell types. For example, in the embeddings of PaCMap, t-SNE, and UMAP, B cells were separated far from Pro B cells (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S25B</ext-link>), and in the embeddings of PoincaréMap and PHATE, naive B cells were adjacent to HSC but not to Pro-B and Pre-B. We observed many compact small clusters in the t-SNE and UMAP embeddings, similar to the observations of the <italic>C. elegans</italic> cells, which indicated these methods mainly focusing on local but not global structures. The embedding of scDHMap was not affected by this problem.</p>
      <fig position="float" id="GR277068TIAF6">
        <label>Figure 6.</label>
        <caption>
          <p>Embedding of scDHMap on Satpathy's scATAC-seq data. (<italic>A</italic>) Embedding quality metrics of different methods. (<italic>B</italic>) scDHMap embedding of Satpathy's scATAC-seq data. (<italic>C</italic>–<italic>F</italic>) scDHMap embedding of different cell types. (<italic>C</italic>) Progenitor cells (cluster 1–9) and myeloid cells (cluster 10–13). (<italic>D</italic>) Progenitor cells and B cells (cluster 14–16). (<italic>E</italic>) Progenitor cells and CD4<sup>+</sup> T cells (cluster 21–25). (<italic>F</italic>) Progenitor cells and CD8<sup>+</sup> T cells (cluster 26–31). (HSC) Hematopoietic stem cell; (LMPP) lymphoid-primed multipotent progenitor; (CLP) common lymphoid progenitor; (MEP) megakaryocyte–erythroid progenitor; (BMP) basophil–mast cell progenitor.</p>
        </caption>
        <graphic xlink:href="232f06" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s2">
    <title>Discussion</title>
    <p>In summary, we have developed a deep hyperbolic manifold learning approach scDHMap for visualizing complex trajectories in single-cell genomics data. The model is a hyperbolic t-SNE parametrized by a model-based deep variational autoencoder. We compared scDHMap with several state-of-the-art dimensionality-reduction methods, including the recently published hyperbolic methods, on various embedding tasks of simulated and real data sets and illustrated scDHMap as having the best performance. By aggregating peaks into genes, we showed that scDHMap could be applied to visualize scATAC-seq data. The model can be optimized per mini-batch, making it efficient for visualizing large data sets. Because of the parametric model, it can easily include out-of-sample points after training. All these advantages make scDHMap a strong candidate for the visualization and discovery of complex hierarchical structures in single-cell genomics data.</p>
    <p>ScDHMap is an end-to-end deep learning approach that accepts the raw count matrix with several building blocks accounting for different tasks. The encoder part accounts for learning a low-dimensional embedding, and the decoder part accounts for denoising dropout events in the count matrix. The wrapped normal prior distribution in the variational inference makes the embedding of scDHMap more favorable for visualization. To eliminate batch effects, we combine two blocks: the conditional autoencoder explicitly accounts for batch IDs, and Harmony corrects batches in the input of the t-SNE regularization. The combination results in better batch alignment and embedding quality. The most critical hyperparameter in the t-SNE regularization is perplexity, which controls how many neighbors will be considered. We showed that scDHMap is robust in a range of different perplexities.</p>
    <p>So far, all results have been obtained on the data sets with continuous hierarchical structures. To illustrate the universality of visualization of single-cell genomics data, we further applied scDHMap to the data sets with different cell types. For this purpose, we used four real scRNA-seq data sets from various species and tissues: 10X PBMC (<xref rid="GR277068TIAC68" ref-type="bibr">Zheng et al. 2017</xref>), mouse embryonic stem cells (<xref rid="GR277068TIAC26" ref-type="bibr">Klein et al. 2015</xref>), mouse bladder cells (<xref rid="GR277068TIAC19" ref-type="bibr">Han et al. 2018</xref>), and worm neuron cells (<xref rid="GR277068TIAC4" ref-type="bibr">Cao et al. 2017</xref>). We found that scDHMap retained good performance in embedding qualities, whereas it was able to separate cell types decently (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Fig. S26A–C</ext-link>). This result suggests that, although not designed for this type of data, scDHMap can still be used for visualizing the data sets with different cell types.</p>
    <p>Our method provides a flexible hyperbolic embedding framework that can be extended in several ways. For example, we can extend it to visualize multi-omic data by joint high-dimensional distances (<xref rid="GR277068TIAC10" ref-type="bibr">Do and Canzar 2021</xref>) or to preserve local densities by adding a regularization term (<xref rid="GR277068TIAC40" ref-type="bibr">Narayan et al. 2021</xref>). Another extension is to include prior information into the embedding learning process. The prior information can be cell types, time points, etc., that could guide the model to group or separate different cells (<xref rid="GR277068TIAC58" ref-type="bibr">Tian et al. 2021</xref>; <xref rid="GR277068TIAC67" ref-type="bibr">Zhai et al. 2022</xref>). Given the efficiency, flexibility, and extensibility, we expect scDHMap to be a valuable tool for the analysis of single-cell genomics data.</p>
    <p>We have illustrated the advantage of the hyperbolic embedding in visualizing of complex hierarchical structures in single-cell data, but the downstream analysis tools are currently underdeveloped. For example, in the pseudotime inference analysis, after obtaining the low-dimensional embedding, practitioners often expect to do clustering, to build a minimum spanning tree (MST), and to run principal curve regression (<xref rid="GR277068TIAC47" ref-type="bibr">Qiu et al. 2017</xref>; <xref rid="GR277068TIAC54" ref-type="bibr">Street et al. 2018</xref>). The hyperbolic version of these popular analytic methods is desired to exploit the advantageous hyperbolic embedding. We hope our study can motivate the development of more supportive tools acting on hyperbolic space, which is a promising future study direction.</p>
  </sec>
  <sec sec-type="methods" id="s3">
    <title>Methods</title>
    <sec id="s3a">
      <title>Feature selection and preprocessing of scRNA-seq data</title>
      <p>Following the method of <xref rid="GR277068TIAC28" ref-type="bibr">Kobak and Berens (2019)</xref>, we apply the mean-variance relationship for feature selection. We first filter out genes that have a non-zero expression in fewer than 10 cells. Given by their mean, genes with large variance are selected. For each gene <italic>g</italic>, we compute the fraction of zero counts
<disp-formula id="GR277068TIAUM1"><mml:math id="UM1" display="block" overflow="scroll"><mml:msub><mml:mi>d</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mrow><mml:munder><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:munder><mml:mo>⁡</mml:mo><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>
where <italic>n</italic> is the number of cells, and the mean of log non-zero expression is
<disp-formula id="GR277068TIAUM2"><mml:math id="UM2" display="block" overflow="scroll"><mml:msub><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>log</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo fence="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo></mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula>
</p>
      <p>To select a predefined number <italic>M</italic> of genes (we set <italic>M</italic> = 1000), we use a heuristic approach of finding a value <italic>b</italic> such that
<disp-formula id="GR277068TIAUM3"><mml:math id="UM3" display="block" overflow="scroll"><mml:msub><mml:mi>d</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mn>0.02</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:math></disp-formula>
is true for exactly <italic>M</italic> genes. Here <italic>b</italic> is found by binary search, and <italic>a</italic> is set to be 1. The feature selection procedure is conducted on raw counts directly, and the selected <italic>n</italic> × <italic>M</italic> raw count matrix is used for the ZINB reconstruction part of the decoder. Following our previous work (<xref rid="GR277068TIAC57" ref-type="bibr">Tian et al. 2019</xref>, <xref rid="GR277068TIAC58" ref-type="bibr">2021</xref>), the input for the autoencoder is library size normalized, log-transformed, and scaled counted. Briefly, we calculate a library size factor of each cell, so cells share the same library size. Next, we log-transform and scale counts, so genes have unit variance and zero mean. The preprocessed count is denoted as <inline-formula id="il1"><mml:math id="IL1" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. These steps are conducted by the Python package SCANPY (<xref rid="GR277068TIAC64" ref-type="bibr">Wolf et al. 2018</xref>).</p>
      <p>We use PCs of the data matrix as the input for the t-SNE regularization part. After selecting top <italic>M</italic> genes, we apply analytic Pearson residual normalization (<xref rid="GR277068TIAC31" ref-type="bibr">Lause et al. 2021</xref>) to correct sequencing depth and stabilize the variance across genes in the count data. Specifically, for gene <italic>g</italic> in cell <italic>i</italic>, Pearson residuals are calculated by
<disp-formula id="GR277068TIAUM4"><mml:math id="UM4" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>⁡</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>⁡</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>
<disp-formula id="GR277068TIAUM5"><mml:math id="UM5" display="block" overflow="scroll"><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>θ</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>
where <italic>X</italic><sub><italic>ig</italic></sub> is the raw count of gene <italic>g</italic> in cell <italic>i</italic>, and θ is the dispersion parameter of the NB distribution and is set to be 100. Pearson residual normalized count data are reduced from <italic>n</italic> × <italic>M</italic> to <italic>n</italic> × 50 by PCA. As <xref rid="GR277068TIAC28" ref-type="bibr">Kobak and Berens (2019)</xref> suggested, the usual number of PCs in single-cell data is around 50, so the top 50 PCs are used as the input for the t-SNE part of our model to keep the structural topology of the data during dimensionality reduction.</p>
    </sec>
    <sec id="s3b">
      <title>Hyperbolic geometry and Poincaré ball</title>
      <p>Hyperbolic geometry is a non-Euclidean geometry having a constant sectional curvature of −1. Because of the geometric analog, hyperbolic space can be considered as a continuous version of discrete trees. Poincaré ball is a projection that represents the hyperbolic space in a unit ball in the Euclidean space: <inline-formula id="il2"><mml:math id="IL2" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mi>M</mml:mi></mml:msup><mml:mo>:=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo fence="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.4em"/><mml:mspace width="0.4em"/><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, where ℙ<sup><italic>M</italic></sup> is a <italic>M</italic> dimensional Poincaré ball, ℝ<sup><italic>M</italic>+1</sup> is a <italic>M</italic> + <italic>1</italic> dimensional Euclidean space, and <bold><italic>z</italic></bold> = (<italic>z</italic><sub>0</sub>, <italic>z</italic><sub>1</sub>, …, <italic>z</italic><sub><italic>M</italic></sub>)<sup><italic>T</italic></sup>. In the Poincaré ball, the distance between two points <bold><italic>z</italic></bold><sub>1</sub>, <bold><italic>z</italic></bold><sub>2</sub> is defined as
<disp-formula id="GR277068TIAUM6"><mml:math id="UM6" display="block" overflow="scroll"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.4em"/><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">arcosh</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mspace width="0.4em"/><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>
where <inline-formula id="il3"><mml:math id="IL3" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">arcosh</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">ln</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>+</mml:mo><mml:msqrt><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:msqrt></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is the inverse hyperbolic cosine function, and <inline-formula id="il4"><mml:math id="IL4" display="inline" overflow="scroll"><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mo>⋅</mml:mo><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:math></inline-formula> is the Euclidean norm. The distance between <bold><italic>z</italic></bold> and the origin is the Poincaré norm
<disp-formula id="GR277068TIAUM7"><mml:math id="UM7" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">arcosh</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula>
</p>
      <p>As we can see, the Poincaré ball represents Euclidean space near the origin of the unit hyperball, and the Poincaré norm grows exponentially when <bold><italic>z</italic></bold> approach to the border <inline-formula id="il5"><mml:math id="IL5" display="inline" overflow="scroll"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mo>→</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. These properties are very useful for representing the hierarchical tree structure.</p>
      <p>The Lorentzian model is a type of hyperbolic space that all points satisfy ℍ<sup><italic>M</italic></sup>: = {<italic>z</italic> ∈ ℝ<sup><italic>M</italic>+1</sup>|<italic>z</italic><sub>0</sub> &gt; 0, 〈<bold><italic>z</italic></bold>, <bold><italic>z</italic></bold>〉<sub>ℍ</sub> = −1}, where ℍ<sup><italic>M</italic></sup> is a <italic>M</italic>-dimensional Lorentzian model, ℝ<sup><italic>M</italic>+1</sup> is a <italic>M</italic> + <italic>1</italic>-dimensional Euclidean space, and<inline-formula id="il6"><mml:math id="IL6" display="inline" overflow="scroll"><mml:mspace width="0.4em"/><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mn>0</mml:mn><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is the Lorentzian inner product. Lorentzian norm is defined as <inline-formula id="il7"><mml:math id="IL7" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:msub><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:msqrt></mml:math></inline-formula>. The origin of the Lorentzian model is <bold><italic>o</italic></bold><sub>0</sub> = (1, 0, …, 0)<sup><italic>T</italic></sup>. The distance between two points <bold><italic>z</italic></bold><sub>1</sub>, <bold><italic>z</italic></bold><sub>2</sub> in the Lorentzian model is defined as
<disp-formula id="GR277068TIAUM8"><mml:math id="UM8" display="block" overflow="scroll"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.4em"/><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">arcosh</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.4em"/><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula>
</p>
      <p>The tangent space at point <bold><italic>o</italic></bold> is defined as all vectors that passing point <bold><italic>o</italic></bold> and are orthogonal to vector <bold><italic>o</italic></bold>
<disp-formula id="GR277068TIAUM9"><mml:math id="UM9" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mi>M</mml:mi></mml:msup><mml:mo>:=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mo fence="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>
and the resulting tangent space is a Euclidean subspace in ℝ<sup><italic>M</italic>+1</sup>. The mapping between hyperbolic space and tangent space can be performed by the exponential map and inverse exponential map (also called logarithm map) (<xref rid="GR277068TIAC42" ref-type="bibr">Nickel and Kiela 2018</xref>; <xref rid="GR277068TIAC13" ref-type="bibr">Grattarola et al. 2019</xref>; <xref rid="GR277068TIAC39" ref-type="bibr">Nagano et al. 2019</xref>). For point <inline-formula id="il8"><mml:math id="IL8" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mi>M</mml:mi></mml:msup></mml:math></inline-formula> and <bold><italic>o</italic></bold> ∈ ℍ<sup><italic>M</italic></sup>, the exponential map is
<disp-formula id="GR277068TIAUM10"><mml:math id="UM10" display="block" overflow="scroll"><mml:msub><mml:mi>exp</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">cosh</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">sinh</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>
where sinh and cosh are hyperbolic sine and cosine, respectively. For point <bold><italic>o</italic></bold>, <bold><italic>z</italic></bold> ∈ ℍ<sup><italic>M</italic></sup> and <bold><italic>o</italic></bold> ≠ <bold><italic>z</italic></bold>, we can obtain the inverse exponential map as
<disp-formula id="GR277068TIAUM11"><mml:math id="UM11" display="block" overflow="scroll"><mml:msubsup><mml:mi>exp</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">arcosh</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mi>η</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msqrt><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>
where <italic>η</italic> = −〈<bold><italic>o</italic></bold>, <bold><italic>z</italic></bold>〉<sub>ℍ</sub>.</p>
      <p>We build our model with a latent representation of the 2D Lorentzian model. For visualization, we can easily project the 2D Lorentzian model to Poincaré ball
<disp-formula id="GR277068TIAUM12"><mml:math id="UM12" display="block" overflow="scroll"><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>→</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula>
We discard the first dimension because it is a constant zero for plotting.</p>
    </sec>
    <sec id="s3c">
      <title>Hyperbolic variational autoencoder with the ZINB reconstruction loss</title>
      <p>ScDHMap receives preprocessed count and reduces it to a two-dimensional hyperbolic space by a ZINB model–based variational autoencoder (VAE). ZINB model–based autoencoder has been applied to scRNA-seq count data in previous studies successfully (<xref rid="GR277068TIAC35" ref-type="bibr">Lopez et al. 2018</xref>; <xref rid="GR277068TIAC11" ref-type="bibr">Eraslan et al. 2019</xref>; <xref rid="GR277068TIAC57" ref-type="bibr">Tian et al. 2019</xref>, <xref rid="GR277068TIAC58" ref-type="bibr">2021</xref>). VAE is a deep generative model that characterizes data by a neural-network parametrized distribution with a low-dimensional latent variable (<xref rid="GR277068TIAC25" ref-type="bibr">Kingma and Welling 2014</xref>), which is appropriate for visualization. The variational inference models the count matrix <bold><italic>X</italic></bold> by the likelihood of ZINB distribution:
<disp-formula id="GR277068TIAUM13"><mml:math id="UM13" display="block" overflow="scroll"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo fence="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∏</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:mrow><mml:mi mathvariant="normal">ZINB</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo fence="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">π</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula>
Here the ZINB likelihood of <italic>X</italic><sub><italic>ig</italic></sub> is calculated by two components: the NB likelihood and the point mass of probability at zero (the probability of dropout events):
<disp-formula id="GR277068TIAUM14"><mml:math id="UM14" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">NB</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo fence="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>!</mml:mo><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>
<disp-formula id="GR277068TIAUM15"><mml:math id="UM15" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">ZINB</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo fence="false">|</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>δ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mrow><mml:mi mathvariant="normal">NB</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo fence="false">|</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula>
ZINB parameters mean <bold>μ</bold>, dispersion <bold>θ</bold>, and dropout probability <bold>π</bold> are parametrized by decoder networks with the latent variable <bold><italic>z</italic></bold>. Specifically,
<disp-formula id="GR277068TIAUM16"><mml:math id="UM16" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">diag</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>
<disp-formula id="GR277068TIAUM17"><mml:math id="UM17" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">softplus</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>
<disp-formula id="GR277068TIAUM18"><mml:math id="UM18" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">π</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">sigmoid</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>π</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>
where <inline-formula id="il9"><mml:math id="IL9" display="inline" overflow="scroll"><mml:msub><mml:mi>l</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula id="il10"><mml:math id="IL10" display="inline" overflow="scroll"><mml:msub><mml:mi>l</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:math></inline-formula>, and <inline-formula id="il11"><mml:math id="IL11" display="inline" overflow="scroll"><mml:msub><mml:mi>l</mml:mi><mml:mi>π</mml:mi></mml:msub></mml:math></inline-formula> are three neural networks that parametrize mean, dispersion, and dropout probability, respectively; <italic>s</italic><sub><italic>i</italic></sub> is the library size factor of cell <italic>i</italic> that is calculated in the preprocessing step; and <italic>l</italic> is the latent decoder. Different activation functions (exponential, softplus, and sigmoid) are appended to the three networks, because mean and dispersion are always positive, and dropout probability is in the range from zero to one. In keeping with the method of <xref rid="GR277068TIAC11" ref-type="bibr">Eraslan et al. (2019)</xref>, the estimated mean <inline-formula id="il12"><mml:math id="IL12" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula> can be used as denoised counts, which eliminates the effect of library size. In the typical VAE model, the latent variable <bold><italic>z</italic></bold> is generated from a standard multivariate normal prior, but in scDHMap model, to represent the continuous hierarchical structure, we use a wrapped normal prior <inline-formula id="il13"><mml:math id="IL13" display="inline" overflow="scroll"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">Wrapped</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo fence="false">|</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula> in the hyperbolic space (<xref rid="GR277068TIAC36" ref-type="bibr">Mathieu et al. 2019</xref>; <xref rid="GR277068TIAC39" ref-type="bibr">Nagano et al. 2019</xref>; <xref rid="GR277068TIAC43" ref-type="bibr">Ovinnikov 2019</xref>; <xref rid="GR277068TIAC8" ref-type="bibr">Ding and Regev 2021</xref>).</p>
      <p>The posterior distribution <inline-formula id="il14"><mml:math id="IL14" display="inline" overflow="scroll"><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo fence="false">|</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula> is parametrized by the encoder and a wrapped normal posterior <inline-formula id="il15"><mml:math id="IL15" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">Wrapped</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo fence="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">σ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula>. Here latent mean <bold><italic>m</italic></bold> and latent variance <bold>σ</bold> are estimated by neural networks
<disp-formula id="GR277068TIAUM19"><mml:math id="UM19" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mspace width="0.4em"/><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>
<disp-formula id="GR277068TIAUM20"><mml:math id="UM20" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">σ</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">softplus</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>σ</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mspace width="0.4em"/><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>
where <italic>f</italic> is the latent encoder. Then <bold><italic>h</italic></bold><sub><italic>i</italic></sub> is projected to hyperbolic space by the exponential map (from the tangent space around the origin <bold><italic>o</italic></bold><sub>0</sub> = (1, 0, …, 0)<sup><italic>T</italic></sup> to the hyperbolic space):
<disp-formula id="GR277068TIAUM21"><mml:math id="UM21" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>cosh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="0.4em"/><mml:mi>sinh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula>
We use <bold><italic>m</italic></bold> as the low-dimensional embedding for the visualization.</p>
      <p>Combining encoder and decoder parts, we can write the learning objective as maximizing the evidence lower bound (ELBO) (<xref rid="GR277068TIAC25" ref-type="bibr">Kingma and Welling 2014</xref>):
<disp-formula id="GR277068TIAM1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mtable columnalign="right left" rowspacing=".5em" columnspacing="thickmathspace" displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="normal">ELBO</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mtext>β</mml:mtext><mml:mo>×</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KL</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mo fence="false">|</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo><mml:mo fence="false">|</mml:mo><mml:mo fence="false">|</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mo fence="false">|</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mspace width="0.4em"/><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mo fence="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mtext>β</mml:mtext><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KL</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo fence="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo fence="false">|</mml:mo><mml:mo fence="false">|</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">ZINB</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo fence="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">π</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
where the KL divergence measures the difference between the wrapped normal prior and the wrapped normal posterior of the latent variable, and β controls the weight of KL divergence (<xref rid="GR277068TIAC20" ref-type="bibr">Higgins et al. 2017</xref>). The second term is the ZINB likelihood of the raw count matrix.</p>
      <p>The wrapped normal prior distribution <inline-formula id="il16"><mml:math id="IL16" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">Wrapped</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula> is built by two steps. First, a standard normal distribution in the tangent space <inline-formula id="il17"><mml:math id="IL17" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mi>M</mml:mi></mml:msup></mml:math></inline-formula> at the origin <bold><italic>o</italic></bold><sub>0</sub> = (1, 0, …, 0)<sup><italic>T</italic></sup> is defined. Next, samples of the standard normal distribution are parallel-transported to the desired locations and projected to the hyperbolic space by the exponential map.</p>
      <p>For sampling the wrapped normal posterior distribution <inline-formula id="il18"><mml:math id="IL18" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">Wrapped</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">σ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula>, where <bold><italic>m</italic></bold> ∈ ℍ<sup><italic>M</italic></sup> and <bold>σ</bold> ∈ ℝ<sup><italic>M</italic></sup>, we use a set of invertible functions to transform samples from a normal distribution <inline-formula id="il19"><mml:math id="IL19" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">σ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula> in ℝ<sup><italic>M</italic></sup> to the hyperbolic space, where <bold><italic>I</italic></bold><sub><italic>M</italic></sub> is the identity matrix in ℝ<sup><italic>M</italic></sup>(<xref rid="GR277068TIAC39" ref-type="bibr">Nagano et al. 2019</xref>). First, we sample <inline-formula id="il20"><mml:math id="IL20" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> from <inline-formula id="il101"><mml:math id="IL21" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">σ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula> and then let <inline-formula id="il21"><mml:math id="IL22" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula>, which can be considered as a sample vector in the tangent space <inline-formula id="il22"><mml:math id="IL23" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mi>M</mml:mi></mml:msup></mml:math></inline-formula>. Next, <bold><italic>z</italic></bold><sub>0</sub> is parallel-transported to <bold><italic>z</italic></bold><sub>1</sub> in the tangent space <inline-formula id="il23"><mml:math id="IL24" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mi>M</mml:mi></mml:msup></mml:math></inline-formula>at <bold><italic>m</italic></bold>:
<disp-formula id="GR277068TIAUM22"><mml:math id="UM22" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>η</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>
where <italic>η</italic> = −〈<bold><italic>o</italic></bold><sub>0</sub>, <bold><italic>m</italic></bold>〉<sub>ℍ</sub>. The parallel-transport keeps the direction and the vector norm. Finally, <bold><italic>z</italic></bold><sub>1</sub> is projected back to the hyperbolic space by the exponential map
<disp-formula id="GR277068TIAUM23"><mml:math id="UM23" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">cosh</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">sinh</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:math></disp-formula>
The likelihood of <bold><italic>z</italic></bold> can be calculated by
<disp-formula id="GR277068TIAUM24"><mml:math id="UM24" display="block" overflow="scroll"><mml:mtable columnalign="right left" rowspacing=".5em" columnspacing="thickmathspace" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo movablelimits="true" form="prefix">det</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo movablelimits="true" form="prefix">det</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>sinh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo></mml:math></disp-formula>
where <italic>d</italic> is the dimension of vectors, <inline-formula id="il24"><mml:math id="IL25" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">Wrapped</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">σ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula> and <inline-formula id="il25"><mml:math id="IL26" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">σ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula>.</p>
      <p>For a given sample <bold><italic>z</italic></bold> from the wrapped normal, we need the corresponding <bold><italic>z</italic></bold><sub>1</sub> and <bold><italic>z</italic></bold><sub>0</sub> to evaluate the wrapped normal density <italic>p</italic>(<bold><italic>z</italic></bold>). These can be obtained by the inverse exponential map and the inverse parallel transport
<disp-formula id="GR277068TIAUM25"><mml:math id="UM25" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">arcosh</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msqrt><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>
<disp-formula id="GR277068TIAUM26"><mml:math id="UM26" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>η</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>
where <italic>η</italic>′ = −〈<bold><italic>m</italic></bold>, <bold><italic>z</italic></bold>〉<sub>ℍ</sub> and η = −〈<bold><italic>o</italic></bold><sub>0</sub>, <bold><italic>m</italic></bold>〉<sub>ℍ</sub>.</p>
      <p>Finally, we have all components to calculate the KL divergence in the ELBO Equation (<xref rid="GR277068TIAM1" ref-type="disp-formula">1</xref>)
<disp-formula id="GR277068TIAUM27"><mml:math id="UM27" display="block" overflow="scroll"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KL</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>∝</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">σ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>
where <italic>p</italic>(<bold><italic>z</italic></bold>;<bold><italic>m</italic></bold>, <bold>σ</bold>) is the wrapped normal density with mean <bold><italic>m</italic></bold> and variance <bold>σ</bold>, and <italic>p</italic>(<bold><italic>z</italic></bold>;<bold>0</bold>, <bold><italic>I</italic></bold>) is the wrapped normal density with mean <bold>0</bold> and variance <bold><italic>I</italic></bold>.</p>
    </sec>
    <sec id="s3d">
      <title>The t-SNE regularization function</title>
      <p>The t-SNE function preserves the similarity of high-dimensional space during dimension reduction (<xref rid="GR277068TIAC61" ref-type="bibr">van der Maaten and Hinton 2008</xref>). Specifically, t-SNE measures a directional similarity of point <italic>j</italic> to point <italic>i</italic> in high-dimensional space (here we use 50 PCs of analytic Pearson residual normalized counts),
<disp-formula id="GR277068TIAUM28"><mml:math id="UM28" display="block" overflow="scroll"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mo fence="false">|</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>
where the variance of the Gaussian kernel <inline-formula id="il26"><mml:math id="IL27" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> is chosen such that the perplexity
<disp-formula id="GR277068TIAUM29"><mml:math id="UM29" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="script">P</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:munder><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mo fence="false">|</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>log</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mspace width="0.4em"/><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mo fence="false">|</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula>
has a predefined value. <inline-formula id="il27"><mml:math id="IL28" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> can be found by the binary search, and perplexity controls the variance of the kernel. Importantly, for a given <inline-formula id="il28"><mml:math id="IL29" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">P</mml:mi></mml:mrow></mml:math></inline-formula>, all but <inline-formula id="il29"><mml:math id="IL30" display="inline" overflow="scroll"><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">P</mml:mi></mml:mrow></mml:math></inline-formula> nearest neighbors of point <italic>i</italic> have a near-to-zero <italic>p</italic><sub><italic>j</italic>|<italic>i</italic></sub>, so <inline-formula id="il30"><mml:math id="IL31" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">P</mml:mi></mml:mrow></mml:math></inline-formula> can guide the t-SNE algorithm to focus on how many neighbors of each point. For mathematical and computational convenience, we use the symmetric SNE
<disp-formula id="GR277068TIAUM30"><mml:math id="UM30" display="block" overflow="scroll"><mml:msubsup><mml:mrow><mml:mspace width="0.4em"/><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo fence="false">|</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mo fence="false">|</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></disp-formula>
<disp-formula id="GR277068TIAUM31"><mml:math id="UM31" display="block" overflow="scroll"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mspace width="0.4em"/><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mspace width="0.4em"/><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:math></disp-formula>
</p>
      <p>The similarity between points in the low-dimensional embedding is measured by a heavy-tailed Student's <italic>t</italic>-distribution
<disp-formula id="GR277068TIAUM32"><mml:math id="UM32" display="block" overflow="scroll"><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∝</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:math></disp-formula>
</p>
      <p>Additionally, to separate trajectory branches more clearly, we provide an optional parameter γ (Cauchy distribution) to strengthen the repulsive force between nonneighboring points
<disp-formula id="GR277068TIAUM33"><mml:math id="UM33" display="block" overflow="scroll"><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∝</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>γ</mml:mi></mml:mfrac></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>γ</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>
where γ controls the heavy-tail property of the Cauchy distribution. If setting γ = 1, the Cauchy distribution reduces to the Student's <italic>t</italic>-distribution. With the larger γ, the repulsive force will also increase.</p>
      <p>The t-SNE algorithm optimizes the low-dimensional embedding such that the similarity between <italic>q</italic><sub><italic>ij</italic></sub> and <italic>p</italic><sub><italic>ij</italic></sub> as close as possible in terms of the KL divergence.</p>
      <p>In the scDHMap model, we use the ZINB model–based hyperbolic variational autoencoder to learn the low-dimensional embeddings <bold><italic>m</italic></bold>, and the low-dimensional similarity is calculated by the hyperbolic distance
<disp-formula id="GR277068TIAUM34"><mml:math id="UM34" display="block" overflow="scroll"><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>Z</mml:mi></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>
<disp-formula id="GR277068TIAUM35"><mml:math id="UM35" display="block" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>γ</mml:mi></mml:mfrac></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.4em"/><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>γ</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>
<disp-formula id="GR277068TIAUM36"><mml:math id="UM36" display="block" overflow="scroll"><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mo>⁡</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula>
and the t-SNE KL divergence is obtained by (<xref rid="GR277068TIAC9" ref-type="bibr">Ding et al. 2018</xref>):
<disp-formula id="GR277068TIAM2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mtable columnalign="right left" rowspacing=".5em" columnspacing="thickmathspace" displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KL</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo fence="false">|</mml:mo><mml:mo fence="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">q</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mspace width="0.4em"/><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>∝</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>∝</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">[</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>γ</mml:mi></mml:mfrac></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.4em"/><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>γ</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>γ</mml:mi></mml:mfrac></mml:mrow><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">(</mml:mo></mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.4em"/><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>γ</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>∝</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">[</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>γ</mml:mi></mml:mfrac></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.4em"/><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>γ</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>γ</mml:mi></mml:mfrac></mml:mrow><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">(</mml:mo></mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.4em"/><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>γ</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
In the t-SNE KL divergence, the first term is an attractive force between point <italic>i</italic> and <italic>j</italic> whenever <italic>p</italic><sub><italic>ji</italic></sub> ≠ 0, and the second term is a repulsive force between point <italic>i</italic> and <italic>j</italic>.</p>
    </sec>
    <sec id="s3e">
      <title>Total loss function</title>
      <p>The total learning objective of scDHMap combines the ELBO of ZINB model–based hyperbolic variational autoencoder (1) and the t-SNE regularization (2):
<disp-formula id="GR277068TIAM3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:munder><mml:mrow><mml:mo form="prefix">min</mml:mo></mml:mrow><mml:mi>W</mml:mi></mml:munder><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">ZINB</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo fence="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">π</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mtext>β</mml:mtext><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KL</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo fence="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo fence="false">|</mml:mo><mml:mo fence="false">|</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KL</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo fence="false">|</mml:mo><mml:mo fence="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">q</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula>
where α controls the weight of the t-SNE part, β controls the weight of the wrapped normal prior, and <italic>W</italic> is trainable parameters in the variational autoencoder. The loss function is optimized per min-batch. For the t-SNE regularization, the variance of the Gaussian kernel is also searched per mini-batch.</p>
    </sec>
    <sec id="s3f">
      <title>Batch correction and evaluation</title>
      <p>We combine Harmony (<xref rid="GR277068TIAC29" ref-type="bibr">Korsunsky et al. 2019</xref>) with a conditional variational autoencoder (<xref rid="GR277068TIAC53" ref-type="bibr">Sohn et al. 2015</xref>) to correct batch effect in the data. The conditional variational autoencoder has been applied in scVI and scPhere for integrating scRNA-seq data of different batches (<xref rid="GR277068TIAC35" ref-type="bibr">Lopez et al. 2018</xref>; <xref rid="GR277068TIAC8" ref-type="bibr">Ding and Regev 2021</xref>). We encode the batch ID by a one-hot encoding <bold><italic>B</italic></bold>, and then the conditional encoder becomes <inline-formula id="il32"><mml:math id="IL32" display="inline" overflow="scroll"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula> and the conditional decoder becomes <italic>l</italic>((<bold><italic>z</italic></bold>, <bold><italic>B</italic></bold>)). As a result, the learned latent embedding should be batch independent. For the input of the t-SNE part, the 50 PCs are corrected by Harmony respecting the batch information. scDHMap combines the strength of conditional variational autoencoder and Harmony for batch correction.</p>
      <p>The result of batch correction is quantified by the silhouette coefficient (SIL) (<xref rid="GR277068TIAC7" ref-type="bibr">Cole et al. 2019</xref>):
<disp-formula id="GR277068TIAUM37"><mml:math id="UM37" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">sil</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">abs</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>b</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo fence="false">{</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.4em"/><mml:mi>b</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo fence="false">}</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>
where <italic>a</italic>(<italic>i</italic>) denotes the average distance (for methods of hyperbolic space, e.g., scDHMap, PoincaréMap, and scPhere, we use Poincaré distance; for other methods, we use Euclidean distance) between the embedding of the <italic>i</italic>th cell and other cells in the same batch, and <italic>b</italic>(<italic>i</italic>) denotes the minimum average distance between the embedding of the <italic>i</italic>th cell and cells in other batches. The SIL of the whole data set is the average of SILs of all cells. The larger SIL means the learned embedding has better alignment between batches.</p>
    </sec>
    <sec id="s3g">
      <title>Model implementation</title>
      <p>The scDHMap model is implemented in Python3 using PyTorch (<xref rid="GR277068TIAC45" ref-type="bibr">Paszke et al. 2017</xref>). All layers are fully connected neural networks. Layer sizes of latent encoder and decoder are (128, 64, 32, 16) and (16, 32, 64, 128), and each layer uses the ELU activation function (<xref rid="GR277068TIAC6" ref-type="bibr">Clevert et al. 2016</xref>) with the batch normalization technic (<xref rid="GR277068TIAC21" ref-type="bibr">Ioffe and Szegedy 2015</xref>). The bottleneck layer size is set to be two for visualization. The model learns the latent representation in the Lorentzian model and then projects the embedding to the 2D Poincaré ball for plotting. The Adam (<xref rid="GR277068TIAC24" ref-type="bibr">Kingma and Ba 2015</xref>) with AMSGrad (<xref rid="GR277068TIAC48" ref-type="bibr">Reddi et al. 2018</xref>) optimizer is used to train the model, with the parameters of learning rate <italic>lr</italic> = 0.001, β<sub>1</sub> = 0.9, and β<sub>2</sub> = 0.999 and a weight decay of 0.001. The model is first pretrained without the t-SNE loss for a predefined number of iterations (400 epochs for most data sets and 200 epochs for large data sets with more than 10,000 cells) and then trained to optimize the total loss. The early stop criterion is set to be the t-SNE KL divergence loss not improving for 150 epochs. The size of mini-batch is 512. The most time-consuming step in the model is to find the best variance of the Gaussian kernel with the given perplexity in each mini-batch. We accelerate this step by parallelly calculating the variance for each sample, which is implemented by the Python package numba (<xref rid="GR277068TIAC30" ref-type="bibr">Lam et al. 2015</xref>). Values of the hyperparameters are α = 1000 for balancing the number of input features and the dimensions of latent embeddings and β = 10. The perplexity is set to be 30, and the default parameter of the Cauchy distribution is γ = 1. We use float64 for all tensor operations in PyTorch to achieve better calculation precision.</p>
    </sec>
    <sec id="s3h">
      <title>Embedding quality metric</title>
      <p>Following the method of <xref rid="GR277068TIAC32" ref-type="bibr">Lee and Verleysen (2010)</xref>, we use the embedding quality metric to quantify the performance of different dimension reduction methods. Basically, the metric is to measure how good preservation of local and global distance on the manifold. In this work, the high-dimensional distance is calculated by the pairwise Euclidean distance of 50 PCs of analytic Pearson residual normalized counts. A good dimensionality-reduction method should have a good preservation of local and global distance on the embedding, which means close neighbors should be placed closed to each other and distant points should be placed separately. Q local and Q global are focused on local and global distance preservation, respectively. The quantities of Q local and Q global range from zero to one; larger values mean better preservations.</p>
    </sec>
    <sec id="s3i">
      <title>Translation in Poincaré space and Poincaré pseudotime</title>
      <p>In the embedding of Poincaré ball, we can perform an isometric transformation of the whole embedding that places a known root to the origin and preserve all pairwise distances. Particularly, if we want to translate the origin of the Poincaré ball to <bold>ν</bold>, point <bold><italic>x</italic></bold> is translated to
<disp-formula id="GR277068TIAUM38"><mml:math id="UM38" display="block" overflow="scroll"><mml:mi>τ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">ν</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">ν</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">ν</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">ν</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">ν</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">ν</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>
where 〈<bold>ν</bold>, <bold><italic>x</italic></bold>〉 is the inner product (〈<bold>ν</bold>, <bold><italic>x</italic></bold>〉 = ν<sub>1</sub><italic>x</italic><sub>1</sub> + ν<sub>2</sub><italic>x</italic><sub>2</sub> + …), and <inline-formula id="il33"><mml:math id="IL33" display="inline" overflow="scroll"><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mo>⋅</mml:mo><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:math></inline-formula> is the Euclidean norm. In the Poincaré ball, the spatial resolution is amplified around the origin, so this translation can be also used as a method to zoom into the interested part of embedding.</p>
      <p>Pseudotime is referred to as a measure of how much process a cell has been differentiated through a trajectory path. Here, we use the Poincaré distance between an individual cell and the root as the Poincaré pseudotime.</p>
    </sec>
    <sec id="s3j">
      <title>Methods comparison</title>
      <p>PoincaréMap (<xref rid="GR277068TIAC27" ref-type="bibr">Klimovskaia et al. 2020</xref>), scPhere (<xref rid="GR277068TIAC8" ref-type="bibr">Ding and Regev 2021</xref>), scvis (<xref rid="GR277068TIAC9" ref-type="bibr">Ding et al. 2018</xref>), PaCMap (<xref rid="GR277068TIAC63" ref-type="bibr">Wang et al. 2021</xref>), t-SNE (<xref rid="GR277068TIAC61" ref-type="bibr">van der Maaten and Hinton 2008</xref>), UMAP (<xref rid="GR277068TIAC37" ref-type="bibr">McInnes et al. 2018</xref>), PHATE (<xref rid="GR277068TIAC38" ref-type="bibr">Moon et al. 2019</xref>), and BC-t-SNE (<xref rid="GR277068TIAC1" ref-type="bibr">Aliverti et al. 2020</xref>) are used for comparisons. All methods reduce inputs to 2D representations. We first select the top 1000 genes by using the mean-variance relationship. Except for scPhare (which accepts raw counts as inputs), all competing methods use 50 PCs of analytic Pearson residual normalized raw counts as inputs. For data sets with batch effects, 50 PCs are corrected by Harmony (<xref rid="GR277068TIAC29" ref-type="bibr">Korsunsky et al. 2019</xref>), respecting the batch IDs (except for scPhere and BC-t-SNE; these two methods can handle batch effects directly).</p>
      <p>PoincaréMap (<uri xlink:href="https://github.com/facebookresearch/PoincareMaps">https://github.com/facebookresearch/Poincare Maps</uri>) is set to the default setting: k_neighbours = 15, sigma = 1, gamma = 2, epochs = 1000, lr = 0.1, and earlystop = 0.0001 (except for the <italic>C. elegans</italic> data set: sigma = 2, gamma = 3, which is suggest by the investigators).</p>
      <p>scPhere (<uri xlink:href="https://github.com/klarman-cell-observatory/scPhere">https://github.com/klarman-cell-observatory/scPhere</uri>) is a VAE-based dimensionality-reduction method, which maps the scRNA-seq data to the hyperbolic space. We project the VAE embedding from the hyperbolic space to the Poincare space as the model output. The parameters for scPhere are latent_dist = “wn,” max_epoch = 250, and the rest of the parameters are set to the default settings: z_dim = 2, observation_dist = “nb,” mb_size = 128, learning_rate = 0.001.</p>
      <p>Scvis (<uri xlink:href="https://github.com/shahcompbio/scvis">https://github.com/shahcompbio/scvis</uri>) is a deep generative dimensionality-reduction model for scRNA-seq data. The parameters are set to use the default settings: optimization = “Adam,” learning_rate = 0.01, batch_size = 512, max_epoch = 100, regularizer_l2 = 0.001, perplexity = 10.</p>
      <p>PaCMap (<uri xlink:href="https://github.com/YingfanWang/PaCMAP">https://github.com/YingfanWang/PaCMAP</uri>), t-SNE (<uri xlink:href="https://github.com/pavlin-policar/openTSNE">https://github.com/pavlin-policar/openTSNE</uri>), and UMAP (<uri xlink:href="https://github.com/lmcinnes/umap">https://github.com/lmcinnes/umap</uri>) are nonlinear dimensionality-reduction methods. The parameters of PaCMap are n_dims = 2, n_neighbors = None, MN_ratio = 0.5, and FP_ratio = 2. The settings of t-SNE are perplexity = 30, initialization = “pca.” UMAP uses default settings, for example, n_neighbors = 15, min_dist = 0.1, n_components = 2.</p>
      <p>PHATE (<uri xlink:href="https://github.com/KrishnaswamyLab/PHATE">https://github.com/KrishnaswamyLab/PHATE</uri>) uses default settings such as n_components = 2, knn = 5, t = “auto,” and gamma = 1.</p>
      <p>BC-t-SNE (<uri xlink:href="https://github.com/emanuelealiverti/BC_tSNE">https://github.com/emanuelealiverti/BC_tSNE</uri>) is a batch-aware t-SNE. The parameters are set to use default settings, k = 50, outDim = 2, perplexity = 30, maxIter = 1000.</p>
    </sec>
    <sec id="s3k">
      <title>Data simulation</title>
      <p>Simulated data sets are generated by the R package Splatter (<xref rid="GR277068TIAC66" ref-type="bibr">Zappia et al. 2017</xref>), and the tree structures are synthesized by dyntoy (<xref rid="GR277068TIAC50" ref-type="bibr">Saelens et al. 2019</xref>).</p>
      <p>For the simulation experiments of various dropout rates, we first synthesized the hierarchical tree structure by the dyntoy function generate_milestone_network (“tree”) and then generated the true and raw count matrix of 4000 cells and 3000 genes by Splatter. The parameters for Splatter were set as n_batches = 1, pct_main_ features = 0.5, dropout_shape = −1, de_prob = 0.2, de_facScale = 0.3, n_steps_per_length = 100, and dropout_mid = (1.5, 2, 2.5, 3) for different dropout rates. True count matrix is the count matrix before dropout, and raw count matrix is the count matrix after dropout. For each setting, we generated 10 data sets with different random seeds. For the simulation experiments of batch effect, we set the parameter of Splatter as n_batches = 6, dropout_mid = 2.5, batchCells = (0.05, 0.1, 0.15, 0.2, 0.25, 0.25)*n_cells, and batch_facScale = 0.15, and others remained the same as previously described.</p>
      <p>For the simulation experiments of three branches, we first synthesized the tree structure by the dyntoy function generate_ milestone_network (“bifurcating”). The parameters for Splatter were n_batches = 1, pct_main_features = 0.5, dropout_mid = 4, dropout_shape = −1, de_prob = 0.1, n_steps_per_length = 100, and de_facScale = 0.4. Ten data sets were generated by different random seeds. In the generated data sets, the step value of each cell in the branch was used as a ground-truth pseudotime.</p>
    </sec>
    <sec id="s3l">
      <title>Single-cell data sets</title>
      <p>Paul cell data (<xref rid="GR277068TIAC46" ref-type="bibr">Paul et al. 2015</xref>) were downloaded from GitHub (<uri xlink:href="https://github.com/theislab/scAnalysisTutorial">https://github.com/theislab/scAnalysisTutorial</uri>). In the data set, investigators profiled 2730 murine myeloid progenitor cells by the MARS-seq (<xref rid="GR277068TIAC22" ref-type="bibr">Jaitin et al. 2014</xref>). We used “data.debatched” matrix as the count matrix, which was regularized for the inter-batch differences. The cell types and the root were annotated by the investigators and provided in the tutorial of SCANPY package <uri xlink:href="https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html">https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html</uri>.</p>
      <p>The colon mucosa cells (<xref rid="GR277068TIAC52" ref-type="bibr">Smillie et al. 2019</xref>) were collected from various individuals and profiled by the 10x Chromium platform (v1 or v2). We downloaded it from <uri xlink:href="https://singlecell.broadinstitute.org/single_cell/study/SCP551/scphere">https://singlecell.broadinstitute.org/single_cell/study/SCP551/scphere</uri>. We selected colon epithelial cells from the 12 healthy individuals, thus giving a matrix of 22,439 cells by 1361 genes.</p>
      <p>The <italic>C. elegans</italic> embryonic cell data set (<xref rid="GR277068TIAC44" ref-type="bibr">Packer et al. 2019</xref>) consists of about 80,000 cells profiled by 10x Chromium (v2). The data set was download from the NCBI Gene Expression Omnibus (GEO; <uri xlink:href="https://www.ncbi.nlm.nih.gov/geo/">https://www.ncbi.nlm.nih.gov/geo/</uri>) under accession number GSE126954. We filtered out cells with fewer than 40 genes; as a result, 67,970 cells by 2766 genes were selected for the analysis.</p>
      <p>Satpathy's scATAC-seq data (<xref rid="GR277068TIAC51" ref-type="bibr">Satpathy et al. 2019</xref>), including the cell barcodes, count matrix, peaks, and fragment files, of all the hematopoiesis cells were downloaded from the NCBI Gene Expression Omnibus (GEO) database under accession number GSE129785. This data set contains 63,882 cells. We extracted the bone marrow and peripheral blood immune cells, having 61,806 cells across 31 cell types. Before preprocessing, unsorted fragment files were sorted by BEDTools, and all the fragment files were indexed by Tabix. We preprocessed the data according to the Signac pipeline (<uri xlink:href="https://stuartlab.org/signac/articles/monocle.html">https://stuartlab.org/signac/articles/monocle.html</uri>). Signac (<xref rid="GR277068TIAC55" ref-type="bibr">Stuart et al. 2021</xref>) and Seurat (<xref rid="GR277068TIAC3" ref-type="bibr">Butler et al. 2018</xref>) were used for all the preprocessing. Specifically, each fragment file was transformed into a Signac's fragment object by the “CreateFragmentObject” function. Then all the fragment objects were combined as a list and input into the “CreateChromatinAssay” function to create a chromatin assay. This assay was then transformed into a Seurat object for all the downstream processes. Quality control was performed to remove the outlier cells. Specifically, the scATAC-seq data were filtered by three criteria: (1) total counts per cell &lt; 50,000, (2) transcriptional start site (TSS) enrichment score &gt; 2, and (3) nucleosome signal (the ratio of mononucleosomal to nucleosome-free fragments) &gt; 5. In addition, cells with a high proportion of reads in the black areas of the genome (the regions always with high artifactual signals) were also removed. Following the processing steps in the original paper, the reads were mapped to the gene regions of human genome 19 (hg19) by the “GeneActivity” function. The final count matrix was 58,711 cells by 20,010 genes used for the embedding analysis. Cell type annotations were provided by the original paper.</p>
      <p>The processed real single-cell data sets used in this study can be found at Figshare (<uri xlink:href="https://figshare.com/s/64694120e3d2b87e21c3">https://figshare.com/s/64694120e3d2b87e21c3</uri>).</p>
      <p>The description of the four real scRNA-seq data sets with different cell types is in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Note 5</ext-link>. These data sets can be found at GitHub (<uri xlink:href="https://github.com/ttgump/scDeepCluster/tree/master/scRNA-seq%20data">https://github.com/ttgump/scDeepCluster/tree/master/scRNA-seq%20data</uri>).</p>
    </sec>
    <sec id="s3m">
      <title>Software availability</title>
      <p>An open-source software implementation of scDHMap is available as <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.277068.122/-/DC1" ext-link-type="uri">Supplemental Code</ext-link> and on GitHub (<uri xlink:href="https://github.com/ttgump/scDHMap">https://github.com/ttgump/scDHMap</uri>).</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="PMC_1" content-type="local-data">
      <caption>
        <title>Supplemental Material</title>
      </caption>
      <media mimetype="text" mime-subtype="html" xlink:href="supp_33_2_232__DC1.html"/>
      <media xlink:role="associated-file" mimetype="application" mime-subtype="pdf" xlink:href="supp_gr.277068.122_Supplementary_materials.pdf"/>
      <media xlink:role="associated-file" mimetype="application" mime-subtype="x-zip-compressed" xlink:href="supp_gr.277068.122_SupplementalCode.zip"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgments</title>
    <p>We thank Dr. Yao Ma from the New Jersey Institute of Technology for valuable suggestions. We thank Dr. Ruihua Cheng from the Tianjin University of Finance and Economics for proofreading, which improved the clarity of the manuscript. This work used the Extreme Science and Engineering Discovery Environment (XSEDE) through the allocation CIE170034, supported by National Science Foundation grant number ACI-1548562. This work was supported by grant R15HG012087 (Z.W.) from the National Institutes of Health (NIH) and was partially supported by the National Center for Advancing Translational Sciences (NCATS), a component of NIH under award number UL1TR003017 (Z.W.).</p>
    <p><italic>Author contributions:</italic> Z.W. and H.H. conceived and supervised the project. T.T. conceived and designed the method. T.T. and C.Z. conducted experiments. X.L. contributed to data processing. T.T., C.Z., Z.W., and H.H. wrote the manuscript. All authors approved the manuscript.</p>
  </ack>
  <fn-group>
    <fn fn-type="supplementary-material">
      <p>[Supplemental material is available for this article.]</p>
    </fn>
    <fn>
      <p>Article published online before print. Article, supplemental material, and publication date are at <ext-link xlink:href="https://www.genome.org/cgi/doi/10.1101/gr.277068.122" ext-link-type="uri">https://www.genome.org/cgi/doi/10.1101/gr.277068.122</ext-link>.</p>
    </fn>
  </fn-group>
  <sec id="s5">
    <title>Competing interest statement</title>
    <p>The authors declare no competing interests.</p>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="GR277068TIAC1">
      <mixed-citation publication-type="journal"><string-name><surname>Aliverti</surname><given-names>E</given-names></string-name>, <string-name><surname>Tilson</surname><given-names>JL</given-names></string-name>, <string-name><surname>Filer</surname><given-names>DL</given-names></string-name>, <string-name><surname>Babcock</surname><given-names>B</given-names></string-name>, <string-name><surname>Colaneri</surname><given-names>A</given-names></string-name>, <string-name><surname>Ocasio</surname><given-names>J</given-names></string-name>, <string-name><surname>Gershon</surname><given-names>TR</given-names></string-name>, <string-name><surname>Wilhelmsen</surname><given-names>KC</given-names></string-name>, <string-name><surname>Dunson</surname><given-names>DB</given-names></string-name>. <year>2020</year>. <article-title>Projected <italic>t</italic>-SNE for batch correction</article-title>. <source>Bioinformatics</source><volume>36</volume>: <fpage>3522</fpage>–<lpage>3527</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa189</pub-id><pub-id pub-id-type="pmid">32176244</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC2">
      <mixed-citation publication-type="journal"><string-name><surname>Amodio</surname><given-names>M</given-names></string-name>, <string-name><surname>van Dijk</surname><given-names>D</given-names></string-name>, <string-name><surname>Srinivasan</surname><given-names>K</given-names></string-name>, <string-name><surname>Chen</surname><given-names>WS</given-names></string-name>, <string-name><surname>Mohsen</surname><given-names>H</given-names></string-name>, <string-name><surname>Moon</surname><given-names>KR</given-names></string-name>, <string-name><surname>Campbell</surname><given-names>A</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X</given-names></string-name>, <string-name><surname>Venkataswamy</surname><given-names>M</given-names></string-name>, <etal/><year>2019</year>. <article-title>Exploring single-cell data with deep multitasking neural networks</article-title>. <source>Nat Methods</source><volume>16</volume>: <fpage>1139</fpage>–<lpage>1145</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-019-0576-7</pub-id><pub-id pub-id-type="pmid">31591579</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC3">
      <mixed-citation publication-type="journal"><string-name><surname>Butler</surname><given-names>A</given-names></string-name>, <string-name><surname>Hoffman</surname><given-names>P</given-names></string-name>, <string-name><surname>Smibert</surname><given-names>P</given-names></string-name>, <string-name><surname>Papalexi</surname><given-names>E</given-names></string-name>, <string-name><surname>Satija</surname><given-names>R</given-names></string-name>. <year>2018</year>. <article-title>Integrating single-cell transcriptomic data across different conditions, technologies, and species</article-title>. <source>Nat Biotechnol</source><volume>36</volume>: <fpage>411</fpage>–<lpage>420</lpage>. <pub-id pub-id-type="doi">10.1038/nbt.4096</pub-id><pub-id pub-id-type="pmid">29608179</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC4">
      <mixed-citation publication-type="journal"><string-name><surname>Cao</surname><given-names>J</given-names></string-name>, <string-name><surname>Packer</surname><given-names>JS</given-names></string-name>, <string-name><surname>Ramani</surname><given-names>V</given-names></string-name>, <string-name><surname>Cusanovich</surname><given-names>DA</given-names></string-name>, <string-name><surname>Huynh</surname><given-names>C</given-names></string-name>, <string-name><surname>Daza</surname><given-names>R</given-names></string-name>, <string-name><surname>Qiu</surname><given-names>X</given-names></string-name>, <string-name><surname>Lee</surname><given-names>C</given-names></string-name>, <string-name><surname>Furlan</surname><given-names>SN</given-names></string-name>, <string-name><surname>Steemers</surname><given-names>FJ</given-names></string-name>, <etal/><year>2017</year>. <article-title>Comprehensive single-cell transcriptional profiling of a multicellular organism</article-title>. <source>Science</source><volume>357</volume>: <fpage>661</fpage>–<lpage>667</lpage>. <pub-id pub-id-type="doi">10.1126/science.aam8940</pub-id><pub-id pub-id-type="pmid">28818938</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC5">
      <mixed-citation publication-type="book"><string-name><surname>Chami</surname><given-names>I</given-names></string-name>, <string-name><surname>Ying</surname><given-names>Z</given-names></string-name>, <string-name><surname>Ré</surname><given-names>C</given-names></string-name>, <string-name><surname>Leskovec</surname><given-names>J</given-names></string-name>. <year>2019</year>. <article-title>Hyperbolic graph convolutional neural networks</article-title>. In <source>Advances in neural information processing systems</source> (ed. <string-name><surname>Wallach</surname><given-names>H</given-names></string-name>, <etal/>). <publisher-name>NIPS</publisher-name>, Vancouver, Canada.</mixed-citation>
    </ref>
    <ref id="GR277068TIAC6">
      <mixed-citation publication-type="other"><string-name><surname>Clevert</surname><given-names>D-A</given-names></string-name>, <string-name><surname>Unterthiner</surname><given-names>T</given-names></string-name>, <string-name><surname>Hochreiter</surname><given-names>S</given-names></string-name>. <year>2016</year>. <article-title>Fast and accurate deep network learning by exponential linear units (ELUs)</article-title>. In <italic>International Conference on Learning Representations (ICLR)</italic>, San Juan, Puerto Rico.</mixed-citation>
    </ref>
    <ref id="GR277068TIAC7">
      <mixed-citation publication-type="journal"><string-name><surname>Cole</surname><given-names>MB</given-names></string-name>, <string-name><surname>Risso</surname><given-names>D</given-names></string-name>, <string-name><surname>Wagner</surname><given-names>A</given-names></string-name>, <string-name><surname>DeTomaso</surname><given-names>D</given-names></string-name>, <string-name><surname>Ngai</surname><given-names>J</given-names></string-name>, <string-name><surname>Purdom</surname><given-names>E</given-names></string-name>, <string-name><surname>Dudoit</surname><given-names>S</given-names></string-name>, <string-name><surname>Yosef</surname><given-names>N</given-names></string-name>. <year>2019</year>. <article-title>Performance assessment and selection of normalization procedures for single-cell RNA-seq</article-title>. <source>Cell Syst</source><volume>8</volume>: <fpage>315</fpage>–<lpage>328.e8</lpage>. <pub-id pub-id-type="doi">10.1016/j.cels.2019.03.010</pub-id><pub-id pub-id-type="pmid">31022373</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC8">
      <mixed-citation publication-type="journal"><string-name><surname>Ding</surname><given-names>J</given-names></string-name>, <string-name><surname>Regev</surname><given-names>A</given-names></string-name>. <year>2021</year>. <article-title>Deep generative model embedding of single-cell RNA-Seq profiles on hyperspheres and hyperbolic spaces</article-title>. <source>Nat Commun</source><volume>12</volume>: <fpage>2554</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-021-22851-4</pub-id><pub-id pub-id-type="pmid">33953202</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC9">
      <mixed-citation publication-type="journal"><string-name><surname>Ding</surname><given-names>J</given-names></string-name>, <string-name><surname>Condon</surname><given-names>A</given-names></string-name>, <string-name><surname>Shah</surname><given-names>SP</given-names></string-name>. <year>2018</year>. <article-title>Interpretable dimensionality reduction of single cell transcriptome data with deep generative models</article-title>. <source>Nat Commun</source><volume>9</volume>: <fpage>2002</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-018-04368-5</pub-id><pub-id pub-id-type="pmid">29784946</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC10">
      <mixed-citation publication-type="journal"><string-name><surname>Do</surname><given-names>VH</given-names></string-name>, <string-name><surname>Canzar</surname><given-names>S</given-names></string-name>. <year>2021</year>. <article-title>A generalization of t-SNE and UMAP to single-cell multimodal omics</article-title>. <source>Genome Biol</source><volume>22</volume>: <fpage>130</fpage>. <pub-id pub-id-type="doi">10.1186/s13059-021-02356-5</pub-id><pub-id pub-id-type="pmid">33941244</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC11">
      <mixed-citation publication-type="journal"><string-name><surname>Eraslan</surname><given-names>G</given-names></string-name>, <string-name><surname>Simon</surname><given-names>LM</given-names></string-name>, <string-name><surname>Mircea</surname><given-names>M</given-names></string-name>, <string-name><surname>Mueller</surname><given-names>NS</given-names></string-name>, <string-name><surname>Theis</surname><given-names>FJ</given-names></string-name>. <year>2019</year>. <article-title>Single-cell RNA-seq denoising using a deep count autoencoder</article-title>. <source>Nat Commun</source><volume>10</volume>: <fpage>390</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-018-07931-2</pub-id><pub-id pub-id-type="pmid">30674886</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC12">
      <mixed-citation publication-type="journal"><string-name><surname>Goldberg</surname><given-names>AD</given-names></string-name>, <string-name><surname>Allis</surname><given-names>CD</given-names></string-name>, <string-name><surname>Bernstein</surname><given-names>E</given-names></string-name>. <year>2007</year>. <article-title>Epigenetics: a landscape takes shape</article-title>. <source>Cell</source><volume>128</volume>: <fpage>635</fpage>–<lpage>638</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2007.02.006</pub-id><pub-id pub-id-type="pmid">17320500</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC13">
      <mixed-citation publication-type="journal"><string-name><surname>Grattarola</surname><given-names>D</given-names></string-name>, <string-name><surname>Livi</surname><given-names>L</given-names></string-name>, <string-name><surname>Alippi</surname><given-names>C</given-names></string-name>. <year>2019</year>. <article-title>Adversarial autoencoders with constant-curvature latent manifolds</article-title>. <source>Appl Soft Comput</source><volume>81</volume>: <fpage>105511</fpage>. <pub-id pub-id-type="doi">10.1016/j.asoc.2019.105511</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC14">
      <mixed-citation publication-type="other"><string-name><surname>Graving</surname><given-names>JM</given-names></string-name>, <string-name><surname>Couzin</surname><given-names>ID</given-names></string-name>. <year>2020</year>. <article-title>VAE-SNE: a deep generative model for simultaneous dimensionality reduction and clustering</article-title>. <italic>bioRxiv</italic><pub-id pub-id-type="doi">10.1101/2020.07.17.207993</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC15">
      <mixed-citation publication-type="book"><string-name><surname>Gromov</surname><given-names>M</given-names></string-name>. <year>2007</year>. <source>Metric structures for Riemannian and non-Riemannian spaces</source>. <publisher-name>Springer Science &amp; Business Media, Birkhäuser, Boston</publisher-name>.</mixed-citation>
    </ref>
    <ref id="GR277068TIAC16">
      <mixed-citation publication-type="journal"><string-name><surname>Haghverdi</surname><given-names>L</given-names></string-name>, <string-name><surname>Buettner</surname><given-names>F</given-names></string-name>, <string-name><surname>Theis</surname><given-names>FJ</given-names></string-name>. <year>2015</year>. <article-title>Diffusion maps for high-dimensional single-cell analysis of differentiation data</article-title>. <source>Bioinformatics</source><volume>31</volume>: <fpage>2989</fpage>–<lpage>2998</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btv325</pub-id><pub-id pub-id-type="pmid">26002886</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC17">
      <mixed-citation publication-type="journal"><string-name><surname>Haghverdi</surname><given-names>L</given-names></string-name>, <string-name><surname>Büttner</surname><given-names>M</given-names></string-name>, <string-name><surname>Wolf</surname><given-names>FA</given-names></string-name>, <string-name><surname>Buettner</surname><given-names>F</given-names></string-name>, <string-name><surname>Theis</surname><given-names>FJ</given-names></string-name>. <year>2016</year>. <article-title>Diffusion pseudotime robustly reconstructs lineage branching</article-title>. <source>Nat Methods</source><volume>13</volume>: <fpage>845</fpage>–<lpage>848</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.3971</pub-id><pub-id pub-id-type="pmid">27571553</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC18">
      <mixed-citation publication-type="journal"><string-name><surname>Haghverdi</surname><given-names>L</given-names></string-name>, <string-name><surname>Lun</surname><given-names>ATL</given-names></string-name>, <string-name><surname>Morgan</surname><given-names>MD</given-names></string-name>, <string-name><surname>Marioni</surname><given-names>JC</given-names></string-name>. <year>2018</year>. <article-title>Batch effects in single-cell RNA-sequencing data are corrected by matching mutual nearest neighbors</article-title>. <source>Nat Biotechnol</source><volume>36</volume>: <fpage>421</fpage>–<lpage>427</lpage>. <pub-id pub-id-type="doi">10.1038/nbt.4091</pub-id><pub-id pub-id-type="pmid">29608177</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC19">
      <mixed-citation publication-type="journal"><string-name><surname>Han</surname><given-names>X</given-names></string-name>, <string-name><surname>Wang</surname><given-names>R</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>Y</given-names></string-name>, <string-name><surname>Fei</surname><given-names>L</given-names></string-name>, <string-name><surname>Sun</surname><given-names>H</given-names></string-name>, <string-name><surname>Lai</surname><given-names>S</given-names></string-name>, <string-name><surname>Saadatpour</surname><given-names>A</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>Z</given-names></string-name>, <string-name><surname>Chen</surname><given-names>H</given-names></string-name>, <string-name><surname>Ye</surname><given-names>F</given-names></string-name>, <etal/><year>2018</year>. <article-title>Mapping the mouse cell atlas by Microwell-seq</article-title>. <source>Cell</source><volume>172</volume>: <fpage>1091</fpage>–<lpage>1107.e17</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2018.02.001</pub-id><pub-id pub-id-type="pmid">29474909</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC20">
      <mixed-citation publication-type="confproc"><string-name><surname>Higgins</surname><given-names>I</given-names></string-name>, <string-name><surname>Matthey</surname><given-names>L</given-names></string-name>, <string-name><surname>Pal</surname><given-names>A</given-names></string-name>, <string-name><surname>Burgess</surname><given-names>C</given-names></string-name>, <string-name><surname>Glorot</surname><given-names>X</given-names></string-name>, <string-name><surname>Botvinick</surname><given-names>M</given-names></string-name>, <string-name><surname>Mohamed</surname><given-names>S</given-names></string-name>, <string-name><surname>Lerchner</surname><given-names>A</given-names></string-name>. <year>2017</year>. <article-title>β-VAE: learning basic visual concepts with a constrained variational framework</article-title>. In <conf-name>International Conference on Learning Representations</conf-name> (<italic>ICLR</italic>), Toulon, France.</mixed-citation>
    </ref>
    <ref id="GR277068TIAC21">
      <mixed-citation publication-type="confproc"><string-name><surname>Ioffe</surname><given-names>S</given-names></string-name>, <string-name><surname>Szegedy</surname><given-names>C</given-names></string-name>. <year>2015</year>. <article-title>Batch normalization: accelerating deep network training by reducing internal covariate shift</article-title>. In <conf-name>Proceedings of the 32nd International Conference on Machine Learning</conf-name> (ed. <string-name><surname>Bach</surname><given-names>F</given-names></string-name>, <string-name><surname>Blei</surname><given-names>D</given-names></string-name>), Vol. <volume>37</volume>, pp. <fpage>448</fpage>–<lpage>456</lpage>. <publisher-name>JMLR</publisher-name>, <conf-loc>Lille, France</conf-loc>.</mixed-citation>
    </ref>
    <ref id="GR277068TIAC22">
      <mixed-citation publication-type="journal"><string-name><surname>Jaitin</surname><given-names>DA</given-names></string-name>, <string-name><surname>Kenigsberg</surname><given-names>E</given-names></string-name>, <string-name><surname>Keren-Shaul</surname><given-names>H</given-names></string-name>, <string-name><surname>Elefant</surname><given-names>N</given-names></string-name>, <string-name><surname>Paul</surname><given-names>F</given-names></string-name>, <string-name><surname>Zaretsky</surname><given-names>I</given-names></string-name>, <string-name><surname>Mildner</surname><given-names>A</given-names></string-name>, <string-name><surname>Cohen</surname><given-names>N</given-names></string-name>, <string-name><surname>Jung</surname><given-names>S</given-names></string-name>, <string-name><surname>Tanay</surname><given-names>A</given-names></string-name>, <etal/><year>2014</year>. <article-title>Massively parallel single-cell RNA-seq for marker-free decomposition of tissues into cell types</article-title>. <source>Science</source><volume>343</volume>: <fpage>776</fpage>–<lpage>779</lpage>. <pub-id pub-id-type="doi">10.1126/science.1247651</pub-id><pub-id pub-id-type="pmid">24531970</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC23">
      <mixed-citation publication-type="journal"><string-name><surname>Jianbo</surname><given-names>S</given-names></string-name>, <string-name><surname>Malik</surname><given-names>J</given-names></string-name>. <year>2000</year>. <article-title>Normalized cuts and image segmentation</article-title>. <source>IEEE Transa Pattern Anal Mach Intell</source><volume>22</volume>: <fpage>888</fpage>–<lpage>905</lpage>. <pub-id pub-id-type="doi">10.1109/34.868688</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC24">
      <mixed-citation publication-type="confproc"><string-name><surname>Kingma</surname><given-names>DP</given-names></string-name>, <string-name><surname>Ba</surname><given-names>J</given-names></string-name>. <year>2015</year>. <article-title>Adam: A method for stochastic optimization</article-title>. In <conf-name>International Conference on Learning Representations (ICLR)</conf-name>, San Diego.</mixed-citation>
    </ref>
    <ref id="GR277068TIAC25">
      <mixed-citation publication-type="confproc"><string-name><surname>Kingma</surname><given-names>DP</given-names></string-name>, <string-name><surname>Welling</surname><given-names>M</given-names></string-name>. <year>2014</year>. <article-title>Auto-encoding variational bayes</article-title>. In <conf-name>International Conference on Learning Representations (ICLR)</conf-name>, Banff, Canada.</mixed-citation>
    </ref>
    <ref id="GR277068TIAC26">
      <mixed-citation publication-type="journal"><string-name><surname>Klein</surname><given-names>AM</given-names></string-name>, <string-name><surname>Mazutis</surname><given-names>L</given-names></string-name>, <string-name><surname>Akartuna</surname><given-names>I</given-names></string-name>, <string-name><surname>Tallapragada</surname><given-names>N</given-names></string-name>, <string-name><surname>Veres</surname><given-names>A</given-names></string-name>, <string-name><surname>Li</surname><given-names>V</given-names></string-name>, <string-name><surname>Peshkin</surname><given-names>L</given-names></string-name>, <string-name><surname>Weitz</surname><given-names>DA</given-names></string-name>, <string-name><surname>Kirschner</surname><given-names>MW</given-names></string-name>. <year>2015</year>. <article-title>Droplet barcoding for single-cell transcriptomics applied to embryonic stem cells</article-title>. <source>Cell</source><volume>161</volume>: <fpage>1187</fpage>–<lpage>1201</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2015.04.044</pub-id><pub-id pub-id-type="pmid">26000487</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC27">
      <mixed-citation publication-type="journal"><string-name><surname>Klimovskaia</surname><given-names>A</given-names></string-name>, <string-name><surname>Lopez-Paz</surname><given-names>D</given-names></string-name>, <string-name><surname>Bottou</surname><given-names>L</given-names></string-name>, <string-name><surname>Nickel</surname><given-names>M</given-names></string-name>. <year>2020</year>. <article-title>Poincaré maps for analyzing complex hierarchies in single-cell data</article-title>. <source>Nat Commun</source><volume>11</volume>: <fpage>2966</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-020-16822-4</pub-id><pub-id pub-id-type="pmid">32528075</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC28">
      <mixed-citation publication-type="journal"><string-name><surname>Kobak</surname><given-names>D</given-names></string-name>, <string-name><surname>Berens</surname><given-names>P</given-names></string-name>. <year>2019</year>. <article-title>The art of using t-SNE for single-cell transcriptomics</article-title>. <source>Nat Commun</source><volume>10</volume>: <fpage>5416</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-13056-x</pub-id><pub-id pub-id-type="pmid">31780648</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC29">
      <mixed-citation publication-type="journal"><string-name><surname>Korsunsky</surname><given-names>I</given-names></string-name>, <string-name><surname>Millard</surname><given-names>N</given-names></string-name>, <string-name><surname>Fan</surname><given-names>J</given-names></string-name>, <string-name><surname>Slowikowski</surname><given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>F</given-names></string-name>, <string-name><surname>Wei</surname><given-names>K</given-names></string-name>, <string-name><surname>Baglaenko</surname><given-names>Y</given-names></string-name>, <string-name><surname>Brenner</surname><given-names>M</given-names></string-name>, <string-name><surname>Loh</surname><given-names>PR</given-names></string-name>, <string-name><surname>Raychaudhuri</surname><given-names>S</given-names></string-name>. <year>2019</year>. <article-title>Fast, sensitive and accurate integration of single-cell data with harmony</article-title>. <source>Nat Methods</source><volume>16</volume>: <fpage>1289</fpage>–<lpage>1296</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-019-0619-0</pub-id><pub-id pub-id-type="pmid">31740819</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC30">
      <mixed-citation publication-type="confproc"><string-name><surname>Lam</surname><given-names>SK</given-names></string-name>, <string-name><surname>Pitrou</surname><given-names>A</given-names></string-name>, <string-name><surname>Seibert</surname><given-names>S</given-names></string-name>. <year>2015</year>. <article-title>Numba: A LLVM-based Python JIT compiler</article-title>. In <conf-name>Proceedings of the second workshop on the LLVM compiler infrastructure in HPC</conf-name> (ed. <string-name><surname>Finkel</surname><given-names>H</given-names></string-name>), pp. <fpage>1</fpage>–<lpage>6</lpage>. <comment>LLVM '15</comment>, Austin, TX.</mixed-citation>
    </ref>
    <ref id="GR277068TIAC31">
      <mixed-citation publication-type="journal"><string-name><surname>Lause</surname><given-names>J</given-names></string-name>, <string-name><surname>Berens</surname><given-names>P</given-names></string-name>, <string-name><surname>Kobak</surname><given-names>D</given-names></string-name>. <year>2021</year>. <article-title>Analytic Pearson residuals for normalization of single-cell RNA-seq UMI data</article-title>. <source>Genome Biol</source><volume>22</volume>: <fpage>258</fpage>. <pub-id pub-id-type="doi">10.1186/s13059-021-02451-7</pub-id><pub-id pub-id-type="pmid">34488842</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC32">
      <mixed-citation publication-type="journal"><string-name><surname>Lee</surname><given-names>JA</given-names></string-name>, <string-name><surname>Verleysen</surname><given-names>M</given-names></string-name>. <year>2010</year>. <article-title>Scale-independent quality criteria for dimensionality reduction</article-title>. <source>Pattern Recognit Lett</source><volume>31</volume>: <fpage>2248</fpage>–<lpage>2257</lpage>. <pub-id pub-id-type="doi">10.1016/j.patrec.2010.04.013</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC33">
      <mixed-citation publication-type="journal"><string-name><surname>Levine</surname><given-names>JH</given-names></string-name>, <string-name><surname>Simonds</surname><given-names>EF</given-names></string-name>, <string-name><surname>Bendall</surname><given-names>SC</given-names></string-name>, <string-name><surname>Davis</surname><given-names>KL</given-names></string-name>, <string-name><surname>Amir el</surname><given-names>AD</given-names></string-name>, <string-name><surname>Tadmor</surname><given-names>MD</given-names></string-name>, <string-name><surname>Litvin</surname><given-names>O</given-names></string-name>, <string-name><surname>Fienberg</surname><given-names>HG</given-names></string-name>, <string-name><surname>Jager</surname><given-names>A</given-names></string-name>, <string-name><surname>Zunder</surname><given-names>ER</given-names></string-name>, <etal/><year>2015</year>. <article-title>Data-driven phenotypic dissection of AML reveals progenitor-like cells that correlate with prognosis</article-title>. <source>Cell</source><volume>162</volume>: <fpage>184</fpage>–<lpage>197</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2015.05.047</pub-id><pub-id pub-id-type="pmid">26095251</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC34">
      <mixed-citation publication-type="journal"><string-name><surname>Liu</surname><given-names>Q</given-names></string-name>, <string-name><surname>Chen</surname><given-names>S</given-names></string-name>, <string-name><surname>Jiang</surname><given-names>R</given-names></string-name>, <string-name><surname>Wong</surname><given-names>WH</given-names></string-name>. <year>2021</year>. <article-title>Simultaneous deep generative modelling and clustering of single cell genomic data</article-title>. <source>Nat Mach Intell</source><volume>3</volume>: <fpage>536</fpage>–<lpage>544</lpage>. <pub-id pub-id-type="doi">10.1038/s42256-021-00333-y</pub-id><pub-id pub-id-type="pmid">34179690</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC35">
      <mixed-citation publication-type="journal"><string-name><surname>Lopez</surname><given-names>R</given-names></string-name>, <string-name><surname>Regier</surname><given-names>J</given-names></string-name>, <string-name><surname>Cole</surname><given-names>MB</given-names></string-name>, <string-name><surname>Jordan</surname><given-names>MI</given-names></string-name>, <string-name><surname>Yosef</surname><given-names>N</given-names></string-name>. <year>2018</year>. <article-title>Deep generative modeling for single-cell transcriptomics</article-title>. <source>Nat Methods</source><volume>15</volume>: <fpage>1053</fpage>–<lpage>1058</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-018-0229-2</pub-id><pub-id pub-id-type="pmid">30504886</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC36">
      <mixed-citation publication-type="book"><string-name><surname>Mathieu</surname><given-names>E</given-names></string-name>, <string-name><surname>Lan</surname><given-names>CL</given-names></string-name>, <string-name><surname>Maddison</surname><given-names>CJ</given-names></string-name>, <string-name><surname>Tomioka</surname><given-names>R</given-names></string-name>, <string-name><surname>Teh</surname><given-names>YW</given-names></string-name>. <year>2019</year>. <article-title>Hierarchical representations with poincaré variational auto-encoders</article-title>. In <source>Advances in neural information processing systems</source> (ed. <string-name><surname>Wallach</surname><given-names>H</given-names></string-name>, <etal/>), Vol. <volume>32</volume>. <publisher-name>NIPS, Vancouver, Canada</publisher-name>.</mixed-citation>
    </ref>
    <ref id="GR277068TIAC37">
      <mixed-citation publication-type="other"><string-name><surname>McInnes</surname><given-names>L</given-names></string-name>, <string-name><surname>Healy</surname><given-names>J</given-names></string-name>, <string-name><surname>Melville</surname><given-names>J</given-names></string-name>. <year>2018</year>. <article-title>UMAP: Uniform Manifold Approximation and Projection for dimension reduction</article-title>. <italic>arXiv</italic>: <comment>1802.03426</comment>.</mixed-citation>
    </ref>
    <ref id="GR277068TIAC38">
      <mixed-citation publication-type="journal"><string-name><surname>Moon</surname><given-names>KR</given-names></string-name>, <string-name><surname>van Dijk</surname><given-names>D</given-names></string-name>, <string-name><surname>Wang</surname><given-names>Z</given-names></string-name>, <string-name><surname>Gigante</surname><given-names>S</given-names></string-name>, <string-name><surname>Burkhardt</surname><given-names>DB</given-names></string-name>, <string-name><surname>Chen</surname><given-names>WS</given-names></string-name>, <string-name><surname>Yim</surname><given-names>K</given-names></string-name>, <string-name><surname>Elzen</surname><given-names>AVD</given-names></string-name>, <string-name><surname>Hirn</surname><given-names>MJ</given-names></string-name>, <string-name><surname>Coifman</surname><given-names>RR</given-names></string-name>, <etal/><year>2019</year>. <article-title>Visualizing structure and transitions in high-dimensional biological data</article-title>. <source>Nat Biotechnol</source><volume>37</volume>: <fpage>1482</fpage>–<lpage>1492</lpage>. <pub-id pub-id-type="doi">10.1038/s41587-019-0336-3</pub-id><pub-id pub-id-type="pmid">31796933</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC39">
      <mixed-citation publication-type="confproc"><string-name><surname>Nagano</surname><given-names>Y</given-names></string-name>, <string-name><surname>Yamaguchi</surname><given-names>S</given-names></string-name>, <string-name><surname>Fujita</surname><given-names>Y</given-names></string-name>, <string-name><surname>Koyama</surname><given-names>M</given-names></string-name>. <year>2019</year>. <article-title>A wrapped normal distribution on hyperbolic space for gradient-based learning</article-title>. In <conf-name>Proceedings of the 36th International Conference on Machine Learning</conf-name> (ed. <string-name><surname>Kamalika</surname><given-names>C</given-names></string-name>, <string-name><surname>Ruslan</surname><given-names>S</given-names></string-name>), Vol. <volume>97</volume>, pp. <fpage>4693</fpage>–<lpage>4702</lpage>. <publisher-name>Proceedings of Machine Learning Research, Long Beach, CA</publisher-name>.</mixed-citation>
    </ref>
    <ref id="GR277068TIAC40">
      <mixed-citation publication-type="journal"><string-name><surname>Narayan</surname><given-names>A</given-names></string-name>, <string-name><surname>Berger</surname><given-names>B</given-names></string-name>, <string-name><surname>Cho</surname><given-names>H</given-names></string-name>. <year>2021</year>. <article-title>Assessing single-cell transcriptomic variability through density-preserving data visualization</article-title>. <source>Nat Biotechnol</source><volume>39</volume>: <fpage>765</fpage>–<lpage>774</lpage>. <pub-id pub-id-type="doi">10.1038/s41587-020-00801-7</pub-id><pub-id pub-id-type="pmid">33462509</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC41">
      <mixed-citation publication-type="confproc"><string-name><surname>Nickel</surname><given-names>M</given-names></string-name>, <string-name><surname>Kiela</surname><given-names>D</given-names></string-name>. <year>2017</year>. <article-title>Poincaré embeddings for learning hierarchical representations</article-title>. In <conf-name>Proceedings of the 31st International Conference on Neural Information Processing Systems</conf-name> (ed. <string-name><surname>Guyon</surname><given-names>I</given-names></string-name>, <etal/>), pp. <fpage>6341</fpage>–<lpage>6350</lpage>. <publisher-name>Curran Associates</publisher-name>, <conf-loc>Long Beach, CA</conf-loc>.</mixed-citation>
    </ref>
    <ref id="GR277068TIAC42">
      <mixed-citation publication-type="confproc"><string-name><surname>Nickel</surname><given-names>M</given-names></string-name>, <string-name><surname>Kiela</surname><given-names>D</given-names></string-name>. <year>2018</year>. <article-title>Learning continuous hierarchies in the Lorentz model of hyperbolic geometry</article-title>. In <conf-name>Proceedings of the 35th International Conference on Machine Learning</conf-name>, <conf-loc>Stockholm</conf-loc> (ed. <string-name><surname>Dy</surname><given-names>JG</given-names></string-name>, <string-name><surname>Krause</surname><given-names>A</given-names></string-name>), Vol. <volume>80</volume>, pp. <fpage>3779</fpage>–<lpage>3788</lpage>. <publisher-name>Proceedings of Machine Learning Research, Stockholm</publisher-name>.</mixed-citation>
    </ref>
    <ref id="GR277068TIAC43">
      <mixed-citation publication-type="other"><string-name><surname>Ovinnikov</surname><given-names>I</given-names></string-name>. <year>2019</year>. <article-title>Poincaré Wasserstein autoencoder</article-title>. In <italic>International Conference on Learning Representations (ICLR)</italic>, Addis Ababa, Ethiopia.</mixed-citation>
    </ref>
    <ref id="GR277068TIAC44">
      <mixed-citation publication-type="journal"><string-name><surname>Packer</surname><given-names>JS</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>Q</given-names></string-name>, <string-name><surname>Huynh</surname><given-names>C</given-names></string-name>, <string-name><surname>Sivaramakrishnan</surname><given-names>P</given-names></string-name>, <string-name><surname>Preston</surname><given-names>E</given-names></string-name>, <string-name><surname>Dueck</surname><given-names>H</given-names></string-name>, <string-name><surname>Stefanik</surname><given-names>D</given-names></string-name>, <string-name><surname>Tan</surname><given-names>K</given-names></string-name>, <string-name><surname>Trapnell</surname><given-names>C</given-names></string-name>, <string-name><surname>Kim</surname><given-names>J</given-names></string-name>, <etal/><year>2019</year>. <article-title>A lineage-resolved molecular atlas of <italic>C. elegans</italic> embryogenesis at single-cell resolution</article-title>. <source>Science</source><volume>365</volume>: <fpage>eaax1971</fpage>. <pub-id pub-id-type="doi">10.1126/science.aax1971</pub-id><pub-id pub-id-type="pmid">31488706</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC45">
      <mixed-citation publication-type="book"><string-name><surname>Paszke</surname><given-names>A</given-names></string-name>, <string-name><surname>Gross</surname><given-names>S</given-names></string-name>, <string-name><surname>Chintala</surname><given-names>S</given-names></string-name>, <string-name><surname>Chanan</surname><given-names>G</given-names></string-name>, <string-name><surname>Yang</surname><given-names>E</given-names></string-name>, <string-name><surname>DeVito</surname><given-names>Z</given-names></string-name>, <string-name><surname>Lin</surname><given-names>Z</given-names></string-name>, <string-name><surname>Desmaison</surname><given-names>A</given-names></string-name>, <string-name><surname>Antiga</surname><given-names>L</given-names></string-name>, <string-name><surname>Lerer</surname><given-names>A</given-names></string-name>. <year>2017</year>. <article-title>Automatic differentiation in PyTorch</article-title>. In <source>Neural information processing systems</source> (ed. <string-name><surname>Wallach</surname><given-names>HM</given-names></string-name>, <etal/>). NIPS, Long Beach, CA.</mixed-citation>
    </ref>
    <ref id="GR277068TIAC46">
      <mixed-citation publication-type="journal"><string-name><surname>Paul</surname><given-names>F</given-names></string-name>, <string-name><surname>Arkin</surname><given-names>Y</given-names></string-name>, <string-name><surname>Giladi</surname><given-names>A</given-names></string-name>, <string-name><surname>Jaitin</surname><given-names>DA</given-names></string-name>, <string-name><surname>Kenigsberg</surname><given-names>E</given-names></string-name>, <string-name><surname>Keren-Shaul</surname><given-names>H</given-names></string-name>, <string-name><surname>Winter</surname><given-names>D</given-names></string-name>, <string-name><surname>Lara-Astiaso</surname><given-names>D</given-names></string-name>, <string-name><surname>Gury</surname><given-names>M</given-names></string-name>, <string-name><surname>Weiner</surname><given-names>A</given-names></string-name>, <etal/><year>2015</year>. <article-title>Transcriptional heterogeneity and lineage commitment in myeloid progenitors</article-title>. <source>Cell</source><volume>163</volume>: <fpage>1663</fpage>–<lpage>1677</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2015.11.013</pub-id><pub-id pub-id-type="pmid">26627738</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC47">
      <mixed-citation publication-type="journal"><string-name><surname>Qiu</surname><given-names>X</given-names></string-name>, <string-name><surname>Mao</surname><given-names>Q</given-names></string-name>, <string-name><surname>Tang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname><given-names>L</given-names></string-name>, <string-name><surname>Chawla</surname><given-names>R</given-names></string-name>, <string-name><surname>Pliner</surname><given-names>HA</given-names></string-name>, <string-name><surname>Trapnell</surname><given-names>C</given-names></string-name>. <year>2017</year>. <article-title>Reversed graph embedding resolves complex single-cell trajectories</article-title>. <source>Nat Methods</source><volume>14</volume>: <fpage>979</fpage>–<lpage>982</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.4402</pub-id><pub-id pub-id-type="pmid">28825705</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC48">
      <mixed-citation publication-type="confproc"><string-name><surname>Reddi</surname><given-names>SJ</given-names></string-name>, <string-name><surname>Kale</surname><given-names>S</given-names></string-name>, <string-name><surname>Kumar</surname><given-names>S</given-names></string-name>. <year>2018</year>. <article-title>On the convergence of Adam and beyond</article-title>. In <conf-name>International Conference on Learning Representations</conf-name>. <publisher-name>ICLR</publisher-name>, <conf-loc>Vancouver, BC, Canada</conf-loc>.</mixed-citation>
    </ref>
    <ref id="GR277068TIAC49">
      <mixed-citation publication-type="journal"><string-name><surname>Risso</surname><given-names>D</given-names></string-name>, <string-name><surname>Perraudeau</surname><given-names>F</given-names></string-name>, <string-name><surname>Gribkova</surname><given-names>S</given-names></string-name>, <string-name><surname>Dudoit</surname><given-names>S</given-names></string-name>, <string-name><surname>Vert</surname><given-names>JP</given-names></string-name>. <year>2018</year>. <article-title>A general and flexible method for signal extraction from single-cell RNA-seq data</article-title>. <source>Nat Commun</source><volume>9</volume>: <fpage>284</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-017-02554-5</pub-id><pub-id pub-id-type="pmid">29348443</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC50">
      <mixed-citation publication-type="journal"><string-name><surname>Saelens</surname><given-names>W</given-names></string-name>, <string-name><surname>Cannoodt</surname><given-names>R</given-names></string-name>, <string-name><surname>Todorov</surname><given-names>H</given-names></string-name>, <string-name><surname>Saeys</surname><given-names>Y</given-names></string-name>. <year>2019</year>. <article-title>A comparison of single-cell trajectory inference methods</article-title>. <source>Nat Biotechnol</source><volume>37</volume>: <fpage>547</fpage>–<lpage>554</lpage>. <pub-id pub-id-type="doi">10.1038/s41587-019-0071-9</pub-id><pub-id pub-id-type="pmid">30936559</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC51">
      <mixed-citation publication-type="journal"><string-name><surname>Satpathy</surname><given-names>AT</given-names></string-name>, <string-name><surname>Granja</surname><given-names>JM</given-names></string-name>, <string-name><surname>Yost</surname><given-names>KE</given-names></string-name>, <string-name><surname>Qi</surname><given-names>Y</given-names></string-name>, <string-name><surname>Meschi</surname><given-names>F</given-names></string-name>, <string-name><surname>McDermott</surname><given-names>GP</given-names></string-name>, <string-name><surname>Olsen</surname><given-names>BN</given-names></string-name>, <string-name><surname>Mumbach</surname><given-names>MR</given-names></string-name>, <string-name><surname>Pierce</surname><given-names>SE</given-names></string-name>, <string-name><surname>Corces</surname><given-names>MR</given-names></string-name>, <etal/><year>2019</year>. <article-title>Massively parallel single-cell chromatin landscapes of human immune cell development and intratumoral T cell exhaustion</article-title>. <source>Nat Biotechnol</source><volume>37</volume>: <fpage>925</fpage>–<lpage>936</lpage>. <pub-id pub-id-type="doi">10.1038/s41587-019-0206-z</pub-id><pub-id pub-id-type="pmid">31375813</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC52">
      <mixed-citation publication-type="journal"><string-name><surname>Smillie</surname><given-names>CS</given-names></string-name>, <string-name><surname>Biton</surname><given-names>M</given-names></string-name>, <string-name><surname>Ordovas-Montanes</surname><given-names>J</given-names></string-name>, <string-name><surname>Sullivan</surname><given-names>KM</given-names></string-name>, <string-name><surname>Burgin</surname><given-names>G</given-names></string-name>, <string-name><surname>Graham</surname><given-names>DB</given-names></string-name>, <string-name><surname>Herbst</surname><given-names>RH</given-names></string-name>, <string-name><surname>Rogel</surname><given-names>N</given-names></string-name>, <string-name><surname>Slyper</surname><given-names>M</given-names></string-name>, <string-name><surname>Waldman</surname><given-names>J</given-names></string-name>, <etal/><year>2019</year>. <article-title>Intra- and inter-cellular rewiring of the human colon during ulcerative colitis</article-title>. <source>Cell</source><volume>178</volume>: <fpage>714</fpage>–<lpage>730.e22</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2019.06.029</pub-id><pub-id pub-id-type="pmid">31348891</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC53">
      <mixed-citation publication-type="journal"><string-name><surname>Sohn</surname><given-names>K</given-names></string-name>, <string-name><surname>Lee</surname><given-names>H</given-names></string-name>, <string-name><surname>Yan</surname><given-names>X</given-names></string-name>. <year>2015</year>. <article-title>Learning structured output representation using deep conditional generative models</article-title>. In <source>Advances in neural information processing systems</source> (ed. <string-name><surname>Cortes</surname><given-names>C</given-names></string-name>, <etal/>), Vol. <volume>28</volume>. NIPS, Montréal, Canada.</mixed-citation>
    </ref>
    <ref id="GR277068TIAC54">
      <mixed-citation publication-type="journal"><string-name><surname>Street</surname><given-names>K</given-names></string-name>, <string-name><surname>Risso</surname><given-names>D</given-names></string-name>, <string-name><surname>Fletcher</surname><given-names>RB</given-names></string-name>, <string-name><surname>Das</surname><given-names>D</given-names></string-name>, <string-name><surname>Ngai</surname><given-names>J</given-names></string-name>, <string-name><surname>Yosef</surname><given-names>N</given-names></string-name>, <string-name><surname>Purdom</surname><given-names>E</given-names></string-name>, <string-name><surname>Dudoit</surname><given-names>S</given-names></string-name>. <year>2018</year>. <article-title>Slingshot: cell lineage and pseudotime inference for single-cell transcriptomics</article-title>. <source>BMC Genomics</source><volume>19</volume>: <fpage>477</fpage>. <pub-id pub-id-type="doi">10.1186/s12864-018-4772-0</pub-id><pub-id pub-id-type="pmid">29914354</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC55">
      <mixed-citation publication-type="journal"><string-name><surname>Stuart</surname><given-names>T</given-names></string-name>, <string-name><surname>Srivastava</surname><given-names>A</given-names></string-name>, <string-name><surname>Madad</surname><given-names>S</given-names></string-name>, <string-name><surname>Lareau</surname><given-names>CA</given-names></string-name>, <string-name><surname>Satija</surname><given-names>R</given-names></string-name>. <year>2021</year>. <article-title>Single-cell chromatin state analysis with Signac</article-title>. <source>Nat Methods</source><volume>18</volume>: <fpage>1333</fpage>–<lpage>1341</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-021-01282-5</pub-id><pub-id pub-id-type="pmid">34725479</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC56">
      <mixed-citation publication-type="journal"><string-name><surname>Tanay</surname><given-names>A</given-names></string-name>, <string-name><surname>Regev</surname><given-names>A</given-names></string-name>. <year>2017</year>. <article-title>Scaling single-cell genomics from phenomenology to mechanism</article-title>. <source>Nature</source><volume>541</volume>: <fpage>331</fpage>–<lpage>338</lpage>. <pub-id pub-id-type="doi">10.1038/nature21350</pub-id><pub-id pub-id-type="pmid">28102262</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC57">
      <mixed-citation publication-type="journal"><string-name><surname>Tian</surname><given-names>T</given-names></string-name>, <string-name><surname>Wan</surname><given-names>J</given-names></string-name>, <string-name><surname>Song</surname><given-names>Q</given-names></string-name>, <string-name><surname>Wei</surname><given-names>Z</given-names></string-name>. <year>2019</year>. <article-title>Clustering single-cell RNA-seq data with a model-based deep learning approach</article-title>. <source>Nature Machine Intelligence</source><volume>1</volume>: <fpage>191</fpage>–<lpage>198</lpage>. <pub-id pub-id-type="doi">10.1038/s42256-019-0037-0</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC58">
      <mixed-citation publication-type="journal"><string-name><surname>Tian</surname><given-names>T</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>J</given-names></string-name>, <string-name><surname>Lin</surname><given-names>X</given-names></string-name>, <string-name><surname>Wei</surname><given-names>Z</given-names></string-name>, <string-name><surname>Hakonarson</surname><given-names>H</given-names></string-name>. <year>2021</year>. <article-title>Model-based deep embedding for constrained clustering analysis of single cell RNA-seq data</article-title>. <source>Nat Commun</source><volume>12</volume>: <fpage>1873</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-021-22008-3</pub-id><pub-id pub-id-type="pmid">33767149</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC59">
      <mixed-citation publication-type="journal"><string-name><surname>Van den Berge</surname><given-names>K</given-names></string-name>, <string-name><surname>Roux de Bézieux</surname><given-names>H</given-names></string-name>, <string-name><surname>Street</surname><given-names>K</given-names></string-name>, <string-name><surname>Saelens</surname><given-names>W</given-names></string-name>, <string-name><surname>Cannoodt</surname><given-names>R</given-names></string-name>, <string-name><surname>Saeys</surname><given-names>Y</given-names></string-name>, <string-name><surname>Dudoit</surname><given-names>S</given-names></string-name>, <string-name><surname>Clement</surname><given-names>L</given-names></string-name>. <year>2020</year>. <article-title>Trajectory-based differential expression analysis for single-cell sequencing data</article-title>. <source>Nat Commun</source><volume>11</volume>: <fpage>1201</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-020-14766-3</pub-id><pub-id pub-id-type="pmid">32139671</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC60">
      <mixed-citation publication-type="confproc"><string-name><surname>van der Maaten</surname><given-names>L</given-names></string-name>. <year>2009</year>. <article-title>Learning a parametric embedding by preserving local structure</article-title>. In <conf-name>Proceedings of the 12th International Conference on Artificial Intelligence and Statistics</conf-name> (ed. <string-name><surname>van Dyk</surname><given-names>DA</given-names></string-name>, <string-name><surname>Max</surname><given-names>W</given-names></string-name>), Vol. <volume>5</volume>, pp. <fpage>384</fpage>–<lpage>391</lpage>. <publisher-name>Proceedings of Machine Learning Research, Clearwater Beach, FL.</publisher-name></mixed-citation>
    </ref>
    <ref id="GR277068TIAC61">
      <mixed-citation publication-type="journal"><string-name><surname>van der Maaten</surname><given-names>L</given-names></string-name>, <string-name><surname>Hinton</surname><given-names>G</given-names></string-name>. <year>2008</year>. <article-title>Visualizing data using t-SNE</article-title>. <source>J Mach Learn Res</source><volume>9</volume>: <fpage>2579</fpage>–<lpage>2605</lpage>.</mixed-citation>
    </ref>
    <ref id="GR277068TIAC62">
      <mixed-citation publication-type="journal"><string-name><surname>Wang</surname><given-names>B</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>J</given-names></string-name>, <string-name><surname>Pierson</surname><given-names>E</given-names></string-name>, <string-name><surname>Ramazzotti</surname><given-names>D</given-names></string-name>, <string-name><surname>Batzoglou</surname><given-names>S</given-names></string-name>. <year>2017</year>. <article-title>Visualization and analysis of single-cell RNA-seq data by kernel-based similarity learning</article-title>. <source>Nat Methods</source><volume>14</volume>: <fpage>414</fpage>–<lpage>416</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.4207</pub-id><pub-id pub-id-type="pmid">28263960</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC63">
      <mixed-citation publication-type="journal"><string-name><surname>Wang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Huang</surname><given-names>H</given-names></string-name>, <string-name><surname>Rudin</surname><given-names>C</given-names></string-name>, <string-name><surname>Shaposhnik</surname><given-names>Y</given-names></string-name>. <year>2021</year>. <article-title>Understanding how dimension reduction tools work: an empirical approach to deciphering t-SNE, UMAP, TriMAP, and PaCMAP for data visualization</article-title>. <source>J Mach Learn Res</source><volume>22</volume>: <fpage>1</fpage>–<lpage>73</lpage>.</mixed-citation>
    </ref>
    <ref id="GR277068TIAC64">
      <mixed-citation publication-type="journal"><string-name><surname>Wolf</surname><given-names>FA</given-names></string-name>, <string-name><surname>Angerer</surname><given-names>P</given-names></string-name>, <string-name><surname>Theis</surname><given-names>FJ</given-names></string-name>. <year>2018</year>. <article-title>SCANPY: large-scale single-cell gene expression data analysis</article-title>. <source>Genome Biol</source><volume>19</volume>: <fpage>15</fpage>. <pub-id pub-id-type="doi">10.1186/s13059-017-1382-0</pub-id><pub-id pub-id-type="pmid">29409532</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC65">
      <mixed-citation publication-type="journal"><string-name><surname>Wolf</surname><given-names>FA</given-names></string-name>, <string-name><surname>Hamey</surname><given-names>FK</given-names></string-name>, <string-name><surname>Plass</surname><given-names>M</given-names></string-name>, <string-name><surname>Solana</surname><given-names>J</given-names></string-name>, <string-name><surname>Dahlin</surname><given-names>JS</given-names></string-name>, <string-name><surname>Göttgens</surname><given-names>B</given-names></string-name>, <string-name><surname>Rajewsky</surname><given-names>N</given-names></string-name>, <string-name><surname>Simon</surname><given-names>L</given-names></string-name>, <string-name><surname>Theis</surname><given-names>FJ</given-names></string-name>. <year>2019</year>. <article-title>PAGA: graph abstraction reconciles clustering with trajectory inference through a topology preserving map of single cells</article-title>. <source>Genome Biol</source><volume>20</volume>: <fpage>59</fpage>. <pub-id pub-id-type="doi">10.1186/s13059-019-1663-x</pub-id><pub-id pub-id-type="pmid">30890159</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC66">
      <mixed-citation publication-type="journal"><string-name><surname>Zappia</surname><given-names>L</given-names></string-name>, <string-name><surname>Phipson</surname><given-names>B</given-names></string-name>, <string-name><surname>Oshlack</surname><given-names>A</given-names></string-name>. <year>2017</year>. <article-title>Splatter: simulation of single-cell RNA sequencing data</article-title>. <source>Genome Biol</source><volume>18</volume>: <fpage>174</fpage>. <pub-id pub-id-type="doi">10.1186/s13059-017-1305-0</pub-id><pub-id pub-id-type="pmid">28899397</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC67">
      <mixed-citation publication-type="journal"><string-name><surname>Zhai</surname><given-names>Z</given-names></string-name>, <string-name><surname>Lei</surname><given-names>YL</given-names></string-name>, <string-name><surname>Wang</surname><given-names>R</given-names></string-name>, <string-name><surname>Xie</surname><given-names>Y</given-names></string-name>. <year>2022</year>. <article-title>Supervised capacity preserving mapping: a clustering guided visualization method for scRNA-seq data</article-title>. <source>Bioinformatics</source><volume>38</volume>: <fpage>2496</fpage>–<lpage>2503</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btac131</pub-id><pub-id pub-id-type="pmid">35253834</pub-id></mixed-citation>
    </ref>
    <ref id="GR277068TIAC68">
      <mixed-citation publication-type="journal"><string-name><surname>Zheng</surname><given-names>GX</given-names></string-name>, <string-name><surname>Terry</surname><given-names>JM</given-names></string-name>, <string-name><surname>Belgrader</surname><given-names>P</given-names></string-name>, <string-name><surname>Ryvkin</surname><given-names>P</given-names></string-name>, <string-name><surname>Bent</surname><given-names>ZW</given-names></string-name>, <string-name><surname>Wilson</surname><given-names>R</given-names></string-name>, <string-name><surname>Ziraldo</surname><given-names>SB</given-names></string-name>, <string-name><surname>Wheeler</surname><given-names>TD</given-names></string-name>, <string-name><surname>McDermott</surname><given-names>GP</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>J</given-names></string-name>, <etal/><year>2017</year>. <article-title>Massively parallel digital transcriptional profiling of single cells</article-title>. <source>Nat Commun</source><volume>8</volume>: <fpage>14049</fpage>. <pub-id pub-id-type="doi">10.1038/ncomms14049</pub-id><pub-id pub-id-type="pmid">28091601</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
