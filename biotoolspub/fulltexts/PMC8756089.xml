<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8756089</article-id>
    <article-id pub-id-type="pmid">34636886</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btab702</article-id>
    <article-id pub-id-type="publisher-id">btab702</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Applications Notes</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Data and Text Mining</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>BioVAE: a pre-trained latent variable language model for biomedical text mining</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2472-6370</contrib-id>
        <name>
          <surname>Trieu</surname>
          <given-names>Hai-Long</given-names>
        </name>
        <xref rid="btab702-cor1" ref-type="corresp"/>
        <aff><institution>Artificial Intelligence Research Center (AIRC), National Institute of Advanced Industrial Science and Technology (AIST)</institution>, Tokyo 135-0064, <country country="JP">Japan</country></aff>
        <aff><institution>National Centre for Text Mining, Computer Science, University of Manchester 131 Princess Street, M7 1DN</institution>, <country country="GB">UK</country></aff>
        <!--long.trieu@aist.go.jp-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Miwa</surname>
          <given-names>Makoto</given-names>
        </name>
        <aff><institution>Artificial Intelligence Research Center (AIRC), National Institute of Advanced Industrial Science and Technology (AIST)</institution>, Tokyo 135-0064, <country country="JP">Japan</country></aff>
        <aff>
          <institution>Department of Advanced Science and Technology, Toyota Technological Institute, Nagoya 468-8511, Japan</institution>
        </aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ananiadou</surname>
          <given-names>Sophia</given-names>
        </name>
        <aff><institution>National Centre for Text Mining, Computer Science, University of Manchester 131 Princess Street, M7 1DN</institution>, <country country="GB">UK</country></aff>
        <aff><institution>Alan Turing Institute</institution>, 96 Euston Road, London, NW1 2DB, <country country="GB">UK</country></aff>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Wren</surname>
          <given-names>Jonathan</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btab702-cor1">To whom correspondence should be addressed. <email>long.trieu@aist.go.jp</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>01</day>
      <month>2</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2021-10-12">
      <day>12</day>
      <month>10</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>12</day>
      <month>10</month>
      <year>2021</year>
    </pub-date>
    <volume>38</volume>
    <issue>3</issue>
    <fpage>872</fpage>
    <lpage>874</lpage>
    <history>
      <date date-type="received">
        <day>14</day>
        <month>7</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>16</day>
        <month>9</month>
        <year>2021</year>
      </date>
      <date date-type="editorial-decision">
        <day>04</day>
        <month>10</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>08</day>
        <month>10</month>
        <year>2021</year>
      </date>
      <date date-type="corrected-typeset">
        <day>29</day>
        <month>10</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btab702.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Summary</title>
        <p>Large-scale pre-trained language models (PLMs) have advanced state-of-the-art (SOTA) performance on various biomedical text mining tasks. The power of such PLMs can be combined with the advantages of deep generative models. These are examples of these combinations. However, they are trained only on general domain text, and biomedical models are still missing. In this work, we describe BioVAE, the first large-scale pre-trained latent variable language model for the biomedical domain, which uses the OPTIMUS framework to train on large volumes of biomedical text. The model shows SOTA performance on several biomedical text mining tasks when compared to existing publicly available biomedical PLMs. In addition, our model can generate more accurate biomedical sentences than the original OPTIMUS output.</p>
      </sec>
      <sec id="s2">
        <title>Availability and implementation</title>
        <p>Our source code and pre-trained models are freely available: <ext-link xlink:href="https://github.com/aistairc/BioVAE" ext-link-type="uri">https://github.com/aistairc/BioVAE</ext-link>.</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>New Energy and Industrial Technology Development Organization (NEDO)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>JPNP20006</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Alan Turing Institute and BBSRC</institution>
          </institution-wrap>
        </funding-source>
        <award-id>BB/P025684/1</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="3"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Large-scale pre-trained language models (PLMs) (<xref rid="btab702-B1" ref-type="bibr">Beltagy <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btab702-B10" ref-type="bibr">Lee <italic toggle="yes">et al.</italic>, 2020</xref>) have shown state-of-the-art (SOTA) performance on various biomedical text mining tasks. These models provide contextualized representations, learned from large volumes of biomedical text, which then can be easily applied to achieve SOTA on downstream tasks such as named entity recognition (NER), relation extraction (REL) and question answering (QA) (<xref rid="btab702-B6" ref-type="bibr">Kim <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btab702-B13" ref-type="bibr">Lin <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btab702-B14" ref-type="bibr">Nentidis <italic toggle="yes">et al.</italic>, 2019</xref>).</p>
    <p>Combining such large-scale PLMs to train latent variables based on deep generative models (DGMs) has been shown to improve representation learning tasks (<xref rid="btab702-B2" ref-type="bibr">Bowman <italic toggle="yes">et al.</italic>, 2016</xref>; <xref rid="btab702-B11" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2020</xref>). A recent framework called OPTIMUS, has successfully combined BERT-based PLMs (<xref rid="btab702-B3" ref-type="bibr">Devlin <italic toggle="yes">et al.</italic>, 2019</xref>) and GPT-2 (<xref rid="btab702-B15" ref-type="bibr">Radford <italic toggle="yes">et al.</italic>, 2019</xref>) with variational autoencoders (VAEs) (<xref rid="btab702-B8" ref-type="bibr">Kingma <italic toggle="yes">et al.</italic>, 2013</xref>) (a powerful model of DGMs), achieving SOTA in both representation learning and language generation tasks when trained on two million Wikipedia sentences. However, the data distributions between general and biomedical domain are different, which makes it challenging to apply these models directly to biomedical text mining tasks (<xref rid="btab702-B10" ref-type="bibr">Lee <italic toggle="yes">et al.</italic>, 2020</xref>). In addition, training such large-scale models on a massive amount of biomedical text is costly (<xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix</xref> SF).</p>
    <p>To leverage the advantages of VAE-based PLMs for biomedical text mining, we release BioVAE, the first large-scale pre-trained latent variable language model for biomedical texts. The model is trained using the OPTIMUS framework on 34 million sentences from PubMed articles. We evaluate our BioVAE model on downstream text mining tasks, i.e. NER, REL and QA, and achieve SOTA on most of the tasks when compared with previous powerful biomedical PLMs, i.e. BioBERT (<xref rid="btab702-B10" ref-type="bibr">Lee <italic toggle="yes">et al.</italic>, 2020</xref>), SciBERT (<xref rid="btab702-B1" ref-type="bibr">Beltagy <italic toggle="yes">et al.</italic>, 2019</xref>) and PubMedBERT (<xref rid="btab702-B5" ref-type="bibr">Gu <italic toggle="yes">et al.</italic>, 2020</xref>). For language generation, BioVAE generates more accurate biomedical sentences than the original OPTIMUS output.</p>
  </sec>
  <sec>
    <title>2 Approach</title>
    <p><bold><italic toggle="yes">OPTIMUS</italic></bold><italic toggle="yes">:</italic> The OPTIMUS framework (<xref rid="btab702-B11" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2020</xref>) is a large-scale VAE-based language model. VAE defines a joint distribution of observed inputs <italic toggle="yes">x</italic> and latent variables <italic toggle="yes">z</italic> with unknown prior distributions <italic toggle="yes">p</italic>(<italic toggle="yes">z</italic>). The objective is to maximize the <italic toggle="yes">Evidence Lower Bound</italic> (ELBO):
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≥</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mo>ϕ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mo>ϕ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is known as decoder, and <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mo>ϕ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is known as encoder.</p>
    <p><bold><italic toggle="yes">BioVAE</italic></bold><italic toggle="yes">:</italic> We used the OPTIMUS framework with the same configurations to train a large-scale VAE language model on biomedical data. We initialize the encoder with the biomedical pre-trained SciBERT (<xref rid="btab702-B1" ref-type="bibr">Beltagy <italic toggle="yes">et al.</italic>, 2019</xref>) and the decoder with the pre-trained GPT-2 (<xref rid="btab702-B15" ref-type="bibr">Radford <italic toggle="yes">et al.</italic>, 2019</xref>). We illustrate our model in <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix</xref> SA.</p>
    <p><italic toggle="yes">Corpus:</italic> We train BioVAE on the latest biomedical abstracts from the PubMed 2021 Baseline (<ext-link xlink:href="https://ftp.ncbi.nlm.nih.gov/pubmed/baseline/" ext-link-type="uri">https://ftp.ncbi.nlm.nih.gov/pubmed/baseline/</ext-link>). Our data contain 34M sentences (3.35M abstracts).</p>
    <p><bold><italic toggle="yes">Settings</italic></bold><italic toggle="yes">:</italic> We follow the same settings used in OPTIMUS. We set the latent size as 32 and 768, and beta as 0.0 and 0.5. For training on large batch sizes, we used the LAMB optimizer (<xref rid="btab702-B16" ref-type="bibr">You <italic toggle="yes">et al.</italic>, 2020</xref>). We used 128 GPUs from the AI Bridging Cloud Infrastructure (ABCI <ext-link xlink:href="https://abci.ai/" ext-link-type="uri">https://abci.ai/</ext-link>), which take 3 days to train 34M sentences for one epoch.</p>
  </sec>
  <sec>
    <title>3 Results</title>
    <p><bold><italic toggle="yes">Tasks</italic></bold><italic toggle="yes">:</italic> The pre-trained BioVAE model is evaluated on three NER tasks, i.e. BC5CDR (<xref rid="btab702-B12" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2016</xref>), JNLPBA (<xref rid="btab702-B7" ref-type="bibr">Kim <italic toggle="yes">et al.</italic>, 2004</xref>) and NCBI-disease (<xref rid="btab702-B4" ref-type="bibr">Doğan <italic toggle="yes">et al.</italic>, 2014</xref>); a REL task, i.e. ChemProt (<xref rid="btab702-B9" ref-type="bibr">Kringelum et al., 2016</xref>); and a QA task, i.e. BioASQ (<xref rid="btab702-B14" ref-type="bibr">Nentidis <italic toggle="yes">et al.</italic>, 2019</xref>). We follow the same evaluation settings used in <xref rid="btab702-B10" ref-type="bibr">Lee <italic toggle="yes">et al.</italic> (2020)</xref> and <xref rid="btab702-B1" ref-type="bibr">Beltagy <italic toggle="yes">et al.</italic> (2019)</xref>.</p>
    <p><bold><italic toggle="yes">Fine-tuning</italic></bold><italic toggle="yes">:</italic> We follow the same settings used in our baseline SciBERT model (<xref rid="btab702-B1" ref-type="bibr">Beltagy <italic toggle="yes">et al.</italic>, 2019</xref>). The final BERT vectors from the encoder of our pre-trained BioVAE model are fed into a classification layer. The pre-trained model is fine-tuned for 2–5 epochs with a batch size of 32 and a learning rate of 2e-5, similarly to SciBERT’s tuning parameters. For QA, we follow the BioBERT settings and evaluation scripts.</p>
    <p><bold><italic toggle="yes">Results</italic></bold><italic toggle="yes">:</italic>  <xref rid="btab702-T1" ref-type="table">Table  1</xref> compares our BioVAE with biomedical pre-trained SciBERT (<xref rid="btab702-B1" ref-type="bibr">Beltagy <italic toggle="yes">et al.</italic>, 2019</xref>), BioBERT (<xref rid="btab702-B10" ref-type="bibr">Lee <italic toggle="yes">et al.</italic>, 2020</xref>) and PubMedBERT (<xref rid="btab702-B5" ref-type="bibr">Gu <italic toggle="yes">et al.</italic>, 2020</xref>) models on the NER, REL and QA tasks. Our baseline is the SciBERT since we use this model to initialize the encoder. BioVAE outperforms the SciBERT on all tasks, i.e. +0.54 F1 (JNLPBA), +0.17 F1 (BC5CDR), +1.55 F1 (for NCBI-disease) and +0.85 F1 (for ChemProt); and +3.57 accuracy (for QA) compared with BioBERT. BioVAE scores are lower than PubMedBERT in REL and QA, but better in NER tasks, and we discuss the reasons in more details in <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix</xref> SC.</p>
    <table-wrap position="float" id="btab702-T1">
      <label>Table 1.</label>
      <caption>
        <p>Results on the text mining test sets</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col valign="top" align="left" span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th rowspan="2" colspan="1">Model</th>
            <th colspan="3" rowspan="1">NER<hr/></th>
            <th rowspan="2" colspan="1">REL</th>
            <th rowspan="2" colspan="1">QA</th>
          </tr>
          <tr>
            <th rowspan="1" colspan="1">BC5CDR</th>
            <th rowspan="1" colspan="1">NCBI</th>
            <th rowspan="1" colspan="1">JNLPBA</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="1" colspan="1">PubMedBERT (<xref rid="btab702-B5" ref-type="bibr">Gu <italic toggle="yes">et al.</italic>, 2020</xref>)</td>
            <td rowspan="1" colspan="1">87.27</td>
            <td rowspan="1" colspan="1">79.96</td>
            <td rowspan="1" colspan="1">71.82</td>
            <td rowspan="1" colspan="1">
              <bold>85.47</bold>
            </td>
            <td rowspan="1" colspan="1">
              <bold>75.00</bold>
            </td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">BioBERT (<xref rid="btab702-B10" ref-type="bibr">Lee <italic toggle="yes">et al.</italic>, 2020</xref>)</td>
            <td rowspan="1" colspan="1">88.85</td>
            <td rowspan="1" colspan="1">89.36</td>
            <td rowspan="1" colspan="1">77.59</td>
            <td rowspan="1" colspan="1">76.68</td>
            <td rowspan="1" colspan="1">69.29</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">SCIBERT (<xref rid="btab702-B1" ref-type="bibr">Beltagy <italic toggle="yes">et al.</italic>, 2019</xref>)</td>
            <td rowspan="1" colspan="1">90.01</td>
            <td rowspan="1" colspan="1">88.57</td>
            <td rowspan="1" colspan="1">77.28</td>
            <td rowspan="1" colspan="1">83.64</td>
            <td rowspan="1" colspan="1">72.14</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">BioVAE (<inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:mo>β</mml:mo><mml:mo>=</mml:mo><mml:mn>0.0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>32</mml:mn></mml:mrow></mml:math></inline-formula>)</td>
            <td rowspan="1" colspan="1">89.85</td>
            <td rowspan="1" colspan="1">
              <underline>88.85</underline>
            </td>
            <td rowspan="1" colspan="1">
              <bold>
                <underline>77.82</underline>
              </bold>
            </td>
            <td rowspan="1" colspan="1">
              <underline>83.68</underline>
            </td>
            <td rowspan="1" colspan="1">
              <underline>72.86</underline>
            </td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">BioVAE (<inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:mo>β</mml:mo><mml:mo>=</mml:mo><mml:mn>0.0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>768</mml:mn></mml:mrow></mml:math></inline-formula>)</td>
            <td rowspan="1" colspan="1">
              <underline>90.10</underline>
            </td>
            <td rowspan="1" colspan="1">88.12</td>
            <td rowspan="1" colspan="1">
              <underline>77.69</underline>
            </td>
            <td rowspan="1" colspan="1">83.05</td>
            <td rowspan="1" colspan="1">72.14</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">BioVAE (<inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:mo>β</mml:mo><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>32</mml:mn></mml:mrow></mml:math></inline-formula>)</td>
            <td rowspan="1" colspan="1">89.69</td>
            <td rowspan="1" colspan="1">
              <underline>89.80</underline>
            </td>
            <td rowspan="1" colspan="1">
              <underline>77.66</underline>
            </td>
            <td rowspan="1" colspan="1">83.54</td>
            <td rowspan="1" colspan="1">72.14</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">BioVAE (<inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:mo>β</mml:mo><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>768</mml:mn></mml:mrow></mml:math></inline-formula>)</td>
            <td rowspan="1" colspan="1">
              <bold>
                <underline>90.18</underline>
              </bold>
            </td>
            <td rowspan="1" colspan="1">
              <bold>
                <underline>90.12</underline>
              </bold>
            </td>
            <td rowspan="1" colspan="1">
              <underline>77.57</underline>
            </td>
            <td rowspan="1" colspan="1">
              <underline>84.49</underline>
            </td>
            <td rowspan="1" colspan="1">
              <underline>72.86</underline>
            </td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="tblfn1">
          <p><italic toggle="yes">Note</italic>: The best scores are in bold, and the scores outperforming the SciBERT baseline are underlined. We report macro F1 scores for NER, micro F1 for REL and accuracy for QA (<italic toggle="yes">d<sub>z</sub></italic>: latent size).</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <p><italic toggle="yes">Text generation:</italic> Given an input sequence, our model can reconstruct the input sequence. We compare sentences that have been reconstructed by both our BioVAE and OPTIMUS models in <xref rid="btab702-T2" ref-type="table">Table  2</xref>. The table shows that sentences generated by BioVAE are more accurate than the original OPTIMUS output. Further samples are presented in <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix</xref> SB.</p>
    <table-wrap position="float" id="btab702-T2">
      <label>Table 2.</label>
      <caption>
        <p>Reconstruction samples generated by OPTIMUS and our BioVAE and corresponding perplexity scores (the lower score is better)</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col valign="top" align="left" span="1"/>
          <col valign="top" align="left" span="1"/>
          <col valign="top" align="left" span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th rowspan="1" colspan="1">Input</th>
            <th align="left" rowspan="1" colspan="1">BioVAE</th>
            <th align="left" rowspan="1" colspan="1">OPTIMUS</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="1" colspan="1">Sequence analysis of CDC4/FBXW7 was carried out on gastric <bold>carcinoma</bold> cell lines and xenografts</td>
            <td rowspan="1" colspan="1">Sequence analysis of CDC4/FBXW7 was carried out on gastric <bold>cancer</bold> cell lines and xenografts</td>
            <td rowspan="1" colspan="1">Electrophysiological studies were performed in the brain and cerebrospinal fluid (CSF)</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Perplexity = 1.000</td>
            <td rowspan="1" colspan="1">Perplexity = 1.113</td>
            <td rowspan="1" colspan="1">Perplexity = 3.534</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p>We have described BioVAE, the first large-scale pre-trained latent variable language model for the biomedical domain. The model is trained using the OPTIMUS framework on large volumes of biomedical text. We achieve SOTA when evaluating the model on text mining tasks such as NER, REL and QA. Our results provide strong evidence that it will be possible to apply the BioVAE model to further biomedical tasks in the future.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btab702_Supplementary_Data</label>
      <media xlink:href="btab702_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgement</title>
    <p>The authors thank Khoa N. A. Duong for the invaluable support in implementing and evaluating the models.</p>
    <sec>
      <title>Funding</title>
      <p>This paper is based on results obtained from a project, JPNP20006, commissioned by the New Energy and Industrial Technology Development Organization (NEDO). This research is also supported by the Alan Turing Institute and BBSRC, Japan Partnering Award, BB/P025684/1.</p>
      <p><italic toggle="yes">Conflict of Interest</italic>: none declared. </p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btab702-B1">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Beltagy</surname><given-names>I.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) SciBERT: a pretrained language model for scientific text. In: <italic toggle="yes">EMNLP-IJCNLP</italic>. ACL, Hong Kong, China, pp. <fpage>3606</fpage>–<lpage>3611</lpage>.</mixed-citation>
    </ref>
    <ref id="btab702-B2">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bowman</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) Generating sentences from a continuous space. In: <italic toggle="yes">CONLL, Berlin, Germany</italic>, pp. <fpage>10</fpage>–<lpage>21</lpage>.</mixed-citation>
    </ref>
    <ref id="btab702-B3">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Devlin</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) BERT: pre-training of deep bidirectional transformers for language understanding. In: <italic toggle="yes">NAACL, Minneapolis</italic>, pp. <fpage>4171</fpage>–<lpage>4186</lpage>.</mixed-citation>
    </ref>
    <ref id="btab702-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Doğan</surname><given-names>R.I.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) <article-title>NCBI disease corpus: a resource for disease name recognition and concept normalization</article-title>. <source>Biomed. Inf</source>., <volume>47</volume>, <fpage>1</fpage>–<lpage>10</lpage>.</mixed-citation>
    </ref>
    <ref id="btab702-B5">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Gu</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) Domain-specific language model pretraining for biomedical natural language processing. <italic toggle="yes">arXiv</italic>, <italic toggle="yes">preprint arXiv:2007.15779.</italic></mixed-citation>
    </ref>
    <ref id="btab702-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>A neural named entity recognition and multi-type normalization tool for biomedical text mining</article-title>. <source>IEEE Access</source>, <volume>7</volume>, <fpage>73729</fpage>–<lpage>73740</lpage>.</mixed-citation>
    </ref>
    <ref id="btab702-B7">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kim</surname><given-names>J.-D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2004</year>) Introduction to the bio-entity recognition task at JNLPBA. In: <italic toggle="yes">NLPBA/BioNLP</italic>, COLING, Geneva, Switzerland, pp. <fpage>70</fpage>–<lpage>75</lpage>.</mixed-citation>
    </ref>
    <ref id="btab702-B8">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kingma</surname><given-names>D.P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) Auto-encoding variational Bayes. In: <italic toggle="yes">ICLR</italic>, Banff, Canada.</mixed-citation>
    </ref>
    <ref id="btab702-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kringelum</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) <article-title>Chemprot-3.0: a global chemical biology diseases mapping</article-title>. <source>Database</source>, <volume>2016</volume>, <fpage>bav123</fpage>.<pub-id pub-id-type="pmid">26876982</pub-id></mixed-citation>
    </ref>
    <ref id="btab702-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>BioBERT: a pre-trained biomedical language representation model for biomedical text mining</article-title>. <source>Bioinformatics</source>, <fpage>36:1234-1240</fpage>.<pub-id pub-id-type="pmid">32000657</pub-id></mixed-citation>
    </ref>
    <ref id="btab702-B11">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) Optimus: organizing sentences via pre-trained modeling of a latent space. In: <italic toggle="yes">EMNLP</italic>, ACL. pp. <fpage>4678</fpage>–<lpage>4699</lpage>.</mixed-citation>
    </ref>
    <ref id="btab702-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) <article-title>Biocreative V CDR task corpus: a resource for chemical disease relation extraction</article-title>. <source>Database</source>, <volume>2016</volume>, <fpage>baw068</fpage>.<pub-id pub-id-type="pmid">27161011</pub-id></mixed-citation>
    </ref>
    <ref id="btab702-B13">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>A Bert-based universal model for both within-and cross-sentence clinical temporal relation extraction</article-title>. In: <italic toggle="yes">Clinical NLP Workshop</italic>, pp. <fpage>65</fpage>–<lpage>71</lpage>.</mixed-citation>
    </ref>
    <ref id="btab702-B14">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Nentidis</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Results of the seventh edition of the bioasq challenge</article-title>. In: <italic toggle="yes">ECMLPKDD</italic>, Springer, Würzburg, Germany, pp. <fpage>553</fpage>–<lpage>568</lpage>.</mixed-citation>
    </ref>
    <ref id="btab702-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Radford</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Language models are unsupervised multitask learners</article-title>. <source>OpenAI Blog</source>.</mixed-citation>
    </ref>
    <ref id="btab702-B16">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>You</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Large batch optimization for deep learning: training Bert in 76 minutes</article-title>. In: <italic toggle="yes">ICLR</italic>.</mixed-citation>
    </ref>
  </ref-list>
</back>
