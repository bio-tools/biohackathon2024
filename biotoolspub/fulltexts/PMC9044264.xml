<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PeerJ Comput Sci</journal-id>
    <journal-id journal-id-type="iso-abbrev">PeerJ Comput Sci</journal-id>
    <journal-id journal-id-type="publisher-id">peerj-cs</journal-id>
    <journal-title-group>
      <journal-title>PeerJ Computer Science</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2376-5992</issn>
    <publisher>
      <publisher-name>PeerJ Inc.</publisher-name>
      <publisher-loc>San Diego, USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9044264</article-id>
    <article-id pub-id-type="publisher-id">cs-902</article-id>
    <article-id pub-id-type="doi">10.7717/peerj-cs.902</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Artificial Intelligence</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Computer Vision</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Data Science</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Emerging Technologies</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Optimization Theory and Computation</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DMPNet: densely connected multi-scale pyramid networks for crowd counting</article-title>
    </title-group>
    <contrib-group>
      <contrib id="author-1" contrib-type="author" corresp="yes">
        <name>
          <surname>Li</surname>
          <given-names>Pengfei</given-names>
        </name>
        <email>lipf@hdu.edu.cn</email>
        <xref rid="aff-1" ref-type="aff"/>
      </contrib>
      <contrib id="author-2" contrib-type="author" corresp="yes">
        <name>
          <surname>Zhang</surname>
          <given-names>Min</given-names>
        </name>
        <email>hz_andy@163.com</email>
        <xref rid="aff-1" ref-type="aff"/>
      </contrib>
      <contrib id="author-3" contrib-type="author">
        <name>
          <surname>Wan</surname>
          <given-names>Jian</given-names>
        </name>
        <xref rid="aff-1" ref-type="aff"/>
      </contrib>
      <contrib id="author-4" contrib-type="author">
        <name>
          <surname>Jiang</surname>
          <given-names>Ming</given-names>
        </name>
        <xref rid="aff-1" ref-type="aff"/>
      </contrib>
      <aff id="aff-1"><institution>Computer &amp; Software School, Hangzhou Dianzi University</institution>, <city>Hangzhou</city>, <state>Zhejiang</state>, <country>China</country></aff>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Raza</surname>
          <given-names>Khalid</given-names>
        </name>
      </contrib>
    </contrib-group>
    <pub-date pub-type="epub" date-type="pub" iso-8601-date="2022-03-18">
      <day>18</day>
      <month>3</month>
      <year iso-8601-date="2022">2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>8</volume>
    <elocation-id>e902</elocation-id>
    <history>
      <date date-type="received" iso-8601-date="2021-10-29">
        <day>29</day>
        <month>10</month>
        <year iso-8601-date="2021">2021</year>
      </date>
      <date date-type="accepted" iso-8601-date="2022-02-07">
        <day>7</day>
        <month>2</month>
        <year iso-8601-date="2022">2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>©2022 Li et al.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Li et al.</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, reproduction and adaptation in any medium and for any purpose provided that it is properly attributed. For attribution, the original author(s), title, publication source (PeerJ Computer Science) and either DOI or URL of the article must be cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="https://peerj.com/articles/cs-902"/>
    <abstract>
      <p>Crowd counting has been widely studied by deep learning in recent years. However, due to scale variation caused by perspective distortion, crowd counting is still a challenging task. In this paper, we propose a Densely Connected Multi-scale Pyramid Network (DMPNet) for count estimation and the generation of high-quality density maps. The key component of our network is the Multi-scale Pyramid Network (MPN), which can extract multi-scale features of the crowd effectively while keeping the resolution of the input feature map and the number of channels unchanged. To increase the information transfer between the network layer, we used dense connections to connect multiple MPNs. In addition, we also designed a novel loss function, which can help our model achieve better convergence. To evaluate our method, we conducted extensive experiments on three challenging benchmark crowd counting datasets. Experimental results show that compared with the state-of-the-art algorithms, DMPNet performs well in both parameters and results. The code is available at: <ext-link xlink:href="https://github.com/lpfworld/DMPNet" ext-link-type="uri">https://github.com/lpfworld/DMPNet</ext-link>.</p>
    </abstract>
    <kwd-group kwd-group-type="author">
      <kwd>Crowd counting</kwd>
      <kwd>Density map</kwd>
      <kwd>Multi-scale</kwd>
      <kwd>Pyramid convolution</kwd>
      <kwd>Group convolution</kwd>
    </kwd-group>
    <funding-group>
      <award-group id="fund-1">
        <funding-source>Zhejiang Provincial Technical Plan Project</funding-source>
        <award-id>No. 2020C03105</award-id>
        <award-id>2021C01129</award-id>
      </award-group>
      <award-group id="fund-2">
        <funding-source>Xiaoshan District Science and Technology Plan Project</funding-source>
        <award-id>No. 2020102</award-id>
      </award-group>
      <funding-statement>This work is supported by the Zhejiang Provincial Technical Plan Project (No. 2020C03105, 2021C01129), and the Xiaoshan District Science and Technology Plan Project (No. 2020102). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro">
    <title>Introduction</title>
    <p>With the increase of the world population, crowd counting has been widely applied in video surveillance, crowd analysis, sporting events, and other public security services (<xref rid="ref-4" ref-type="bibr">Chan, Liang &amp; Vasconcelos, 2008</xref>; <xref rid="ref-2" ref-type="bibr">Boominathan, Kruthiventi &amp; Babu, 2016</xref>; <xref rid="ref-3" ref-type="bibr">Cao et al., 2018</xref>; <xref rid="ref-44" ref-type="bibr">Xiong et al., 2019</xref>). In addition, it has been extended to cell or bacterial counts in the medical field and vehicle counts in transportation field (<xref rid="ref-43" ref-type="bibr">Xie, Noble &amp; Zisserman, 2018</xref>; <xref rid="ref-13" ref-type="bibr">Hu et al., 2020</xref>). However, crowd counting still is a challenging task due to scale variations, cluttered backgrounds, and heavy occlusion. Among these challenges scale variation is the most important research issue, as shown in <xref rid="fig-1" ref-type="fig">Fig. 1</xref>.</p>
    <fig position="float" id="fig-1">
      <object-id pub-id-type="doi">10.7717/peerjcs.902/fig-1</object-id>
      <label>Figure 1</label>
      <caption>
        <title>Different scales of heads exist in crowd counting datasets.</title>
        <p>The first row shows samples of crowd images, The second row shows corresponding ground truth density maps. The samples are from ShanghaiTech Part A and Part B (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>).</p>
      </caption>
      <graphic xlink:href="peerj-cs-08-902-g001" position="float"/>
    </fig>
    <p>Convolutional Neural Network (CNN)-based methods have made remarkable progress in crowd counting in recent years. To extract multi-scale features of crowds, researchers designed multi-column or multi-branch networks (<xref rid="ref-32" ref-type="bibr">Sam, Surya &amp; Babu, 2016</xref>; <xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>; <xref rid="ref-25" ref-type="bibr">Liu, Salzmann &amp; Fua, 2019</xref>; <xref rid="ref-18" ref-type="bibr">Jiang et al., 2020</xref>). However, most networks are limited in their ability to extract multi-scale features due to the similarity of different columns or branches (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>; <xref rid="ref-32" ref-type="bibr">Sam, Surya &amp; Babu, 2016</xref>). In addition, multi-scale extraction modules in these networks require a lot of computation because of the complexity of the network structure (<xref rid="ref-22" ref-type="bibr">Li, Zhang &amp; Chen, 2018</xref>; <xref rid="ref-12" ref-type="bibr">Guo et al., 2019</xref>). Our MPN also adopts a multi-branch structure to ensure multi-scale feature extraction, in which pyramid convolution and group convolution are used to effectively reduce parameters.</p>
    <p>The higher resolution feature map contains finer details and the resulting density map is of higher quality, which is helpful for count estimation (<xref rid="ref-3" ref-type="bibr">Cao et al., 2018</xref>; <xref rid="ref-40" ref-type="bibr">Wan &amp; Chan, 2019</xref>). To increase receptive fields of networks, pooling operations are adopted. However, the resolution of feature maps generated by the network become smaller, resulting in the loss of crowd image details. To keep the input and output resolutions unchanged, the encoder–decoder structure is usually utilized (<xref rid="ref-17" ref-type="bibr">Jiang et al., 2019</xref>; <xref rid="ref-29" ref-type="bibr">Thanasutives et al., 2021</xref>). The network of encoder–decoder structure uses encoder to extract input image features and combine them, and then decodes the higher-level features required by these features through a specially designed decoder. Take M-SFANet (Multi-Scale-Aware Fusion Network with Attention mechanism) (<xref rid="ref-29" ref-type="bibr">Thanasutives et al., 2021</xref>) for example, the encoder of M-SFANet (<xref rid="ref-29" ref-type="bibr">Thanasutives et al., 2021</xref>) is enhanced with ASSP (Atrous Spatial Pyramid Pooling, ASSP) (<xref rid="ref-6" ref-type="bibr">Chen et al., 2017</xref>), which can extract multi-scale features of the target object and fuse large context information. In order to further deal with the scale variation of the input image, they used the context module called CAN (Context Aware Network, CAN) (<xref rid="ref-25" ref-type="bibr">Liu, Salzmann &amp; Fua, 2019</xref>) as the decoder. Similar to these works, we keep the input and output resolutions of MPN unchanged to ensure that the final density map generated by DMPNet contains sufficient detailed crowd information.</p>
    <p>Different layers of neural network contain different crowd information, but with the increase of network depth, some details are gradually lost. DSNet (Dense Scale Network, DSNet) (<xref rid="ref-7" ref-type="bibr">Dai et al., 2021</xref>) proposed that using dense connected networks in the field of crowd counting can effectively extract long-distance context information and maximize the retention of network layer information. We follow this operation and connect MPNs with dense connections.</p>
    <p>Euclidean loss is the most common loss function in crowd counting SOTA methods, which is based on pixel independence (<xref rid="ref-3" ref-type="bibr">Cao et al., 2018</xref>; <xref rid="ref-24" ref-type="bibr">Liu et al., 2020</xref>; <xref rid="ref-45" ref-type="bibr">Zhang et al., 2020</xref>). However, texture features and pixel correlation of different regions in crowd images are different. Euclidean loss ignores the local correlation of the crowd image and does not consider the global counting error of the crowd image (<xref rid="ref-3" ref-type="bibr">Cao et al., 2018</xref>; <xref rid="ref-7" ref-type="bibr">Dai et al., 2021</xref>). Therefore, when designing the loss function, we not only consider the local density consistency of the image, but also consider the global counting loss of the image.</p>
    <p>In this paper, we propose the densely connected Multi-scale <xref rid="fig-2" ref-type="fig">Fig. 2</xref>. The important component Multi-scale Pyramid Network (MPN) consists of Local Pyramid Network (LPN), Global Pyramid Network (GPN), and Multi-scale Feature Fusion Network (MFFN). LPN is used to capture small heads and extract multi-scale fine-grained features, while GPN is used to capture large heads and global features. They are composed of multiple levels, and each level has filters of different sizes and depths, whose output local and global features are combined by MFFN. To maximize the flow of information between layers of the network, MPNs in the network are densely connected, with each MPN receiving as input the results of MPNs before it. To optimize the loss function, we combine Euclidean loss, density level consistency loss, and MAE loss to improve the performance of DMPNet. Experiments on three datasets (ShanghaiTech Part A and Part B, <xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>; UCF-QNRF, <xref rid="ref-16" ref-type="bibr">Idrees et al., 2018</xref>; UCF_CC_50, <xref rid="ref-15" ref-type="bibr">Idrees et al., 2013</xref>) prove the effectiveness and robustness of the proposed method.</p>
    <fig position="float" id="fig-2">
      <object-id pub-id-type="doi">10.7717/peerjcs.902/fig-2</object-id>
      <label>Figure 2</label>
      <caption>
        <title>The architecture of DMPNet for crowd counting and high-quality density map.</title>
        <p>It contains VGG16 (<xref rid="ref-35" ref-type="bibr">Simonyan &amp; Zisserman, 2014</xref>) as the front-end network and three MPNs stacked by dense connections as the back-end network. MPN is composed of LPN, GPN and MFFN. It is used to extract human head features at different scales, and the resolution and channel number of input feature maps remain unchanged. The samples are from ShanghaiTech Part B (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>).</p>
      </caption>
      <graphic xlink:href="peerj-cs-08-902-g002" position="float"/>
    </fig>
  </sec>
  <sec>
    <title>Related Work</title>
    <p>Generally, the existing crowd counting methods can be mainly classified into two categories: traditional methods and CNN-based methods (<xref rid="ref-36" ref-type="bibr">Sindagi &amp; Patel, 2017a</xref>; <xref rid="ref-37" ref-type="bibr">Sindagi &amp; Patel, 2017b</xref>; <xref rid="ref-11" ref-type="bibr">Gao et al., 2020</xref>). In this section, we give a brief review of crowd counting methods and explain the differences between our methods.</p>
    <sec>
      <title>Traditional methods</title>
      <p>In early studies, detection-based methods used sliding windows to detect the target, and manually extract features of the human body or specific body parts (<xref rid="ref-42" ref-type="bibr">Wu &amp; Nevatia, 2007</xref>; <xref rid="ref-9" ref-type="bibr">Enzweiler &amp; Gavrila, 2009</xref>; <xref rid="ref-10" ref-type="bibr">Felzenszwalb et al., 2010</xref>). However, even if only heads or smaller body parts of pedestrians are detected, these methods often fail to make accurate counts of dense crowd scenes due to occlusion and illumination. To improve the performance of crowd counting, feature-based regression methods attempted to extract various features from local image blocks and generate low-level information (<xref rid="ref-5" ref-type="bibr">Chan &amp; Vasconcelos, 2009</xref>; <xref rid="ref-31" ref-type="bibr">Ryan et al., 2009</xref>; <xref rid="ref-19" ref-type="bibr">Ke et al., 2012</xref>). <xref rid="ref-15" ref-type="bibr">Idrees et al. (2013)</xref> tried to fuse the features obtained by Fourier analysis and Scale-invariant feature transform (SIFT) interest points. However, they ignored the scale information. To overcome the problem, density estimation-based method considers the relationship between image features and data regression. <xref rid="ref-21" ref-type="bibr">Lempitsky &amp; Zisserman (2010)</xref> adopted the method of extracting features in local areas and establishing linear mapping between features and density maps. <xref rid="ref-28" ref-type="bibr">Pham et al. (2015)</xref> tried to use random forest regression to get a nonlinear map.</p>
    </sec>
    <sec>
      <title>CNN-based methods</title>
      <p>The CNN-based methods can be classified into the multi-column CNN-based methods and the single-column CNN-based methods. The multi-column CNN-based methods use multi-column networks to extract the human head features of different scales and then fuse them to generate density maps. <xref rid="ref-46" ref-type="bibr">Zhang et al. (2016)</xref> (Multi-Column Convolutional Neural Network, MCNN) proposed to extract features using three-column networks with convolution kernels of different sizes respectively, and fused them through 1×1 convolution. <xref rid="ref-32" ref-type="bibr">Sam, Surya &amp; Babu (2016)</xref> (Switching Convolutional Neural Network, Switch-CNN) proposed to design an additional switch based on MCNN, that is to use the switch to select the most appropriate CNN column for different input images to improve the counting accuracy. Inspired by the image generation methods, <xref rid="ref-39" ref-type="bibr">Viresh, Le &amp; Hoai (2018)</xref> (Iterative Crowd Counting CNN, ic-CNN) proposed a two-column networks to gradually refine the obtained low-resolution density map to high-resolution density map. <xref rid="ref-37" ref-type="bibr">Sindagi &amp; Patel (2017b)</xref> (Contextual Pyramid CNN, CP-CNN) used global and local feature information to generate density maps for crowd images. <xref rid="ref-45" ref-type="bibr">Zhang et al. (2020)</xref> Relational Attention Network (RANet) proposed to use the stacked hourglass structure in human pose, optimized outputs from each hourglass module with local attention LSA and global attention GSA, and then fused the two features with a relational module. <xref rid="ref-47" ref-type="bibr">Zhu et al. (2019)</xref> (Multi-Scale Fusion Network with Attention mechanism, SFANet) proposed a dual path multi-scale fusion network architecture with attention mechanism, which contains a VGG as the front-end feature map extractor and a dual path multi-scale fusion networks as the back-end to generate density map. <xref rid="ref-18" ref-type="bibr">Jiang et al. (2020)</xref> (Attention Scaling Network, ASNet) proposed to use different columns to generate density maps and scale factors, then multiply them by the mask of the region of interest to output multiple attention-based density maps, and add the density maps to obtain a high-quality density map. These methods have a strong ability in extracting multi-scale features and improving the performance of crowd counting. However, they also have some disadvantages: these networks usually have a lot of parameters, and the similarity of networks with different columns results in limited feature extraction ability. In addition, training multiple CNNs at the same time will lead to slower training speed (<xref rid="ref-22" ref-type="bibr">Li, Zhang &amp; Chen, 2018</xref>; <xref rid="ref-3" ref-type="bibr">Cao et al., 2018</xref>; <xref rid="ref-18" ref-type="bibr">Jiang et al., 2020</xref>).</p>
      <p>The single-column CNN methods try to use the multi-branch structure for optimization, which can extract multi-scale information and effectively reduce parameters (<xref rid="ref-22" ref-type="bibr">Li, Zhang &amp; Chen, 2018</xref>; <xref rid="ref-3" ref-type="bibr">Cao et al., 2018</xref>; <xref rid="ref-25" ref-type="bibr">Liu, Salzmann &amp; Fua, 2019</xref>). <xref rid="ref-22" ref-type="bibr">Li, Zhang &amp; Chen (2018)</xref> (Congested Scene Recognition Network, CSRNet) proposed the network structure of the front and back end, in which the front-end network adopts VGG16 (<xref rid="ref-35" ref-type="bibr">Simonyan &amp; Zisserman, 2014</xref>), and the back-end network uses dilated convolution to increase the receptive field and extract multi-scale features. <xref rid="ref-3" ref-type="bibr">Cao et al. (2018)</xref> (Scale Aggregation Network, SANet) proposed to extract multi-scale features by using convolution containing multiple levels, and the convolution kernel of each level is different in size. At the back end of SANet, the resolution of the feature map is restored to the size of the input image by deconvolution, and the final density map is obtained. <xref rid="ref-25" ref-type="bibr">Liu, Salzmann &amp; Fua (2019)</xref> (Context Aware Network, CAN) proposed a pooling pyramid network to extract multi-scale features and adaptively assign weights to crowd regions of different scales in images. <xref rid="ref-34" ref-type="bibr">Shi et al. (2019)</xref> (Perspective-Aware CNN, PACNN) proposed a perspective-aware network, which can integrate the perspective information into density regression to provide additional knowledge of scale variations in images. <xref rid="ref-27" ref-type="bibr">Miao et al. (2020)</xref> (Shallow feature based Dense Attention Network, SDANet) proposed to reduce the influence of background by introducing an attentional model based on shallow features, and to capture multi-scale information through dense connections of hierarchical features. <xref rid="ref-29" ref-type="bibr">Thanasutives et al., (2021)</xref> (M-SFANet) proposed to use ASPP (<xref rid="ref-6" ref-type="bibr">Chen et al., 2017</xref>) containing parallel atrous convolutional layers with different sampling rates to enhance the network, which can extract multi-scale features of the target object and incorporate larger context. <xref rid="ref-17" ref-type="bibr">Jiang et al., (2019)</xref> (Trellis Encoder-Decoder Network, TEDNet) proposed to build multiple decoding paths in different coding stages to aggregate features of different layers. <xref rid="ref-26" ref-type="bibr">Ma et al. (2019)</xref> (Bayesian Loss, BL) regarded crowd counting as a probability problem, the predicted density map is a probability map, each point represents the probability of existence at the point, and each point of the density map is regarded as the sample observation value.</p>
      <p>Our DMPNet is a single-column network with multi-branch, similar to some works (<xref rid="ref-3" ref-type="bibr">Cao et al., 2018</xref>; <xref rid="ref-25" ref-type="bibr">Liu, Salzmann &amp; Fua, 2019</xref>; <xref rid="ref-7" ref-type="bibr">Dai et al., 2021</xref>). We differ them from three aspects: (1) Each branch of our convolution kernel is not only different in size, but also different in the number of channels, which improves the ability of feature extraction of similar networks. (2) We use group convolution to process convolution kernels of different sizes, effectively reducing network parameters, and the calculation process is similar to Google MixNet (Mixed Depthwise Convolutional Network, MixNet) (<xref rid="ref-38" ref-type="bibr">Tan &amp; Le, 2019</xref>). (3) Our DMPNet is an end-to-end architecture, without adding extra perspective maps or attention maps (<xref rid="ref-33" ref-type="bibr">Shi et al., 2018</xref>).</p>
    </sec>
  </sec>
  <sec sec-type="methods">
    <title>Methods</title>
    <p>The basic idea of our approach is to implement an end-to-end network that can capture multi-scale features and generate a high-quality density map, to achieve accurate crowd estimation. In this section, we first introduce our proposed DMPNet architecture, then present our loss function.</p>
    <sec>
      <title>DMPNet architecture</title>
      <p>Similar to CSRNet (<xref rid="ref-22" ref-type="bibr">Li, Zhang &amp; Chen, 2018</xref>), our DMPNet architecture includes a front-end network and a back-end network (see <xref rid="fig-2" ref-type="fig">Fig. 2</xref>). In the front-end network, the first ten layers with three pooling layers of VGG16 are used to extract features from crowd images. Several works have proved that VGG16 achieves a trade-off between accuracy and computation, and is suitable for crowd counting (<xref rid="ref-3" ref-type="bibr">Cao et al., 2018</xref>; <xref rid="ref-39" ref-type="bibr">Viresh, Le &amp; Hoai, 2018</xref>; <xref rid="ref-40" ref-type="bibr">Wan &amp; Chan, 2019</xref>). In the back-end network, MPNs that can extract multi-scale features are connected in a dense way to improve information flow between layers. The integration between the different layers in the network can also be further retained multi-scale features (<xref rid="ref-14" ref-type="bibr">Huang et al., 2016</xref>; <xref rid="ref-27" ref-type="bibr">Miao et al., 2020</xref>; <xref rid="ref-1" ref-type="bibr">Amaranageswarao, Deivalakshmi &amp; Ko, 2020</xref>). In ablation experiments, we demonstrated the effectiveness of dense connections.</p>
    </sec>
    <sec>
      <title>Multi-scale pyramid network (MPN)</title>
      <p>MPN consists of three parts: Local Pyramid Network (LPN), Global Pyramid Network (GPN), and Multi-scale Feature Fusion Network (MFFN), illustrated in <xref rid="fig-2" ref-type="fig">Fig. 2</xref>. The design principle of MPN is to keep the resolution and channel number of input and output features unchanged, and effectively extract multi-scale features.</p>
    </sec>
    <sec>
      <title>Pyramid convolution and group convolution</title>
      <p>Pyramid convolution has been applied to image segmentation, image classification and other fields, and achieved remarkable results (<xref rid="ref-23" ref-type="bibr">Lin et al., 2017</xref>; <xref rid="ref-8" ref-type="bibr">Duta et al., 2020</xref>; <xref rid="ref-41" ref-type="bibr">Wang et al., 2020</xref>; <xref rid="ref-30" ref-type="bibr">Richardson et al., 2020</xref>). Inspired by this, we propose to apply pyramid convolution to crowd counting. Compared to standard convolution, pyramid convolution is composed of convolution kernels of different sizes and depths in N level, without increasing computational cost and complexity, illustrated in <xref rid="fig-3" ref-type="fig">Fig. 3</xref>.</p>
      <fig position="float" id="fig-3">
        <object-id pub-id-type="doi">10.7717/peerjcs.902/fig-3</object-id>
        <label>Figure 3</label>
        <caption>
          <title>Compare the calculation process of standard convolution and pyramid convolution.</title>
          <p>In pyramid convolution, the input feature map is calculated with convolution kernels of different sizes, and then the obtained feature map is connected by channel as the output feature map. The size of convolution kernel is decreasing, and the depth of convolution kernel is increasing.</p>
        </caption>
        <graphic xlink:href="peerj-cs-08-902-g003" position="float"/>
      </fig>
      <p>Each level of pyramid convolution is computed with all input features. To use different depths of the kernels at each level of pyramid convolution, we do this using group convolution. The input features are divided into four groups, and the convolution kernels are applied separately for each input group, illustrated in <xref rid="fig-4" ref-type="fig">Fig. 4</xref>.</p>
      <fig position="float" id="fig-4">
        <object-id pub-id-type="doi">10.7717/peerjcs.902/fig-4</object-id>
        <label>Figure 4</label>
        <caption>
          <title>Compare the calculation process of standard convolution and group convolution.</title>
          <p>In grouping convolution, the input feature map is divided into N groups, and the convolution kernel is also divided into N groups accordingly. The calculation is carried out in the corresponding group. Each group will generate a feature map, and a total of N feature maps are generated.</p>
        </caption>
        <graphic xlink:href="peerj-cs-08-902-g004" position="float"/>
      </fig>
      <p>We compare the parameters of standard convolution and group convolution. (1) Standard convolution contains a single type of convolution kernel (with height K, width K), and the depth is equal to the number of channels of input features <italic toggle="yes">C</italic><sub>1</sub>.<italic toggle="yes">C</italic><sub>2</sub> such convolution kernels and input features (with height H, width W) are calculated to get output features (with height <italic toggle="yes">H</italic>′, width <italic toggle="yes">W</italic>′). Therefore, the parameter number of standard convolution is <italic toggle="yes">k</italic><sup>2</sup><italic toggle="yes">C</italic><sub>1</sub><italic toggle="yes">C</italic><sub>2</sub>. (2) Group Convolution divides the input feature map (with height H, width W) into <italic toggle="yes">g</italic> groups, the depth is equal to the number of channels of input features <italic toggle="yes">C</italic><sub>1</sub>, and then performs convolution calculation within each group. The convolution kernels (with height K, width K, and the number of channels <italic toggle="yes">C</italic><sub>2</sub>) are also divided into corresponding <italic toggle="yes">g</italic> groups. Each group of convolution generates feature maps (with height W’, width H’, and the number of channels <italic toggle="yes">C</italic><sub>2</sub>/<italic toggle="yes">g</italic>). Therefore, the parameter number of group convolution is <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-902-i001.jpg"/><tex-math id="tex-ieqn-11">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${k}^{2}\mathrm{ \ast } \left( \frac{{C}_{1}}{g} \right) \mathrm{ \ast } \left( \frac{{C}_{2}}{g} \right) \mathrm{ \ast }g={k}^{2}{C}_{1}{C}_{2}/g$\end{document}</tex-math><mml:math id="mml-ieqn-11" overflow="scroll"><mml:msup><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal"> ∗</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mi mathvariant="normal"> ∗</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mi mathvariant="normal"> ∗</mml:mi><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>g</mml:mi></mml:math></alternatives></inline-formula>. The width and height of the output depend on the convolution step size, and these two values are not considered here. The above calculation results prove that group convolution can generate feature maps with fewer parameters. The more feature maps, the more information that can be encoded for the crowd counting network.</p>
    </sec>
    <sec>
      <title>LPN, GPN, and MFFN</title>
      <p>Based on the ability of pyramid convolution and group convolution, we design LPN and MPN to extract local and global features of crowd images, and use MFFN to combine the two, as shown in <xref rid="fig-5" ref-type="fig">Fig. 5</xref>.</p>
      <p>(a) LPN is mainly used for fine-grained feature extraction. Detailed information is shown in <xref rid="fig-5" ref-type="fig">Fig. 5A</xref>. First, we use 1x1 convolution to reduce the channel of <italic toggle="yes">F</italic><sub><italic toggle="yes">I</italic></sub> to 512. Then, four-level pyramid convolution with different convolution kernels sizes (9 × 9, 7 × 7, 5 × 5, and 3 × 3) is used to extract multi-scale features. The corresponding channel number is 32,64,128,256, and the group convolution size is 16,8,4,1. Finally, we use 1x1 convolution to increase the channel numbers of the four-level features to 512, and the output feature <italic toggle="yes">F</italic><sub><italic toggle="yes">L</italic></sub> is obtained. All convolution operations are followed by BN and ReLU.</p>
      <p>(b) GPN is mainly used for coarse-grained feature extraction. Detailed information is shown in <xref rid="fig-5" ref-type="fig">Fig. 5B</xref>. The intermediate processing of GPN and LPN is the same, but the difference is that the input feature <italic toggle="yes">F</italic><sub><italic toggle="yes">I</italic></sub> first goes through a layer of 9x9 adaptive average pool to ensure that complete global information can be obtained. In addition, to restore the resolution of the output feature map, we use bilinear interpolation for up-sampling to obtain the final output <italic toggle="yes">F</italic><sub><italic toggle="yes">G</italic></sub>.</p>
      <p>(c) MFFN is mainly used for global and local feature fusion (fine-grain and coarse-grain features). Detailed information is shown in <xref rid="fig-5" ref-type="fig">Fig. 5C</xref>. First, the output of LPN and GPN is combined into the features with 1024 channels as the input of MFFN. Then, through a layer of 3×3 convolution output the features of 256 channels. Finally, we use 1×1 convolution to restore the channel numbers to 512 and obtain feature <italic toggle="yes">F</italic><sub><italic toggle="yes">o</italic></sub>.</p>
      <fig position="float" id="fig-5">
        <object-id pub-id-type="doi">10.7717/peerjcs.902/fig-5</object-id>
        <label>Figure 5</label>
        <caption>
          <title>Three main components of multi-scale pyramid network.</title>
          <p><italic toggle="yes">F</italic><sub><italic toggle="yes">I</italic></sub> is the input features of LPN and GPN. <italic toggle="yes">F</italic><sub><italic toggle="yes">L</italic></sub> and <italic toggle="yes">F</italic><sub><italic toggle="yes">G</italic></sub> are output features of LPN and GPN, respectively. <italic toggle="yes">F</italic><sub><italic toggle="yes">O</italic></sub> is output features of MFFN.</p>
        </caption>
        <graphic xlink:href="peerj-cs-08-902-g005" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>Loss function</title>
      <p>Euclidean loss is the most common loss function in crowd counting. It evaluates the difference between the ground truth and the estimated density map based on pixel independence, without considering the local density correlation of images. However, the local features of the crowd are generally consistent. In addition, Euclidean loss does not consider the counting error of the image (<xref rid="ref-3" ref-type="bibr">Cao et al., 2018</xref>; <xref rid="ref-7" ref-type="bibr">Dai et al., 2021</xref>). Therefore, we combine density-level consistency loss and MAE loss with Euclidean loss in the loss function.</p>
    </sec>
    <sec>
      <title>Euclidean loss</title>
      <p>Euclidean loss can estimate the pixel-level error between the estimated density map and the ground truth. It is the most common loss function in crowd counting. The Euclidean loss function can be defined as follow:</p>
      <p>
        <inline-formula>
          <alternatives>
            <inline-graphic xlink:href="peerj-cs-08-902-i002.jpg"/>
            <tex-math id="tex-ieqn-27">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${L}_{E}= \frac{1}{N} {\mathop{\sum }\nolimits }_{i=1}^{N}{|}{|}\mathrm{F}({X}_{i};\theta )-{F}_{i}{|}{\mathop{{|}}\nolimits }_{2}^{2}$\end{document}</tex-math>
            <mml:math id="mml-ieqn-27" overflow="scroll">
              <mml:msub>
                <mml:mrow>
                  <mml:mi>L</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>E</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>=</mml:mo>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>N</mml:mi>
                </mml:mrow>
              </mml:mfrac>
              <mml:msubsup>
                <mml:mrow>
                  <mml:mo>∑</mml:mo>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>i</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>N</mml:mi>
                </mml:mrow>
              </mml:msubsup>
              <mml:mo>|</mml:mo>
              <mml:mo>|</mml:mo>
              <mml:mi mathvariant="normal">F</mml:mi>
              <mml:mrow>
                <mml:mfenced separators="" open="(" close=")">
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>X</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>;</mml:mo>
                  <mml:mi>θ</mml:mi>
                </mml:mfenced>
              </mml:mrow>
              <mml:mo>−</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>F</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>i</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>|</mml:mo>
              <mml:msubsup>
                <mml:mrow>
                  <mml:mo>|</mml:mo>
                </mml:mrow>
                <mml:mrow>
                  <mml:mn>2</mml:mn>
                </mml:mrow>
                <mml:mrow>
                  <mml:mn>2</mml:mn>
                </mml:mrow>
              </mml:msubsup>
            </mml:math>
          </alternatives>
        </inline-formula>
      </p>
      <p>where N is the size of training batch, <italic toggle="yes">θ</italic> is the variable parameters of DMPNet. <italic toggle="yes">X</italic><sub><italic toggle="yes">i</italic></sub> is the input image, <italic toggle="yes">F</italic><sub><italic toggle="yes">i</italic></sub> represent the ground truth, and F(<italic toggle="yes">X</italic><sub><italic toggle="yes">i</italic></sub>; <italic toggle="yes">θ</italic>) is the output of DMPNet.</p>
    </sec>
    <sec>
      <title>Density level consistency loss</title>
      <p>Due to the imbalance of crowd distribution, the density map has a local correlation, and the density level of different sub-regions is not the same. Therefore, the density map generated by the model should be consistent with the ground truth (<xref rid="ref-40" ref-type="bibr"> Wan &amp; Chan, 2019</xref>; <xref rid="ref-18" ref-type="bibr">Jiang et al., 2020</xref>). Referring to the setting of reference (<xref rid="ref-7" ref-type="bibr">Dai et al., 2021</xref>), we divide the density map into sub-regions of different sizes and formed pool representations. Three outputs of different sizes are used (1 × 1, 2 × 2, 4 × 4), with 1 × 1 representing the global density level of the density map and the other two representing the density level of different local sizes in the density map. The density level consistency loss can be defined as follow: <disp-formula id="NONUM-d2e905"><alternatives><graphic xlink:href="peerj-cs-08-902-e001.jpg" position="float"/><tex-math id="tex-NONUM-d2e905">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{L}_{D}= \frac{1}{N} \sum _{i=1}^{N}\sum _{j=1}^{S} \frac{1}{{k}_{j}^{2}} {|}{|}{P}_{ave} \left( F \left( {X}_{i};\theta \right) ,{k}_{j} \right) -{P}_{ave}({D}_{i}^{GT},{k}_{j}){|}{{|}}_{1} \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-NONUM-d2e905" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo mathsize="big" movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mrow><mml:mo mathsize="big" movablelimits="false"> ∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>F</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></alternatives></disp-formula>
</p>
      <p>where <italic toggle="yes">S</italic> represents the number of scale levels, <italic toggle="yes">P</italic><sub><italic toggle="yes">ave</italic></sub> is the average pooling operation, and <italic toggle="yes">k</italic><sub><italic toggle="yes">j</italic></sub> represents the specified output size of average pooling.</p>
    </sec>
    <sec>
      <title>MAE loss</title>
      <p>Mean absolute error (MAE) loss can estimate the real count and the estimated count. The MAE loss can be defined as follow:</p>
      <p>
        <inline-formula>
          <alternatives>
            <inline-graphic xlink:href="peerj-cs-08-902-i003.jpg"/>
            <tex-math id="tex-ieqn-40">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${L}_{A}= \frac{1}{N} {\mathop{\sum }\nolimits }_{i=1}^{N}{|}\mathrm{C}({I}_{i})-{C}^{\mathrm{GT}}({I}_{i}^{{^{\prime}}}){|}$\end{document}</tex-math>
            <mml:math id="mml-ieqn-40" overflow="scroll">
              <mml:msub>
                <mml:mrow>
                  <mml:mi>L</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>A</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>=</mml:mo>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>N</mml:mi>
                </mml:mrow>
              </mml:mfrac>
              <mml:msubsup>
                <mml:mrow>
                  <mml:mo>∑</mml:mo>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>i</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>N</mml:mi>
                </mml:mrow>
              </mml:msubsup>
              <mml:mo>|</mml:mo>
              <mml:mi mathvariant="normal">C</mml:mi>
              <mml:mrow>
                <mml:mfenced separators="" open="(" close=")">
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>I</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mfenced>
              </mml:mrow>
              <mml:mo>−</mml:mo>
              <mml:msup>
                <mml:mrow>
                  <mml:mi>C</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi mathvariant="normal">GT</mml:mi>
                </mml:mrow>
              </mml:msup>
              <mml:mrow>
                <mml:mfenced separators="" open="(" close=")">
                  <mml:msubsup>
                    <mml:mrow>
                      <mml:mi>I</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>′</mml:mo>
                    </mml:mrow>
                  </mml:msubsup>
                </mml:mfenced>
              </mml:mrow>
              <mml:mo>|</mml:mo>
            </mml:math>
          </alternatives>
        </inline-formula>
      </p>
      <p>where <italic toggle="yes">I</italic><sub><italic toggle="yes">i</italic></sub> and <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-902-i004.jpg"/><tex-math id="tex-ieqn-42">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${I}_{i}^{{^{\prime}}}$\end{document}</tex-math><mml:math id="mml-ieqn-42" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> represent the density map generated by DMPNet and the real density map of <italic toggle="yes">X</italic><sub><italic toggle="yes">i</italic></sub> separately. <italic toggle="yes">C</italic> represents the sum of all pixels. C(<italic toggle="yes">I</italic><sub><italic toggle="yes">i</italic></sub>) and <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-902-i005.jpg"/><tex-math id="tex-ieqn-46">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${C}^{\mathrm{GT}} \left( {I}_{i}^{{^{\prime}}} \right) $\end{document}</tex-math><mml:math id="mml-ieqn-46" overflow="scroll"><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">GT</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math></alternatives></inline-formula> represent the estimated count and the real count of <italic toggle="yes">X</italic><sub><italic toggle="yes">i</italic></sub> separately.</p>
    </sec>
    <sec>
      <title>The final loss</title>
      <p>The final loss consists of <italic toggle="yes">L</italic><sub><italic toggle="yes">s</italic></sub>, <italic toggle="yes">L</italic><sub><italic toggle="yes">c</italic></sub>, and <italic toggle="yes">L</italic><sub><italic toggle="yes">E</italic></sub>. <italic toggle="yes">α</italic> and <italic toggle="yes">β</italic> are weighting factors of <italic toggle="yes">L</italic><sub><italic toggle="yes">s</italic></sub> and <italic toggle="yes">L</italic><sub><italic toggle="yes">c</italic></sub>. According to our experiments, they are set as 10-4 and 10-3, separately. <disp-formula id="NONUM-d2e1349"><alternatives><graphic xlink:href="peerj-cs-08-902-e002.jpg" position="float"/><tex-math id="tex-NONUM-d2e1349">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}L \left( \Theta \right) ={L}_{E}+a{L}_{D}+\beta {L}_{A}. \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-NONUM-d2e1349" overflow="scroll"><mml:mstyle displaystyle="true"><mml:mi>L</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:math></alternatives></disp-formula>
</p>
    </sec>
  </sec>
  <sec>
    <title>Experimental and Discussion</title>
    <sec>
      <title>Training methods</title>
    </sec>
    <sec>
      <title>Ground truth generation</title>
      <p>The ground truth density map can represent the image containing N people. Following the methods in (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>; <xref rid="ref-25" ref-type="bibr">Liu, Salzmann &amp; Fua, 2019</xref>; <xref rid="ref-18" ref-type="bibr">Jiang et al., 2020</xref>), We convolve <italic toggle="yes">δ</italic>(<italic toggle="yes">x</italic> − <italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub>) with a Gaussian kernel <italic toggle="yes">G</italic><sub><italic toggle="yes">σ</italic><sub><italic toggle="yes">i</italic></sub></sub>(x) (which is normalized to 1) with parameter <italic toggle="yes">σ</italic><sub><italic toggle="yes">i</italic></sub> to blur each head annotation. The ground truth density map can be defined as follow:</p>
      <p><inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-902-i006.jpg"/><tex-math id="tex-ieqn-57">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$F \left( x \right) ={\mathop{\sum }\nolimits }_{i=1}^{N}\delta (x-{x}_{i})\mathrm{ \ast }{G}_{{\sigma }_{i}}$\end{document}</tex-math><mml:math id="mml-ieqn-57" overflow="scroll"><mml:mi>F</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow><mml:mi mathvariant="normal"> ∗</mml:mi><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>(x) with <italic toggle="yes">σ</italic><sub><italic toggle="yes">i</italic></sub> = <italic toggle="yes">β</italic>
<inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-902-i007.jpg"/><tex-math id="tex-ieqn-61">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\bar {{d}^{i}}$\end{document}</tex-math><mml:math id="mml-ieqn-61" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo> ¯</mml:mo></mml:mrow></mml:mover></mml:math></alternatives></inline-formula></p>
      <p>where <italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub> represents the position of pixel, <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-902-i008.jpg"/><tex-math id="tex-ieqn-63">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\bar {{d}^{i}}$\end{document}</tex-math><mml:math id="mml-ieqn-63" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo> ¯</mml:mo></mml:mrow></mml:mover></mml:math></alternatives></inline-formula> is the average distance of <italic toggle="yes">k</italic> nearest neighbors, <italic toggle="yes">β</italic> is a constant. We set <italic toggle="yes">k</italic> = 3 and <italic toggle="yes">β</italic> = 0.3. <italic toggle="yes">σ</italic><sub><italic toggle="yes">i</italic></sub> is standard deviation, the setups are shown in <xref rid="table-1" ref-type="table">Table 1</xref>.</p>
      <table-wrap position="float" id="table-1">
        <object-id pub-id-type="doi">10.7717/peerjcs.902/table-1</object-id>
        <label>Table 1</label>
        <caption>
          <title>The setups for different datasets.</title>
          <p>Parameter settings for density maps generated from different datasets.</p>
        </caption>
        <alternatives>
          <graphic xlink:href="peerj-cs-08-902-g008" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col span="1"/>
              <col span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Datasets</th>
                <th rowspan="1" colspan="1">Parameter settings</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">ShanghaiTech Part_A (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>)</td>
                <td rowspan="1" colspan="1"><italic toggle="yes">σ</italic><sub><italic toggle="yes">i</italic></sub>=4</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ShanghaiTech Part_B (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>)</td>
                <td rowspan="1" colspan="1"><italic toggle="yes">σ</italic><sub><italic toggle="yes">i</italic></sub>=15</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">UCF_QRNF (<xref rid="ref-16" ref-type="bibr">Idrees et al., 2018</xref>)</td>
                <td rowspan="1" colspan="1">Geometry-adaptive kernels</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">UCF_CC_50 (<xref rid="ref-15" ref-type="bibr">Idrees et al., 2013</xref>)</td>
                <td rowspan="1" colspan="1">Geometry-adaptive kernels</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
    </sec>
    <sec>
      <title>Training details</title>
      <p>Our DMPNet is implemented based on the PyTorch framework. It consists of a front-end network with the first 10 layers of VGG16 (<xref rid="ref-35" ref-type="bibr">Simonyan &amp; Zisserman, 2014</xref>) and a back-end network with three densely connected MPNs. The training batch size is 1, optimized by Adam (<xref rid="ref-20" ref-type="bibr">Kingma &amp; Ba, 2014</xref>), and the learning rate is 5e−6 and the weight decay of 5e−4. Random Gaussian initialization with 0.01 standard deviation is used. Besides, we perform data enhancement on the image, and the enhancement principle followed CSRNet (<xref rid="ref-22" ref-type="bibr">Li, Zhang &amp; Chen, 2018</xref>). Considering the illumination changes, we carry out gamma transform and gray transform on images, and the transformation principle follows DSNet (<xref rid="ref-7" ref-type="bibr">Dai et al., 2021</xref>).</p>
    </sec>
    <sec>
      <title>Evaluation metrics</title>
      <p>Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) can evaluate the performance of crowd counting (<xref rid="ref-40" ref-type="bibr">Wan &amp; Chan, 2019</xref>; <xref rid="ref-18" ref-type="bibr">Jiang et al., 2020</xref>). MAE and RMSE represent the accuracy and robustness of the network respectively, and they can be defined as follows:</p>
      <p>
        <inline-formula>
          <alternatives>
            <inline-graphic xlink:href="peerj-cs-08-902-i009.jpg"/>
            <tex-math id="tex-ieqn-71">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$MAE= \frac{1}{N} {|}{D}_{i}-{D}_{i}^{GT}{|}$\end{document}</tex-math>
            <mml:math id="mml-ieqn-71" overflow="scroll">
              <mml:mi>M</mml:mi>
              <mml:mi>A</mml:mi>
              <mml:mi>E</mml:mi>
              <mml:mo>=</mml:mo>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>N</mml:mi>
                </mml:mrow>
              </mml:mfrac>
              <mml:mo>|</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>D</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>i</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>−</mml:mo>
              <mml:msubsup>
                <mml:mrow>
                  <mml:mi>D</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>i</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>G</mml:mi>
                  <mml:mi>T</mml:mi>
                </mml:mrow>
              </mml:msubsup>
              <mml:mo>|</mml:mo>
            </mml:math>
          </alternatives>
        </inline-formula>
      </p>
      <p>
        <inline-formula>
          <alternatives>
            <inline-graphic xlink:href="peerj-cs-08-902-i010.jpg"/>
            <tex-math id="tex-ieqn-72">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$RMSE=\sqrt{ \frac{1}{N} {\mathop{\sum }\nolimits }_{i=1}^{N}({D}_{i}-{D}_{i}^{GT})^{2}}$\end{document}</tex-math>
            <mml:math id="mml-ieqn-72" overflow="scroll">
              <mml:mi>R</mml:mi>
              <mml:mi>M</mml:mi>
              <mml:mi>S</mml:mi>
              <mml:mi>E</mml:mi>
              <mml:mo>=</mml:mo>
              <mml:msqrt>
                <mml:mrow>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>N</mml:mi>
                    </mml:mrow>
                  </mml:mfrac>
                  <mml:msubsup>
                    <mml:mrow>
                      <mml:mo>∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>N</mml:mi>
                    </mml:mrow>
                  </mml:msubsup>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mrow>
                        <mml:mfenced separators="" open="(" close=")">
                          <mml:msub>
                            <mml:mrow>
                              <mml:mi>D</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>i</mml:mi>
                            </mml:mrow>
                          </mml:msub>
                          <mml:mo>−</mml:mo>
                          <mml:msubsup>
                            <mml:mrow>
                              <mml:mi>D</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>i</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>G</mml:mi>
                              <mml:mi>T</mml:mi>
                            </mml:mrow>
                          </mml:msubsup>
                        </mml:mfenced>
                      </mml:mrow>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mn>2</mml:mn>
                    </mml:mrow>
                  </mml:msup>
                </mml:mrow>
              </mml:msqrt>
            </mml:math>
          </alternatives>
        </inline-formula>
      </p>
      <p>where N is the number of test images. <italic toggle="yes">D</italic><sub><italic toggle="yes">i</italic></sub> and <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-902-i011.jpg"/><tex-math id="tex-ieqn-74">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${D}_{i}^{GT}$\end{document}</tex-math><mml:math id="mml-ieqn-74" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> represent the actual and estimated numbers of people in the i-th image respectively.</p>
    </sec>
    <sec>
      <title>Datasets</title>
      <p>We evaluate DMPNet on three benchmark crowd counting datasets: ShanghaiTech (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>), UCF-QNRF (<xref rid="ref-16" ref-type="bibr">Idrees et al., 2018</xref>), UCF CC 50 (<xref rid="ref-15" ref-type="bibr">Idrees et al., 2013</xref>). (1) ShanghaiTech: It includes Part A and Part B, with a total 1,198 images and 330,165 annotations. Part A contains 300 training images and 182 testing images for congested crowd scenes, counting from 33 to 3,139. Part B contains 400 training images and 316 testing images, for sparse crowd scenes, counting from 9 to 578. (2) UCF-QNRF: It is the largest and most recently released dataset on crowd counting with 1,535 dense crowd images from various websites, counting from 49 to 12,865. (3) UCF CC 50: It contains 50 images with 63,974 annotations, counting from 94 to 4,543. The average number of people in the image is 1,280.</p>
    </sec>
    <sec>
      <title>Comparison with State-of-the-Art</title>
      <p>We evaluate and compare our DMPNet and SOTA methods on three challenging crowd counting datasets. The experimental results are shown in <xref rid="table-2" ref-type="table">Table 2</xref>. As you can see, our DMPNet is in the top two in multiple comparisons. (1) On ShanghaiTech part A (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>), MAE of our model is 98.3, which is the second best result. RMSE is 63.7, 7.2% higher than that of the optimal model RANet (<xref rid="ref-47" ref-type="bibr">Zhu et al., 2019</xref>). On ShanghaiTech Part B (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>), MAE and RMSE are 13.4% and 15.6% higher than DSNet (<xref rid="ref-7" ref-type="bibr">Dai et al., 2021</xref>) and SDANet (<xref rid="ref-27" ref-type="bibr">Miao et al., 2020</xref>), respectively. The images of Part A are from the Internet with highly congested scenes. The images of Part B come from streets captured by fixed cameras with relatively sparse crowd scenes. It indicates that our DMPNet can perform well both congested and sparse crowd scenes. (2) On UCF_QNRF (<xref rid="ref-16" ref-type="bibr">Idrees et al., 2018</xref>), although we do not reach the best, we still have a good performance. MAE and RMSE are 98.7 and 179.8, respectively, 15.3% and 18.9% higher than M-SFANet (<xref rid="ref-29" ref-type="bibr">Thanasutives et al., 2021</xref>). UCF_QNRF has lots of different scenes, in which the viewpoint and lighting variations are more diverse. In addition, due to the great change of crowd density, the perspective distortion of the head is more serious. Our model can handle this data well, which proves that our model has a certain adaptability to multiple scenes. In the face of crowd images close to real high-density scenes in UCF_QNRF, DMPNet can produce more accurate counting. (3) On UCF_CC_50 (<xref rid="ref-15" ref-type="bibr">Idrees et al., 2013</xref>), 5-fold cross-validation is used to evaluate our DMPNet and achieve the second-best results of MAE and RMSE, 24.7% and 25.3% higher than M-SFANet (<xref rid="ref-29" ref-type="bibr">Thanasutives et al., 2021</xref>) and DSNet (<xref rid="ref-7" ref-type="bibr">Dai et al., 2021</xref>) respectively. UCF_CC_50 is a challenging dataset with few samples and low image resolution. The results of this data demonstrate that we can also achieve high results on small datasets.</p>
      <table-wrap position="float" id="table-2">
        <object-id pub-id-type="doi">10.7717/peerjcs.902/table-2</object-id>
        <label>Table 2</label>
        <caption>
          <title>Comparisons of our DMPNet with SOTA methods.</title>
          <p>The empirical comparison of three mainstream datasets shows that our method is more effective on MAE and MSE. We have bolded the best two results from each dataset.</p>
        </caption>
        <alternatives>
          <graphic xlink:href="peerj-cs-08-902-g009" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th align="center" colspan="2" rowspan="1">ShanghaiTech Part A</th>
                <th align="center" colspan="2" rowspan="1">ShanghaiTech Part B</th>
                <th align="center" colspan="2" rowspan="1">UCF_QNRF</th>
                <th align="center" colspan="2" rowspan="1">UCF_CC_50</th>
              </tr>
              <tr>
                <th rowspan="1" colspan="1">Methods</th>
                <th rowspan="1" colspan="1">MAE</th>
                <th rowspan="1" colspan="1">RMSE</th>
                <th rowspan="1" colspan="1">MAE</th>
                <th rowspan="1" colspan="1">RMSE</th>
                <th rowspan="1" colspan="1">MAE</th>
                <th rowspan="1" colspan="1">RMSE</th>
                <th rowspan="1" colspan="1">MAE</th>
                <th rowspan="1" colspan="1">RMSE</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">MCNN</td>
                <td rowspan="1" colspan="1">110.2</td>
                <td rowspan="1" colspan="1">173.2</td>
                <td rowspan="1" colspan="1">26.4</td>
                <td rowspan="1" colspan="1">41.3</td>
                <td rowspan="1" colspan="1">277.0</td>
                <td rowspan="1" colspan="1">426.0</td>
                <td rowspan="1" colspan="1">377.6</td>
                <td rowspan="1" colspan="1">509.1</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Switch-CNN</td>
                <td rowspan="1" colspan="1">90.4</td>
                <td rowspan="1" colspan="1">135.0</td>
                <td rowspan="1" colspan="1">21.6</td>
                <td rowspan="1" colspan="1">33.4</td>
                <td rowspan="1" colspan="1">228.0</td>
                <td rowspan="1" colspan="1">445.0</td>
                <td rowspan="1" colspan="1">318.1</td>
                <td rowspan="1" colspan="1">439.2</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CP-CNN</td>
                <td rowspan="1" colspan="1">73.6</td>
                <td rowspan="1" colspan="1">106.4</td>
                <td rowspan="1" colspan="1">20.1</td>
                <td rowspan="1" colspan="1">30.1</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">295.8</td>
                <td rowspan="1" colspan="1">320.9</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ic-CNN</td>
                <td rowspan="1" colspan="1">68.5</td>
                <td rowspan="1" colspan="1">116.2</td>
                <td rowspan="1" colspan="1">10.7</td>
                <td rowspan="1" colspan="1">16.0</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">260.9</td>
                <td rowspan="1" colspan="1">365.5</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CSRNet</td>
                <td rowspan="1" colspan="1">68.2</td>
                <td rowspan="1" colspan="1">115.0</td>
                <td rowspan="1" colspan="1">10.6</td>
                <td rowspan="1" colspan="1">16.0</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">266.1</td>
                <td rowspan="1" colspan="1">397.5</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">SANet</td>
                <td rowspan="1" colspan="1">67.0</td>
                <td rowspan="1" colspan="1">104.5</td>
                <td rowspan="1" colspan="1">8.4</td>
                <td rowspan="1" colspan="1">13.6</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">258.4</td>
                <td rowspan="1" colspan="1">334.9</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">BL</td>
                <td rowspan="1" colspan="1">62.8</td>
                <td rowspan="1" colspan="1">101.8</td>
                <td rowspan="1" colspan="1">7.7</td>
                <td rowspan="1" colspan="1">12.7</td>
                <td rowspan="1" colspan="1">
                  <bold>88.7</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>154.8</bold>
                </td>
                <td rowspan="1" colspan="1">229.3</td>
                <td rowspan="1" colspan="1">308.2</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">RANet</td>
                <td rowspan="1" colspan="1">
                  <bold>59.4</bold>
                </td>
                <td rowspan="1" colspan="1">102.0</td>
                <td rowspan="1" colspan="1">7.9</td>
                <td rowspan="1" colspan="1">12.9</td>
                <td rowspan="1" colspan="1">111</td>
                <td rowspan="1" colspan="1">190</td>
                <td rowspan="1" colspan="1">239.8</td>
                <td rowspan="1" colspan="1">319.4</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">SDANet</td>
                <td rowspan="1" colspan="1">63.6</td>
                <td rowspan="1" colspan="1">101.8</td>
                <td rowspan="1" colspan="1">7.8</td>
                <td rowspan="1" colspan="1">
                  <bold>10.2</bold>
                </td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">227.6</td>
                <td rowspan="1" colspan="1">316.4</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">SFANet</td>
                <td rowspan="1" colspan="1">
                  <bold>59.8</bold>
                </td>
                <td rowspan="1" colspan="1">99.3</td>
                <td rowspan="1" colspan="1">6.9</td>
                <td rowspan="1" colspan="1">10.9</td>
                <td rowspan="1" colspan="1">100.8</td>
                <td rowspan="1" colspan="1">174.5</td>
                <td rowspan="1" colspan="1">219.6</td>
                <td rowspan="1" colspan="1">316.2</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">PACNN</td>
                <td rowspan="1" colspan="1">66.3</td>
                <td rowspan="1" colspan="1">106.4</td>
                <td rowspan="1" colspan="1">8.9</td>
                <td rowspan="1" colspan="1">13.5</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">241.7</td>
                <td rowspan="1" colspan="1">320.7</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">TEDNet</td>
                <td rowspan="1" colspan="1">64.2</td>
                <td rowspan="1" colspan="1">109.1</td>
                <td rowspan="1" colspan="1">8.2</td>
                <td rowspan="1" colspan="1">12.8</td>
                <td rowspan="1" colspan="1">113</td>
                <td rowspan="1" colspan="1">188</td>
                <td rowspan="1" colspan="1">249.4</td>
                <td rowspan="1" colspan="1">354.5</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">DSNet</td>
                <td rowspan="1" colspan="1">61.7</td>
                <td rowspan="1" colspan="1">102.6</td>
                <td rowspan="1" colspan="1">
                  <bold>6.7</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>10.5</bold>
                </td>
                <td rowspan="1" colspan="1">91.4</td>
                <td rowspan="1" colspan="1">160.4</td>
                <td rowspan="1" colspan="1">
                  <bold>183.3</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>240.6</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">M-SFANet</td>
                <td rowspan="1" colspan="1">59.69</td>
                <td rowspan="1" colspan="1">
                  <bold>95.66</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>6.76</bold>
                </td>
                <td rowspan="1" colspan="1">11.89</td>
                <td rowspan="1" colspan="1">
                  <bold>85.60</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>151.23</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>162.33</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>276.76</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">DMPNet</td>
                <td rowspan="1" colspan="1">63.7</td>
                <td rowspan="1" colspan="1">
                  <bold>98.3</bold>
                </td>
                <td rowspan="1" colspan="1">7.6</td>
                <td rowspan="1" colspan="1">11.8</td>
                <td rowspan="1" colspan="1">98.7</td>
                <td rowspan="1" colspan="1">179.8</td>
                <td rowspan="1" colspan="1">202.4</td>
                <td rowspan="1" colspan="1">301.5</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>The visualization results of our DMPNet are shown in <xref rid="fig-6" ref-type="fig">Fig. 6</xref>, and the quality of density maps generated by DMPNet and SOTA methods is compared on ShanghaiTech Part A and Part B datasets (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>) are shown in <xref rid="fig-7" ref-type="fig">Fig. 7</xref>. The comparison of visualization results and counting results shows that DMPNet can extract different types of crowd image information, and the density map is closer to the ground truth and higher in counting accuracy than MCNN (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>) and CSRNet (<xref rid="ref-22" ref-type="bibr">Li, Zhang &amp; Chen, 2018</xref>). Our DMPNet has well solved the problems of crowd occlusion, perspective distortion, and scale variations.</p>
      <fig position="float" id="fig-6">
        <object-id pub-id-type="doi">10.7717/peerjcs.902/fig-6</object-id>
        <label>Figure 6</label>
        <caption>
          <title>The visualization results and the corresponding counting results of our DMPNet.</title>
          <p>The first row illustrates different test images from left to right: ShanghaiTech Part A (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>), ShanghaiTech Part B (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>), UCF-QNRF (<xref rid="ref-16" ref-type="bibr">Idrees et al., 2018</xref>), and UCF_CC_50 (<xref rid="ref-15" ref-type="bibr">Idrees et al., 2013</xref>). The second and third lines are the ground truth map and the estimated density map generated by DMPNet, respectively.</p>
        </caption>
        <graphic xlink:href="peerj-cs-08-902-g006" position="float"/>
      </fig>
      <fig position="float" id="fig-7">
        <object-id pub-id-type="doi">10.7717/peerjcs.902/fig-7</object-id>
        <label>Figure 7</label>
        <caption>
          <title>Comparison of density maps generated by different SOTA methods on ShanghaiTech Part A and Part B dataset (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>).</title>
          <p>The six rows show: (1) The test images; (2) the ground truth; (3) density maps produced by MCNN (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>); (4) density maps produced by CSRNet (<xref rid="ref-22" ref-type="bibr">Li, Zhang &amp; Chen, 2018</xref>); (5) density maps produced by DSNet (<xref rid="ref-7" ref-type="bibr">Dai et al., 2021</xref>); (6) density maps produced by our DMPNet.</p>
        </caption>
        <graphic xlink:href="peerj-cs-08-902-g007" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>Ablation experiments</title>
      <p>In this subsection, we perform several ablation experiments including Multi-scale Pyramid Network (<italic toggle="yes">i.e.,</italic> LPN, GPN, and LPN+GPN), connected network (<italic toggle="yes">i.e.,</italic> dense connection and without dense connection), and loss function. Following the previous works (<xref rid="ref-22" ref-type="bibr">Li, Zhang &amp; Chen, 2018</xref>; <xref rid="ref-18" ref-type="bibr">Jiang et al., 2020</xref>; <xref rid="ref-45" ref-type="bibr">Zhang et al., 2020</xref>), ablation experiments are conducted on ShanghaiTech Part A (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>).</p>
    </sec>
    <sec>
      <title>Effect of LPN and GPN</title>
      <p>To verify the effects of LPN and GPN, we adjust the network structure with three different combinations. The results of LPN and GPN are summarized in <xref rid="table-3" ref-type="table">Table 3</xref>. In comparison, LPN achieves better results than GPN, with MAE and MSE lower 4.4% and 7.1%, respectively. When the two networks are used together, the results are further reduced by 5.4% and 6.7% relative to LPN. The results show that the proposed multi-scale extraction module is effective in capturing coarse-grained and fine-grained scales.</p>
    </sec>
    <sec>
      <title>Effect of Dense connection</title>
      <p>To verify the effects of dense connections, we compare two structures, one with dense connections and the other without dense connections, and the results are shown in <xref rid="table-4" ref-type="table">Table 4</xref>. Results are significantly better when dense connections are used, with MAE and MSE decreasing by 6.8% and 9.5%, respectively. This indicates that dense connection effectively prevents feature loss, increases information flow between different network layers, further enlarges scale diversity, and makes the feature more effective.</p>
    </sec>
    <sec>
      <title>Effect of loss function</title>
      <p>To verify the effect of different loss function combinations, we design four different combinations, and the results are shown in <xref rid="table-5" ref-type="table">Table 5</xref>. MSE Loss, as the most common loss function in crowd counting, still plays a major role. However, after density level consistency loss and MAE loss are added, the effect is improved to a certain extent. When both are used, MAE and MSE decrease by 9.0% and 9.3%, respectively, indicating that the combination of density level consistency loss and MAE loss can help the model to better converge and improve the counting performance.</p>
      <table-wrap position="float" id="table-3">
        <object-id pub-id-type="doi">10.7717/peerjcs.902/table-3</object-id>
        <label>Table 3</label>
        <caption>
          <title>The estimation errors of LPN and GPN are compared on ShanghaiTech Part A (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>).</title>
          <p>In the following training, MFFN is used.</p>
        </caption>
        <alternatives>
          <graphic xlink:href="peerj-cs-08-902-g010" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Methods</th>
                <th rowspan="1" colspan="1">MAE</th>
                <th rowspan="1" colspan="1">RMSE</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">w/ LPN, w/o GPN</td>
                <td rowspan="1" colspan="1">67.3</td>
                <td rowspan="1" colspan="1">105.4</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">w/ GPN, w/o LPN</td>
                <td rowspan="1" colspan="1">70.4</td>
                <td rowspan="1" colspan="1">113.5</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">w/ (LPN+GPN)</td>
                <td rowspan="1" colspan="1">63.7</td>
                <td rowspan="1" colspan="1">98.3</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <table-wrap position="float" id="table-4">
        <object-id pub-id-type="doi">10.7717/peerjcs.902/table-4</object-id>
        <label>Table 4</label>
        <caption>
          <title>The estimation errors of dense connections are compared on ShanghaiTech Part A (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>).</title>
          <p>In the following training, we used three MPNs.</p>
        </caption>
        <alternatives>
          <graphic xlink:href="peerj-cs-08-902-g011" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Method</th>
                <th rowspan="1" colspan="1">MAE</th>
                <th rowspan="1" colspan="1">RMSE</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">w/o Dense connection</td>
                <td rowspan="1" colspan="1">68.4</td>
                <td rowspan="1" colspan="1">108.7</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">w/ Dense connection</td>
                <td rowspan="1" colspan="1">63.7</td>
                <td rowspan="1" colspan="1">98.3</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <table-wrap position="float" id="table-5">
        <object-id pub-id-type="doi">10.7717/peerjcs.902/table-5</object-id>
        <label>Table 5</label>
        <caption>
          <title>The estimation errors of different loss function combinations are compared on ShanghaiTech Part A (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>).</title>
        </caption>
        <alternatives>
          <graphic xlink:href="peerj-cs-08-902-g012" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Method</th>
                <th rowspan="1" colspan="1">MAE</th>
                <th rowspan="1" colspan="1">RMSE</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">
                  <bold>
                    <italic toggle="yes">L</italic>
                  </bold>
                  <sub>
                    <bold>
                      <italic toggle="yes">E</italic>
                    </bold>
                  </sub>
                </td>
                <td rowspan="1" colspan="1">70.0</td>
                <td rowspan="1" colspan="1">108.4</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"><bold><italic toggle="yes">L</italic></bold><sub><bold><italic toggle="yes">E</italic></bold></sub> + <bold><italic toggle="yes">L</italic></bold><sub><bold><italic toggle="yes">D</italic></bold></sub></td>
                <td rowspan="1" colspan="1">67.3</td>
                <td rowspan="1" colspan="1">105.8</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"><bold><italic toggle="yes">L</italic></bold><sub><bold><italic toggle="yes">E</italic></bold></sub> + <bold><italic toggle="yes">L</italic></bold><sub><bold><italic toggle="yes">A</italic></bold></sub></td>
                <td rowspan="1" colspan="1">69.6</td>
                <td rowspan="1" colspan="1">107.6</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"><bold><italic toggle="yes">L</italic></bold><sub><bold><italic toggle="yes">E</italic></bold></sub> + <bold><italic toggle="yes">L</italic></bold><sub><bold><italic toggle="yes">D</italic></bold></sub> + <bold><italic toggle="yes">L</italic></bold><sub><bold><italic toggle="yes">A</italic></bold></sub></td>
                <td rowspan="1" colspan="1">63.7</td>
                <td rowspan="1" colspan="1">98.3</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
    </sec>
    <sec>
      <title>Effect of the number of MPN</title>
      <p>In order to verify the influence of the number of MPNs on the results, the number of MPNs is gradually increased and dense connections are used in different structures. The results are shown in <xref rid="table-6" ref-type="table">Table 6</xref>. When the number of N is not greater than 3, the result of crowd counting is better as the number of MPN increases. When <italic toggle="yes">N</italic> = 3, MAE and RMSE are 63.7 and 98.3, respectively. When <italic toggle="yes">N</italic> = 4, the results were 64.4 and 97.7, with no significant improvement. In DMPNet, we use dense connection, so there is no need to set too many MPN numbers, which will cause the increase of parameters and the redundancy of calculation.</p>
      <table-wrap position="float" id="table-6">
        <object-id pub-id-type="doi">10.7717/peerjcs.902/table-6</object-id>
        <label>Table 6</label>
        <caption>
          <title>The estimation errors of different MPN numbers are compared on ShanghaiTech Part A (<xref rid="ref-46" ref-type="bibr">Zhang et al., 2016</xref>).</title>
          <p>MPN(n) represents that the network contains n MPNs.</p>
        </caption>
        <alternatives>
          <graphic xlink:href="peerj-cs-08-902-g013" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Method</th>
                <th rowspan="1" colspan="1">MAE</th>
                <th rowspan="1" colspan="1">RMSE</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1"><bold><italic toggle="yes">MPN</italic></bold>(1)</td>
                <td rowspan="1" colspan="1">71.0</td>
                <td rowspan="1" colspan="1">111.3</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"><bold><italic toggle="yes">MPN</italic></bold>(2)</td>
                <td rowspan="1" colspan="1">66.2</td>
                <td rowspan="1" colspan="1">103.4</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"><bold><italic toggle="yes">MPN</italic></bold>(3)</td>
                <td rowspan="1" colspan="1">63.7</td>
                <td rowspan="1" colspan="1">98.3</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"><bold><italic toggle="yes">MPN</italic></bold>(4)</td>
                <td rowspan="1" colspan="1">64.4</td>
                <td rowspan="1" colspan="1">97.7</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
    </sec>
  </sec>
  <sec>
    <title>Conclusion</title>
    <p>In this paper, we proposed a novel end-to-end model called DMPNet for accurate crowd counting and high-quality density map generation. The front-end network of DMPNet is VGG16, and the back-end network is stacked by three densely connected MPNs. As an important component module of DMPNet, MPN can effectively extract multi-scale features while keeping the input and output resolution unchanged. The ability of the network is further enhanced by densely connecting multiple MPNs. In addition, we combined Euclidean loss with density level consistency loss and MAE loss to further improve the effect of the model. Experimental results on three challenging datasets validate the adaptability and robustness of our method in different crowd scenes. Although we deal with scale variation well, we did not eliminate background noise in the crowd density map, which will affect the counting accuracy to some extent. In future work, we will introduce attention mechanism to deal with background noise.</p>
  </sec>
  <sec sec-type="supplementary-material" id="supplemental-information">
    <title>Supplemental Information</title>
    <supplementary-material id="supp-1" position="float" content-type="local-data">
      <object-id pub-id-type="doi">10.7717/peerj-cs.902/supp-1</object-id>
      <label>Supplemental Information 1</label>
      <caption>
        <title>Code files used to build the DMPNet model, process the original data set to produce a high-quality density map, implement gaussian convolution function, and test the test set to obtain MAE and RMSE</title>
      </caption>
      <media xlink:href="peerj-cs-08-902-s001.tar">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <sec sec-type="additional-information">
    <title>Additional Information and Declarations</title>
    <fn-group content-type="competing-interests">
      <title>Competing Interests</title>
      <fn id="conflict-1" fn-type="COI-statement">
        <p>The authors declare there are no competing interests.</p>
      </fn>
    </fn-group>
    <fn-group content-type="author-contributions">
      <title>Author Contributions</title>
      <fn id="contribution-1" fn-type="con">
        <p><xref rid="author-1" ref-type="contrib">Pengfei Li</xref> conceived and designed the experiments, performed the experiments, analyzed the data, performed the computation work, prepared figures and/or tables, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
      <fn id="contribution-2" fn-type="con">
        <p><xref rid="author-2" ref-type="contrib">Min Zhang</xref> conceived and designed the experiments, performed the experiments, analyzed the data, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
      <fn id="contribution-3" fn-type="con">
        <p><xref rid="author-3" ref-type="contrib">Jian Wan</xref> performed the experiments, analyzed the data, performed the computation work, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
      <fn id="contribution-4" fn-type="con">
        <p><xref rid="author-4" ref-type="contrib">Ming Jiang</xref> performed the experiments, prepared figures and/or tables, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
    </fn-group>
    <fn-group content-type="other">
      <title>Data Availability</title>
      <fn id="addinfo-1">
        <p>The following information was supplied regarding data availability:</p>
        <p>The code is available at GitHub: <ext-link xlink:href="https://github.com/lpfworld/DMPNet" ext-link-type="uri">https://github.com/lpfworld/DMPNet</ext-link>.</p>
        <p>The ShanghaiTech Part A and Part B is available at Kaggle: <ext-link xlink:href="https://www.kaggle.com/tthien/shanghaitech" ext-link-type="uri">https://www.kaggle.com/tthien/shanghaitech</ext-link>.</p>
        <p>The UCF-QNRF is available at: <ext-link xlink:href="https://www.crcv.ucf.edu/data/ucf-qnrf/" ext-link-type="uri">https://www.crcv.ucf.edu/data/ucf-qnrf/</ext-link>
</p>
        <p>The UCF_CC_50 is available at: <ext-link xlink:href="https://www.crcv.ucf.edu/data/ucf-cc-50/" ext-link-type="uri">https://www.crcv.ucf.edu/data/ucf-cc-50/</ext-link>.</p>
      </fn>
    </fn-group>
  </sec>
  <ref-list content-type="authoryear">
    <title>References</title>
    <ref id="ref-1">
      <label>Amaranageswarao, Deivalakshmi &amp; Ko (2020)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Amaranageswarao</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Deivalakshmi</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ko</surname>
            <given-names>SB</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>Deep dilated and densely connected parallel convolutional groups for compression artifacts reduction</article-title>
        <source>Digital Signal Processing</source>
        <volume>106</volume>
        <fpage>102804</fpage>
        <pub-id pub-id-type="doi">10.1016/j.dsp.2020.102804</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-2">
      <label>Boominathan, Kruthiventi &amp; Babu (2016)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Boominathan</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Kruthiventi</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Babu</surname>
            <given-names>RV</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2016">2016</year>
        <article-title>CrowdNet: a deep convolutional network for dense crowd counting</article-title>
        <conf-name>Proceedings of the 24th ACM international conference on multimedia</conf-name>
        <fpage>640</fpage>
        <lpage>644</lpage>
        <pub-id pub-id-type="doi">10.1145/2964284.2967300</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-3">
      <label>Cao et al. (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Cao</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>Scale aggregation network for accurate and efficient crowd counting</article-title>
        <conf-name>Proceedings of the european conference on computer vision (ECCV)</conf-name>
        <fpage>734</fpage>
        <lpage>750</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-030-01228-1_45</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-4">
      <label>Chan, Liang &amp; Vasconcelos (2008)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Chan</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>ZS</given-names>
          </name>
          <name>
            <surname>Vasconcelos</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2008">2008</year>
        <article-title>Privacy preserving crowd monitoring: counting people without people models or tracking</article-title>
        <conf-name>2008 IEEE conference on computer vision and pattern recognition (CVPR)</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor> IEEE</conf-sponsor>
        <fpage>1</fpage>
        <lpage>7</lpage>
        <pub-id pub-id-type="doi">10.1109/CVPR.2008.4587569</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-5">
      <label>Chan &amp; Vasconcelos (2009)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Chan</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Vasconcelos</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2009">2009</year>
        <article-title>Bayesian Poisson regression for crowd counting</article-title>
        <conf-name>2009 IEEE 12th international conference on computer vision</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor> IEEE</conf-sponsor>
        <fpage>545</fpage>
        <lpage>551</lpage>
        <pub-id pub-id-type="doi">10.1109/ICCV.2009.5459191</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-6">
      <label>Chen et al. (2017)</label>
      <element-citation publication-type="workingpaper">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>LC</given-names>
          </name>
          <name>
            <surname>Papandreou</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Schroff</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Adam</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2017">2017</year>
        <article-title>Rethinking atrous convolution for semantic image segmentation</article-title>
        <pub-id pub-id-type="arxiv">1706.05587</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-7">
      <label>Dai et al. (2021)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Dai</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Xi</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Qiang</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2021">2021</year>
        <article-title>Dense scale network for crowd counting</article-title>
        <conf-name>International conference on multimedia retrieval</conf-name>
        <fpage>64</fpage>
        <lpage>72</lpage>
        <pub-id pub-id-type="doi">10.1145/3460426.3463628</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-8">
      <label>Duta et al. (2020)</label>
      <element-citation publication-type="workingpaper">
        <person-group person-group-type="author">
          <name>
            <surname>Duta</surname>
            <given-names>IC</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Shao</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>Pyramidal convolution: rethinking convolutional neural networks for visual recognition</article-title>
        <source>CoRR abs/2006.11538</source>
      </element-citation>
    </ref>
    <ref id="ref-9">
      <label>Enzweiler &amp; Gavrila (2009)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Enzweiler</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gavrila</surname>
            <given-names>GDM</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2009">2009</year>
        <article-title>Monocular pedestrian detection: survey and experiments</article-title>
        <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor>IEEE</conf-sponsor>
        <fpage>2179</fpage>
        <lpage>2195</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2008.260</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-10">
      <label>Felzenszwalb et al. (2010)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Felzenszwalb</surname>
            <given-names>PF</given-names>
          </name>
          <name>
            <surname>Girshick</surname>
            <given-names>RB</given-names>
          </name>
          <name>
            <surname>McAllester</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ramanan</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2010">2010</year>
        <article-title>Object detection with discriminatively trained part-based models</article-title>
        <source>IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</source>
        <volume>32</volume>
        <issue>9</issue>
        <fpage>1627</fpage>
        <lpage>1645</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2009.167</pub-id>
        <pub-id pub-id-type="pmid">20634557</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-11">
      <label>Gao et al. (2020)</label>
      <element-citation publication-type="workingpaper">
        <person-group person-group-type="author">
          <name>
            <surname>Gao</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>CNN-based density estimation and crowd counting: a survey</article-title>
        <pub-id pub-id-type="arxiv">2003.12783</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-12">
      <label>Guo et al. (2019)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Guo</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Zha</surname>
            <given-names>ZJ</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>DADNet: dilated-Attention-Deformable ConvNet for crowd counting</article-title>
        <conf-name>Proceedings of the 27th ACM international conference on multimedia</conf-name>
        <fpage>1823</fpage>
        <lpage>1832</lpage>
        <pub-id pub-id-type="doi">10.1145/3343031.3350881</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-13">
      <label>Hu et al. (2020)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Doermann</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>NAS-Count: counting-by-density with neural architecture search</article-title>
        <conf-name>Proceedings of the European conference on computer vision (ECCV)</conf-name>
        <fpage>747</fpage>
        <lpage>766</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-030-58542-6_45</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-14">
      <label>Huang et al. (2016)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Laurens</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname> Weinberger</surname>
            <given-names>KQ</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2016">2016</year>
        <article-title>Densely connected convolutional networks</article-title>
        <conf-name>IEEE conference on computer vision and pattern recognition (CVPR)</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor>IEEE</conf-sponsor>
        <pub-id pub-id-type="doi">10.1109/CVPR.2017.243</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-15">
      <label>Idrees et al. (2013)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Idrees</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Saleemi</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Seibert</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Shah</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2013">2013</year>
        <article-title>Multi-source multi-scale counting in extremely dense crowd images</article-title>
        <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor>IEEE</conf-sponsor>
        <fpage>2547</fpage>
        <lpage>2554</lpage>
        <pub-id pub-id-type="doi">10.1109/CVPR.2013.329</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-16">
      <label>Idrees et al. (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Idrees</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Tayyab</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Athrey</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Al-Maadeed</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Rajpoot</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Shah</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>Composition loss for counting, density map estimation and localization in dense crowds</article-title>
        <conf-name>Proceedings of the European conference on computer vision (ECCV)</conf-name>
        <fpage>532</fpage>
        <lpage>546</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-030-01216-8_33</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-17">
      <label>Jiang et al. (2019)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zhen</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Doermann</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Shao</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>Crowd counting and density estimation by trellis encoder–decoder networks</article-title>
        <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR). 2020</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor>IEEE</conf-sponsor>
        <fpage>6133</fpage>
        <lpage>6142</lpage>
        <pub-id pub-id-type="doi">10.1109/CVPR.2019.00629</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-18">
      <label>Jiang et al. (2020)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname> Lv</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Pang</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>Attention scaling for crowd counting</article-title>
        <conf-name>2020 IEEE/CVF conference on computer vision and pattern recognition (CVPR)</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor>IEEE</conf-sponsor>
        <fpage>4705</fpage>
        <lpage>4714</lpage>
        <pub-id pub-id-type="doi">10.1109/CVPR42600.2020.00476</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-19">
      <label>Ke et al. (2012)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Ke</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>CL</given-names>
          </name>
          <name>
            <surname>Gong</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Xiang</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2012">2012</year>
        <article-title>Feature mining for localised crowd counting</article-title>
        <conf-name>British machine vision conference (BMVC)</conf-name>
        <pub-id pub-id-type="doi">10.5244/C.26.21</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-20">
      <label>Kingma &amp; Ba (2014)</label>
      <element-citation publication-type="workingpaper">
        <person-group person-group-type="author">
          <name>
            <surname>Kingma</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ba</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2014">2014</year>
        <article-title>Adam: a method for stochastic optimization. Computer Science</article-title>
        <pub-id pub-id-type="arxiv">1412.6980</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-21">
      <label>Lempitsky &amp; Zisserman (2010)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Lempitsky</surname>
            <given-names>VS</given-names>
          </name>
          <name>
            <surname>Zisserman</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2010">2010</year>
        <article-title>Learning to count objects in images</article-title>
        <conf-name>24th annual conference on neural information processing systems</conf-name>
        <fpage>1324</fpage>
        <lpage>1332</lpage>
      </element-citation>
    </ref>
    <ref id="ref-22">
      <label>Li, Zhang &amp; Chen (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>CSRNet: dilated convolutional neural networks for understanding the highly congested scenes</article-title>
        <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor>IEEE</conf-sponsor>
        <fpage>1091</fpage>
        <lpage>1100</lpage>
        <pub-id pub-id-type="doi">10.1109/CVPR.2018.00120</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-23">
      <label>Lin et al. (2017)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>TY</given-names>
          </name>
          <name>
            <surname>Dollar</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Girshick</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname> He</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Hariharan</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Belongie</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2017">2017</year>
        <article-title>Feature pyramid networks for object detection</article-title>
        <conf-name>2017 IEEE conference on computer vision and pattern recognition (CVPR)</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor>IEEE</conf-sponsor>
        <pub-id pub-id-type="doi">10.1109/CVPR.2017.106</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-24">
      <label>Liu et al. (2020)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ouyang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>Crowd counting with deep structured scale integration network</article-title>
        <conf-name>2019 IEEE/CVF international confere nce on computer vision (ICCV)</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor>IEEE</conf-sponsor>
        <fpage>1774</fpage>
        <lpage>1783</lpage>
        <pub-id pub-id-type="doi">10.1109/ICCV.2019.00186</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-25">
      <label>Liu, Salzmann &amp; Fua (2019)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Salzmann</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Fua</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>Context-aware crowd counting</article-title>
        <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor>IEEE</conf-sponsor>
        <fpage>5099</fpage>
        <lpage>5108</lpage>
        <pub-id pub-id-type="doi">10.1109/CVPR.2019.00524</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-26">
      <label>Ma et al. (2019)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Ma</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Hong</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Gong</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>Bayesian loss for crowd count estimation with point supervision</article-title>
        <conf-name>Proceedings of the IEEE international conference on computer vision (ICCV)</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor>IEEE</conf-sponsor>
        <fpage>6142</fpage>
        <lpage>6151</lpage>
        <pub-id pub-id-type="doi">10.1109/ICCV.2019.00624</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-27">
      <label>Miao et al. (2020)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Miao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>Shallow feature based dense attention network for crowd counting</article-title>
        <source>Proceedings of the AAAI Conference on Artificial Intelligence</source>
        <volume>34</volume>
        <issue>7</issue>
        <fpage>11765</fpage>
        <lpage>11772</lpage>
        <pub-id pub-id-type="doi">10.1609/aaai.v34i07.6848</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-28">
      <label>Pham et al. (2015)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Pham</surname>
            <given-names>VQ</given-names>
          </name>
          <name>
            <surname>Kozakaya</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Yamaguchi</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Okada</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2015">2015</year>
        <article-title>Count forest: co-voting uncertain number of targets using random forest for crowd density estimation</article-title>
        <conf-name>2015 IEEE international conference on computer vision (ICCV)</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor>IEEE</conf-sponsor>
        <fpage>3253</fpage>
        <lpage>3261</lpage>
        <pub-id pub-id-type="doi">10.1109/ICCV.2015.372</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-29">
      <label>Thanasutives et al. (2021)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Thanasutives</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Fukui</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Numao</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kijsirikul</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2021">2021</year>
        <article-title>Encoder-decoder based convolutional neural networks with multi-scale-aware modules for crowd counting</article-title>
        <conf-name>25th international conference on pattern recognition (ICPR)</conf-name>
        <fpage>2382</fpage>
        <lpage>2389</lpage>
        <pub-id pub-id-type="doi">10.1109/ICPR48806.2021.9413286</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-30">
      <label>Richardson et al. (2020)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Richardson</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Azar</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Avioz</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Geron</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Ronen</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Avraham</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Shapiro</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>It’s all about the scale-efficient text detection using adaptive scaling</article-title>
        <conf-name>2020 IEEE winter conference on applications of computer vision (WACV)</conf-name>
        <fpage>1833</fpage>
        <lpage>1842</lpage>
        <pub-id pub-id-type="doi">10.1109/WACV45572.2020.9093534</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-31">
      <label>Ryan et al. (2009)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Ryan</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Denman</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Fookes</surname>
            <given-names>CB</given-names>
          </name>
          <name>
            <surname>Sridharan</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2009">2009</year>
        <article-title>Crowd counting using multiple local features</article-title>
        <source>2009 digital image computing: techniques and applications</source>
        <publisher-name>IEEE</publisher-name>
        <publisher-loc>Piscataway</publisher-loc>
        <fpage>81</fpage>
        <lpage>88</lpage>
        <pub-id pub-id-type="doi">10.1109/DICTA.2009.22</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-32">
      <label>Sam, Surya &amp; Babu (2016)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Sam</surname>
            <given-names>DB</given-names>
          </name>
          <name>
            <surname>Surya</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Babu</surname>
            <given-names>RV</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2016">2016</year>
        <article-title>Switching convolutional neural network for crowd counting</article-title>
        <conf-name>2017 IEEE conference on computer vision and pattern recognition (CVPR)</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor>IEEE</conf-sponsor>
        <fpage>4031</fpage>
        <lpage>4039</lpage>
        <pub-id pub-id-type="doi">10.1109/CVPR.2017.429</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-33">
      <label>Shi et al. (2018)</label>
      <element-citation publication-type="workingpaper">
        <person-group person-group-type="author">
          <name>
            <surname>Shi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>Perspective-Aware CNN for crowd counting</article-title>
        <pub-id pub-id-type="arxiv">1807.01989</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-34">
      <label>Shi et al. (2019)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Shi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>Revisiting perspective information for efficient crowd counting</article-title>
        <conf-name>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor>IEEE</conf-sponsor>
        <pub-id pub-id-type="doi">10.1109/CVPR.2019.00745</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-35">
      <label>Simonyan &amp; Zisserman (2014)</label>
      <element-citation publication-type="workingpaper">
        <person-group person-group-type="author">
          <name>
            <surname>Simonyan</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Zisserman</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2014">2014</year>
        <article-title>Very deep convolutional networks for large-scale image recognition. Computer Science</article-title>
        <pub-id pub-id-type="arxiv">1409.1556</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-36">
      <label>Sindagi &amp; Patel (2017a)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Sindagi</surname>
            <given-names>VA</given-names>
          </name>
          <name>
            <surname>Patel</surname>
            <given-names>VM</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2017">2017a</year>
        <article-title>Generating high-quality crowd density maps using contextual pyramid CNNs</article-title>
        <conf-name>2017 IEEE international conference on computer vision (ICCV)</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor>IEEE</conf-sponsor>
        <fpage>1879</fpage>
        <lpage>1888</lpage>
        <pub-id pub-id-type="doi">10.1109/ICCV.2017.206</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-37">
      <label>Sindagi &amp; Patel (2017b)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sindagi</surname>
            <given-names>VA</given-names>
          </name>
          <name>
            <surname>Patel</surname>
            <given-names>VM</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2017">2017b</year>
        <article-title>A survey of recent advances in CNN-based single image crowd counting and density estimation</article-title>
        <source>Pattern Recognition Letters</source>
        <volume>107</volume>
        <fpage>3</fpage>
        <lpage>16</lpage>
        <pub-id pub-id-type="doi">10.1016/j.patrec.2017.07.007</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-38">
      <label>Tan &amp; Le (2019)</label>
      <element-citation publication-type="workingpaper">
        <person-group person-group-type="author">
          <name>
            <surname>Tan</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>QV</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>MixConv: mixed depthwise convolutional kernels</article-title>
        <pub-id pub-id-type="arxiv">1907.09595</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-39">
      <label>Viresh, Le &amp; Hoai (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Viresh</surname>
            <given-names>RJ</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Hoai</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>Iterative crowd counting</article-title>
        <conf-name>15th european conference on computer vision</conf-name>
        <fpage>278</fpage>
        <lpage>293</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-030-01234-2_17</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-40">
      <label>Wan &amp; Chan (2019)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Wan</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>AB</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>Adaptive density map generation for crowd counting</article-title>
        <conf-name>2019 IEEE/CVF international conference on computer vision (ICCV)</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor>IEEE</conf-sponsor>
        <fpage>1130</fpage>
        <lpage>1139</lpage>
        <pub-id pub-id-type="doi">10.1109/ICCV.2019.00122</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-41">
      <label>Wang et al. (2020)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>Scale-equalizing pyramid convolution for object detection</article-title>
        <conf-name>2020 IEEE/CVF conference on computer vision and pattern recognition (CVPR)</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor>IEEE</conf-sponsor>
        <fpage>13356</fpage>
        <lpage>13365</lpage>
        <pub-id pub-id-type="doi">10.1109/CVPR42600.2020.01337</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-42">
      <label>Wu &amp; Nevatia (2007)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Nevatia</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2007">2007</year>
        <article-title>Detection and tracking of multiple, partially occluded humans by bayesian combination of edgelet based part detectors</article-title>
        <source>International Journal of Computer Vision</source>
        <volume>75</volume>
        <issue>2</issue>
        <fpage>247</fpage>
        <lpage>266</lpage>
        <pub-id pub-id-type="doi">10.1007/s11263-006-0027-7</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-43">
      <label>Xie, Noble &amp; Zisserman (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xie</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Noble</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Zisserman</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>Microscopy cell counting and detection with fully convolutional regression networks</article-title>
        <source>Computer Methods in Biomechanics and Biomedical Engineering: Imaging &amp; Visualization</source>
        <volume>6</volume>
        <issue>3</issue>
        <fpage>283</fpage>
        <lpage>292</lpage>
        <pub-id pub-id-type="doi">10.1080/21681163.2016.1149104</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-44">
      <label>Xiong et al. (2019)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Xiong</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>From open set to closed set: counting objects by spatial divide-and-conquer</article-title>
        <conf-name>2019 IEEE/CVF international conference on computer vision (ICCV)</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor>IEEE</conf-sponsor>
        <fpage>8362</fpage>
        <lpage>8371</lpage>
        <pub-id pub-id-type="doi">10.1109/ICCV.2019.00845</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-45">
      <label>Zhang et al. (2020)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Zhen</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Shao</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>Relational attention network for crowd counting</article-title>
        <conf-name>2019 IEEE/CVF international conference on computer vision (ICCV), 2020</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor>IEEE</conf-sponsor>
        <pub-id pub-id-type="doi">10.1109/ICCV.2019.00689</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-46">
      <label>Zhang et al. (2016)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2016">2016</year>
        <article-title>Single-image crowd counting via multi-column convolutional neural network</article-title>
        <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</conf-name>
        <conf-loc>Piscataway</conf-loc>
        <conf-sponsor>IEEE</conf-sponsor>
        <fpage>589</fpage>
        <lpage>597</lpage>
        <pub-id pub-id-type="doi">10.1109/CVPR.2016.70</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-47">
      <label>Zhu et al. (2019)</label>
      <element-citation publication-type="workingpaper">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>Dual path multi-scale fusion networks with attention for crowd counting</article-title>
        <pub-id pub-id-type="arxiv">1902.01115</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
