<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Genet</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Genet</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Genet.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Genetics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1664-8021</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6883002</article-id>
    <article-id pub-id-type="pmid">31824573</article-id>
    <article-id pub-id-type="doi">10.3389/fgene.2019.01182</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Genetics</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Graph Embedding Deep Learning Guides Microbial Biomarkers' Identification</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Zhu</surname>
          <given-names>Qiang</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="https://loop.frontiersin.org/people/773223"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jiang</surname>
          <given-names>Xingpeng</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn001">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="https://loop.frontiersin.org/people/603158"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhu</surname>
          <given-names>Qing</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pan</surname>
          <given-names>Min</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>He</surname>
          <given-names>Tingting</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>School of Information Management, Central China Normal University</institution>, <addr-line>Wuhan</addr-line>, <country>China</country></aff>
    <aff id="aff2"><sup>2</sup><institution>School of Computer, Central China Normal University</institution>, <addr-line>Wuhan</addr-line>, <country>China</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Hubei Provincial Key Laboratory of Artificial Intelligence and Smart Learning, Central China Normal University</institution>, <addr-line>Wuhan</addr-line>, <country>China</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Quan Zou, University of Electronic Science and Technology of China, China</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Lingling Jin, Thompson Rivers University, Canada; Xishuang Dong, Prairie View A&amp;M University, United States; Xiangrong Liu, Xiamen University, China</p>
      </fn>
      <corresp id="fn001">*Correspondence: Xingpeng Jiang, <email xlink:href="mailto:xpjiang@mail.ccnu.edu.cn" xlink:type="simple">xpjiang@mail.ccnu.edu.cn</email>
</corresp>
      <fn fn-type="other" id="fn002">
        <p>This article was submitted to Bioinformatics and Computational Biology, a section of the journal Frontiers in Genetics</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>22</day>
      <month>11</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>10</volume>
    <elocation-id>1182</elocation-id>
    <history>
      <date date-type="received">
        <day>12</day>
        <month>8</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>24</day>
        <month>10</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2019 Zhu, Jiang, Zhu, Pan and He</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>Zhu, Jiang, Zhu, Pan and He</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>The microbiome-wide association studies are to figure out the relationship between microorganisms and humans, with the goal of discovering relevant biomarkers to guide disease diagnosis. However, the microbiome data is complex, with high noise and dimensions. Traditional machine learning methods are limited by the models' representation ability and cannot learn complex patterns from the data. Recently, deep learning has been widely applied to fields ranging from text processing to image recognition due to its efficient flexibility and high capacity. But the deep learning models must be trained with enough data in order to achieve good performance, which is impractical in reality. In addition, deep learning is considered as black box and hard to interpret. These factors make deep learning not widely used in microbiome-wide association studies. In this work, we construct a sparse microbial interaction network and embed this graph into deep model to alleviate the risk of overfitting and improve the performance. Further, we explore a Graph Embedding Deep Feedforward Network (GEDFN) to conduct feature selection and guide meaningful microbial markers' identification. Based on the experimental results, we verify the feasibility of combining the microbial graph model with the deep learning model, and demonstrate the feasibility of applying deep learning and feature selection on microbial data. Our main contributions are: firstly, we utilize different methods to construct a variety of microbial interaction networks and combine the network <italic>via</italic> graph embedding deep learning. Secondly, we introduce a feature selection method based on graph embedding and validate the biological meaning of microbial markers. The code is available at <uri xlink:type="simple" xlink:href="https://github.com/MicroAVA/GEDFN.git">https://github.com/MicroAVA/GEDFN.git</uri>. </p>
    </abstract>
    <counts>
      <fig-count count="6"/>
      <table-count count="1"/>
      <equation-count count="12"/>
      <ref-count count="43"/>
      <page-count count="11"/>
      <word-count count="5368"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="s1">
    <title>graph embedding, deep learning, feature selection, biomarkers, microbiomeIntroduction</title>
    <p>A large number of microorganisms are parasite on various parts of the human body, mainly concentrated in the intestine, oral cavity, reproductive tract, epidermis and skin. The microbial communities existing in different parts of the body or in different host environments are very different (<xref rid="B39" ref-type="bibr">Turnbaugh et al., 2007</xref>; <xref rid="B26" ref-type="bibr">Lloyd-Price et al., 2017</xref>). These microorganisms include bacteria, fungi, viruses and protozoa. All genetic material in the particular microbial community is called the microbiome. Recent studies have shown that microorganisms are directly or indirectly related to many diseases. For example, the gut microbiome may be closely related to irritable bowel syndrome and its imbalance may lead to chronic kidney diseases. Microorganisms may also be closely related to digestive tract diseases, endocrine diseases, circulatory diseases, reproductive system diseases, respiratory and psychiatric diseases (<xref rid="B19" ref-type="bibr">Kho and Lal, 2018</xref>). Since the microbiome plays a central role in the hosts' health, understanding the distribution and composition of microbial communities in humans, especially under different diseases or physiological conditions, is of great significance for disease diagnosis, prevention and treatment. The microbiome-wide association studies are to find disease-associated microbial markers to guide disease diagnosis and treatment (<xref rid="B14" ref-type="bibr">Gilbert et al., 2016</xref>; <xref rid="B40" ref-type="bibr">Wang and Jia, 2016</xref>). Compared with the human genome, the microbiome is an ideal target and more convenient to regulate. Therefore, the microbiome is often named “the second human genome” (<xref rid="B4" ref-type="bibr">Brüls and Weissenbach, 2011</xref>). However, there are many types of microorganisms and most of them cannot be cultured. Therefore, a high-throughput sequencing method is a feasible means of understanding microbial communities. Through high-throughput sequencing, we can understand the types of microorganisms and even their functions in the community (<xref rid="B35" ref-type="bibr">Ranjan et al., 2016</xref>).</p>
    <p>The microbiome data is from high-throughput sequencing methods such as 16s or shotgun sequencing, which is often with high dimensions with noise. As a result, it is difficult to mine microbial signatures from these data. Traditionally, statistical-based methods identify markers mainly through microbial abundance differential expression (<xref rid="B34" ref-type="bibr">Paulson et al., 2013</xref>). However, the statistical approaches often have strong assumptions and the real data often do not satisfy these assumptions (<xref rid="B18" ref-type="bibr">Hawinkel et al., 2017</xref>; <xref rid="B42" ref-type="bibr">Weiss et al., 2017</xref>). Other machine learning methods are widely explored (<xref rid="B33" ref-type="bibr">Pasolli et al., 2016</xref>). Recently, deep learning has received great attention, especially its end-to-end automatic learning ability. At present, deep learning is widely used in automatic driving, image recognition and text processing, which has received exciting results (<xref rid="B23" ref-type="bibr">LeCun et al., 2015</xref>). The deep models can learn specific patterns directly from the data, thus avoiding the artificial feature engineering (<xref rid="B16" ref-type="bibr">Goodfellow et al., 2016</xref>; <xref rid="B20" ref-type="bibr">Kong and Yu, 2018</xref>). In the analysis of biomedical data, especially the analysis of various omics data, deep learning has achieved good improvement, but still faces many problems and challenges (<xref rid="B2" ref-type="bibr">Angermueller et al., 2016</xref>; <xref rid="B5" ref-type="bibr">Camacho et al., 2018</xref>; <xref rid="B9" ref-type="bibr">Eraslan et al., 2019</xref>). First, deep learning requires a large amount of training data to learn useful information while the biological sample size is often limited and cannot fully utilize its capabilities. Second, the training process is often considered a black box and people can only control the input and models' parameters. More specifically, deep learning involves complex network structures and nonlinear transformations, as well as a large number of hyperparameters, which hinder people from understanding how deep neural networks are making predictions. Although deep neural networks perform well on some classification tasks, biological problems should be paid more attention to which features lead to better classification (<xref rid="B6" ref-type="bibr">Ching et al., 2018</xref>).</p>
    <p>In this paper, we propose a feature selection method based on Graph Embedding Deep Feedforward Network (GEDFN) to conduct microbiome-wide association studies. Firstly, we construct three different microbial co-occurrence interaction networks. We utilize a graph embedding method to embed the network as <italic>a priori</italic> knowledge into Deep Feedforward Neural Network to reduce parameters, alleviate the overfitting problem and improve the models' performance. Secondly, we propose a feature selection approach based on GEDFN. Experiments show the microbial feature markers obtained <italic>via</italic> this method have biological significance. In other words, our results demonstrate graph embedding deep learning could guide feature selection.</p>
  </sec>
  <sec id="s2">
    <title>Related Work</title>
    <sec id="s2_1">
      <title>Microbial Interaction Network</title>
      <p>Because of the various relationships between microorganisms, such as symbiosis, competition and so on, as well as the complex structure and function of microorganisms due to their dynamic properties, the network is a good way to represent complex relationships. Understanding microbial interaction can help us understand microbial functions. System-oriented graph theory can facilitate microbial analysis and enhance our understanding of complex ecosystems and evolutionary processes (<xref rid="B11" ref-type="bibr">Faust et al., 2012</xref>; <xref rid="B22" ref-type="bibr">Layeghifard et al., 2017</xref>). However, most microorganisms are uncultured, we can only construct microbial interaction networks from high-throughput sequencing data. At present, there are many computational methods to construct microbial interaction networks. In theory, any method of calculating features' relationships can be used. For example, Bray–Curtis can be used to measure species abundance similarity (<xref rid="B3" ref-type="bibr">Bray and Curtis, 1957</xref>). The Pearson correlation coefficient is used to evaluate the linear relationship and the Spearman correlation coefficient can measure the rank relationship (<xref rid="B28" ref-type="bibr">Mukaka, 2012</xref>). CoNet uses an ensemble approach and combines with different comparison metrics to detect different relationships (<xref rid="B10" ref-type="bibr">Faust and Raes, 2016</xref>). Maximum mutual information is designed to capture broader relationships, not limited to specific function families (<xref rid="B36" ref-type="bibr">Reshef et al., 2011</xref>). MENA applies random matrix theory to conduct microbial analysis and experiments show it is robust to the noise and threshold (<xref rid="B7" ref-type="bibr">Deng et al., 2012</xref>). Sparse Correlations for Compositional data (SparCC) is a tool based on Aitchison's log ratio transformation to conduct microbial composition analysis (<xref rid="B12" ref-type="bibr">Friedman and Alm, 2012</xref>). SParse InversE Covariance Estimation for Ecological Association Inference (SPIEC-EASI) combines data logarithmic transformation with graph model inference framework to build a correlation network (<xref rid="B21" ref-type="bibr">Kurtz et al., 2015</xref>).</p>
    </sec>
    <sec id="s2_2">
      <title>Feature Selection</title>
      <p>Real biomedical data, especially various omics data with high dimensions and noise, often has feature redundancy problem. Feature selection is a step of data preprocessing, which involves selecting related features from a large number of features to improve subsequent learning tasks (<xref rid="B25" ref-type="bibr">Li et al., 2017</xref>).</p>
      <p>There are mainly three kinds of feature selection methods, including filter, wrapper and embedded method. The filter approach selects subset features and then trains the learner. The feature selection process is independent of the subsequent learner. This is equivalent to filter the initial feature with the feature selection process and train the model with the filtered features. However, filter methods often ignore some features that are helpful for classification. At the same time, many filter methods are based on a single-featured greedy algorithm. The assumption is that each feature is independent while this is often not the case in microbiological data. The wrapper feature selection directly takes the performance of the learner to be used as the evaluation criterion of the feature subset. In other words, the purpose of the wrapper feature selection is to select a feature subset that is most efficient in its performance for a given learner. Compared to the filter method, the wrapper method can evaluate the result of feature selection to improve the classification performance; however, the feature selection process requires to train the learner iteratively and the calculation is huge (<xref rid="B25" ref-type="bibr">Li et al., 2017</xref>). The embedded feature selection combines the feature selection in the learning and training process, both of which are completed in the same optimization. In other words, the feature selection is automatically performed during the training.</p>
      <p>Feature selection is a traditional machine learning research field with many methods. For more information, please refer to the literature (<xref rid="B25" ref-type="bibr">Li et al., 2017</xref>). The previous work proposed a feature selection method based on Deep Forest (<xref rid="B43" ref-type="bibr">Zhu et al., 2018</xref>); however, there is less work on microbiome-wide association studies <italic>via</italic> Deep Neural Network and less research is done from the perspective of embedding approach for feature selection. The challenge of feature selection based on microbial network is that there is no microbial network available at present. The commonly used statistical-based interaction network methods may lead to high false positive rate due to the compositional bias (<xref rid="B15" ref-type="bibr">Gloor et al., 2017</xref>).</p>
    </sec>
  </sec>
  <sec sec-type="materials|methods" id="s3">
    <title>Materials and Methods</title>
    <p>We mainly explain the feature selection method based on GEDFN from the following three aspects (<xref ref-type="fig" rid="f1"><bold>Figure 1</bold></xref>). First, we will introduce the construction method of microbial interaction network, including sparcc, SPIEC-EASI and Maximal Information Coefficient (MIC) then, we will introduce a deep embedding structure to embed the graph into Deep Feedforward Network. Finally, we will propose a feature selection approach for GEDFN.</p>
    <fig id="f1" position="float">
      <label>Figure 1</label>
      <caption>
        <p>The workflow of graph embedding deep network to conduct feature selection. 1. Construct microbial interaction network. The input is Operational Taxonomic Unit (OTU) abundance. Different approaches are adopted to obtain different interaction networks. The vertexes are species and the edges are correlation coefficient. 2. Graph embedding and model training. The feature graph is embedded into the first hidden layer in order to achieve sparse connection instead of fully-connected between the input layer and the first hidden layer. The first hidden layer (graph embedding layer) has the same neurons as the input layer. 3. Feature selection. The neurons (features) are ranked according to their importance score which is calculated <italic>via</italic> each neuron's connection weights.</p>
      </caption>
      <graphic xlink:href="fgene-10-01182-g001"/>
    </fig>
    <sec id="s3_1">
      <title>Microbial Correlation Network</title>
      <p>The total amount of genetic material extracted from the microbial community and the sequencing depth will affect the whole reads. It is often necessary to normalize the reads in the sample. As a result, the microbial abundance obtained by 16s sequencing is relative rather than absolute, which is not independent. The traditional statistical measures for detecting microbial interactions, for example, Pearson correlation, will lead to false positives (<xref rid="B15" ref-type="bibr">Gloor et al., 2017</xref>).</p>
      <sec id="s3_1_1">
        <title>Sparcc</title>
        <p>Assuming that the network is sparse, sparcc constructs the association network by using standard logarithmic ratio transformation and iteratively calculates the variance matrix of compositional dependence. For details of the algorithm, please refer to the literature (<xref rid="B12" ref-type="bibr">Friedman and Alm, 2012</xref>).</p>
      </sec>
      <sec id="s3_1_2">
        <title>SPIEC-EASI</title>
        <p>SPIEC-EASI assumes the network is sparse and combines logarithmic transformation of compositional data with graph inference framework to construct the network. It consists of two steps: first, logarithmic ratio transforms the data; then, SPIEC-EASI uses the neighborhood selection and sparse inverse covariance selection to infer the interaction graph from the transformed data (<xref rid="B21" ref-type="bibr">Kurtz et al., 2015</xref>).</p>
      </sec>
      <sec id="s3_1_3">
        <title>Maximal Information Coefficient</title>
        <p>The maximal information coefficient (MIC) is used to measure the degree of linear and nonlinear correlation between two variables (<xref rid="B36" ref-type="bibr">Reshef et al., 2011</xref>). The main idea of the MIC method is based on the recognition that if there is some correlation between two variables, the distribution of the data in the grid can be reflected after meshing the scatter plots formed by the two variables. The MIC divides the scatter plot of the variable pair (x, y) and uses dynamic programming to calculate and search for the maximum mutual information value that can be achieved under different split modes. Finally, the maximum mutual information value is normalized and the result is MIC.</p>
      </sec>
    </sec>
    <sec id="s3_2">
      <title>The Framework of Graph Embedding Deep Feedforward Network</title>
      <sec id="s3_2_1">
        <title>Deep Feedforward Neural Network</title>
        <p>Deep Feedforward Network, also known as feedforward neural network or multilayer perceptron, is a typical deep learning model. In this model, the information moves only in one direction from the input nodes to the output nodes through the hidden nodes. There is no loop in the network. A feedforward neural network structure with <italic>l</italic> hidden layers is:</p>
        <disp-formula>
          <label>(1)</label>
          <mml:math id="M1">
            <mml:mrow>
              <mml:mtext>P</mml:mtext>
              <mml:mrow>
                <mml:mo>(</mml:mo>
                <mml:mrow>
                  <mml:mtext>y|X</mml:mtext>
                  <mml:mo>,</mml:mo>
                  <mml:mi>θ</mml:mi>
                </mml:mrow>
                <mml:mo>)</mml:mo>
              </mml:mrow>
              <mml:mo>=</mml:mo>
              <mml:mi>f</mml:mi>
              <mml:mrow>
                <mml:mo>(</mml:mo>
                <mml:mrow>
                  <mml:msub>
                    <mml:mi>Z</mml:mi>
                    <mml:mrow>
                      <mml:mi>o</mml:mi>
                      <mml:mi>u</mml:mi>
                      <mml:mi>t</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>W</mml:mi>
                    <mml:mrow>
                      <mml:mi>o</mml:mi>
                      <mml:mi>u</mml:mi>
                      <mml:mi>t</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>+</mml:mo>
                  <mml:msub>
                    <mml:mi>b</mml:mi>
                    <mml:mrow>
                      <mml:mi>o</mml:mi>
                      <mml:mi>u</mml:mi>
                      <mml:mi>t</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
                <mml:mo>)</mml:mo>
              </mml:mrow>
            </mml:mrow>
          </mml:math>
        </disp-formula>
        <disp-formula>
          <label>(2)</label>
          <mml:math id="M2">
            <mml:mrow>
              <mml:msub>
                <mml:mi>Z</mml:mi>
                <mml:mrow>
                  <mml:mi>o</mml:mi>
                  <mml:mi>u</mml:mi>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>=</mml:mo>
              <mml:mo> </mml:mo>
              <mml:mi>σ</mml:mi>
              <mml:mrow>
                <mml:mo>(</mml:mo>
                <mml:mrow>
                  <mml:msub>
                    <mml:mi>Z</mml:mi>
                    <mml:mi>l</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>W</mml:mi>
                    <mml:mi>l</mml:mi>
                  </mml:msub>
                  <mml:mo>+</mml:mo>
                  <mml:mo> </mml:mo>
                  <mml:msub>
                    <mml:mi>b</mml:mi>
                    <mml:mi>l</mml:mi>
                  </mml:msub>
                </mml:mrow>
                <mml:mo>)</mml:mo>
              </mml:mrow>
            </mml:mrow>
          </mml:math>
        </disp-formula>
        <disp-formula>
          <label>(3)</label>
          <mml:math id="M3">
            <mml:mrow>
              <mml:msub>
                <mml:mi>Z</mml:mi>
                <mml:mrow>
                  <mml:mi>k</mml:mi>
                  <mml:mo>+</mml:mo>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
              </mml:msub>
              <mml:mo>=</mml:mo>
              <mml:mo> </mml:mo>
              <mml:mi>σ</mml:mi>
              <mml:mrow>
                <mml:mo>(</mml:mo>
                <mml:mrow>
                  <mml:msub>
                    <mml:mi>Z</mml:mi>
                    <mml:mi>k</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>W</mml:mi>
                    <mml:mi>k</mml:mi>
                  </mml:msub>
                  <mml:mo>+</mml:mo>
                  <mml:mo> </mml:mo>
                  <mml:msub>
                    <mml:mi>b</mml:mi>
                    <mml:mi>k</mml:mi>
                  </mml:msub>
                </mml:mrow>
                <mml:mo>)</mml:mo>
              </mml:mrow>
            </mml:mrow>
          </mml:math>
        </disp-formula>
        <disp-formula>
          <label>(4)</label>
          <mml:math id="M4">
            <mml:mrow>
              <mml:mo> </mml:mo>
              <mml:mo> </mml:mo>
              <mml:mo> </mml:mo>
              <mml:mo> </mml:mo>
              <mml:mo> </mml:mo>
              <mml:mo> </mml:mo>
              <mml:mo> </mml:mo>
              <mml:msub>
                <mml:mi>Z</mml:mi>
                <mml:mn>1</mml:mn>
              </mml:msub>
              <mml:mo>=</mml:mo>
              <mml:mo> </mml:mo>
              <mml:mi>σ</mml:mi>
              <mml:mrow>
                <mml:mo>(</mml:mo>
                <mml:mrow>
                  <mml:mi>X</mml:mi>
                  <mml:msub>
                    <mml:mi>W</mml:mi>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mi>n</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>+</mml:mo>
                  <mml:mo> </mml:mo>
                  <mml:msub>
                    <mml:mi>b</mml:mi>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mi>n</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
                <mml:mo>)</mml:mo>
              </mml:mrow>
              <mml:mo> </mml:mo>
              <mml:mo> </mml:mo>
              <mml:mo> </mml:mo>
              <mml:mo> </mml:mo>
              <mml:mo> </mml:mo>
              <mml:mo> </mml:mo>
              <mml:mo> </mml:mo>
            </mml:mrow>
          </mml:math>
        </disp-formula>
        <p>where X∈<italic>R</italic>
<italic><sup>nxp</sup></italic> is an input matrix with <italic>n</italic> samples and <italic>p</italic> features, y∈<italic>R</italic>
<italic><sup>n</sup></italic> is the output label for the classification task. In this work, it is a binary classification. The label for each sample is normal or disease. Z<italic><sub>out</sub></italic> and Z<italic><sub>k</sub></italic>
<italic>,</italic>(<italic>k</italic>=1,…,<italic>l</italic>-1) are the neurons in the hidden layer. W<italic><sub>k</sub></italic> is the weight matrix. b<italic><sub>k</sub></italic> is the bias. θ is the parameters. <italic>σ</italic>(·)is the activation function(such as, sigmoid, tanh, rectifiers). <italic>F</italic>(·) is a softmax function which is used to convert the output layer value into the predicted probability.</p>
        <p>The model uses a stochastic gradient descent (SGD) algorithm to minimize the cross entropy loss function to update the parameter <italic>θ</italic>. When a feedforward neural network is used to receive input x and produce an output 
<inline-formula><mml:math id="im1"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mtext> y </mml:mtext></mml:mrow><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>
. During training, forward propagation can continue until it produces a scalar cost function <italic>J</italic>(<italic>θ</italic>). The backpropagation algorithm runs information from the cost function and flow backward through the network to calculate the gradient in order to update the weight parameters (<xref rid="B16" ref-type="bibr">Goodfellow et al., 2016</xref>).</p>
        <disp-formula>
          <label>(5)</label>
          <mml:math id="M5">
            <mml:mrow>
              <mml:mi>J</mml:mi>
              <mml:mrow>
                <mml:mo>(</mml:mo>
                <mml:mi>θ</mml:mi>
                <mml:mo>)</mml:mo>
              </mml:mrow>
              <mml:mo>=</mml:mo>
              <mml:mo> </mml:mo>
              <mml:mo>−</mml:mo>
              <mml:mfrac>
                <mml:mn>1</mml:mn>
                <mml:mi>n</mml:mi>
              </mml:mfrac>
              <mml:mstyle displaystyle="true">
                <mml:msubsup>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mi>n</mml:mi>
                </mml:msubsup>
                <mml:mrow>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mi>y</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                      <mml:mi>l</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>g</mml:mi>
                      <mml:msub>
                        <mml:mover accent="true">
                          <mml:mi>p</mml:mi>
                          <mml:mo>^</mml:mo>
                        </mml:mover>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                      <mml:mo>+</mml:mo>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mrow>
                          <mml:mn>1</mml:mn>
                          <mml:mo>−</mml:mo>
                          <mml:msub>
                            <mml:mi>y</mml:mi>
                            <mml:mi>i</mml:mi>
                          </mml:msub>
                        </mml:mrow>
                        <mml:mo>)</mml:mo>
                      </mml:mrow>
                      <mml:mtext>log</mml:mtext>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mrow>
                          <mml:mn>1</mml:mn>
                          <mml:mo>−</mml:mo>
                          <mml:msub>
                            <mml:mover accent="true">
                              <mml:mi>p</mml:mi>
                              <mml:mo>^</mml:mo>
                            </mml:mover>
                            <mml:mi>i</mml:mi>
                          </mml:msub>
                        </mml:mrow>
                        <mml:mo>)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                    <mml:mo>)</mml:mo>
                  </mml:mrow>
                </mml:mrow>
              </mml:mstyle>
            </mml:mrow>
          </mml:math>
        </disp-formula>
      </sec>
      <sec id="s3_2_2">
        <title>Graph Embedding Deep Feedforward Network</title>
        <p>The fully connected deep feedforward neural network has many parameters and requires a large number of training data, but often the biological sample size is limited, which often leads to overfitting. Therefore, we construct a microbial sparse network and embed this graph network into the model. There are two main advantages. First, the sparse graph embedding will greatly reduce the parameters of deep feedforward network and mitigate the overfitting risk. Second, the sparse graph structure is derived from existing prior information and combining the priori information into the network can improve the reliability of the model. The main idea of graph embedding is to replace the full connections between the input layer and the first hidden layer with a sparse graph (<xref ref-type="fig" rid="f2"><bold>Figure 2</bold></xref>).</p>
        <fig id="f2" position="float">
          <label>Figure 2</label>
          <caption>
            <p>Graph embedding deep feedforward network (GEDFN). The graph embedding layer (first hidden layer) has same neurons with the input layer. The sparse connect between the input layer and the first hidden layer is marked as black. Other hidden layers are fully-connected.</p>
          </caption>
          <graphic xlink:href="fgene-10-01182-g002"/>
        </fig>
        <p>Consider a graph G=(V,E), V is the vertical set with <italic>p</italic> features. E is a collection of all edges. A common way of representing a graph is to use an adjacency matrix. Given a graph G with <italic>p</italic> vertices, a <italic>pxp</italic> adjacency matrix A is:</p>
        <disp-formula>
          <mml:math id="M6">
            <mml:mrow>
              <mml:msub>
                <mml:mi>A</mml:mi>
                <mml:mrow>
                  <mml:mi>i</mml:mi>
                  <mml:mi>j</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>=</mml:mo>
              <mml:mrow>
                <mml:mo>{</mml:mo>
                <mml:mrow>
                  <mml:mtable>
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mrow>
                          <mml:mn>1</mml:mn>
                          <mml:mo>,</mml:mo>
                          <mml:mtext> </mml:mtext>
                          <mml:mi>i</mml:mi>
                          <mml:mi>f</mml:mi>
                          <mml:mo> </mml:mo>
                          <mml:msub>
                            <mml:mtext>V</mml:mtext>
                            <mml:mtext>i</mml:mtext>
                          </mml:msub>
                          <mml:mo> </mml:mo>
                          <mml:mi>a</mml:mi>
                          <mml:mi>n</mml:mi>
                          <mml:mi>d</mml:mi>
                          <mml:mo> </mml:mo>
                          <mml:msub>
                            <mml:mtext>V</mml:mtext>
                            <mml:mtext>j</mml:mtext>
                          </mml:msub>
                          <mml:mo> </mml:mo>
                          <mml:mi>c</mml:mi>
                          <mml:mi>o</mml:mi>
                          <mml:mi>n</mml:mi>
                          <mml:mi>n</mml:mi>
                          <mml:mi>e</mml:mi>
                          <mml:mi>c</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>e</mml:mi>
                          <mml:mi>d</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mtext> </mml:mtext>
                          <mml:mo>∀</mml:mo>
                          <mml:mi>i</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mi>j</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mn>1</mml:mn>
                          <mml:mo>,</mml:mo>
                          <mml:mo>…</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mi>p</mml:mi>
                        </mml:mrow>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mrow>
                          <mml:mn>0</mml:mn>
                          <mml:mo>,</mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mo> </mml:mo>
                          <mml:mi>o</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>h</mml:mi>
                          <mml:mi>e</mml:mi>
                          <mml:mi>r</mml:mi>
                          <mml:mi>w</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mi>s</mml:mi>
                          <mml:mi>e</mml:mi>
                          <mml:mo>.</mml:mo>
                        </mml:mrow>
                      </mml:mtd>
                    </mml:mtr>
                  </mml:mtable>
                </mml:mrow>
              </mml:mrow>
            </mml:mrow>
          </mml:math>
        </disp-formula>
        <p>G is an undirected graph and A is a symmetric matrix. At the same time, we consider <italic>A</italic>
<italic><sub>ii</sub></italic>=1 which indicates that the vertex itself is connected. We construct a feedforward neural network in which the first hidden layer has the same dimensions as the input layer, <italic>h</italic>
<italic><sub>in=</sub></italic>
<italic>p</italic>, similarly,<italic>W</italic>
<italic><sub>in</sub></italic> is a <italic>pxp</italic> matrix. The input X is sparsely connected with Z<sub>1</sub> (<xref ref-type="fig" rid="f2"><bold>Figure 2</bold></xref>). In other words, the original fully connected layer:</p>
        <disp-formula>
          <label>(6)</label>
          <mml:math id="M7">
            <mml:mrow>
              <mml:msub>
                <mml:mi>Z</mml:mi>
                <mml:mn>1</mml:mn>
              </mml:msub>
              <mml:mo>=</mml:mo>
              <mml:mi>σ</mml:mi>
              <mml:mrow>
                <mml:mo>(</mml:mo>
                <mml:mrow>
                  <mml:mi>X</mml:mi>
                  <mml:msub>
                    <mml:mi>W</mml:mi>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mi>n</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>+</mml:mo>
                  <mml:msub>
                    <mml:mi>b</mml:mi>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mi>n</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
                <mml:mo>)</mml:mo>
              </mml:mrow>
            </mml:mrow>
          </mml:math>
        </disp-formula>
        <p>is changed to:</p>
        <disp-formula>
          <label>(7)</label>
          <mml:math id="M8">
            <mml:mrow>
              <mml:msub>
                <mml:mi>Z</mml:mi>
                <mml:mn>1</mml:mn>
              </mml:msub>
              <mml:mo>=</mml:mo>
              <mml:mi>σ</mml:mi>
              <mml:mrow>
                <mml:mo>(</mml:mo>
                <mml:mrow>
                  <mml:mi>X</mml:mi>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mi>W</mml:mi>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mi>n</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>⊙</mml:mo>
                      <mml:mi>A</mml:mi>
                    </mml:mrow>
                    <mml:mo>)</mml:mo>
                  </mml:mrow>
                  <mml:mo>+</mml:mo>
                  <mml:msub>
                    <mml:mi>b</mml:mi>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mi>n</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
                <mml:mo>)</mml:mo>
              </mml:mrow>
            </mml:mrow>
          </mml:math>
        </disp-formula>
        <p>Where 
<inline-formula><mml:math id="im2"><mml:mrow><mml:mtext> </mml:mtext><mml:mo>⊙</mml:mo></mml:mrow></mml:math></inline-formula>
is element-wise product. Therefore, the connection between the input and first hidden layer of the feedforward network is filtered by the graph adjacency matrix. Each feature is corresponding to a hidden neuron. All features have corresponding hidden neurons in the first hidden layer. The feature can only provide information to the connected graph. In this way, the graph helps to achieve the sparsity of the connection between the input layer and the first hidden layer (<xref rid="B20" ref-type="bibr">Kong and Yu, 2018</xref>).</p>
      </sec>
    </sec>
    <sec id="s3_3">
      <title>Feature Selection Based on GEDFN</title>
      <p>In addition to improving classification, it is also meaningful to find features that contribute significantly to classification because they reveal potential biological mechanisms. However, Deep neural network is a “black box”, the interpretability of deep learning hasn't been well-defined (<xref rid="B17" ref-type="bibr">Guidotti et al., 2019</xref>). In our experiment, we focus on how the input features influence the prediction and we borrow the idea from <xref rid="B31" ref-type="bibr">Olden and Jackson (2002)</xref> and <xref rid="B20" ref-type="bibr">Kong and Yu (2018)</xref>. The feature importance score is the quantification values of the contributions of features to a model prediction, which links the input features and output prediction. They highlight the parts of a given input that are most influential for the model prediction and thereby help to explain why such a prediction was made. The feature selection is based on feature score, which means the score is high if the feature is important. As a result, we develop a feature ranking method based on the feature relative importance score, similar to the connection weights method introduced by <xref rid="B31" ref-type="bibr">Olden and Jackson (2002)</xref> and <xref rid="B20" ref-type="bibr">Kong and Yu (2018)</xref>. What is learned by neural networks is contained in the connection weights. Based on idea of connection weight, we propose a graphical connect weight method that emphasizes the importance of the features of our proposed neural network architecture.</p>
      <p>The main idea of a graphical connect weight is: the contribution of a particular variable directly reflects the magnitude of the connection weights associated with the corresponding hidden neurons in the graph embedding layer. The sum of the absolute values of the directly related weights for a neuron (or feature) gives its relative importance:</p>
      <disp-formula>
        <label>(8)</label>
        <mml:math id="M9">
          <mml:mrow>
            <mml:msub>
              <mml:mi>s</mml:mi>
              <mml:mi>j</mml:mi>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:msub>
              <mml:mi>γ</mml:mi>
              <mml:mi>j</mml:mi>
            </mml:msub>
            <mml:mstyle displaystyle="true">
              <mml:msubsup>
                <mml:mo>∑</mml:mo>
                <mml:mrow>
                  <mml:mi>k</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
                <mml:mi>p</mml:mi>
              </mml:msubsup>
              <mml:mrow>
                <mml:mrow>
                  <mml:mo>|</mml:mo>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mi>w</mml:mi>
                      <mml:mrow>
                        <mml:mi>k</mml:mi>
                        <mml:mi>j</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mi>i</mml:mi>
                        <mml:mi>n</mml:mi>
                        <mml:mo stretchy="false">)</mml:mo>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mi>I</mml:mi>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:msub>
                      <mml:mi>A</mml:mi>
                      <mml:mrow>
                        <mml:mi>k</mml:mi>
                        <mml:mi>j</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>|</mml:mo>
                  <mml:mrow>
                    <mml:mo>+</mml:mo>
                    <mml:msubsup>
                      <mml:mi>Σ</mml:mi>
                      <mml:mrow>
                        <mml:mi>m</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mi>h</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:msub>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                </mml:mrow>
              </mml:mrow>
            </mml:mstyle>
            <mml:mrow>
              <mml:mo>|</mml:mo>
              <mml:mrow>
                <mml:msubsup>
                  <mml:mi>w</mml:mi>
                  <mml:mrow>
                    <mml:mi>j</mml:mi>
                    <mml:mi>m</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mn>1</mml:mn>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                </mml:msubsup>
              </mml:mrow>
              <mml:mo>|</mml:mo>
            </mml:mrow>
            <mml:mo>,</mml:mo>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <disp-formula>
        <label>(9)</label>
        <mml:math id="M10">
          <mml:mrow>
            <mml:msub>
              <mml:mi>γ</mml:mi>
              <mml:mi>j</mml:mi>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:mi>min</mml:mi>
            <mml:mo>⁡</mml:mo>
            <mml:mrow>
              <mml:mo>(</mml:mo>
              <mml:mrow>
                <mml:mi>c</mml:mi>
                <mml:mo stretchy="false">/</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:msubsup>
                    <mml:mo>∑</mml:mo>
                    <mml:mrow>
                      <mml:mi>k</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mi>p</mml:mi>
                  </mml:msubsup>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mi>A</mml:mi>
                          <mml:mrow>
                            <mml:mi>k</mml:mi>
                            <mml:mi>j</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mo>)</mml:mo>
                    </mml:mrow>
                    <mml:mo>,</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:mstyle>
              </mml:mrow>
              <mml:mo>)</mml:mo>
            </mml:mrow>
            <mml:mo>,</mml:mo>
            <mml:mo> </mml:mo>
            <mml:mi>j</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mn>1</mml:mn>
            <mml:mo>,</mml:mo>
            <mml:mo> </mml:mo>
            <mml:mo>…</mml:mo>
            <mml:mo>,</mml:mo>
            <mml:mi>p</mml:mi>
            <mml:mo>.</mml:mo>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <p>Where s<italic><sub>j</sub></italic> is importance score of the feature <italic>j w</italic>
<sup>(</sup>
<italic><sup>in</sup></italic>
<sup>)</sup> indicates the weights between the input layer and the first hidden layer, while w<sup>(1)</sup> indicates the weights between the first and second hidden layer. The constant <italic>c</italic> is to penalize vertices with too many connections so that they don’t over impact the result. In the following experiments, we set the parameter <italic>c </italic>= 50.</p>
    </sec>
  </sec>
  <sec id="s4">
    <title>Experiments and Results</title>
    <sec id="s4_1">
      <title>Data Set</title>
      <p>Inflammatory bowel diseases (IBD) are a group of specific chronic intestinal diseases, mainly including Crohn's disease and ulcerative colitis. The occurrence and development of IBD are closely related to intestinal microorganisms (<xref rid="B13" ref-type="bibr">Gevers et al., 2014</xref>). In our experiment, OTU BIOM files and metadata were downloaded from the QIITA (<uri xlink:type="simple" xlink:href="https://qiita.ucsd.edu/">https://qiita.ucsd.edu/</uri>) database (study id: 1939). The detailed experiment was described in <xref rid="B13" ref-type="bibr">Gevers et al., 2014</xref>. The IBD data set consists of 1,359 metagenomic samples, including rectal, ileal biopsy and fecal samples (<xref rid="B13" ref-type="bibr">Gevers et al., 2014</xref>). We retained samples of mucosal tissue biopsies (terminal ileum and rectum) samples under the age of 18. The control group were without inflammatory conditions, such as abdominal pain and diarrhea. The final data set consisted of 657 IBD samples and 316 normal samples, respectively. We used QIIME's taxa collapse to filter the strain's species, limiting features at genus level.</p>
    </sec>
    <sec id="s4_2" sec-type="results">
      <title>Results</title>
      <sec id="s4_2_1">
        <title>The Hyperparameters of Graph Embedding Deep Feedforward Neural Network</title>
        <p>The structure of the graph embedding deep feedforward neural network (GEDFN) is shown in Figure 2. The most important part of GEDFN is that the number of neurons in the first hidden layer is the same as the number of neurons in the input layer and they are sparsely connected, which is different with normal fully connected feed forward neural network. The second layer, third and fourth hidden layers are consisting of 128, 64 and 16 neurons respectively and they are fully connected.</p>
        <p>We use three different methods to construct a microbial co-occurrence interaction network from microbial abundance data. When the sparcc method is used to build the network, we reserve the vertexes if the correlation of two vertexes is larger than 0.3. We get an adjacency network with 63 vertexes and 315 edges. We adopt the mictools (<xref rid="B1" ref-type="bibr">Albanese et al., 2018</xref>) to build the MIC relevant network and we get 279 vertexes and 3230 edges when the correlation threshold is 0.2. The network constructed by sparcc and SPEC-EASI methods is sparse while MIC gets relatively a dense network. Different methods get different interaction networks. We find the higher the threshold, the more reliable is the network. However, the high threshold will make the network too sparse. As a result, we combine three kinds of networks to get a larger network with 736 vertexes and 18,034 edges. In this way, the connections between the input layer and the first hidden layer are more reliable and less dense than the fully connected approach.</p>
        <p>Other hyperparameters of GEDFN are as follows: the learning rate is 0.0001, the activation function is Rectified Linear Unit (ReLU) and the weight initializer is he_uniform, the drop out is 0.2. the code is implemented in keras and available at <uri xlink:type="simple" xlink:href="https://github.com/MicroAVA/GEDFN.git">https://github.com/MicroAVA/GEDFN.git</uri>. </p>
      </sec>
      <sec id="s4_2_2">
        <title>The Evaluation of Classification</title>
        <p>Traditional classification methods such as Random Forest has been shown to be the best performers in omics data classification tasks and the results show that Random Forest has achieved the best performance on microbial classification (<xref rid="B33" ref-type="bibr">Pasolli et al., 2016</xref>). Therefore, we compare GEDFN with Deep Forest (DF), Random Forest (RF) and Support Vector Machines (SVM). For the binary classification, we calculate the Area Under the Receiver Operating Characteristics (AUROC) and classification accuracy for each method (<xref ref-type="fig" rid="f3"><bold>Figure 3</bold></xref>).</p>
        <fig id="f3" position="float">
          <label>Figure 3</label>
          <caption>
            <p>The Area Under Receiver Operating Characteristic curve (left) and accuracy of classification (right) for GEDFN, Deep Forest (DF), Random Forest (RF) and Support Vector Machines (SVM). Left: the grey dash line is the chance discrimination that located on diagonal line (AUC = 0.5). The maximum AUC = 1 means the classifier could discriminate the diseased and non-diseased perfectly while AUC = 0 means the classifier incorrectly classified all subjects with diseased as negative and all subjects with non-diseased as positive. The AUC is averaged through a five-fold cross validation. Right: the boxplot for classifiers’ classification accuracy.</p>
          </caption>
          <graphic xlink:href="fgene-10-01182-g003"/>
        </fig>
        <p>AUROC curve is a performance measurement for classification problem at various thresholds settings, which can evaluate classifiers considering all true positives (TP), false positives (FP), true negatives (TN) and false negatives (FN). Receiver Operating Characteristics (ROC) is a probability curve and Area Under the Curve (AUC) represents degree or measure of separability. It tells how much a model is capable of distinguishing between classes. The higher the AUC, the better the model is at predicting 0s as 0s and 1s as 1s. By analogy, the higher the AUC, the better the model is at distinguishing between patients with disease and no disease. The ROC curve is plotted with true positive rate (TPR) against the false positive rate (FPR) where TPR is on the y-axis and FPR is on the x-axis.</p>
        <disp-formula>
          <mml:math id="M11">
            <mml:mrow>
              <mml:mtext>TPR</mml:mtext>
              <mml:mo>=</mml:mo>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mi>T</mml:mi>
                  <mml:mi>P</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>T</mml:mi>
                  <mml:mi>P</mml:mi>
                  <mml:mo>+</mml:mo>
                  <mml:mi>F</mml:mi>
                  <mml:mi>N</mml:mi>
                </mml:mrow>
              </mml:mfrac>
              <mml:mo> </mml:mo>
              <mml:mo>,</mml:mo>
              <mml:mo> </mml:mo>
              <mml:mi>F</mml:mi>
              <mml:mi>P</mml:mi>
              <mml:mi>R</mml:mi>
              <mml:mo>=</mml:mo>
              <mml:mo> </mml:mo>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mi>F</mml:mi>
                  <mml:mi>P</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>T</mml:mi>
                  <mml:mi>N</mml:mi>
                  <mml:mo>+</mml:mo>
                  <mml:mi>F</mml:mi>
                  <mml:mi>P</mml:mi>
                </mml:mrow>
              </mml:mfrac>
            </mml:mrow>
          </mml:math>
        </disp-formula>
        <p>The classification accuracy means the percentage of correct predictions from the total number of predictions made.</p>
        <disp-formula>
          <mml:math id="M12">
            <mml:mrow>
              <mml:mtext>ACC</mml:mtext>
              <mml:mo>=</mml:mo>
              <mml:mtext> </mml:mtext>
              <mml:mfrac>
                <mml:mn>1</mml:mn>
                <mml:mi>m</mml:mi>
              </mml:mfrac>
              <mml:munderover>
                <mml:mstyle displaystyle="true">
                  <mml:mo>∑</mml:mo>
                </mml:mstyle>
                <mml:mrow>
                  <mml:mi>i</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
                <mml:mi>m</mml:mi>
              </mml:munderover>
              <mml:mi>I</mml:mi>
              <mml:mrow>
                <mml:mo>(</mml:mo>
                <mml:mrow>
                  <mml:mover accent="true">
                    <mml:mi>y</mml:mi>
                    <mml:mo>^</mml:mo>
                  </mml:mover>
                  <mml:mo>=</mml:mo>
                  <mml:mi>y</mml:mi>
                </mml:mrow>
                <mml:mo>)</mml:mo>
              </mml:mrow>
            </mml:mrow>
          </mml:math>
        </disp-formula>
        <p>Where 
<inline-formula><mml:math id="im3"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>
is the predicted label and <italic>y</italic>
<italic><sub>i</sub></italic> is the true label for the sample <italic>i</italic>. The <italic>m</italic> means the sample size and <italic>I</italic>(·)is the indicator function.</p>
        <p>In this experiment, we adopt a five-fold cross-validation. We use the implementation of Random Forest in python's scikit-learn package. We set the estimator parameter to 300. The Deep Forest is based on the work (<xref rid="B43" ref-type="bibr">Zhu et al., 2018</xref>). From the AUC value, we find that the Graph Embedding Deep Feedforward Network (GEDFN) is much better than SVM (AUC = 0.663). Compared with Deep Forest and Random Forest, GEDFN is also very competitive. GEDFN achieves an AUC value of 0.843, which is slightly better than Deep Forest (AUC = 0.834) and Random Forest (AUC = 0.823). In terms of classification accuracy, GEDFN achieves an average accuracy of 79.52%, Deep Forest achieves 76.6% and Random Forest achieves 75.16%. GEDFN outperforms 2–4% than Deep Forest and Random Forest. These methods are much better than SVM (67.5%).</p>
      </sec>
      <sec id="s4_2_3">
        <title>The Evaluation of Feature Selection</title>
        <p>In our experiment, we compare GEDFN with traditional feature selection methods, such as minimum redundancy and maximum Relevance (mRMR) (<xref rid="B8" ref-type="bibr">Ding and Peng, 2005</xref>), Random Forest and Deep Forest respectively. Each method selects 50 features. We want to know if the features obtained by the traditional machine learning feature selection method can also be selected by GEDFN. As can be seen from the Venn diagram (<xref ref-type="fig" rid="f4"><bold>Figure 4</bold></xref>), most of the features selected by the mRMR are different from those selected by the other three methods. Among these 50 features selected by GEDFN, there are 25 and 21 features which are consistent with the Random Forest and Deep Forest respectively.</p>
        <fig id="f4" position="float">
          <label>Figure 4</label>
          <caption>
            <p>The feature selection based on Graph Embedding Deep Feedforward Network (GEDFN). The Venn diagram for top the 50 features selected <italic>via</italic> minimum Redundancy and Maximum Relevance (mRMR), Random Forest (RF), Deep Forest (DF) and GEDFN.</p>
          </caption>
          <graphic xlink:href="fgene-10-01182-g004"/>
        </fig>
        <p>In addition, we compare the performance of GEDFN + SVM, RF + SVM, RF + SVM and RF + DF. Our approach is to select top 10, top 15, top 20,…, top 50 feature subsets from GEDFN and RF respectively, and test them on SVM and Deep Forest (DF) classifiers with five-fold cross-validation (<xref rid="T1" ref-type="table"><bold>Table 1</bold></xref>). GEDFN + SVM, means GEDFN is utilized to conduct feature selection and SVM is the classifier. RF + SVM, means RF is utilized to conduct feature selection and SVM is the classifier. GEDFN + DF, means GEDFN is utilized to conduct feature selection and DF is the classifier. RF + DF, means RF is utilized to conduct feature selection and DF is the classifier.</p>
        <table-wrap id="T1" position="float">
          <label>Table 1</label>
          <caption>
            <p>The performance among GEDFN + SVM, RF + SVM, GEDFN + DF and RF + DF.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th valign="top" rowspan="1" colspan="1">#</th>
                <th valign="top" colspan="3" rowspan="1">GEDFN + SVM</th>
                <th valign="top" colspan="3" rowspan="1">RF + SVM</th>
                <th valign="top" colspan="3" rowspan="1">
                  <bold>GEDGN+DF</bold>
                </th>
                <th valign="top" colspan="3" rowspan="1">
                  <bold>RF+DF</bold>
                </th>
              </tr>
              <tr>
                <th valign="top" rowspan="1" colspan="1"/>
                <th valign="top" rowspan="1" colspan="1">P</th>
                <th valign="top" rowspan="1" colspan="1">R</th>
                <th valign="top" rowspan="1" colspan="1">F1</th>
                <th valign="top" rowspan="1" colspan="1">P</th>
                <th valign="top" rowspan="1" colspan="1">R</th>
                <th valign="top" rowspan="1" colspan="1">F1</th>
                <th valign="top" rowspan="1" colspan="1">P</th>
                <th valign="top" rowspan="1" colspan="1">R</th>
                <th valign="top" rowspan="1" colspan="1">F1</th>
                <th valign="top" rowspan="1" colspan="1">P</th>
                <th valign="top" rowspan="1" colspan="1">R</th>
                <th valign="top" rowspan="1" colspan="1">F1</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td valign="top" rowspan="1" colspan="1">10</td>
                <td valign="top" rowspan="1" colspan="1">0.733</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">
                  <bold>0.846</bold>
                </td>
                <td valign="top" rowspan="1" colspan="1">0.675</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">0.806</td>
                <td valign="top" rowspan="1" colspan="1">0.733</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">
                  <bold>0.846</bold>
                </td>
                <td valign="top" rowspan="1" colspan="1">0.785</td>
                <td valign="top" rowspan="1" colspan="1">0.871</td>
                <td valign="top" rowspan="1" colspan="1">0.825</td>
              </tr>
              <tr>
                <td valign="top" rowspan="1" colspan="1">15</td>
                <td valign="top" rowspan="1" colspan="1">0.745</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">
                  <bold>0.854</bold>
                </td>
                <td valign="top" rowspan="1" colspan="1">0.675</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">0.806</td>
                <td valign="top" rowspan="1" colspan="1">0.745</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">
                  <bold>0.854</bold>
                </td>
                <td valign="top" rowspan="1" colspan="1">0.722</td>
                <td valign="top" rowspan="1" colspan="1">0.909</td>
                <td valign="top" rowspan="1" colspan="1">0.800</td>
              </tr>
              <tr>
                <td valign="top" rowspan="1" colspan="1">20</td>
                <td valign="top" rowspan="1" colspan="1">0.752</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">
                  <bold>0.858</bold>
                </td>
                <td valign="top" rowspan="1" colspan="1">0.675</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">0.806</td>
                <td valign="top" rowspan="1" colspan="1">0.750</td>
                <td valign="top" rowspan="1" colspan="1">0.991</td>
                <td valign="top" rowspan="1" colspan="1">0.854</td>
                <td valign="top" rowspan="1" colspan="1">0.717</td>
                <td valign="top" rowspan="1" colspan="1">0.927</td>
                <td valign="top" rowspan="1" colspan="1">0.805</td>
              </tr>
              <tr>
                <td valign="top" rowspan="1" colspan="1">25</td>
                <td valign="top" rowspan="1" colspan="1">0.706</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">0.828</td>
                <td valign="top" rowspan="1" colspan="1">0.675</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">0.806</td>
                <td valign="top" rowspan="1" colspan="1">0.705</td>
                <td valign="top" rowspan="1" colspan="1">0.991</td>
                <td valign="top" rowspan="1" colspan="1">0.824</td>
                <td valign="top" rowspan="1" colspan="1">0.765</td>
                <td valign="top" rowspan="1" colspan="1">0.907</td>
                <td valign="top" rowspan="1" colspan="1">
                  <bold>0.829</bold>
                </td>
              </tr>
              <tr>
                <td valign="top" rowspan="1" colspan="1">30</td>
                <td valign="top" rowspan="1" colspan="1">0.707</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">
                  <bold>0.828</bold>
                </td>
                <td valign="top" rowspan="1" colspan="1">0.675</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">0.806</td>
                <td valign="top" rowspan="1" colspan="1">0.707</td>
                <td valign="top" rowspan="1" colspan="1">0.983</td>
                <td valign="top" rowspan="1" colspan="1">0.823</td>
                <td valign="top" rowspan="1" colspan="1">0.718</td>
                <td valign="top" rowspan="1" colspan="1">0.957</td>
                <td valign="top" rowspan="1" colspan="1">0.821</td>
              </tr>
              <tr>
                <td valign="top" rowspan="1" colspan="1">35</td>
                <td valign="top" rowspan="1" colspan="1">0.698</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">
                  <bold>0.822</bold>
                </td>
                <td valign="top" rowspan="1" colspan="1">0.675</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">0.806</td>
                <td valign="top" rowspan="1" colspan="1">0.698</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">
                  <bold>0.822</bold>
                </td>
                <td valign="top" rowspan="1" colspan="1">0.692</td>
                <td valign="top" rowspan="1" colspan="1">0.977</td>
                <td valign="top" rowspan="1" colspan="1">0.810</td>
              </tr>
              <tr>
                <td valign="top" rowspan="1" colspan="1">40</td>
                <td valign="top" rowspan="1" colspan="1">0.704</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">
                  <bold>0.826</bold>
                </td>
                <td valign="top" rowspan="1" colspan="1">0.675</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">0.806</td>
                <td valign="top" rowspan="1" colspan="1">0.709</td>
                <td valign="top" rowspan="1" colspan="1">0.985</td>
                <td valign="top" rowspan="1" colspan="1">0.824</td>
                <td valign="top" rowspan="1" colspan="1">0.706</td>
                <td valign="top" rowspan="1" colspan="1">0.962</td>
                <td valign="top" rowspan="1" colspan="1">0.813</td>
              </tr>
              <tr>
                <td valign="top" rowspan="1" colspan="1">45</td>
                <td valign="top" rowspan="1" colspan="1">0.707</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">
                  <bold>0.828</bold>
                </td>
                <td valign="top" rowspan="1" colspan="1">0.675</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">0.806</td>
                <td valign="top" rowspan="1" colspan="1">0.707</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">
                  <bold>0.828</bold>
                </td>
                <td valign="top" rowspan="1" colspan="1">0.687</td>
                <td valign="top" rowspan="1" colspan="1">0.991</td>
                <td valign="top" rowspan="1" colspan="1">0.811</td>
              </tr>
              <tr>
                <td valign="top" rowspan="1" colspan="1">50</td>
                <td valign="top" rowspan="1" colspan="1">0.697</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">
                  <bold>0.822</bold>
                </td>
                <td valign="top" rowspan="1" colspan="1">0.675</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">0.806</td>
                <td valign="top" rowspan="1" colspan="1">0.697</td>
                <td valign="top" rowspan="1" colspan="1">1</td>
                <td valign="top" rowspan="1" colspan="1">
                  <bold>0.822</bold>
                </td>
                <td valign="top" rowspan="1" colspan="1">0.695</td>
                <td valign="top" rowspan="1" colspan="1">0.974</td>
                <td valign="top" rowspan="1" colspan="1">0.810</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <p>#, number of top features; P, precision; R, recall; F1= 
<inline-formula><mml:math id="im4"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>
. The best F1 scores are marked as bold.</p>
          </table-wrap-foot>
        </table-wrap>
        <p>From <xref rid="T1" ref-type="table"><bold>Table 1</bold></xref>, the combination of GEDFN and SVM achieves the best f1 score, while RF + SVM gets the worst performance. Meanwhile, GEDFN + SVM and GEDFN + DF have consistent performance. We find GEDFN prefers the sparse features while RF prefers the dense features. In other words, RF has a bias in the feature selection process where multivalued features are favored (<xref rid="B29" ref-type="bibr">Nguyen et al., 2015</xref>). In addition, RF is biased in the presence of correlation and often identifies non-predictive features that are independent from each other (<xref rid="B30" ref-type="bibr">Nicodemus and Malley, 2009</xref>). Actually, the microbial data is sparse and the features are dependent, which makes RF not the best choice to conduct feature selection in microbiome. However, GEDFN is to embed the <italic>priori</italic> sparse correlation network and find biomarkers as a whole, which makes it more suitable for microbiome-wide association studies than RF-based models.</p>
        <p>The cophenetic similarity or cophenetic distance of two objects is a measure of how similar those two objects have to be in order to be grouped into the same cluster (<xref rid="B38" ref-type="bibr">Sokal and Rohlf, 1962</xref>; <xref rid="B37" ref-type="bibr">Saraçli et al., 2013</xref>). We calculate the cophenetic distance of the feature subsets. The specific process is as follows: we select different feature subsets obtained by Random Forest, Deep Forest and GEDFN, such as top 10–50 features, and then calculate node-node pairwise distance. The distance is characterized by the leaf nodes of the phylogenetic tree. We use the cophenetic method of the ape package in R to calculate the node-node pairwise cophenetic distance. The value in the matrix is the sum of the branch lengths separating each pair of species. We compare the top 50 features of Random Forest, Deep Forest and GEDFN respectively. We find the feature subsets of GEDFN has smallest cophenetic distances among these methods, which means that the subset of these features is better cohesive and we speculate that this cohesion may be functional meaningful (<xref ref-type="fig" rid="f5"><bold>Figure 5</bold></xref>). Deep Forest and Random Forest have similar cophenetic distance because Deep Forest is a cascade structure based on Random Forest.</p>
        <fig id="f5" position="float">
          <label>Figure 5</label>
          <caption>
            <p>The cophenetic distance for top 50 features selected <italic>via</italic> Random Forest (RF), Deep Forest (DF) and Graph Embedding Deep Feedforward Network (GEDFN) respectively (The cophenetic distance is the sum of the features' pair-wise distance.). The cophenetic distance of two objects is a measure of how similar those two objects have to be in order to be grouped into the same cluster.</p>
          </caption>
          <graphic xlink:href="fgene-10-01182-g005"/>
        </fig>
        <p>In addition, we utilize interactive Tree Of Life (iTOL) (<xref rid="B24" ref-type="bibr">Letunic and Bork, 2016</xref>) to visualize the top 20 features selected by GEDFN (<xref ref-type="fig" rid="f6"><bold>Figure 6</bold></xref>). The features are ranked according to their importance score. We average each species' relative abundance for diseased and normal groups respectively. We find that <italic>Neisseria</italic>, <italic>Pasteurellaceae</italic>, <italic>Bamesiellaceae</italic>, <italic>S24-7</italic>, <italic>Fusobacterium</italic>, <italic>Anaeroplasma</italic> and <italic>Gemellaceae</italic> had high abundance compared to the normal group, while other microorganisms are lowly expressed in the disease group. The <italic>Neisseria</italic>, <italic>Pasteurellaceae</italic>, <italic>Fusobacterium</italic> and <italic>Gemellaceae</italic> increased in Crohn's disease, which was reported in the research (<xref rid="B13" ref-type="bibr">Gevers et al., 2014</xref>). The <italic>Clostridiales</italic>, <italic>Eubacterium</italic>, <italic>Erysipelotrichaceae</italic> and <italic>Peptostreptococcaceae</italic>, <italic>Christensenellaceae</italic> were found in lower relative abundance in Crohn's disease (<xref rid="B13" ref-type="bibr">Gevers et al., 2014</xref>; <xref rid="B27" ref-type="bibr">Matsuoka and Kanai, 2015</xref>; <xref rid="B32" ref-type="bibr">Pascal et al., 2017</xref>). However, there is no unified option on the Crohn's disease-related microbial biomarkers. As a result, our findings must need further experiments to explore and verify.</p>
        <fig id="f6" position="float">
          <label>Figure 6</label>
          <caption>
            <p>The top 20 species selected <italic>via</italic> Graph Embedding Deep Feedforward Network (GEDFN). The species in red circle are higher relative abundance while species in blue star are lower relative abundance in diseased group. These species are visualized on the phylogenetic tree.</p>
          </caption>
          <graphic xlink:href="fgene-10-01182-g006"/>
        </fig>
      </sec>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="s5">
    <title>Conclusions</title>
    <p>In this work, we propose a method of embedding a microbial graph into a Deep Feedforward Network to achieve feature selection purpose. We have verified the feasibility of this method through experiments. The main contributions of our work are as follows: Firstly, the feasibility of this method is verified through combining microbial interaction structure and deep learning, and a sparse network structure is proposed. Secondly, the feature selection method is introduced into the microbial sparse network and the reliability of the feature selection results is verified, indicating that deep neural networks can also conduct feature selection. We hope our work will bring another perspective to the interpretability of deep learning.</p>
    <p>The problems still exist in the research work. First of all, our work does not compare the influence of various methods of constructing microbial networks on feature selection (<xref rid="B41" ref-type="bibr">Weiss et al., 2016</xref>). The networks constructed by various methods are varying. We found that the reliability of the microbial network directly affected the subsequent results. Secondly, the threshold of association network was traded off and there was no relevant guidance suggestion. In general, the higher the threshold, the more reliable the network, but it would make the network too sparse. It would be required to balance the threshold and the network's sparseness. Finally, we only consider the influence of the weight parameters of the Deep Neural Network on the feature selection without considering the threshold of the neuron. Because it would involve the nonlinear transformation which could make the problem complicated and difficult. Therefore, our future work will focus on how to build a more reliable microbial interaction network and get more meaningful microbial markers.</p>
  </sec>
  <sec sec-type="data-availability" id="s6">
    <title>Data Availability Statement</title>
    <p>The datasets generated for this study are available on request to the corresponding author.</p>
  </sec>
  <sec id="s7">
    <title>Author Contributions</title>
    <p>Qiang Z, XJ and TH conceived the concept of the work and designed the experiments. Qing Z and MP performed literature search. Qing Z, XJ, MP and TH collected and analyzed the data. Qiang Z, XJ and MP wrote the paper. All authors have approved the final manuscript.</p>
  </sec>
  <sec sec-type="funding-information" id="s8">
    <title>Funding</title>
    <p>This research is supported by the National Key Research and Development Program of China (2017YFC0909502) and the National Natural Science Foundation of China (No. 61532008 and 61872157).</p>
  </sec>
  <sec id="s9">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albanese</surname><given-names>D.</given-names></name><name><surname>Riccadonna</surname><given-names>S.</given-names></name><name><surname>Donati</surname><given-names>C.</given-names></name><name><surname>Franceschi</surname><given-names>P.</given-names></name></person-group> (<year>2018</year>). <article-title>A practical tool for maximal information coefficient analysis</article-title>. <source>GigaScience</source>
<volume>7</volume> (<issue>4</issue>), <fpage>giy032</fpage>. <pub-id pub-id-type="doi">10.1093/gigascience/giy032</pub-id>
</mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angermueller</surname><given-names>C.</given-names></name><name><surname>Pärnamaa</surname><given-names>T.</given-names></name><name><surname>Parts</surname><given-names>L.</given-names></name><name><surname>Stegle</surname><given-names>O.</given-names></name></person-group> (<year>2016</year>). <article-title>Deep learning for computational biology</article-title>. <source>Mol. Syst. Biol.</source>
<volume>12</volume> (<issue>7</issue>), <fpage>878</fpage>. <pub-id pub-id-type="doi">10.15252/msb.20156651</pub-id>
<pub-id pub-id-type="pmid">27474269</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bray</surname><given-names>J. R.</given-names></name><name><surname>Curtis</surname><given-names>J. T.</given-names></name></person-group> (<year>1957</year>). <article-title>An ordination of the upland forest communities of southern wisconsin</article-title>. <source>Ecol. Monographs</source>
<volume>27</volume> (<issue>4</issue>), <fpage>325</fpage>–<lpage>349</lpage>. <pub-id pub-id-type="doi">10.2307/1942268</pub-id>
</mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brüls</surname><given-names>T.</given-names></name><name><surname>Weissenbach</surname><given-names>J.</given-names></name></person-group> (<year>2011</year>). <article-title>The human metagenome: our other genome</article-title>. <source>Hum. Mol. Genet.</source>
<volume>20 </volume>(<issue>R2</issue>), <fpage>R142</fpage>–<lpage>R148</lpage>. <pub-id pub-id-type="doi">10.1093/hmg/ddr353</pub-id>
<pub-id pub-id-type="pmid">21840927</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Camacho</surname><given-names>D. M.</given-names></name><name><surname>Collins</surname><given-names>K. M.</given-names></name><name><surname>Powers</surname><given-names>R. K.</given-names></name><name><surname>Costello</surname><given-names>J. C.</given-names></name><name><surname>Collins</surname><given-names>J. J.</given-names></name></person-group> (<year>2018</year>). <article-title>Next-generation machine learning for biological networks</article-title>. <source>Cell</source>
<volume>173</volume> (<issue>7</issue>), <fpage>1581</fpage>–<lpage>1592</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2018.05.015</pub-id>
<pub-id pub-id-type="pmid">29887378</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ching</surname><given-names>T.</given-names></name><name><surname>Himmelstein</surname><given-names>D. S.</given-names></name><name><surname>Beaulieu-Jones</surname><given-names>B. K.</given-names></name><name><surname>Kalinin</surname><given-names>A. A.</given-names></name><name><surname>Do</surname><given-names>B. T.</given-names></name><name><surname>Way</surname><given-names>G. P.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Opportunities and obstacles for deep learning in biology and medicine</article-title>. <source>J. Royal Soc. Inter.</source>
<volume>15</volume> (<issue>141</issue>), <fpage>20170387</fpage>. <pub-id pub-id-type="doi">10.1098/rsif.2017.0387</pub-id>
</mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>Y.</given-names></name><name><surname>Jiang</surname><given-names>Y. H.</given-names></name><name><surname>Yang</surname><given-names>Y.</given-names></name><name><surname>He</surname><given-names>Z.</given-names></name><name><surname>Luo</surname><given-names>F.</given-names></name><name><surname>Zhou</surname><given-names>J.</given-names></name></person-group> (<year>2012</year>). <article-title>Molecular ecological network analyses</article-title>. <source>BMC Bioinformatics</source>
<volume>13</volume> (<issue>1</issue>), <fpage>113</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2105-13-113</pub-id>
<pub-id pub-id-type="pmid">22646978</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>C.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name></person-group> (<year>2005</year>). <article-title>Minimum redundancy feature selection from microarray gene expression data</article-title>. <source>J. Bioinform. Comput. Biol.</source>
<volume>3</volume> (<issue>02</issue>), <fpage>185</fpage>–<lpage>205</lpage>. <pub-id pub-id-type="doi">10.1142/S0219720005001004</pub-id>
<pub-id pub-id-type="pmid">15852500</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eraslan</surname><given-names>G.</given-names></name><name><surname>Avsec</surname><given-names>Ž.</given-names></name><name><surname>Gagneur</surname><given-names>J.</given-names></name><name><surname>Theis</surname><given-names>F. J.</given-names></name></person-group> (<year>2019</year>). <article-title>Deep learning: new computational modelling techniques for genomics</article-title>. <source>Nat. Rev. Genet.</source>
<volume>20</volume> (<issue>7</issue>), <fpage>389</fpage>–<lpage>403</lpage>. <pub-id pub-id-type="doi">10.1038/s41576-019-0122-6</pub-id>
<pub-id pub-id-type="pmid">30971806</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faust</surname><given-names>K.</given-names></name><name><surname>Raes</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). <article-title>CoNet app: inference of biological association networks using cytoscape</article-title>. <source>F1000 Research</source>
<volume>5</volume>, <fpage>1519</fpage>. <pub-id pub-id-type="doi">10.12688/f1000research.9050.2</pub-id> F1000.<pub-id pub-id-type="pmid">27853510</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faust</surname><given-names>K.</given-names></name><name><surname>Sathirapongsasuti</surname><given-names>J. F.</given-names></name><name><surname>Izard</surname><given-names>J.</given-names></name><name><surname>Segata</surname><given-names>N.</given-names></name><name><surname>Gevers</surname><given-names>D.</given-names></name><name><surname>Raes</surname><given-names>J.</given-names></name><etal/></person-group> (<year>2012</year>). <article-title>Microbial co-occurrence relationships in the human microbiome</article-title>. <source>PLoS Comput. Biol.</source>
<volume>8</volume> (<issue>7</issue>), <elocation-id>e1002606</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1002606</pub-id>
<pub-id pub-id-type="pmid">22807668</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedman</surname><given-names>J.</given-names></name><name><surname>Alm</surname><given-names>E. J.</given-names></name></person-group> (<year>2012</year>). <article-title>Inferring correlation networks from genomic survey data</article-title>. <source>PLoS Comput. Biol.</source>
<volume>8</volume> (<issue>9</issue>), <elocation-id>e1002687</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1002687</pub-id>
<pub-id pub-id-type="pmid">23028285</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gevers</surname><given-names>D.</given-names></name><name><surname>Kugathasan</surname><given-names>S.</given-names></name><name><surname>Denson</surname><given-names>L. A.</given-names></name><name><surname>Vázquez-Baeza</surname><given-names>Y.</given-names></name><name><surname>Treuren</surname><given-names>W. V.</given-names></name><name><surname>Ren</surname><given-names>B.</given-names></name><etal/></person-group> (<year>2014</year>). <article-title>The treatment-naive microbiome in new-onset crohn's disease</article-title>. <source>Cell Host Microbe</source>
<volume>15</volume> (<issue>3</issue>), <fpage>382</fpage>–<lpage>392</lpage>. <pub-id pub-id-type="doi">10.1016/j.chom.2014.02.005</pub-id>
<pub-id pub-id-type="pmid">24629344</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname><given-names>J. A.</given-names></name><name><surname>Quinn</surname><given-names>R. A.</given-names></name><name><surname>Debelius</surname><given-names>J.</given-names></name><name><surname>Xu</surname><given-names>Z. Z.</given-names></name><name><surname>Morton</surname><given-names>J.</given-names></name><name><surname>Garg</surname><given-names>N.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Microbiome-wide association studies link dynamic microbial consortia to disease</article-title>. <source>Nature</source>
<volume>535</volume> (<issue>7610</issue>), <fpage>94</fpage>. <pub-id pub-id-type="doi">10.1038/nature18850</pub-id>
<pub-id pub-id-type="pmid">27383984</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gloor</surname><given-names>G. B.</given-names></name><name><surname>Macklaim</surname><given-names>J. M.</given-names></name><name><surname>Pawlowsky-Glahn</surname><given-names>V.</given-names></name><name><surname>Egozcue</surname><given-names>J. J.</given-names></name></person-group> (<year>2017</year>). <article-title>Microbiome datasets are compositional: and this is not optional</article-title>. <source>Front. Microbiol.</source>
<volume>8</volume>, <elocation-id>2224</elocation-id>. <pub-id pub-id-type="doi">10.3389/fmicb.2017.02224</pub-id>
<pub-id pub-id-type="pmid">29187837</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Courville</surname><given-names>A.</given-names></name></person-group> (<year>2016</year>). <source>Deep Learning</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>MIT press</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guidotti</surname><given-names>R.</given-names></name><name><surname>Monreale</surname><given-names>A.</given-names></name><name><surname>Ruggieri</surname><given-names>S.</given-names></name><name><surname>Turini</surname><given-names>F.</given-names></name><name><surname>Giannotti</surname><given-names>F.</given-names></name><name><surname>Pedreschi</surname><given-names>D.</given-names></name></person-group> (<year>2019</year>). <article-title>A Survey of methods for explaining black box models</article-title>. <source>ACM Computing Surveys (CSUR)</source>
<volume>51</volume> (<issue>5</issue>), <fpage>93</fpage>. <pub-id pub-id-type="doi">10.1145/3236009</pub-id>
</mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawinkel</surname><given-names>S.</given-names></name><name><surname>Mattiello</surname><given-names>F.</given-names></name><name><surname>Bijnens</surname><given-names>L.</given-names></name><name><surname>Thas</surname><given-names>O.</given-names></name></person-group> (<year>2017</year>). <article-title>A broken promise: microbiome differential abundance methods do not control the false discovery rate</article-title>. <source>Brief. Bioinform.</source>
<volume>20</volume> (<issue>1</issue>), <fpage>210</fpage>–<lpage>221</lpage>. <pub-id pub-id-type="doi">10.1093/bib/bbx104</pub-id>
</mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kho</surname><given-names>Z. Y.</given-names></name><name><surname>Lal, Sunil </surname><given-names>K.</given-names></name></person-group>, (<year>2018</year>). <article-title>The human gut microbiome-a potential controller of wellness and disease</article-title>. <source>Front. Microbiol.</source><volume>9</volume>, <elocation-id>1835</elocation-id>. <pub-id pub-id-type="doi">10.3389/fmicb.2018.01835</pub-id><pub-id pub-id-type="pmid">30154767</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kong</surname><given-names>Y.</given-names></name><name><surname>Yu</surname><given-names>T.</given-names></name></person-group> (<year>2018</year>). <article-title>A graph-embedded deep feedforward network for disease outcome classification and feature selection using gene expression data</article-title>. <source>Bioinformatics</source>
<volume>34</volume> (<issue>21</issue>), <fpage>3727</fpage>–<lpage>3737</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bty429</pub-id>
<pub-id pub-id-type="pmid">29850911</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurtz</surname><given-names>Z. D.</given-names></name><name><surname>Müller</surname><given-names>C. L.</given-names></name><name><surname>Miraldi</surname><given-names>E. R.</given-names></name><name><surname>Littman</surname><given-names>D. R.</given-names></name><name><surname>Blaser</surname><given-names>M. J.</given-names></name><name><surname>Bonneau</surname><given-names>R. A.</given-names></name></person-group> (<year>2015</year>). <article-title>Sparse and compositionally robust inference of microbial ecological Networks</article-title>. <source>PLoS Comput. Biol.</source>
<volume>11</volume> (<issue>5</issue>), <elocation-id>e1004226</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004226</pub-id>
<pub-id pub-id-type="pmid">25950956</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Layeghifard</surname><given-names>M.</given-names></name><name><surname>Hwang</surname><given-names>D. M.</given-names></name><name><surname>Guttman</surname><given-names>D. S.</given-names></name></person-group> (<year>2017</year>). <article-title>Disentangling interactions in the microbiome: a network perspective</article-title>. <source>Trends Microbiol.</source>
<volume>25</volume> (<issue>3</issue>), <fpage>217</fpage>–<lpage>228</lpage>. <pub-id pub-id-type="doi">10.1016/j.tim.2016.11.008</pub-id>
<pub-id pub-id-type="pmid">27916383</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group> (<year>2015</year>). <article-title>Deep learning</article-title>. <source>Nature</source>
<volume>521</volume> (<issue>7553</issue>), <fpage>436</fpage>. <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
<pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Letunic</surname><given-names>I.</given-names></name><name><surname>Bork</surname><given-names>P.</given-names></name></person-group> (<year>2016</year>). <article-title>Interactive tree of life (itol) v3: an online tool for the display and annotation of phylogenetic and other trees</article-title>. <source>Nucleic Acids Res.</source>
<volume>44</volume> (<issue>W1</issue>), <fpage>W242</fpage>–<fpage>W245</fpage>. <pub-id pub-id-type="doi">10.1093/nar/gkw290</pub-id>
<pub-id pub-id-type="pmid">27095192</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Cheng</surname><given-names>K.</given-names></name><name><surname>Wang</surname><given-names>S.</given-names></name><name><surname>Morstatter</surname><given-names>R. P. T.</given-names></name><name><surname>Tang</surname><given-names>J.</given-names></name><name><surname>Liu</surname><given-names>H.</given-names></name></person-group> (<year>2017</year>). <article-title>Feature selection: a data perspective</article-title>. <source>ACM Comput. Surveys</source>
<volume>50</volume> (<issue>6</issue>), <fpage>1</fpage>–<lpage>45</lpage>. <pub-id pub-id-type="doi">10.1145/3136625</pub-id>
</mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lloyd-Price</surname><given-names>J.</given-names></name><name><surname>Mahurkar</surname><given-names>A.</given-names></name><name><surname>Rahnavard</surname><given-names>G.</given-names></name><name><surname>Crabtree</surname><given-names>J.</given-names></name><name><surname>Orvis</surname><given-names>J.</given-names></name><name><surname>Hall</surname><given-names>A. B.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Strains, functions and dynamics in the expanded human microbiome project</article-title>. <source>Nature</source>
<volume>550</volume> (<issue>7674</issue>), <fpage>61</fpage>. <pub-id pub-id-type="doi">10.1038/nature23889</pub-id>
<pub-id pub-id-type="pmid">28953883</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Matsuoka</surname><given-names>K.</given-names></name><name><surname>Kanai</surname><given-names>T.</given-names></name></person-group> (<year>2015</year>). “<article-title>The Gut Microbiota and Inflammatory Bowel Disease</article-title>,” in <source>Seminars in immunopathology</source>, vol. <volume>37</volume> (<publisher-loc>Verlag GmbH Germany</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>47</fpage>–<lpage>55</lpage>. <pub-id pub-id-type="doi">10.1007/s00281-014-0454-4</pub-id>
<pub-id pub-id-type="pmid">25420450</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mukaka</surname><given-names>M. M.</given-names></name></person-group> (<year>2012</year>). <article-title>A guide to appropriate use of correlation coefficient in medical research</article-title>. <source>Malawi Med. J.</source>
<volume>24</volume> (<issue>3</issue>), <fpage>69</fpage>–<lpage>71</lpage>.<pub-id pub-id-type="pmid">23638278</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>T. T.</given-names></name><name><surname>Huang</surname><given-names>J. Z.</given-names></name><name><surname>Nguyen</surname><given-names>T. T.</given-names></name></person-group> (<year>2015</year>). <article-title>Unbiased feature selection in learning random forests for high-dimensional data</article-title>. <source>Sci. World J</source>. <fpage>471371</fpage>. 10.1155/2015/471371 2015.
</mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nicodemus</surname><given-names>K. K.</given-names></name><name><surname>Malley</surname><given-names>J. D.</given-names></name></person-group> (<year>2009</year>). <article-title>Predictor correlation impacts machine learning algorithms: implications for genomic studies</article-title>. <source>Bioinformatics</source>
<volume>25</volume> (<issue>15</issue>), <fpage>1884</fpage>–<lpage>1890</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btp331</pub-id>
<pub-id pub-id-type="pmid">19460890</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olden</surname><given-names>J. D.</given-names></name><name><surname>Jackson</surname><given-names>D. A.</given-names></name></person-group> (<year>2002</year>). <article-title>Illuminating the 'black box': a randomization approach for understanding variable contributions in artificial neural networks</article-title>. <source>Ecol. Model.</source>
<volume>154</volume> (<issue>1–2</issue>), <fpage>135</fpage>–<lpage>150</lpage>. <pub-id pub-id-type="doi">10.1016/S0304-3800(02)00064-9</pub-id>
</mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pascal</surname><given-names>V.</given-names></name><name><surname>Pozuelo</surname><given-names>M.</given-names></name><name><surname>Borruel</surname><given-names>N.</given-names></name><name><surname>Casellas</surname><given-names>F.</given-names></name><name><surname>Campos</surname><given-names>D.</given-names></name><name><surname>Santiago</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>A microbial signature for crohn's disease</article-title>. <source>Gut</source>
<volume>66</volume> (<issue>5</issue>), <fpage>813</fpage>–<lpage>822</lpage>. <pub-id pub-id-type="doi">10.1136/gutjnl-2016-313235</pub-id>
<pub-id pub-id-type="pmid">28179361</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasolli</surname><given-names>E.</given-names></name><name><surname>Truong</surname><given-names>D. T.</given-names></name><name><surname>Malik</surname><given-names>F.</given-names></name><name><surname>Waldron</surname><given-names>L.</given-names></name><name><surname>Segata</surname><given-names>N.</given-names></name></person-group> (<year>2016</year>). <article-title>Machine learning meta-analysis of large metagenomic datasets: tools and biological insights</article-title>. <source>PLoS Comput. Biol.</source>
<volume>12</volume> (<issue>7</issue>), <elocation-id>e1004977</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004977</pub-id>
<pub-id pub-id-type="pmid">27400279</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paulson</surname><given-names>J. N.</given-names></name><name><surname>Stine</surname><given-names>O. C.</given-names></name><name><surname>Bravo</surname><given-names>H. C.</given-names></name><name><surname>Pop</surname><given-names>M.</given-names></name></person-group> (<year>2013</year>). <article-title>Differential abundance Analysis for microbial marker-gene surveys</article-title>. <source>Nat. Methods</source>
<volume>10</volume> (<issue>12</issue>), <fpage>1200</fpage>. <pub-id pub-id-type="doi">10.1038/nmeth.2658</pub-id>
<pub-id pub-id-type="pmid">24076764</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ranjan</surname><given-names>R.</given-names></name><name><surname>Rani</surname><given-names>A.</given-names></name><name><surname>Metwally</surname><given-names>A.</given-names></name><name><surname>McGee</surname><given-names>H. S.</given-names></name><name><surname>Perkins</surname><given-names>D. L.</given-names></name></person-group> (<year>2016</year>). <article-title>Analysis of the microbiome: advantages of whole genome shotgun versus 16s amplicon sequencing</article-title>. <source>Biochem. Biophys. Res. Commun.</source>
<volume>469</volume> (<issue>4</issue>), <fpage>967</fpage>–<lpage>977</lpage>. <pub-id pub-id-type="doi">10.1016/j.bbrc.2015.12.083</pub-id>
<pub-id pub-id-type="pmid">26718401</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reshef</surname><given-names>D. N.</given-names></name><name><surname>Reshef</surname><given-names>Y. A.</given-names></name><name><surname>Finucane</surname><given-names>H. K.</given-names></name><name><surname>Grossman</surname><given-names>S. R.</given-names></name><name><surname>McVean</surname><given-names>G.</given-names></name><name><surname>Turnbaugh</surname><given-names>P. J.</given-names></name><etal/></person-group> (<year>2011</year>). <article-title>Detecting novel associations in large data sets</article-title>. <source>Science</source>
<volume>334</volume> (<issue>6062</issue>), <fpage>1518</fpage>–<lpage>1524</lpage>. <pub-id pub-id-type="doi">10.1126/science.1205438</pub-id>
<pub-id pub-id-type="pmid">22174245</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saraçli</surname><given-names>S.</given-names></name><name><surname>Doğan</surname><given-names>N.</given-names></name><name><surname>Doğan</surname><given-names>İ</given-names></name></person-group>, (<year>2013</year>). <article-title>Comparison of hierarchical cluster analysis methods by cophenetic correlation</article-title>. <source>J. Inequal. Appl.</source><volume>2013</volume> (<issue>1</issue>), <fpage>203</fpage>. <pub-id pub-id-type="doi">10.1186/1029-242X-2013-203</pub-id>
</mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sokal</surname><given-names>R. R.</given-names></name><name><surname>Rohlf</surname><given-names>F. J.</given-names></name></person-group> (<year>1962</year>). <article-title>The comparison of dendrograms by objective methods</article-title>. <source>Taxon.</source>
<volume>11</volume> (<issue>2</issue>), <fpage>33</fpage>–<lpage>40</lpage>. <pub-id pub-id-type="doi">10.2307/1217208</pub-id>
</mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turnbaugh</surname><given-names>P. J.</given-names></name><name><surname>Ley</surname><given-names>R. E.</given-names></name><name><surname>Hamady</surname><given-names>M.</given-names></name><name><surname>Fraser-Liggett</surname><given-names>C. M.</given-names></name><name><surname>Knight</surname><given-names>R.</given-names></name><name><surname>Gordon</surname><given-names>J. I.</given-names></name></person-group> (<year>2007</year>). <article-title>The human microbiome project</article-title>. <source>Nature</source>
<volume>449</volume> (<issue>7164</issue>), <fpage>804</fpage>. <pub-id pub-id-type="doi">10.1038/nature06244</pub-id>
<pub-id pub-id-type="pmid">17943116</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Jia</surname><given-names>H.</given-names></name></person-group> (<year>2016</year>). <article-title>Metagenome-wide association studies: fine-mining the microbiome</article-title>. <source>Nat. Rev. Microbiol.</source>
<volume>14</volume> (<issue>8</issue>), <fpage>508</fpage>. <pub-id pub-id-type="doi">10.1038/nrmicro.2016.83</pub-id>
<pub-id pub-id-type="pmid">27396567</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiss</surname><given-names>S.</given-names></name><name><surname>Treuren</surname><given-names>W. V.</given-names></name><name><surname>Lozupone</surname><given-names>C.</given-names></name><name><surname>Faust</surname><given-names>K.</given-names></name><name><surname>Friedman</surname><given-names>J.</given-names></name><name><surname>Deng</surname><given-names>Y.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Correlation detection strategies in microbial data sets vary widely in sensitivity and precision</article-title>. <source>ISME J.</source>
<volume>10</volume> (<issue>7</issue>), <fpage>1669</fpage>. <pub-id pub-id-type="doi">10.1038/ismej.2015.235</pub-id>
<pub-id pub-id-type="pmid">26905627</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiss</surname><given-names>S.</given-names></name><name><surname>Xu</surname><given-names>Z</given-names></name><name><surname>Zech Peddada</surname><given-names>S.</given-names></name><name><surname>Amir</surname><given-names>A.</given-names></name><name><surname>Bittinger</surname><given-names>K.</given-names></name><name><surname>Gonzalez</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Normalization and microbial differential abundance strategies depend upon data characteristics</article-title>. <source>Microbiome</source>
<volume>5</volume> (<issue>1</issue>), <fpage>27</fpage>. <pub-id pub-id-type="doi">10.1186/s40168-017-0237-y</pub-id>
<pub-id pub-id-type="pmid">28253908</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>Q.</given-names></name><name><surname>Pan</surname><given-names>M.</given-names></name><name><surname>Liu</surname><given-names>L.</given-names></name><name><surname>Li</surname><given-names>B.</given-names></name><name><surname>He</surname><given-names>T.</given-names></name><name><surname>Jiang</surname><given-names>X.</given-names></name><etal/></person-group> (<year>2018</year>). “<article-title>An ensemble feature selection method based on deep forest for microbiome-wide association studies</article-title>,” in <source>2018 IEEE international conference on Bioinformatics and Biomedicine (BIBM)</source>, vol. <volume>248–253</volume> (<publisher-loc>Washington, D.C.</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>), <fpage>248</fpage>–<lpage>253</lpage>. <pub-id pub-id-type="doi">10.1109/BIBM.2018.8621461</pub-id>
</mixed-citation>
    </ref>
  </ref-list>
</back>
